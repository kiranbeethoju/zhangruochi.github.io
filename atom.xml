<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RUOCHI.AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhangruochi.com/"/>
  <updated>2020-03-24T21:11:18.878Z</updated>
  <id>https://zhangruochi.com/</id>
  
  <author>
    <name>Ruochi Zhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2020-03-24</title>
    <link href="https://zhangruochi.com/2020-03-24/2020/03/24/"/>
    <id>https://zhangruochi.com/2020-03-24/2020/03/24/</id>
    <published>2020-03-24T20:43:34.000Z</published>
    <updated>2020-03-24T21:11:18.878Z</updated>
    
    <content type="html"><![CDATA[<p>疫情到了现在这个阶段，已经让世界发生了巨大的改变，等疫情结束后，世界可能就不是现在这个世界了。今天看了老雷的视频，觉得自己也应该每天记录点什么。</p><p>今天起我已经准备开始打持久战了。五月一号回国的机票虽然定了，但是到时候能不能走还不知道，飞机上有多危险也不知道。我倾向于先留下，至少等安全了再回去。如果五月走不了，那会有很长的时间需要在美国这边自我隔离。这就需要好好规划下接下来要做些什么。</p><p>现在的想法是, 靠 <code>学习</code> + <code>游戏</code> + <code>运动</code> + <code>科研</code> + <code>工作</code> 充实自己。</p><ol><li>刷题还是要继续，万维那边的情况不知道怎么样，可能以后我随时需要找实习。那就每天5个leetcode吧。</li><li>很多网课需要上。<ul><li>Probablistic Graphic Models</li><li>Programming Languages</li><li>Scala Specialication</li><li>Cloud Computing</li><li>Operating System</li><li>Compiler</li></ul></li><li>游戏的话，多尝试尝试，不要每天就打个NBA2k。</li><li>健身一定要坚持，每天早上和晚上两个时间健身。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;疫情到了现在这个阶段，已经让世界发生了巨大的改变，等疫情结束后，世界可能就不是现在这个世界了。今天看了老雷的视频，觉得自己也应该每天记录点什么。&lt;/p&gt;
&lt;p&gt;今天起我已经准备开始打持久战了。五月一号回国的机票虽然定了，但是到时候能不能走还不知道，飞机上有多危险也不知道。我
      
    
    </summary>
    
    
      <category term="Diary" scheme="https://zhangruochi.com/categories/Diary/"/>
    
    
  </entry>
  
  <entry>
    <title>Bayesian networks</title>
    <link href="https://zhangruochi.com/Bayesian-networks/2020/03/23/"/>
    <id>https://zhangruochi.com/Bayesian-networks/2020/03/23/</id>
    <published>2020-03-23T20:03:48.000Z</published>
    <updated>2020-03-23T22:48:28.393Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Probabilistic-modeling-with-Bayesian-networks"><a href="#Probabilistic-modeling-with-Bayesian-networks" class="headerlink" title="Probabilistic modeling with Bayesian networks"></a>Probabilistic modeling with Bayesian networks</h2><p>Directed graphical models (a.k.a. Bayesian networks) are a family of probability distributions that admit a compact parametrization that can be naturally described using a directed graph.</p><p>The general idea behind this parametrization is surprisingly simple. Recall that by the chain rule, we can write any probability $p$ as:</p><script type="math/tex; mode=display">p(x_1, x_2, \dotsc, x_n) = p(x_1) p(x_2 \mid x_1) \cdots p(x_n \mid x_{n-1}, \dotsc, x_2, x_1).</script><p>A <strong>compact</strong> Bayesian network is a distribution in which each factor on the right hand side depends only on a small number of <em>ancestor variables</em> $x_{A_i}$:</p><script type="math/tex; mode=display">p(x_i \mid x_{i-1}, \dotsc, x_1) = p(x_i \mid x_{A_i}).</script><p>For example, in a model with five variables, we may choose to approximate the factor $p(x_5 \mid x_4, x_3, x_2, x_1)$ with $p(x_5 \mid x_4, x_3)$. In this case, we write $x_{A_5} = \{x_4, x_3\}$.</p><h2 id="Graphical-representation"><a href="#Graphical-representation" class="headerlink" title="Graphical representation"></a>Graphical representation</h2><p>As an example, consider a model of a student’s grade <script type="math/tex">g</script> on an exam. This grade depends on the exam’s difficulty $d$ and the student’s intelligence $i$; it also affects the quality $l$ of the reference letter from the professor who taught the course. The student’s intelligence $i$ affects the SAT score $s$ as well. Each variable is binary, except for $g$, which takes 3 possible values.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Bayes net model describing the performance of a student on an exam. The distribution can be represented a product of conditional probability distributions specified by tables. The form of these distributions is described by edges in the graph.</div></center><p>The joint probability distribution over the 5 variables naturally factorizes as follows:</p><script type="math/tex; mode=display">p(l, g, i, d, s) = p(l \mid g)\, p(g \mid i, d)\, p(i)\, p(d)\, p(s \mid i).</script><p>The graphical representation of this distribution is a DAG that visually specifies how random variables depend on each other. The graph clearly indicates that the letter depends on the grade, which in turn depends on the student’s intelligence and the difficulty of the exam.</p><p>Another way to interpret directed graphs is in terms of stories for how the data was generated. In the above example, to determine the quality of the reference letter, we may first sample an intelligence level and an exam difficulty; then, a student’s grade is sampled given these parameters; finally, the recommendation letter is generated based on that grade.</p><h2 id="Formal-definition"><a href="#Formal-definition" class="headerlink" title="Formal definition."></a>Formal definition.</h2><p>Formally, a Bayesian network is a directed graph $G = (V,E)$ together with</p><ul><li>A random variable $x_i$ for each node $i \in V$.</li><li>One conditional probability distribution (CPD) $p(x_i \mid x_{A_i})$ per node, specifying the probability of $x_i$ conditioned on its parents’ values.</li></ul><p>Thus, a Bayesian network defines a probability distribution $p$. Conversely, we say that a probability $p$ <strong>factorizes</strong> over a DAG $G$ if it can be decomposed into a product of factors, as specified by $G$.</p><p>It is not hard to see that a probability represented by a Bayesian network will be valid: clearly, it will be non-negative and one can show using an induction argument (and using the fact that the CPDs are valid probabilities) that the sum over all variable assignments will be one. Conversely, we can also show by counter-example that when <script type="math/tex">G</script> contains cycles, its associated probability may not sum to one.</p><h2 id="The-dependencies-of-a-Bayes-net"><a href="#The-dependencies-of-a-Bayes-net" class="headerlink" title="The dependencies of a Bayes net"></a>The dependencies of a Bayes net</h2><p>To summarize, Bayesian networks represent probability distributions that can be formed via products of smaller, local conditional probability distributions (one for each variable). By expressing a probability in this form, we are introducing into our model assumptions that certain variables are independent.</p><p>This raises the question: which independence assumptions are we exactly making by using a Bayesian network model with a given structure described by $G$? This question is important for two reasons: we should know precisely what model assumptions we are making (and whether they are correct); also, this information will help us design more efficient inference algorithms later on.</p><p>Let us use the notation $I(p)$ to denote the set of all independencies that hold for a joint distribution $p$. For example, if $p(x,y) = p(x) p(y)$, then we say that $x \perp y \in I(p)$.</p><h3 id="Independencies-described-by-directed-graphs"><a href="#Independencies-described-by-directed-graphs" class="headerlink" title="Independencies described by directed graphs"></a>Independencies described by directed graphs</h3><p>It turns out that a Bayesian network $p$ very elegantly describes many independencies in $I(p)$; these independencies can be recovered from the graph by looking at three types of structures.</p><p>For simplicity, let’s start by looking at a Bayes net $G$ with three nodes: $A$, $B$, and $C$. In this case, <script type="math/tex">G</script> essentially has only three possible structures, each of which leads to different independence assumptions. The interested reader can easily prove these results using a bit of algebra.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Bayesian networks over three variables, encoding different types of dependencies: cascade (a,b), common parent (c), and v-structure (d).</div></center><ul><li><p><strong>Common parent.</strong> If $G$ is of the form $A \leftarrow B \rightarrow C$, and $B$ is observed, then $A \perp C \mid B$. However, if $B$ is unobserved, then $A \not\perp C$. Intuitively this stems from the fact that $B$ contains all the information that determines the outcomes of $A$ and $C$; once it is observed, there is nothing else that affects these variables’ outcomes.</p></li><li><p><strong>Cascade.</strong>: If $G$ equals $A \rightarrow B \rightarrow C$, and $B$ is again observed, then, again $A \perp C \mid B$. However, if $B$ is unobserved, then $A \not\perp C$. Here, the intuition is again that $B$ holds all the information that determines the outcome of $C$; thus, it does not matter what value $A$ takes.</p></li><li><strong>V-structure.</strong> (also known as <em>explaining away</em>): If $G$ is $A \rightarrow C \leftarrow B$, then knowing $C$ couples $A$ and $B$. In other words, $A \perp B$ if $C$ is unobserved, but $A \not\perp B \mid C$ if $C$ is observed.</li></ul><p>The latter case requires additional explanation. Suppose that $C$ is a Boolean variable that indicates whether our lawn is wet one morning; $A$ and $B$ are two explanations for it being wet: either it rained (indicated by $A$), or the sprinkler turned on (indicated by $B$). If we know that the grass is wet ($C$ is true) and the sprinkler didn’t go on ($B$ is false), then the probability that $A$ is true must be one, because that is the only other possible explanation. Hence, $A$ and $B$ are not independent given $C$.</p><p>These structures clearly describe the independencies encoded by a three-variable Bayesian net. </p><h3 id="d-separation"><a href="#d-separation" class="headerlink" title="$d$-separation"></a>$d$-separation</h3><p>We can extend them to general networks by applying them recursively over any larger graph. This leads to a notion called $d$-separation (where $d$ stands for directed).</p><p>Let $Q$, $W$, and $O$ be three sets of nodes in a Bayesian Network $G$. We say that $Q$ and $W$ are $d$-separated given $O$ (<em>i.e.</em> the variables $O$ are observed) if $Q$ and $W$ are not connected by an <em>active path</em>. An undirected path in $G$ is called <em>active</em> given observed variables $O$ if for every consecutive triple of variables $X,Y,Z$ on the path, one of the following holds:</p><ul><li>$X \leftarrow Y \leftarrow Z$, and $Y$ is unobserved $Y \not\in O$</li><li>$X \rightarrow Y \rightarrow Z$, and $Y$ is unobserved $Y \not\in O$</li><li>$X \leftarrow Y \rightarrow Z$, and $Y$ is unobserved $Y \not\in O$</li><li>$X \rightarrow Y \leftarrow Z$, and $Y$ or any of its descendants are observed.</li></ul><p>In other words: A trail $X1, \cdots, X_n$ is active given Z if:</p><ul><li>for any v-structure we have that $X_i$ or one of its descendants<br>$\in$ Z</li><li>no other $X_i$ is in Z</li></ul><p>In this example, $X_1$ and $X_6$ are $d$-separated given $X_2, X_3$.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="7.png" width="70%" height="70%"></center><p>However, $X_2, X_3$ are not $d$-separated given $X_1, X_6$. There is an active pass which passed through the V-structure created when $X_6$ is observed.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="8.png" width="70%" height="70%"></center><p>For example, in the graph below, $X_1$ and $X_6$ are $d$-separated given $X_2, X_3$. However, $X_2, X_3$ are not $d$-separated given $X_1, X_6$, because we can find an active path $(X_2, X_6, X_5, X_3)$</p><p>The notion of $d$-separation is useful, because it lets us describe a large fraction of the dependencies that hold in our model. Let $I(G) = \{(X \perp Y \mid Z) : \text{$X,Y$ are $d$-sep given $Z$}\}$ be a set of variables that are $d$-separated in $G$.</p><h4 id="Two-theorem"><a href="#Two-theorem" class="headerlink" title="Two theorem"></a>Two theorem</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="9.png" width="70%" height="70%"></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="10.png" width="70%" height="70%"></center><h3 id="I-map"><a href="#I-map" class="headerlink" title="$I$-map"></a>$I$-map</h3><blockquote><p>If $p$ factorizes over $G$, then $I(G) \subseteq I(p)$. In this case, we say that $G$ is an $I$-map (independence map) for $p$.</p></blockquote><p>In other words, all the independencies encoded in $G$ are sound: variables that are $d$-separated in $G$ are truly independent in $p$. However, the converse is not true: a distribution may factorize over $G$, yet have independencies that are not captured in $G$.</p><p>In a way this is almost a trivial statement. If $p(x,y) = p(x)p(y)$, then this distribution still factorizes over the graph $y \rightarrow x$, since we can always write it as $p(x,y) = p(x\mid y)p(y)$ with a CPD $p(x\mid y)$ in which the probability of $x$ does not actually vary with $y$. However, we can construct a graph that matches the structure of $p$ by simply removing that unnecessary edge.</p><h4 id="Theorem"><a href="#Theorem" class="headerlink" title="Theorem"></a>Theorem</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="11.png" width="70%" height="70%"></center><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Two equivalent views of graph structure:</p><ul><li><strong>Factorization</strong>: $G$ allows $P$ to be represented</li><li><strong>I-map</strong>: Independencies encoded by G hold in P<ul><li>If $P$ factorizes over a graph $G$, we can read from the graph independencies that must hold in $P$ (an independency map)</li></ul></li></ul><h2 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="12.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Naive Bayes Probabilistic Graphical Model</div></center><p>From the graph,</p><script type="math/tex; mode=display">(x_i \perp x_j | c) \quad \text{for all} \quad x_i, x_j</script><p>Then, we can get</p><script type="math/tex; mode=display">P(C, x_i, \cdots, x_n) = P(c)\prod_{i=1}^{n}P(x_i | C)</script><p>Therefore, the raito of two class is:</p><script type="math/tex; mode=display">\frac{P(C = c^{1} | x_i, \cdots, x_n)}{P(C = c^{2} | x_i, \cdots, x_n)} = \frac{P(C = c^{1})}{P(C = c^{2})}\prod_{i=1}^{n}\frac{P(x_i | C = c^{1})}{P( x_i | C = c^{2})}</script><p>Indtroduce <code>Bernoulli</code>(or others) to calcute probabilities</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="13.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Bernoulli Naive Bayes for Text</div></center><ul><li>Simple approach for classification <ul><li>Computationally efficient</li><li>Easy to construct</li></ul></li><li>Surprisingly effective in domains with many <strong>weakly</strong> relevant features</li><li>Strong independence assumptions reduce performance when many features are strongly correlated</li></ul><h2 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h2><h4 id="Calculate-the-number-of-parameters-of-a-distribution-model"><a href="#Calculate-the-number-of-parameters-of-a-distribution-model" class="headerlink" title="Calculate the number of parameters of a distribution model"></a>Calculate the number of parameters of a distribution model</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="100%" height="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">The number of parameters</div></center><h4 id="Inter-causal-reasoning"><a href="#Inter-causal-reasoning" class="headerlink" title="Inter-causal reasoning"></a>Inter-causal reasoning</h4><center>    <img style="border-radius: 0.1em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="100%" height="100%">    <img style="border-radius: 0.1em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="100%" height="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Inter-causal reasoning</div></center><h4 id="Independencies-in-a-graph"><a href="#Independencies-in-a-graph" class="headerlink" title="Independencies in a graph"></a>Independencies in a graph</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="100%" height="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Independencies in a graph</div></center><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://ermongroup.github.io/cs228-notes/" target="_blank" rel="noopener">https://ermongroup.github.io/cs228-notes/</a></li><li>Course note from Coursera course <a href="https://www.coursera.org/learn/probabilistic-graphical-models/" target="_blank" rel="noopener">Probabilistic graphical models</a> lectured by Daphne Koller</li></ul>]]></content>
    
    <summary type="html">
    
      how do we choose a probability distribution to model some interesting aspect of the world?
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Probabilistic Graphical Models" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Probabilistic-Graphical-Models/"/>
    
    
  </entry>
  
  <entry>
    <title>Fucking distributions</title>
    <link href="https://zhangruochi.com/Fucking-distributions/2020/03/22/"/>
    <id>https://zhangruochi.com/Fucking-distributions/2020/03/22/</id>
    <published>2020-03-22T07:44:30.000Z</published>
    <updated>2020-03-22T07:50:53.670Z</updated>
    
    <content type="html"><![CDATA[<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="overview.png" width="100%" height="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">overview</div></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line"><span class="keyword">import</span> operator <span class="keyword">as</span> op</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations</span><br><span class="line"><span class="keyword">import</span> scipy.special <span class="keyword">as</span> sps</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure><h2 id="Uniform-distribution-continuous"><a href="#Uniform-distribution-continuous" class="headerlink" title="Uniform distribution(continuous)"></a>Uniform distribution(continuous)</h2><ul><li>Uniform distribution has same probaility value on [a, b], easy probability.</li></ul><script type="math/tex; mode=display">f(x)=\begin{cases}  \frac{1}{b - a} & \mathrm{for}\ a \le x \le b, \\[8pt]  0 & \mathrm{for}\ x<a\ \mathrm{or}\ x>b  \end{cases}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">uniform</span><span class="params">(x, a, b)</span>:</span></span><br><span class="line">    y = [<span class="number">1</span> / (b-a) <span class="keyword">if</span> a &lt;= val <span class="keyword">and</span> val &lt;= b <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> val <span class="keyword">in</span> x]</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-100</span>,<span class="number">100</span>)</span><br><span class="line"><span class="keyword">for</span> dis <span class="keyword">in</span> [(<span class="number">-50</span>, <span class="number">50</span>), (<span class="number">10</span>, <span class="number">20</span>)]:</span><br><span class="line">    a, b = dis[<span class="number">0</span>], dis[<span class="number">1</span>]</span><br><span class="line">    x, y, u, s = uniform(x, a, b)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$\mu=%.2f,\ \sigma=%.2f$'</span> % (u, s))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_5_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,<span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">15</span>, density=<span class="keyword">True</span>)</span><br><span class="line">plt.plot(bins, np.ones_like(bins), linewidth=<span class="number">2</span>, color=<span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x11eae0cc0&gt;]</code></pre><p><img src="output_6_1.png" alt="png"></p><h2 id="Bernoulli-distribution-discrete"><a href="#Bernoulli-distribution-discrete" class="headerlink" title="Bernoulli distribution(discrete)"></a>Bernoulli distribution(discrete)</h2><ul><li>Bernoulli distribution is not considered about prior probability P(X). Therefore, if we optimize to the maximum likelihood, we will be vulnerable to overfitting.</li><li>We use binary cross entropy to classify binary classification. It has same form like taking a negative log of the bernoulli distribution.</li></ul><script type="math/tex; mode=display">f(k;p) = \begin{cases}   p & \text{if }k=1, \\   q = 1-p & \text {if } k = 0. \end{cases}</script><ul><li>For Logistic Regression<script type="math/tex; mode=display">p=p(y|x,\theta)=p_{1}^{y_{i}}\ast p_{0}^{1-y_{i}}</script><script type="math/tex; mode=display">max \sum_{i=1}^{m}({y_{i}\log{p_{1}}+(1-y_{i})\log{p_{0})}}</script></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bernoulli</span><span class="params">(p, k)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> p <span class="keyword">if</span> k <span class="keyword">else</span> <span class="number">1</span> - p</span><br><span class="line"></span><br><span class="line">n_experiment = <span class="number">100</span></span><br><span class="line">p = <span class="number">0.6</span></span><br><span class="line">x = np.arange(n_experiment)</span><br><span class="line">y = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(n_experiment):</span><br><span class="line">    pick = bernoulli(p, k=bool(random.getrandbits(<span class="number">1</span>)))</span><br><span class="line">    y.append(pick)</span><br><span class="line"></span><br><span class="line">u, s = np.mean(y), np.std(y)</span><br><span class="line">plt.scatter(x, y, label=<span class="string">r'$p=%.2f,\ \mu=%.2f,\ \sigma=%.2f$'</span> % (p,u, s))</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_8_0.png" alt="png"></p><h2 id="Binomial-distribution-discrete"><a href="#Binomial-distribution-discrete" class="headerlink" title="Binomial distribution(discrete)"></a>Binomial distribution(discrete)</h2><ul><li>Binomial distribution with parameters <strong>n</strong> and <strong>p</strong> is the discrete probability distribution of the number of successes in a sequence of n independent experiments.</li><li>Binomial distribution is distribution considered prior probaility by specifying the number to be picked in advance.</li></ul><script type="math/tex; mode=display">f(k,n,p) = \Pr(k;n,p) = \Pr(X = k) = \binom{n}{k}p^k(1-p)^{n-k}</script><p>for k = 0, 1, 2, …, n, where</p><script type="math/tex; mode=display">\binom{n}{k} =\frac{n!}{k!(n-k)!}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">const</span><span class="params">(n, r)</span>:</span></span><br><span class="line">    r = min(r, n-r)</span><br><span class="line">    numer = reduce(op.mul, range(n, n-r, <span class="number">-1</span>), <span class="number">1</span>)</span><br><span class="line">    denom = reduce(op.mul, range(<span class="number">1</span>, r+<span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> numer / denom</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binomial</span><span class="params">(n, p)</span>:</span></span><br><span class="line">    q = <span class="number">1</span> - p</span><br><span class="line">    y = [const(n, k) * (p ** k) * (q ** (n-k)) <span class="keyword">for</span> k <span class="keyword">in</span> range(n)]</span><br><span class="line">    <span class="keyword">return</span> y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ls <span class="keyword">in</span> [(<span class="number">0.5</span>, <span class="number">20</span>), (<span class="number">0.7</span>, <span class="number">40</span>), (<span class="number">0.5</span>, <span class="number">40</span>)]:</span><br><span class="line">    p, n_experiment = ls[<span class="number">0</span>], ls[<span class="number">1</span>]</span><br><span class="line">    x = np.arange(n_experiment)</span><br><span class="line">    y, u, s = binomial(n_experiment, p)</span><br><span class="line">    plt.scatter(x, y, label=<span class="string">r'$n_&#123;experiment&#125;=%d,\ p=%.2f,\ \mu=%.2f,\ \sigma=%.2f$'</span> % (n_experiment,p, u, s))</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_10_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.binomial(<span class="number">10</span>, <span class="number">0.8</span>, <span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">20</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_11_0.png" alt="png"></p><h2 id="Multi-Bernoulli-distribution-Categorical-distribution-discrete"><a href="#Multi-Bernoulli-distribution-Categorical-distribution-discrete" class="headerlink" title="Multi-Bernoulli distribution, Categorical distribution(discrete)"></a>Multi-Bernoulli distribution, Categorical distribution(discrete)</h2><ul><li>Multi-bernoulli called categorical distribution, is a probability expanded more than 2.</li><li><strong>cross entopy</strong> has same form like taking a negative log of the Multi-Bernoulli distribution.</li></ul><script type="math/tex; mode=display">f(x\mid \boldsymbol{p} ) = \prod_{i=1}^k p_i^{[x=i]}</script><p>where $[x = i]$ evaluates to 1 if $x = i$, 0 otherwise. There are various advantages of this formulation</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categorical</span><span class="params">(p, k)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> p[k]</span><br><span class="line"></span><br><span class="line">n_experiment = <span class="number">100</span></span><br><span class="line">p = [<span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0.7</span>]</span><br><span class="line">x = np.arange(n_experiment)</span><br><span class="line">y = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(n_experiment):</span><br><span class="line">    pick = categorical(p, k = random.randint(<span class="number">0</span>, len(p) - <span class="number">1</span>))</span><br><span class="line">    y.append(pick)</span><br><span class="line"></span><br><span class="line">u, s = np.mean(y), np.std(y)</span><br><span class="line">plt.scatter(x, y, label=<span class="string">r'$p=[0.2, 0.1, 0.7],\mu=%.2f,\ \sigma=%.2f$'</span> % (u, s))</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_13_0.png" alt="png"></p><h2 id="Multinomial-distribution-discrete"><a href="#Multinomial-distribution-discrete" class="headerlink" title="Multinomial distribution(discrete)"></a>Multinomial distribution(discrete)</h2><ul><li>The multinomial distribution has the same relationship with the categorical distribution as the relationship between Bernoull and Binomial.</li><li>For example, it models the probability of counts for each side of a <strong>k-sided</strong> die rolled n times. For n independent trials each of which leads to a success for exactly one of k categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of <strong>any particular combination</strong> of numbers of successes for the various categories.</li><li>When k is 2 and n is 1, the multinomial distribution is the Bernoulli distribution. When k is 2 and n is bigger than 1, it is the binomial distribution. When k is bigger than 2 and n is 1, it is the categorical distribution.</li></ul><script type="math/tex; mode=display">\begin{align}f(x_1,\ldots,x_k;n,p_1,\ldots,p_k) & {} = \Pr(X_1 = x_1 \text{ and } \dots \text{ and } X_k = x_k) \\& {} = \begin{cases} { \displaystyle {n! \over x_1!\cdots x_k!}p_1^{x_1}\times\cdots\times p_k^{x_k}}, \quad &\text{when } \sum_{i=1}^k x_i=n \\  \\0 & \text{otherwise,} \end{cases}\end{align}</script><p>for non-negative integers $x_1, \cdots, x_k$.</p><p>The probability mass function can be expressed using the gamma function as:</p><script type="math/tex; mode=display">f(x_1,\dots, x_{k}; p_1,\ldots, p_k) = \frac{\Gamma(\sum_i x_i + 1)}{\prod_i \Gamma(x_i+1)} \prod_{i=1}^k p_i^{x_i}.</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">factorial</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> reduce(op.mul, range(<span class="number">1</span>, n + <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">const</span><span class="params">(n, a, b, c)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        return n! / a! b! c!, where a+b+c == n</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">assert</span>  a + b + c == n</span><br><span class="line"></span><br><span class="line">    numer = factorial(n)</span><br><span class="line">    denom = factorial(a) * factorial(b) * factorial(c)</span><br><span class="line">    <span class="keyword">return</span> numer / denom</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multinomial</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param x : list, sum(x) should be `n`</span></span><br><span class="line"><span class="string">    :param n : number of trial</span></span><br><span class="line"><span class="string">    :param p: list, sum(p) should be `1`</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># get all a,b,c where a+b+c == n, a&lt;b&lt;c</span></span><br><span class="line">    ls = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(j, n + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> i + j + k == n:</span><br><span class="line">                    ls.append([i, j, k])</span><br><span class="line"></span><br><span class="line">    y = [const(n, l[<span class="number">0</span>], l[<span class="number">1</span>], l[<span class="number">2</span>]) <span class="keyword">for</span> l <span class="keyword">in</span> ls]</span><br><span class="line">    x = np.arange(len(y))</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n_experiment <span class="keyword">in</span> [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>]:</span><br><span class="line">    x, y, u, s = multinomial(n_experiment)</span><br><span class="line">    plt.scatter(x, y, label=<span class="string">r'$trial=%d$'</span> % (n_experiment))</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_15_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.multinomial(<span class="number">20</span>, [<span class="number">1</span>/<span class="number">6.</span>]*<span class="number">6</span>, size=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>array([[1, 1, 2, 3, 8, 5],       [2, 4, 3, 3, 6, 2],       [1, 6, 3, 2, 3, 5],       [5, 3, 4, 4, 2, 2],       [3, 8, 4, 2, 0, 3],       [2, 4, 1, 5, 1, 7],       [6, 3, 2, 4, 3, 2],       [8, 2, 1, 1, 4, 4],       [3, 6, 4, 1, 4, 2],       [3, 2, 3, 3, 6, 3]])</code></pre><h2 id="Beta-distribution-continuous"><a href="#Beta-distribution-continuous" class="headerlink" title="Beta distribution(continuous)"></a>Beta distribution(continuous)</h2><ul><li>Beta distribution is conjugate to the binomial and Bernoulli distributions.</li><li>Using conjucation, we can get the posterior distribution more easily using the prior distribution we know.</li><li>Uniform distiribution is same when beta distribution met special case(alpha=1, beta=1).</li></ul><script type="math/tex; mode=display">\begin{align}f(x;\alpha,\beta) & = \mathrm{constant}\cdot x^{\alpha-1}(1-x)^{\beta-1} \\[3pt]& = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{\displaystyle \int_0^1 u^{\alpha-1} (1-u)^{\beta-1}\, du} \\[6pt]& = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\, x^{\alpha-1}(1-x)^{\beta-1} \\[6pt]& = \frac{1}{B(\alpha,\beta)} x^{\alpha-1}(1-x)^{\beta-1}\end{align}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma_function</span><span class="params">(n)</span>:</span></span><br><span class="line">    cal = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        cal *= i</span><br><span class="line">    <span class="keyword">return</span> cal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">beta</span><span class="params">(x, a, b)</span>:</span></span><br><span class="line"></span><br><span class="line">    gamma = gamma_function(a + b) / \</span><br><span class="line">            (gamma_function(a) * gamma_function(b))</span><br><span class="line">    y = gamma * (x ** (a - <span class="number">1</span>)) * ((<span class="number">1</span> - x) ** (b - <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ls <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">3</span>), (<span class="number">5</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">5</span>)]:</span><br><span class="line">    a, b = ls[<span class="number">0</span>], ls[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x in [0, 1], trial is 1/0.001 = 1000</span></span><br><span class="line">    x = np.arange(<span class="number">0</span>, <span class="number">1</span>, <span class="number">0.001</span>, dtype=np.float)</span><br><span class="line">    x, y, u, s = beta(x, a=a, b=b)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$\mu=%.2f,\ \sigma=%.2f,'</span></span><br><span class="line">                         <span class="string">r'\ \alpha=%d,\ \beta=%d$'</span> % (u, s, a, b))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_18_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.beta(<span class="number">2</span>, <span class="number">5</span>, size=<span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">20</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_19_0.png" alt="png"></p><h2 id="Gamma-distribution-continuous"><a href="#Gamma-distribution-continuous" class="headerlink" title="Gamma distribution(continuous)"></a>Gamma distribution(continuous)</h2><ul><li><p>Gamma distribution will be beta distribution, if $\frac{Gamma(a,1)}{Gamma(a,1) + Gamma(b,1)}$ is same with $Beta(a,b)$.</p></li><li><p>The exponential distribution and chi-squared distribution are special cases of the gamma distribution.</p></li></ul><p>A random variable X that is gamma-distributed with shape α and rate β is denoted:</p><script type="math/tex; mode=display">X \sim \Gamma(\alpha, \beta) \equiv \operatorname{Gamma}(\alpha,\beta)</script><p>The corresponding probability density function in the shape-rate parametrization is:</p><script type="math/tex; mode=display">\begin{align}f(x;\alpha,\beta) & = \frac{ \beta^\alpha x^{\alpha-1} e^{-\beta x}}{\Gamma(\alpha)} \quad \text{ for } x > 0 \quad \alpha, \beta > 0, \\[6pt]\end{align}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma_function</span><span class="params">(n)</span>:</span></span><br><span class="line">    cal = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        cal *= i</span><br><span class="line">    <span class="keyword">return</span> cal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma</span><span class="params">(x, a, b)</span>:</span></span><br><span class="line">    c = (b ** a) / gamma_function(a)</span><br><span class="line">    y = c * (x ** (a - <span class="number">1</span>)) * np.exp(-b * x)</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ls <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">1</span>), (<span class="number">3</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>)]:</span><br><span class="line">    a, b = ls[<span class="number">0</span>], ls[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    x = np.arange(<span class="number">0</span>, <span class="number">20</span>, <span class="number">0.01</span>, dtype=np.float)</span><br><span class="line">    x, y, u, s = gamma(x, a=a, b=b)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$\mu=%.2f,\ \sigma=%.2f,'</span></span><br><span class="line">                         <span class="string">r'\ \alpha=%d,\ \beta=%d$'</span> % (u, s, a, b))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_21_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a, b = <span class="number">2.</span>, <span class="number">2.</span></span><br><span class="line">s = np.random.gamma(a, b, <span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">50</span>, density=<span class="keyword">True</span>)</span><br><span class="line">y = bins**(a<span class="number">-1</span>)*(np.exp(-bins/b) /</span><br><span class="line">                      (sps.gamma(a)*b**b))</span><br><span class="line">plt.plot(bins, y, linewidth=<span class="number">2</span>, color=<span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x12cb80c50&gt;]</code></pre><p><img src="output_22_1.png" alt="png"></p><h2 id="Dirichlet-distribution-continuous"><a href="#Dirichlet-distribution-continuous" class="headerlink" title="Dirichlet distribution(continuous)"></a>Dirichlet distribution(continuous)</h2><ul><li>Dirichlet distribution is conjugate to the MultiNomial distributions. 即Dirichlet分布乘上一个多项分布的似然函数后，得到的后验分布仍然是一个Dirichlet分布。</li><li>If k=2, it will be Beta distribution.</li></ul><script type="math/tex; mode=display">f \left(x_1,\ldots, x_{K}; \alpha_1,\ldots, \alpha_K \right) = \frac{1}{\mathrm{B}(\boldsymbol\alpha)} \prod_{i=1}^K x_i^{\alpha_i - 1}</script><p>where $\{x_k\}_{k=1}^{k=K}$ belong to the standard $K-1$ simplex, or in other words: </p><script type="math/tex; mode=display">\sum_{i=1}^{K} x_i=1 \mbox{ and } x_i \ge 0 \mbox{ for all } i \in [1,K]</script><p>The normalizing constant is the multivariate beta function, which can be expressed in terms of the gamma function</p><script type="math/tex; mode=display">\mathrm{B}(\boldsymbol\alpha) = \frac{\prod_{i=1}^K \Gamma(\alpha_i)}{\Gamma\left(\sum_{i=1}^K \alpha_i\right)},\qquad\boldsymbol{\alpha}=(\alpha_1,\ldots,\alpha_K).</script><blockquote><p>Dirichlet分布可以看做是分布之上的分布。如何理解这句话，我们可以先举个例子：假设我们有一个骰子，其有六面，分别为{1,2,3,4,5,6}。现在我们做了10000次投掷的实验，得到的实验结果是六面分别出现了{2000,2000,2000,2000,1000,1000}次，如果用每一面出现的次数与试验总数的比值估计这个面出现的概率，则我们得到六面出现的概率，分别为{0.2,0.2,0.2,0.2,0.1,0.1}。现在，我们还不满足，我们想要做10000次试验，每次试验中我们都投掷骰子10000次。我们想知道，骰子六面出现概率为{0.2,0.2,0.2,0.2,0.1,0.1}的概率是多少（说不定下次试验统计得到的概率为{0.1, 0.1, 0.2, 0.2, 0.2, 0.2}这样了）。这样我们就在思考骰子六面出现概率分布这样的分布之上的分布。而这样一个分布就是Dirichlet分布。 </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalization</span><span class="params">(x, s)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :return: normalizated list, where sum(x) == s</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> [(i * s) / sum(x) <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sampling</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> normalization([random.randint(<span class="number">1</span>, <span class="number">100</span>),</span><br><span class="line">            random.randint(<span class="number">1</span>, <span class="number">100</span>), random.randint(<span class="number">1</span>, <span class="number">100</span>)], s=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma_function</span><span class="params">(n)</span>:</span></span><br><span class="line">    cal = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        cal *= i</span><br><span class="line">    <span class="keyword">return</span> cal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">beta_function</span><span class="params">(alpha)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param alpha: list, len(alpha) is k</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    numerator = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> alpha:</span><br><span class="line">        numerator *= gamma_function(a)</span><br><span class="line">    denominator = gamma_function(sum(alpha))</span><br><span class="line">    <span class="keyword">return</span> numerator / denominator</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dirichlet</span><span class="params">(x, a, n)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param x: list of [x[1,...,K], x[1,...,K], ...], shape is (n_trial, K)</span></span><br><span class="line"><span class="string">    :param a: list of coefficient, a_i &gt; 0</span></span><br><span class="line"><span class="string">    :param n: number of trial</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    c = (<span class="number">1</span> / beta_function(a))</span><br><span class="line">    y = [c * (xn[<span class="number">0</span>] ** (a[<span class="number">0</span>] - <span class="number">1</span>)) * (xn[<span class="number">1</span>] ** (a[<span class="number">1</span>] - <span class="number">1</span>))</span><br><span class="line">         * (xn[<span class="number">2</span>] ** (a[<span class="number">2</span>] - <span class="number">1</span>)) <span class="keyword">for</span> xn <span class="keyword">in</span> x]</span><br><span class="line">    x = np.arange(n)</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line">n_experiment = <span class="number">1200</span></span><br><span class="line"><span class="keyword">for</span> ls <span class="keyword">in</span> [(<span class="number">6</span>, <span class="number">2</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">7</span>, <span class="number">5</span>), (<span class="number">6</span>, <span class="number">2</span>, <span class="number">6</span>), (<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)]:</span><br><span class="line">    alpha = list(ls)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># random samping [x[1,...,K], x[1,...,K], ...], shape is (n_trial, K)</span></span><br><span class="line">    <span class="comment"># each sum of row should be one.</span></span><br><span class="line">    x = [sampling() <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>, n_experiment + <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    x, y, u, s = dirichlet(x, alpha, n=n_experiment)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$\alpha=(%d,%d,%d)$'</span> % (ls[<span class="number">0</span>], ls[<span class="number">1</span>], ls[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_24_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.dirichlet((<span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.5</span>), <span class="number">20</span>).transpose()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s.shape</span><br></pre></td></tr></table></figure><pre><code>(3, 20)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.barh(range(<span class="number">20</span>), s[<span class="number">0</span>])</span><br><span class="line">plt.barh(range(<span class="number">20</span>), s[<span class="number">1</span>], left=s[<span class="number">0</span>], color=<span class="string">'g'</span>)</span><br><span class="line">plt.barh(range(<span class="number">20</span>), s[<span class="number">2</span>], left=s[<span class="number">0</span>]+s[<span class="number">1</span>], color=<span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;BarContainer object of 20 artists&gt;</code></pre><p><img src="output_27_1.png" alt="png"></p><h2 id="Exponential-distribution-continuous"><a href="#Exponential-distribution-continuous" class="headerlink" title="Exponential distribution(continuous)"></a>Exponential distribution(continuous)</h2><ul><li>Exponential distribution is special cases of the gamma distribution when alpha is 1.</li></ul><script type="math/tex; mode=display">f(x;\lambda) = \begin{cases}\lambda e^{-\lambda x} & x \ge 0, \\0 & x < 0.\end{cases}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exponential</span><span class="params">(x, lamb)</span>:</span></span><br><span class="line">    y = lamb * np.exp(-lamb * x)</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> lamb <span class="keyword">in</span> [<span class="number">0.5</span>, <span class="number">1</span>, <span class="number">1.5</span>]:</span><br><span class="line"></span><br><span class="line">    x = np.arange(<span class="number">0</span>, <span class="number">20</span>, <span class="number">0.01</span>, dtype=np.float)</span><br><span class="line">    x, y, u, s = exponential(x, lamb=lamb)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$\mu=%.2f,\ \sigma=%.2f,'</span></span><br><span class="line">                         <span class="string">r'\ \lambda=%d$'</span> % (u, s, lamb))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_29_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.exponential(scale = <span class="number">0.5</span>, size=<span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">20</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_30_0.png" alt="png"></p><h2 id="Gaussian-distribution-continuous"><a href="#Gaussian-distribution-continuous" class="headerlink" title="Gaussian distribution(continuous)"></a>Gaussian distribution(continuous)</h2><script type="math/tex; mode=display">f(x) = \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian</span><span class="params">(x, n)</span>:</span></span><br><span class="line">    u = x.mean()</span><br><span class="line">    s = x.std()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># divide [x.min(), x.max()] by n</span></span><br><span class="line">    x = np.linspace(x.min(), x.max(), n)</span><br><span class="line"></span><br><span class="line">    a = ((x - u) ** <span class="number">2</span>) / (<span class="number">2</span> * (s ** <span class="number">2</span>))</span><br><span class="line">    y = <span class="number">1</span> / (s * np.sqrt(<span class="number">2</span> * np.pi)) * np.exp(-a)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, y, x.mean(), x.std()</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-100</span>, <span class="number">100</span>) <span class="comment"># define range of x</span></span><br><span class="line">x, y, u, s = gaussian(x, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y, label=<span class="string">r'$\mu=%.2f,\ \sigma=%.2f$'</span> % (u, s))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_32_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mu, sigma = <span class="number">0</span>, <span class="number">0.1</span> <span class="comment"># mean and standard deviation</span></span><br><span class="line">s = np.random.default_rng().normal(mu, sigma, <span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">30</span>, density=<span class="keyword">True</span>)</span><br><span class="line">plt.plot(bins, <span class="number">1</span>/(sigma * np.sqrt(<span class="number">2</span> * np.pi)) *</span><br><span class="line">         np.exp( - (bins - mu)**<span class="number">2</span> / (<span class="number">2</span> * sigma**<span class="number">2</span>) ),</span><br><span class="line">         linewidth=<span class="number">2</span>, color=<span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x11122d4e0&gt;]</code></pre><p><img src="output_33_1.png" alt="png"></p><h2 id="Poisson-distribution"><a href="#Poisson-distribution" class="headerlink" title="Poisson distribution"></a>Poisson distribution</h2><ul><li>在一个时间段内事件平均发生的次数服从泊松分布</li></ul><script type="math/tex; mode=display">\!f(k; \lambda)= \Pr(X = k)= \frac{\lambda^k e^{-\lambda}}{k!},</script><ul><li>e is Euler’s number (e = 2.71828…)</li><li>k! is the factorial of k.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.poisson(<span class="number">5</span>, <span class="number">10000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">14</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_35_0.png" alt="png"></p><h2 id="Chi-squared-distribution-continuous"><a href="#Chi-squared-distribution-continuous" class="headerlink" title="Chi-squared distribution(continuous)"></a>Chi-squared distribution(continuous)</h2><ul><li>Chi-square distribution with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables.</li><li>Chi-square distribution is special case of Beta distribution</li></ul><p>If $Z_1, \cdots, Z_k$ are independent, standard normal random variables, then the sum of their squares,</p><script type="math/tex; mode=display">Q\ = \sum_{i=1}^k Z_i^2 ,</script><p>is distributed according to the chi-square distribution with k degrees of freedom. This is usually denoted as</p><script type="math/tex; mode=display">Q\ \sim\ \chi^2(k)\ \ \text{or}\ \ Q\ \sim\ \chi^2_k .</script><p>The chi-square distribution has one parameter: a positive integer k that specifies the number of degrees of freedom (the number of $Z_i$ s).</p><p>The probability density function (pdf) of the chi-square distribution is</p><script type="math/tex; mode=display">f(x;\,k) =\begin{cases}  \dfrac{x^{\frac k 2 -1} e^{-\frac x 2}}{2^{\frac k 2} \Gamma\left(\frac k 2 \right)},  & x > 0; \\ 0, & \text{otherwise}.\end{cases}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma_function</span><span class="params">(n)</span>:</span></span><br><span class="line">    cal = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        cal *= i</span><br><span class="line">    <span class="keyword">return</span> cal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chi_squared</span><span class="params">(x, k)</span>:</span></span><br><span class="line"></span><br><span class="line">    c = <span class="number">1</span> / (<span class="number">2</span> ** (k/<span class="number">2</span>)) * gamma_function(k//<span class="number">2</span>)</span><br><span class="line">    y = c * (x ** (k/<span class="number">2</span> - <span class="number">1</span>)) * np.exp(-x /<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>]:</span><br><span class="line">    x = np.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">0.01</span>, dtype=np.float)</span><br><span class="line">    x, y, _, _ = chi_squared(x, k)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$k=%d$'</span> % (k))</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_37_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.chisquare(<span class="number">4</span>,<span class="number">10000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">20</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_38_0.png" alt="png"></p><h2 id="Student-t-distribution-continuous"><a href="#Student-t-distribution-continuous" class="headerlink" title="Student-t distribution(continuous)"></a>Student-t distribution(continuous)</h2><ul><li>Definition</li></ul><p>Let $X_1, \cdots, X_n$ be independent and identically distributed as $N(\mu, \sigma^2)$, i.e. this is a sample of size $n$ from a normally distributed population with expected mean value $\mu$ and variance $\sigma^{2}$</p><p>Let</p><script type="math/tex; mode=display">\bar X = \frac 1 n \sum_{i=1}^n X_i</script><p>be the sample mean and let</p><script type="math/tex; mode=display">S^2 = \frac 1 {n-1} \sum_{i=1}^n (X_i - \bar X)^2</script><p>be the (Bessel-corrected) sample variance. </p><p>Then the random variable</p><script type="math/tex; mode=display">\frac{ \bar X - \mu} {S /\sqrt{n}}</script><p>has a standard normal distribution</p><p>Student’s t-distribution has the probability density function given by</p><script type="math/tex; mode=display">f(t) = \frac{\Gamma(\frac{\nu+1}{2})} {\sqrt{\nu\pi}\,\Gamma(\frac{\nu}{2})} \left(1+\frac{t^2}{\nu} \right)^{\!-\frac{\nu+1}{2}}</script><ul><li>$\nu$ is the number of degrees of freedom </li><li>$\Gamma$ is the gamma function.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma_function</span><span class="params">(n)</span>:</span></span><br><span class="line">    cal = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        cal *= i</span><br><span class="line">    <span class="keyword">return</span> cal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">student_t</span><span class="params">(x, freedom, n)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># divide [x.min(), x.max()] by n</span></span><br><span class="line">    x = np.linspace(x.min(), x.max(), n)</span><br><span class="line"></span><br><span class="line">    c = gamma_function((freedom + <span class="number">1</span>) // <span class="number">2</span>) \</span><br><span class="line">        / np.sqrt(freedom * np.pi) * gamma_function(freedom // <span class="number">2</span>)</span><br><span class="line">    y = c * (<span class="number">1</span> + x**<span class="number">2</span> / freedom) ** (-((freedom + <span class="number">1</span>) / <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> freedom <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>]:</span><br><span class="line"></span><br><span class="line">    x = np.arange(<span class="number">-10</span>, <span class="number">10</span>) <span class="comment"># define range of x</span></span><br><span class="line">    x, y, _, _ = student_t(x, freedom=freedom, n=<span class="number">10000</span>)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$v=%d$'</span> % (freedom))</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_40_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Suppose the daily energy intake for 11 women in kilojoules (kJ) is:</span></span><br><span class="line"></span><br><span class="line">intake = np.array([<span class="number">5260.</span>, <span class="number">5470</span>, <span class="number">5640</span>, <span class="number">6180</span>, <span class="number">6390</span>, <span class="number">6515</span>, <span class="number">6805</span>, <span class="number">7515</span>, \</span><br><span class="line">                    <span class="number">7515</span>, <span class="number">8230</span>, <span class="number">8770</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## Does their energy intake deviate systematically from the recommended value of 7725 kJ?</span></span><br><span class="line"><span class="comment">## We have 10 degrees of freedom, so is the sample mean within 95% of the recommended value?</span></span><br><span class="line"></span><br><span class="line">s = np.random.standard_t(<span class="number">10</span>, size=<span class="number">100000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Calculate the t statistic, setting the ddof parameter to the unbiased value so the divisor in the standard deviation will be degrees of freedom, N-1.</span></span><br><span class="line"></span><br><span class="line">t = (np.mean(intake)<span class="number">-7725</span>)/(intake.std(ddof=<span class="number">1</span>)/np.sqrt(len(intake)))</span><br><span class="line"></span><br><span class="line">h = plt.hist(s, bins=<span class="number">100</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_41_0.png" alt="png"></p><p>So the p-value is about 0.009, which says the null hypothesis has a probability of about 99% of being true.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.sum(s&lt;t) / float(len(s))</span><br></pre></td></tr></table></figure><pre><code>0.0086</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/graykode/distribution-is-all-you-need" target="_blank" rel="noopener">https://github.com/graykode/distribution-is-all-you-need</a></li><li><a href="https://blog.csdn.net/deropty/article/details/50266309" target="_blank" rel="noopener">https://blog.csdn.net/deropty/article/details/50266309</a></li></ul>]]></content>
    
    <summary type="html">
    
      Understanding distributions
    
    </summary>
    
    
      <category term="Math" scheme="https://zhangruochi.com/categories/Math/"/>
    
      <category term="Statistics" scheme="https://zhangruochi.com/categories/Math/Statistics/"/>
    
    
  </entry>
  
  <entry>
    <title>Gaussian Mixed Model Introduction</title>
    <link href="https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/"/>
    <id>https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/</id>
    <published>2020-03-15T04:44:46.000Z</published>
    <updated>2020-03-15T07:08:58.707Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Gaussian-Mixture-Models-高斯混合模型"><a href="#Gaussian-Mixture-Models-高斯混合模型" class="headerlink" title="Gaussian Mixture Models(高斯混合模型)"></a>Gaussian Mixture Models(高斯混合模型)</h2><p>高斯模型即正态分布，高斯混合模型就是几个正态分布的叠加，每一个正态分布代表一个类别，所以和K-means很像，高斯混合模型也可以用来做无监督的聚类分析。</p><h2 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h2><h3 id="Jensen’s-inequality"><a href="#Jensen’s-inequality" class="headerlink" title="Jensen’s inequality"></a>Jensen’s inequality</h3><p>For any concave function, we have</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Property of concave function</div></center><script type="math/tex; mode=display">f(\alpha a + (1-\alpha)b) \geq \alpha f(a) + (1 - \alpha) f(b)</script><p>Then, we have:</p><script type="math/tex; mode=display">f(\mathbb{E}_{p(t)}t) \geq \mathbb{E}_{p(t)}f(t)</script><h3 id="Kullback–Leibler-divergence"><a href="#Kullback–Leibler-divergence" class="headerlink" title="Kullback–Leibler divergence"></a>Kullback–Leibler divergence</h3><script type="math/tex; mode=display">\mathcal K \mathcal L (q || p) = \int q(x) log\frac{q(x)}{p(x)}dx</script><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><script type="math/tex; mode=display">max_{\theta} \prod_{i=1}^{N} p(x_i | \theta) = \prod_{i = 1}^{N} (\pi_1 \mathcal{N} (x_i | \mu_1, \mathbb{E_1}) + \cdots )</script><script type="math/tex; mode=display">\text{subject to} \qquad \pi_1 + \pi_2 + \pi_3 = 1; \pi_k \geq 0; k = 1,2,3</script><h3 id="Introducing-latent-variable"><a href="#Introducing-latent-variable" class="headerlink" title="Introducing latent variable"></a>Introducing latent variable</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Latent Variable</div></center><script type="math/tex; mode=display">p(t=c| \theta) = \pi_c</script><script type="math/tex; mode=display">p(x | t = c , \theta) = \mathcal N ( x | \mu_c,\mathbb{E_c} )</script><h3 id="General-form-of-Expectation-Maximization"><a href="#General-form-of-Expectation-Maximization" class="headerlink" title="General form of Expectation Maximization"></a>General form of Expectation Maximization</h3><script type="math/tex; mode=display">p(x_i | \theta) = \sum_{c=1}^{3}p(x_i | t_i = c , \theta) p(t_i = c | \theta)</script><h2 id="基本步骤"><a href="#基本步骤" class="headerlink" title="基本步骤"></a>基本步骤</h2><h3 id="概率角度"><a href="#概率角度" class="headerlink" title="概率角度"></a>概率角度</h3><ol><li>初始化$\theta^{old}$</li><li>E step: 用 $\theta^{old}$计算样本对应隐变量的概率分布，即求后验概率：$p(Z|X,\theta^{old})$。然后计算完全数据的对数似然对后验概率的期望，它是变量$\theta$的函数:<script type="math/tex; mode=display">Q(\theta, \theta^{old}) = \sum_{Z}p(Z|X, \theta^{old})ln p(X,Z|\theta)</script></li><li>M step: 极大化Q函数,得到$\theta^{new}$</li><li>若不收敛、持续迭代。</li></ol><h3 id="程序角度"><a href="#程序角度" class="headerlink" title="程序角度"></a>程序角度</h3><ol><li>猜测有几个类别，既有几个高斯分布;</li><li>针对每一个高斯分布，随机给其均值和方差进行赋值;</li><li>针对每一个样本，计算其在各个高斯分布下的概率;<script type="math/tex; mode=display">f(x)=\frac{1 }{\sqrt\times\sigma}e^{\frac{1}{2}(\frac{x-\mu}{\sigma})^2}</script></li><li>针对每一个高斯分布，每一个样本对该高斯分布的贡献可以由其下的概率表示，如概率大则表示贡献大，反之亦然。这样把样本对该高斯分布的贡献作为权重来计算加权的均值和方差。之后替代其原本的均值和方差;</li><li>重复3~4直到每一个高斯分布的均值和方差收敛;</li><li>当高斯混合模型的特征值维数大于一维时，在计算加权的时候还要计算协方差，即要考虑不同维度之间的相互关联.</li></ol><blockquote><p>即通过模型来计算数据的期望值。通过更新参数μ和σ来让期望值最大化。这个过程可以不断迭代直到两次迭代中生成的参数变化非常小为止。该过程和k-means的算法训练过程很相似（k-means不断更新类中心来让结果最大化），只不过在这里的高斯模型中，我们需要同时更新两个参数：分布的均值和标准差。</p></blockquote><h2 id="GMM-VS-KMeans"><a href="#GMM-VS-KMeans" class="headerlink" title="GMM VS KMeans"></a>GMM VS KMeans</h2><p>KMeans 将样本分到离其最近的聚类中心所在的簇，也就是每个样本数据属于某簇的概率非零即1。对比KMeans，高斯混合的不同之处在于，样本点属于某簇的概率不是非零即1的，而是属于不同簇有不同的概率值。高斯混合模型假设所有样本点是由K个高斯分布混合而成的。</p><h2 id="Implementing-the-EM-Expectation-Maximization-algorithm-for-Gaussian-mixture-models"><a href="#Implementing-the-EM-Expectation-Maximization-algorithm-for-Gaussian-mixture-models" class="headerlink" title="Implementing the EM(Expectation Maximization) algorithm for Gaussian mixture models"></a>Implementing the EM(Expectation Maximization) algorithm for Gaussian mixture models</h2><h3 id="Log-likelihood"><a href="#Log-likelihood" class="headerlink" title="Log likelihood"></a>Log likelihood</h3><p>We provide a function to calculate log likelihood for mixture of Gaussians. The log likelihood quantifies the probability of observing a given set of data under a particular setting of the parameters in our model. We will use this to assess convergence of our EM algorithm; specifically, we will keep looping through EM update steps until the log likehood ceases to increase at a certain rate.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_sum_exp</span><span class="params">(Z)</span>:</span></span><br><span class="line">    <span class="string">""" Compute log(\sum_i exp(Z_i)) for some array Z."""</span></span><br><span class="line">    <span class="keyword">return</span> np.max(Z) + np.log(np.sum(np.exp(Z - np.max(Z))))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loglikelihood</span><span class="params">(data, weights, means, covs)</span>:</span></span><br><span class="line">    <span class="string">""" Compute the loglikelihood of the data for a Gaussian mixture model with the given parameters. """</span></span><br><span class="line">    num_clusters = len(means)</span><br><span class="line">    num_dim = len(data[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    ll = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">        </span><br><span class="line">        Z = np.zeros(num_clusters)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(num_clusters):</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Compute (x-mu)^T * Sigma^&#123;-1&#125; * (x-mu)</span></span><br><span class="line">            delta = np.array(d) - means[k]</span><br><span class="line">            exponent_term = np.dot(delta.T, np.dot(np.linalg.inv(covs[k]), delta))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Compute loglikelihood contribution for this data point and this cluster</span></span><br><span class="line">            Z[k] += np.log(weights[k])</span><br><span class="line">            Z[k] -= <span class="number">1</span>/<span class="number">2.</span> * (num_dim * np.log(<span class="number">2</span>*np.pi) + np.log(np.linalg.det(covs[k])) + exponent_term)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Increment loglikelihood contribution of this data point across all clusters</span></span><br><span class="line">        ll += log_sum_exp(Z)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> ll</span><br></pre></td></tr></table></figure><h3 id="E-step-assign-cluster-responsibilities-given-current-parameters"><a href="#E-step-assign-cluster-responsibilities-given-current-parameters" class="headerlink" title="E-step: assign cluster responsibilities, given current parameters"></a>E-step: assign cluster responsibilities, given current parameters</h3><p>The first step in the EM algorithm is to compute cluster responsibilities. Let $r_{ik}$ denote the responsibility of cluster $k$ for data point $i$. Note that cluster responsibilities are fractional parts: Cluster responsibilities for a single data point $i$ should sum to 1.</p><script type="math/tex; mode=display">r_{i1} + r_{i2} + \ldots + r_{iK} = 1</script><p>To figure how much a cluster is responsible for a given data point, we compute the likelihood of the data point under the  particular cluster assignment, multiplied by the weight of the cluster. For data point $i$ and cluster $k$, this quantity is</p><script type="math/tex; mode=display">r_{ik} \propto \pi_k N(x_i | \mu_k, \Sigma_k)</script><p>where $N(x_i | \mu_k, \Sigma_k)$ is the Gaussian distribution for cluster $k$ (with mean $\mu_k$ and covariance $\Sigma_k$).</p><p>We used $\propto$ because the quantity $N(x_i | \mu_k, \Sigma_k)$ is not yet the responsibility we want. To ensure that all responsibilities over each data point add up to 1, we add the normalization constant in the denominator:</p><script type="math/tex; mode=display">r_{ik} = \frac{\pi_k N(x_i | \mu_k, \Sigma_k)}{\sum_{k=1}^{K} \pi_k N(x_i | \mu_k, \Sigma_k)}.</script><p>Complete the following function that computes $r_{ik}$ for all data points $i$ and clusters $k$.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_responsibilities</span><span class="params">(data, weights, means, covariances)</span>:</span></span><br><span class="line">    <span class="string">'''E-step: compute responsibilities, given the current parameters'''</span></span><br><span class="line">    num_data = len(data)</span><br><span class="line">    num_clusters = len(means)</span><br><span class="line">    resp = np.zeros((num_data, num_clusters))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update resp matrix so that resp[i,k] is the responsibility of cluster k for data point i.</span></span><br><span class="line">    <span class="comment"># Hint: To compute likelihood of seeing data point i given cluster k, use multivariate_normal.pdf.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_data):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(num_clusters):</span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            resp[i, k] = weights[k]*multivariate_normal.pdf(data[i], mean=means[k], cov=covariances[k])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add up responsibilities over each data point and normalize</span></span><br><span class="line">    row_sums = resp.sum(axis=<span class="number">1</span>)[:, np.newaxis]</span><br><span class="line">    resp = resp / row_sums</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> resp</span><br></pre></td></tr></table></figure><h3 id="M-step-Update-parameters-given-current-cluster-responsibilities"><a href="#M-step-Update-parameters-given-current-cluster-responsibilities" class="headerlink" title="M-step: Update parameters, given current cluster responsibilities"></a>M-step: Update parameters, given current cluster responsibilities</h3><p>Once the cluster responsibilities are computed, we update the parameters (weights, means, and covariances) associated with the clusters.</p><p><strong>Computing soft counts</strong>. Before updating the parameters, we first compute what is known as “soft counts”. The soft count of a cluster is the sum of all cluster responsibilities for that cluster:</p><script type="math/tex; mode=display">N^{\text{soft}}_k = r_{1k} + r_{2k} + \ldots + r_{Nk} = \sum_{i=1}^{N} r_{ik}</script><p>where we loop over data points. Note that, unlike k-means, we must loop over every single data point in the dataset. This is because all clusters are represented in all data points, to a varying degree.</p><p>We provide the function for computing the soft counts:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_soft_counts</span><span class="params">(resp)</span>:</span></span><br><span class="line">    <span class="comment"># Compute the total responsibility assigned to each cluster, which will be useful when </span></span><br><span class="line">    <span class="comment"># implementing M-steps below. In the lectures this is called N^&#123;soft&#125;</span></span><br><span class="line">    counts = np.sum(resp, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> counts</span><br></pre></td></tr></table></figure><p><strong>Updating weights.</strong> The cluster weights show us how much each cluster is represented over all data points. The weight of cluster $k$ is given by the ratio of the soft count $N^{\text{soft}}_{k}$ to the total number of data points $N$:</p><script type="math/tex; mode=display">\hat{\pi}_k = \frac{N^{\text{soft}}_{k}}{N}</script><p>Notice that $N$ is equal to the sum over the soft counts $N^{\text{soft}}_{k}$ of all clusters.</p><p>Complete the following function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_weights</span><span class="params">(counts)</span>:</span></span><br><span class="line">    num_clusters = len(counts)</span><br><span class="line">    weights = [<span class="number">0.</span>] * num_clusters</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(num_clusters):</span><br><span class="line">        <span class="comment"># Update the weight for cluster k using the M-step update rule for the cluster weight, \hat&#123;\pi&#125;_k.</span></span><br><span class="line">        <span class="comment"># HINT: compute # of data points by summing soft counts.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        weights[k] = counts[k] / np.sum(counts)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure><p><strong>Updating means</strong>. The mean of each cluster is set to the <a href="https://en.wikipedia.org/wiki/Weighted_arithmetic_mean" target="_blank" rel="noopener">weighted average</a> of all data points, weighted by the cluster responsibilities:</p><script type="math/tex; mode=display">\hat{\mu}_k = \frac{1}{N_k^{\text{soft}}} \sum_{i=1}^N r_{ik}x_i</script><p>Complete the following function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_means</span><span class="params">(data, resp, counts)</span>:</span></span><br><span class="line">    num_clusters = len(counts)</span><br><span class="line">    num_data = len(data)</span><br><span class="line">    means = [np.zeros(len(data[<span class="number">0</span>]))] * num_clusters</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(num_clusters):</span><br><span class="line">        <span class="comment"># Update means for cluster k using the M-step update rule for the mean variables.</span></span><br><span class="line">        <span class="comment"># This will assign the variable means[k] to be our estimate for \hat&#123;\mu&#125;_k.</span></span><br><span class="line">        weighted_sum = <span class="number">0.</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_data):</span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            weighted_sum += data[i] * resp[i][k]</span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        means[k] = weighted_sum / counts[k]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> means</span><br></pre></td></tr></table></figure><p><strong>Updating covariances</strong>.  The covariance of each cluster is set to the weighted average of all <a href="https://people.duke.edu/~ccc14/sta-663/LinearAlgebraReview.html" target="_blank" rel="noopener">outer products</a>, weighted by the cluster responsibilities:</p><script type="math/tex; mode=display">\hat{\Sigma}_k = \frac{1}{N^{\text{soft}}_k}\sum_{i=1}^N r_{ik} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T</script><p>The “outer product” in this context refers to the matrix product</p><script type="math/tex; mode=display">(x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T.</script><p>Letting $(x_i - \hat{\mu}_k)$ to be $d \times 1$ column vector, this product is a $d \times d$ matrix. Taking the weighted average of all outer products gives us the covariance matrix, which is also $d \times d$.</p><p>Complete the following function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_covariances</span><span class="params">(data, resp, counts, means)</span>:</span></span><br><span class="line">    num_clusters = len(counts)</span><br><span class="line">    num_dim = len(data[<span class="number">0</span>])</span><br><span class="line">    num_data = len(data)</span><br><span class="line">    covariances = [np.zeros((num_dim,num_dim))] * num_clusters</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(num_clusters):</span><br><span class="line">        <span class="comment"># Update covariances for cluster k using the M-step update rule for covariance variables.</span></span><br><span class="line">        <span class="comment"># This will assign the variable covariances[k] to be the estimate for \hat&#123;\Sigma&#125;_k.</span></span><br><span class="line">        weighted_sum = np.zeros((num_dim, num_dim))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_data):</span><br><span class="line">            <span class="comment"># YOUR CODE HERE (Hint: Use np.outer on the data[i] and this cluster's mean)</span></span><br><span class="line">            weighted_sum += resp[i][k]*np.outer(data[i] - means[k], data[i] - means[k])</span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        covariances[k] = weighted_sum / counts[k]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> covariances</span><br></pre></td></tr></table></figure><h3 id="The-EM-algorithm"><a href="#The-EM-algorithm" class="headerlink" title="The EM algorithm"></a>The EM algorithm</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SOLUTION</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">EM</span><span class="params">(data, init_means, init_covariances, init_weights, maxiter=<span class="number">1000</span>, thresh=<span class="number">1e-4</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Make copies of initial parameters, which we will update during each iteration</span></span><br><span class="line">    means = init_means[:]</span><br><span class="line">    covariances = init_covariances[:]</span><br><span class="line">    weights = init_weights[:]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Infer dimensions of dataset and the number of clusters</span></span><br><span class="line">    num_data = len(data)</span><br><span class="line">    num_dim = len(data[<span class="number">0</span>])</span><br><span class="line">    num_clusters = len(means)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize some useful variables</span></span><br><span class="line">    resp = np.zeros((num_data, num_clusters))</span><br><span class="line">    ll = loglikelihood(data, weights, means, covariances)</span><br><span class="line">    ll_trace = [ll]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> range(maxiter):</span><br><span class="line">        <span class="keyword">if</span> it % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Iteration %s"</span> % it)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># E-step: compute responsibilities</span></span><br><span class="line">        resp = compute_responsibilities(data, weights, means, covariances)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># M-step</span></span><br><span class="line">        <span class="comment"># Compute the total responsibility assigned to each cluster, which will be useful when </span></span><br><span class="line">        <span class="comment"># implementing M-steps below. In the lectures this is called N^&#123;soft&#125;</span></span><br><span class="line">        counts = compute_soft_counts(resp)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the weight for cluster k using the M-step update rule for the cluster weight, \hat&#123;\pi&#125;_k.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        weights = compute_weights(counts)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update means for cluster k using the M-step update rule for the mean variables.</span></span><br><span class="line">        <span class="comment"># This will assign the variable means[k] to be our estimate for \hat&#123;\mu&#125;_k.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        means = compute_means(data, resp, counts)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update covariances for cluster k using the M-step update rule for covariance variables.</span></span><br><span class="line">        <span class="comment"># This will assign the variable covariances[k] to be the estimate for \hat&#123;\Sigma&#125;_k.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        covariances = compute_covariances(data, resp, counts, means)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the loglikelihood at this iteration</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        ll_latest = loglikelihood(data, weights, means, covariances)</span><br><span class="line">        ll_trace.append(ll_latest)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check for convergence in log-likelihood and store</span></span><br><span class="line">        <span class="keyword">if</span> (ll_latest - ll) &lt; thresh <span class="keyword">and</span> ll_latest &gt; -np.inf:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        ll = ll_latest</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> it % <span class="number">5</span> != <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Iteration %s"</span> % it)</span><br><span class="line">    </span><br><span class="line">    out = &#123;<span class="string">'weights'</span>: weights, <span class="string">'means'</span>: means, <span class="string">'covs'</span>: covariances, <span class="string">'loglik'</span>: ll_trace, <span class="string">'resp'</span>: resp&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p><strong>Reference from <a href="https://zhuanlan.zhihu.com/p/29538307" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29538307</a></strong><br><strong>Reference from <a href="https://zhuanlan.zhihu.com/p/31103654" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31103654</a></strong><br><strong>Reference from coursera course Machine Learning Foundation from University of Washington</strong>  </p>]]></content>
    
    <summary type="html">
    
      Gaussian Mixture Models Introduction
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Machine-Learning/"/>
    
    
      <category term="Machine Learning Algorithm" scheme="https://zhangruochi.com/tags/Machine-Learning-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Lifelong Learning</title>
    <link href="https://zhangruochi.com/Lifelong-Learning/2020/03/14/"/>
    <id>https://zhangruochi.com/Lifelong-Learning/2020/03/14/</id>
    <published>2020-03-14T06:28:06.000Z</published>
    <updated>2020-03-14T07:42:44.810Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Three-problem-need-to-solve-when-realize-lifelong-learnig"><a href="#Three-problem-need-to-solve-when-realize-lifelong-learnig" class="headerlink" title="Three problem need to solve when realize lifelong learnig"></a>Three problem need to solve when realize lifelong learnig</h2><ol><li>Knowledge Retention</li><li>Knowledge Transfer</li><li>Model Expansion</li></ol><h2 id="Knowledge-Retention"><a href="#Knowledge-Retention" class="headerlink" title="Knowledge Retention"></a>Knowledge Retention</h2><p>When a network learn a new task. It has the inclination to forget the skills it has learned. The phenomenon that the model will forget the previous skills is called <strong>Catastrophic Forgetting</strong>.</p><p>The reason for resulting the catastrophic forgetting is not the model’s size which has no enough capacity to learn the new skills. To prove this we can make the model to learn a multi-task problem and get a good results.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">catastrophic forgetting</div></center><h3 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h3><p>The ideas to solve knowledge retention</p><h3 id="Elastic-Weight-Consolidation"><a href="#Elastic-Weight-Consolidation" class="headerlink" title="Elastic Weight Consolidation"></a>Elastic Weight Consolidation</h3><script type="math/tex; mode=display">L^{\prime}(\theta) = L(\theta) + \lambda \sum_{i} b_i(\theta_i - \theta_i^{b})^2</script><ul><li>$L^{\prime}(\theta)$ : the loss to be optimized</li><li>$L(\theta)$ : the loss of current task</li><li>$\theta_i$ : the parameters to be learning</li><li>$\theta_i^{b}$ : the parameters leaned from previous tasks</li><li>$b_i$ how important the parameter is</li></ul><p>the idea of EWC is: learning the new parameters which are not far from the previous parameters</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">EWC</div></center><h4 id><a href="#" class="headerlink" title=" "></a> </h4><h4 id="How-to-set-b-i"><a href="#How-to-set-b-i" class="headerlink" title="How to set $b_i$ ?"></a>How to set $b_i$ ?</h4><ul><li>small 2nd derivative -&gt; set $b_i$ to small or large</li><li>large 2nd derivative -&gt;  set $b_i$ to small</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">important of parameters</div></center><h3 id="Generating-Data"><a href="#Generating-Data" class="headerlink" title="Generating Data"></a>Generating Data</h3><p>Conducting multi-task learning by generating pseudo-data using generative model. </p><p>We know the multi-task learning is a good way to solve life long task(sometimes it is the upper bound). If a new task come, we can regard it as a multi-task problem combined with previous tasks and build a model to solve it. But the premise of dong this is we have the data of previous tasks. In reality, we can not store all the dataset of previous tasks. Therefore, We can use generative model to generate previous dataset.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Generating Data</div></center><h2 id="Knowledge-Transfer"><a href="#Knowledge-Transfer" class="headerlink" title="Knowledge Transfer"></a>Knowledge Transfer</h2><p>The difference of knowledge transfer between lifelong learning with transfer learning is that <code>transfer learning</code> is just concentrate on new task while the lifelong learning shuild consider the catastrophic forgetting</p><h3 id="Gredient-Episodic-Memory"><a href="#Gredient-Episodic-Memory" class="headerlink" title="Gredient Episodic Memory"></a>Gredient Episodic Memory</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">catastrophic forgetting</div></center><p>The idea of GEM is: When we update the parameters by gredient descent we can find a direction which can benifits the previous tasks and new tasks to update. The disadvantages of GEM is we need store a little bit of data of previous tasks.</p><h2 id="Model-Expansion"><a href="#Model-Expansion" class="headerlink" title="Model Expansion"></a>Model Expansion</h2><h3 id="Progressive-Neural-Networks"><a href="#Progressive-Neural-Networks" class="headerlink" title="Progressive Neural Networks"></a>Progressive Neural Networks</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">catastrophic forgetting</div></center><p>An example of model expansion is the <code>Progressive Neural Networks</code> proposed in 2016. We fix the parameters after learning some tasks, and then train the new task. We build a new model and use the output of previous task as the input of new task. However, there is a disadvantage that you can not train too many new tasks because it will cause a lot of load.</p><h3 id="Expert-Gate"><a href="#Expert-Gate" class="headerlink" title="Expert Gate"></a>Expert Gate</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="7.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">catastrophic forgetting</div></center><p>Exper Gate’s method: We still have one model for each task. For example, we have three tasks, and the fourth task is similar to the first one (we use Gate to determine which new task is similar to the old one), then we will use the model of the first task as the fourth Initialization of each task model, this has formed a certain migration effect. However, this method is still a task corresponding to a model, which still causes a lot of load on storage.</p><h3 id="Tasknomy"><a href="#Tasknomy" class="headerlink" title="Tasknomy"></a>Tasknomy</h3><p>The order of learning tasks is just like the order of our textbooks, which has a great impact on the final results. (This is a optimal order for the learning tasks).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Three-problem-need-to-solve-when-realize-lifelong-learnig&quot;&gt;&lt;a href=&quot;#Three-problem-need-to-solve-when-realize-lifelong-learnig&quot; clas
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Transfer Learning" scheme="https://zhangruochi.com/tags/Transfer-Learning/"/>
    
      <category term="Lifelong Learning" scheme="https://zhangruochi.com/tags/Lifelong-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Object Detection Summary</title>
    <link href="https://zhangruochi.com/Object-Detection-Summary/2020/03/06/"/>
    <id>https://zhangruochi.com/Object-Detection-Summary/2020/03/06/</id>
    <published>2020-03-06T20:53:08.000Z</published>
    <updated>2020-03-09T16:58:07.340Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基于REGION-PROPOSAL的方法"><a href="#基于REGION-PROPOSAL的方法" class="headerlink" title="基于REGION PROPOSAL的方法"></a>基于REGION PROPOSAL的方法</h2><h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><p>R-CNN的算法步骤:</p><ol><li>采用Selective Search方法从一张图像中提取约2K个候选区域； </li><li>首先归一化为统一尺寸，再对每个候选区域，使用深度网络提取特征； </li><li>将提取出的特征送入每一类的SVM 分类器，判别是否属于该类； </li><li>使用回归器精细修正候选框位置。因为目标检测问题的衡量标准是重叠面积：许多看似准确的检测结果，往往因为候选框不够准确，导致重叠面积很小。</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">R-CNN Architecture</div></center><h3 id="SPP-NET"><a href="#SPP-NET" class="headerlink" title="SPP-NET"></a>SPP-NET</h3><p>Kaiming He最先对此作出改进，提出了SPP-net（全称：Spatial Pyramid Pooling），最大的改进是只需要将原图输入一次，就可以得到每个候选区域的特征。</p><p>在R-CNN中，候选区域需要经过变形缩放，以适应CNN输入，可以通过修改网络结构，使得任意大小的图片都能输入到CNN中。Kaiming He在论文中提出了SPP结构来适应任何大小的图片输入。SPP-net对R-CNN最大的改进就是特征提取步骤做了修改，其他模块仍然和R-CNN一样。特征提取不再需要每个候选区域都经过CNN，只需要将整张图片输入到CNN就可以了，ROI特征直接从特征图获取。和R-CNN相比，速度提高了24~102倍。</p><p>SPP-Net的算法步骤:</p><ol><li>首先通过选择性搜索，对待检测的图片进行搜索出2000个候选窗口。这一步和R-CNN一样。</li><li>特征提取阶段。这一步就是和R-CNN最大的区别了，这一步骤的具体操作如下：把整张待检测的图片，输入CNN中，进行一次性特征提取，得到feature maps，然后在feature maps中找到各个候选框的区域(ROI)，再对各个候选框采用金字塔空间池化，提取出固定长度的特征向量。而R-CNN输入的是每个候选框，然后在进入CNN，因为SPP-Net只需要一次对整张图片进行特征提取，速度会大大提升。</li><li>最后一步也是和R-CNN一样，采用SVM算法进行特征向量分类识别。</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">SPP Architecture</div></center><h3 id="FAST-R-CNN"><a href="#FAST-R-CNN" class="headerlink" title="FAST R-CNN"></a>FAST R-CNN</h3><p>FAST R-CNN的算法步骤：</p><ol><li>通过selective search生成region proposal，每张图片大约2000个RoI； </li><li>Fast-RCNN把整张图片送入CNN，进行特征提取，把region proposal映射到CNN的最后一层卷积feature map上； </li><li>通过RoI pooling层（也可以称为单层的SPP layer）使得每个建议窗口生成固定大小的feature map； </li><li>继续经过两个全连接层（FC）得到特征向量。特征向量经由各自的FC层，得到两个输出向量，第一个是分类，使用softmax，第二个是每一类的bounding box回归。利用Softmax Loss（探测分类概率）和Smooth L1 Loss（探测边框回归）对分类概率和边框回归（Bounding Box Regression）联合训练。</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Fast-RCNN Architecture</div></center><h3 id="FASTER-R-CNN"><a href="#FASTER-R-CNN" class="headerlink" title="FASTER R-CNN"></a>FASTER R-CNN</h3><p>由于Fast R-CNN仍然是基于Selective Search方法提取region proposal，而Selective Search方法提取region proposal的计算是无法用GPU进行的，无法借助GPU的高度并行运算能力，所以效率极低。而且选取2000个候选区域，也加重了后面深度学习的处理压力。Faster-RCNN = RPN（区域生成网络）+ Fast-RCNN，用RPN网络代替Fast-RCNN中的Selective Search是Faster-RCNN的核心思想。</p><p>FASTER R-CNN 的算法步骤:</p><ol><li>输入测试图像； </li><li>将整张图片输入CNN，进行特征提取； </li><li>用RPN生成建议窗口（proposals），每张图片生成300个建议窗口； </li><li>把建议窗口映射到CNN的最后一层卷积feature map上； </li><li>通过RoI pooling层使每个RoI生成固定尺寸的feature map； </li><li>利用Softmax Loss（探测分类概率）和Smooth L1 Loss（探测边框回归）对分类概率和边框回归（Bounding Box Regression）联合训练。</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Faster-RCNN Architecture</div></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="frcnn.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Faster-RCNN Architecture2</div></center><h4 id="Anchors"><a href="#Anchors" class="headerlink" title="Anchors"></a>Anchors</h4><p>所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。直接运行作者demo中的generate_anchors.py可以得到以下输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[ -84.  -40.   99.   55.]</span><br><span class="line"> [-176.  -88.  191.  103.]</span><br><span class="line"> [-360. -184.  375.  199.]</span><br><span class="line"> [ -56.  -56.   71.   71.]</span><br><span class="line"> [-120. -120.  135.  135.]</span><br><span class="line"> [-248. -248.  263.  263.]</span><br><span class="line"> [ -36.  -80.   51.   95.]</span><br><span class="line"> [ -80. -168.   95.  183.]</span><br><span class="line"> [-168. -344.  183.  359.]]</span><br></pre></td></tr></table></figure><p>其中每行的4个值 $(x_1, y_1, x_2, y_2)$ 表矩形<code>左上</code>和<code>右下</code>角点坐标。9个矩形共有3种形状，长宽比为大约为 $\frac{width}{height}  \in \{1:1, 1:2, 2:1\}$三种，如图。实际上通过anchors就引入了检测中常用到的多尺度方法。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Anchor</div></center><p>遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Anchor2</div></center><p>假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分positive和negative，所以每个点由256d feature转化为cls=2k scores；而每个anchor都有(x, y, w, h)对应4个偏移量，所以reg=4k coordinates. 补充一点，全部anchors拿去训练太多了，训练程序会在合适的anchors中随机选取128个postive anchors+128个negative anchors进行训练。</p><h4 id="Bounding-box-regression"><a href="#Bounding-box-regression" class="headerlink" title="Bounding box regression"></a>Bounding box regression</h4><p>对于窗口一般使用四维向量 (x,y,w,h) 表示，分别表示窗口的中心点坐标和宽高。对于下图，红色的框A代表原始的positive Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G’，即</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="8.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Bouding Box Regression</div></center><ul><li>给定 anchor $A = (A_x, A_y, A_w, A_h)$ 和 $GT =( G_x, G_y, G_w, G_h)$</li><li>寻找一种变换 F, 使得: <script type="math/tex; mode=display">F(A_x, A_y, A_w, A_h)$ = ( G_x^{\prime}, G_y^{\prime}, G_w^{\prime}, G_h^{\prime})</script><script type="math/tex; mode=display">( G_x^{\prime}, G_y^{\prime}, G_w^{\prime}, G_h^{\prime}) \approx ( G_x, G_y, G_w, G_h)</script></li></ul><p>那么经过何种变换F才能从图10中的anchor A变为G’呢？ 比较简单的思路就是:</p><ol><li>先做平移</li></ol><script type="math/tex; mode=display">G_x^{\prime} = A_w \dot d_x(A) + A_x</script><script type="math/tex; mode=display">G_y^{\prime} = A_h \dot d_y(A) + A_y</script><ol><li>再做缩放</li></ol><script type="math/tex; mode=display">G_w^{\prime} = A_w \dot exp(d_w(A))</script><script type="math/tex; mode=display">G_h^{\prime} = A_h \dot exp(d_h(A))</script><p>需要学习的是 $d_x(A)$, $d_y(A)$, $d_w(A)$, $d_h(A)$ 这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调。</p><p>在 faster RCNN的原文中，positive anchor与ground truth之间的平移量$(t_x, t_y)$与尺度因子$(t_w, t_h)$如下:</p><script type="math/tex; mode=display">t_x = \frac{G_x - x_a}{W_a}</script><script type="math/tex; mode=display">t_y = \frac{G_y - y_a}{h_a}</script><script type="math/tex; mode=display">t_w = log(\frac{G_w}{W_a})</script><script type="math/tex; mode=display">t_h = log(\frac{h}{h_a})</script><p>对于训练bouding box regression网络回归分支，输入是X(cnn feature), label 是上述尺度变换因子，训练的目标是在输入fetaure X的条件下，回归网络分支的输出就是每个Anchor的平移量和变换尺度$(t_x, t_y, t_w, t_h)$ 显然即可用来修正Anchor位置了。</p><h4 id="Faster-RCNN-训练"><a href="#Faster-RCNN-训练" class="headerlink" title="Faster RCNN  训练"></a>Faster RCNN  训练</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="train.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Training</div></center><p>Faster R-CNN的训练，是在已经训练好的model（如VGG_CNN_M_1024，VGG，ZF）的基础上继续进行训练。实际中训练过程分为6个步骤：</p><ol><li>在已经训练好的model上，训练RPN网络</li><li>利用步骤1中训练好的RPN网络</li><li>第一次训练Fast RCNN网络</li><li>第二训练RPN网络</li><li>再次利用步骤4中训练好的RPN网络</li><li>第二次训练Fast RCNN网络</li></ol><h3 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h3><p>YOLO的核心思想就是利用整张图作为网络的输入，直接在输出层回归bounding box的位置和bounding box所属的类别。</p><p>YOLO的算法步骤:</p><ol><li>首先，将一幅图像分成S×S个网格（grid cell），如果某个物体的中心落在这个网格中，则这个网格就负责预测这个物体。<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="yolo.png" width="50%" height="50%"> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">YOLO</div></center></li><li>每个网格要预测B个bounding box，每个bounding box除了要回归自身的位置之外，还要附带预测一个confidence。这个confidence代表了所预测的box中含有object的置信度和这个box预测的有多准这两重信息。</li><li>每个bounding box要预测(x, y, w, h)和confidence共5个值，每个cell还要预测一个类别信息，记为C类。则S×S个网格，每个网格要预测B个bounding box还要预测C个categories。输出就是S×S×(5×B+C)的一个tensor。 </li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="yolo_arch.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">YOLO Architecture</div></center><h3 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h3><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>可以提高模型收敛速度，减少过拟合； </p><h4 id="High-Resolution-Classifier"><a href="#High-Resolution-Classifier" class="headerlink" title="High Resolution Classifier"></a>High Resolution Classifier</h4><p>首先采用448×448分辨率的ImageNet数据finetune使网络适应高分辨率输入；然后将该网络用于目标检测任务finetune； </p><h4 id="Convolutional-With-Anchor-Boxes"><a href="#Convolutional-With-Anchor-Boxes" class="headerlink" title="Convolutional With Anchor Boxes"></a>Convolutional With Anchor Boxes</h4><p>去除了YOLO的全连接层，采用固定框（anchor boxes）来预测bounding boxes；借鉴Faster RCNN的做法，YOLO2也尝试采用先验框（anchor）。在每个grid预先设定一组不同大小和宽高比的边框，来覆盖整个图像的不同位置和多种尺度，这些先验框作为预定义的候选区在神经网络中将检测其中是否存在对象，以及微调边框的位置。</p><h4 id="Dimension-Clusters"><a href="#Dimension-Clusters" class="headerlink" title="Dimension Clusters**"></a>Dimension Clusters**</h4><p>Anchor boxes是通过k-means在训练集中学得的，并且作者定义了新的距离公式，使用k-means获取anchor boxes来预测bounding boxes让模型更容易学习如何预测bounding boxes；聚类算法最重要的是选择如何计算两个边框之间的“距离”，对于常用的欧式距离，大边框会产生更大的误差，但我们关心的是边框的IOU。所以，YOLO2在聚类时采用以下公式来计算两个边框之间的“距离”。 </p><script type="math/tex; mode=display">d(box, centroid) = 1 - IOU(box, centroid)</script><h4 id="Direct-location-prediction"><a href="#Direct-location-prediction" class="headerlink" title="Direct location prediction"></a>Direct location prediction</h4><p>借鉴于Faster RCNN的先验框方法，在训练的早期阶段，其位置预测容易不稳定。其位置预测公式为：</p><script type="math/tex; mode=display">x = (t_x * w_a) + x_a</script><script type="math/tex; mode=display">y = (t_y * h_a) + y_a</script><p>其中,(x,y)是预测边框的中心，$x_a$, $y_a$是先验框（anchor）的中心点坐标，(w_a,h_a)是先验框（anchor）的宽和高。$t_x$,$t_y$是要学习的尺度变换因子。<br>由于$t_x$,$t_y$没有约束，因此预测边框的中心可能出现在任何位置，训练早期阶段不容易稳定。YOLO调整了预测公式，将预测边框的中心约束在特定gird网格内。</p><script type="math/tex; mode=display">b_x = \sigma(t_x) + c_x</script><script type="math/tex; mode=display">b_y = \sigma(t_y) + c_y</script><script type="math/tex; mode=display">b_w  = p_w e^{t_w}</script><script type="math/tex; mode=display">b_h  = p_h e^{t_h}</script><script type="math/tex; mode=display">Pr(object)\dot IOU(b,object) = \sigma(t_o)</script><p>其中，$b_X,b_y,b_w,b_h$是预测边框的中心和宽高,$Pr(object)\dot IOU(b,object) = \sigma(t_o)$ 是预测边框的置信度，YOLO1是直接预测置信度的值，这里对预测参数$t_o$进行$\sigma$变换后作为置信度的值。$c_x$,$c_y$是当前网格左上角到图像左上角的距离，要先将网格大小归一化，即令一个网格的宽=1，高=1。$p_w,p_h$是先验框的宽和高。</p><p><strong>see explanantion in <a href="https://www.jianshu.com/p/86b8208f634f" target="_blank" rel="noopener">https://www.jianshu.com/p/86b8208f634f</a></strong></p><h4 id="Fine-Grained-Features"><a href="#Fine-Grained-Features" class="headerlink" title="Fine-Grained Features"></a>Fine-Grained Features</h4><p>YOLOv2通过添加一个pass through layer，将前一个卷积块的特征图的信息融合起来；对象检测面临的一个问题是图像中对象会有大有小，输入图像经过多层网络提取特征，最后输出的特征图中（比如YOLO2中输入416<em>416经过卷积网络下采样最后输出是13</em>13），较小的对象可能特征已经不明显甚至被忽略掉了。为了更好的检测出一些比较小的对象，最后输出的特征图需要保留一些更细节的信息。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="pass.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">pass through</div></center><h4 id="Multi-Scale-Training"><a href="#Multi-Scale-Training" class="headerlink" title="Multi-Scale Training"></a>Multi-Scale Training</h4><p>YOLOv2网络只用到了卷积层和池化层，因此可以进行动态调整输入图像的尺寸，作者希望YOLOv2对于不同尺寸图像的检测都有较好的鲁棒性，因此做了针对性训练。这种策略让YOLOv2网络不得不学着对不同尺寸的图像输入都要预测得很好，这意味着同一个网络可以胜任不同分辨率的检测任务，在网络训练好之后，在使用时只需要根据需求，修改网络输入图像尺寸（width和height的值）即可。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="box.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">YOLO v2 bounding box regression</div></center><h3 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="yolov3.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">YOLO v3 Architecture</div></center><ul><li><strong>DBL</strong>: 如图1左下角所示，也就是代码中的Darknetconv2d_BN_Leaky，是yolo_v3的基本组件。就是卷积+BN+Leaky relu。对于v3来说，BN和leaky relu已经是和卷积层不可分离的部分了(最后一层卷积除外)，共同构成了最小组件。</li><li><strong>resn</strong>：n代表数字，有res1，res2, … ,res8等等，表示这个res_block里含有多少个res_unit。这是yolo_v3的大组件，yolo_v3开始借鉴了ResNet的残差结构，使用这种结构可以让网络结构更深(从v2的darknet-19上升到v3的darknet-53，前者没有残差结构)。对于res_block的解释，可以在图1的右下角直观看到，其基本组件也是DBL。</li><li><strong>concat</strong>：张量拼接。将darknet中间层和后面的某一层的上采样进行拼接。拼接的操作和残差层add的操作是不一样的，拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。</li></ul><h4 id="多尺度特征图"><a href="#多尺度特征图" class="headerlink" title="多尺度特征图"></a>多尺度特征图</h4><ul><li>YOLOv3输出了3个不同的尺度特征图，在它们上面进行融合预测物体框。这借鉴了特征金字塔网络（FPN, feature pyramid networks），采用多尺度来对不同大小的目标进行检测，越精细的grid cell就可以检测出越精细的物体。</li><li>生成3个尺度特征图：<ul><li>尺度y1：尺度1的feature map做卷积后直接得到box信息</li><li>尺度y2：对尺度1输出的卷积进行上采样，然后和尺度2的feature map相加，再经过卷积输出box信息，整个feature map大小相对于尺度1扩大了两倍</li><li>尺度y3：同理y2</li></ul></li><li>3个尺度特征图深度都是255，边长的规律是13:26:52。</li><li>采用了多尺度的特征融合，所以边界框的数量要比之前多很多.以输入图像为416x416为例的(13x13+26x26+52x52)x3=10647，要比v2的13x13x5更多。</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="10.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">YOLO v3 Architecture</div></center><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://blog.csdn.net/v1_vivian/article/details/73275259" target="_blank" rel="noopener">https://blog.csdn.net/v1_vivian/article/details/73275259</a></li><li><a href="https://blog.csdn.net/zhang_can/article/details/79490735" target="_blank" rel="noopener">https://blog.csdn.net/zhang_can/article/details/79490735</a></li><li><a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31426458</a></li><li><a href="https://zhuanlan.zhihu.com/p/47575929" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47575929</a></li><li><a href="https://blog.csdn.net/leviopku/article/details/82660381" target="_blank" rel="noopener">https://blog.csdn.net/leviopku/article/details/82660381</a></li><li><a href="https://www.jianshu.com/p/86b8208f634f" target="_blank" rel="noopener">https://www.jianshu.com/p/86b8208f634f</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;基于REGION-PROPOSAL的方法&quot;&gt;&lt;a href=&quot;#基于REGION-PROPOSAL的方法&quot; class=&quot;headerlink&quot; title=&quot;基于REGION PROPOSAL的方法&quot;&gt;&lt;/a&gt;基于REGION PROPOSAL的方法&lt;/h2&gt;&lt;
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>AI - Search II</title>
    <link href="https://zhangruochi.com/AI-Search-II/2020/03/05/"/>
    <id>https://zhangruochi.com/AI-Search-II/2020/03/05/</id>
    <published>2020-03-05T16:44:39.000Z</published>
    <updated>2020-03-06T01:05:44.631Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h2><p>Definition: search problem</p><ul><li>$S_{start}$: staring state</li><li>Actions(s): possible actions</li><li>Cons(s,a): action cost</li><li>Succ(s,a): successor</li><li>IsEnd(s): reached end state?</li></ul><p>Objective: find the minimum cost path from $S_start$ to an $s$ that satisfying IsEnd(s).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransportationProblem</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, N)</span>:</span></span><br><span class="line">        <span class="comment"># N = number of blocks</span></span><br><span class="line">        self.N = N</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">startState</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isEnd</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> state == self.N</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">succAndCost</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="comment"># return list of (action, newState, cost) triples</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">if</span> state+<span class="number">1</span>&lt;=self.N:</span><br><span class="line">            result.append((<span class="string">'walk'</span>, state+<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">if</span> state*<span class="number">2</span>&lt;=self.N:</span><br><span class="line">            result.append((<span class="string">'tram'</span>, state*<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><h2 id="Learning-problem"><a href="#Learning-problem" class="headerlink" title="Learning problem"></a>Learning problem</h2><p>Now suppose we don’t know what the costs are, but we observe someone getting from 1 to n via some sequence of walking and tram-taking. Can we figure out what the costs are? This is the goal of learning.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="learning.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Learning Problem</div></center><p>Let’s cast the problem as predicting an output y given an input x. Here, the input x is the search problem (visualized as a search tree) without the costs provided. The output y is the desired solution path. The question is what the costs should be set to so that y is actually the minimum cost path of the resulting search problem.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="algo.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Structured Perceptron</div></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.setrecursionlimit(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### Model (search problem)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransportationProblem</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, N, weights)</span>:</span></span><br><span class="line">        <span class="comment"># N = number of blocks</span></span><br><span class="line">        <span class="comment"># weights = weights of different actions</span></span><br><span class="line">        self.N = N</span><br><span class="line">        self.weights = weights</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">startState</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isEnd</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> state == self.N</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">succAndCost</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="comment"># return list of (action, newState, cost) triples</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">if</span> state+<span class="number">1</span>&lt;=self.N:</span><br><span class="line">            result.append((<span class="string">'walk'</span>, state+<span class="number">1</span>, self.weights[<span class="string">'walk'</span>]))</span><br><span class="line">        <span class="keyword">if</span> state*<span class="number">2</span>&lt;=self.N:</span><br><span class="line">            result.append((<span class="string">'tram'</span>, state*<span class="number">2</span>, self.weights[<span class="string">'tram'</span>]))</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dynamicProgramming</span><span class="params">(problem)</span>:</span></span><br><span class="line">    cache = &#123;&#125; <span class="comment"># state -&gt; futureCost(state), action, newState, cost</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">futureCost</span><span class="params">(state)</span>:</span></span><br><span class="line">        <span class="comment"># Base case</span></span><br><span class="line">        <span class="keyword">if</span> problem.isEnd(state):</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">in</span> cache: <span class="comment"># Exponential savings</span></span><br><span class="line">            <span class="keyword">return</span> cache[state][<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># Actually doing work</span></span><br><span class="line">        result = min((cost+futureCost(newState), action, newState, cost) \</span><br><span class="line">                <span class="keyword">for</span> action, newState, cost <span class="keyword">in</span> problem.succAndCost(state))</span><br><span class="line">        cache[state] = result</span><br><span class="line">        <span class="keyword">return</span> result[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    state = problem.startState()</span><br><span class="line">    totalCost = futureCost(state)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Recover history</span></span><br><span class="line">    history = []</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> problem.isEnd(state):</span><br><span class="line">        _, action, newState, cost = cache[state]</span><br><span class="line">        history.append((action, newState, cost))</span><br><span class="line">        state = newState</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (futureCost(problem.startState()), history)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### Main</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(N, weights)</span>:</span></span><br><span class="line">    <span class="comment"># f(x)</span></span><br><span class="line">    <span class="comment"># Input (x): N (number of blocks)</span></span><br><span class="line">    <span class="comment"># Output (y): path (sequence of actions)</span></span><br><span class="line">    problem = TransportationProblem(N, weights)</span><br><span class="line">    totalCost, history = dynamicProgramming(problem)</span><br><span class="line">    <span class="keyword">return</span> [action <span class="keyword">for</span> action, newState, cost <span class="keyword">in</span> history]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateExamples</span><span class="params">()</span>:</span></span><br><span class="line">    trueWeights = &#123;<span class="string">'walk'</span>: <span class="number">1</span>, <span class="string">'tram'</span>: <span class="number">5</span>&#125;</span><br><span class="line">    <span class="keyword">return</span> [(N, predict(N, trueWeights)) <span class="keyword">for</span> N <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">30</span>)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">structuredPerceptron</span><span class="params">(examples)</span>:</span></span><br><span class="line">    weights = &#123;<span class="string">'walk'</span>: <span class="number">0</span>, <span class="string">'tram'</span>: <span class="number">0</span>&#125;</span><br><span class="line">    iteration = <span class="number">100</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(iteration):</span><br><span class="line">        numMistakes = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> N, trueActions <span class="keyword">in</span> examples:</span><br><span class="line">            <span class="comment"># Make a prediction</span></span><br><span class="line">            predActions = predict(N, weights)</span><br><span class="line">            <span class="keyword">if</span> predActions != trueActions:</span><br><span class="line">                numMistakes += <span class="number">1</span></span><br><span class="line">            <span class="comment"># Update weights</span></span><br><span class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> trueActions:</span><br><span class="line">                weights[action] -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> predActions:</span><br><span class="line">                weights[action] += <span class="number">1</span></span><br><span class="line">        print(<span class="string">'Iteration &#123;&#125;, numMistakes = &#123;&#125;, weights = &#123;&#125;'</span>.format(t, numMistakes, weights))</span><br><span class="line">        <span class="keyword">if</span> numMistakes == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">examples = generateExamples()</span><br><span class="line">print(<span class="string">'Training dataset:'</span>)</span><br><span class="line"><span class="keyword">for</span> example <span class="keyword">in</span> examples:</span><br><span class="line">    print(<span class="string">'  '</span>, example)</span><br><span class="line">structuredPerceptron(examples)</span><br></pre></td></tr></table></figure><h2 id="A-Star-Search"><a href="#A-Star-Search" class="headerlink" title="A Star Search"></a>A Star Search</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="aStar.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Exploring states</div></center><p><strong>UCS</strong>: explore states in order of PastCost(s)<br><strong>A start</strong>: explore in order of PastCost(s) + h(s)</p><pre><code>- A heuristic h(s) is any estimate of FutureCost(s).</code></pre><ul><li>First, some terminology: PastCost(s) is the minimum cost from the start state to s, and FutureCost(s) is the minimum cost from s to an end state. Without loss of generality, we can just assume we have one end state. (If we have multiple ones, create a new official goal state which is the successor of all the original end states.)</li><li>Recall that UCS explores states in order of PastCost(s). It’d be nice if we could explore states in order of PastCost(s) + FutureCost(s), which would definitely take the end state into account, but computing FutureCost(s) would be as expensive as solving the original problem.</li><li>A star relies on a heuristic h(s), which is an estimate of FutureCost(s). For A star to work, h(s) must satisfy some conditions, but for now, just think of h(s) as an approximation. We will soon show that A star will explore states in order of PastCost(s) + h(s). This is nice, because now states which are estimated (by h(s)) to be really far away from the end state will be explored later, even if their PastCost(s) is small.</li></ul><h3 id="F-G-H"><a href="#F-G-H" class="headerlink" title="F = G + H"></a>F = G + H</h3><p>One important aspect of A star is f = g + h. The f, g, and h variables are in our Node class and get calculated every time we create a new node. Quickly I’ll go over what these variables mean.</p><ul><li>F is the total cost of the node.</li><li>G is the distance between the current node and the start node.</li><li>H is the heuristic — estimated distance from the current node to the end node.</li></ul><h3 id="Consistent-heuristics"><a href="#Consistent-heuristics" class="headerlink" title="Consistent heuristics"></a>Consistent heuristics</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="consistent.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Consistent heuristics</div></center><ul><li>We need h(s) to be consistent, which means two things. First, the modified edge costs are non-negative (this is the main property). This is important for UCS to find the minimum cost path (remember that UCS only works when all the edge costs are non-negative).</li><li>Second, h(send) = 0, which is just saying: be reasonable. The minimum cost from the end state to the end state is trivially 0, so just use 0.</li></ul><h3 id="Correctness-of-A"><a href="#Correctness-of-A" class="headerlink" title="Correctness of A*"></a>Correctness of A*</h3><p><strong>If h is consistent, A star returns the minimum cost path.</strong></p><h3 id="Admissibility"><a href="#Admissibility" class="headerlink" title="Admissibility"></a>Admissibility</h3><p>A heuristic h(s) is admissible if </p><script type="math/tex; mode=display">h(s) \leq FutureCost(s)</script><p>Theorem: <strong>consistency implies admissibility</strong></p><blockquote><p>If a heuristic h(s) is consistent, then h(s) is admissible.</p></blockquote><h3 id="Relaxation"><a href="#Relaxation" class="headerlink" title="Relaxation"></a>Relaxation</h3><p>With an arbitrary configuration of walls, we can’t compute FutureCost(s) except by doing search. However, if we just relaxed the original problem by removing the walls, then we can compute FutureCost(s) in closed form: it’s just the Manhattan distance between $s$ and $s_{end}$. Specifically, ManhattanDistance((r1, c1), (r2, c2)) = |r1 − r2| + |c1 − c2|.</p><ul><li>More formally, we define a relaxed search problem as one where the relaxed edge costs are no larger than the original edge costs.</li><li>The relaxed heuristic is simply the future cost of the relaxed search problem, which by design should be efficiently computable.</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="algo2.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">A star Search Algorithm</div></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq, collections, re, sys, time, os, random</span><br><span class="line"></span><br><span class="line"><span class="comment"># Data structure for supporting uniform cost search.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PriorityQueue</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.DONE = <span class="number">-100000</span></span><br><span class="line">        self.heap = []</span><br><span class="line">        self.priorities = &#123;&#125;  <span class="comment"># Map from state to priority</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, node)</span>:</span></span><br><span class="line">        state, newPriority = node.position, node.f</span><br><span class="line">        oldPriority = self.priorities.get(state)</span><br><span class="line">        <span class="keyword">if</span> oldPriority == <span class="keyword">None</span> <span class="keyword">or</span> newPriority &lt; oldPriority:</span><br><span class="line">            self.priorities[state] = newPriority</span><br><span class="line">            heapq.heappush(self.heap, node)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeMin</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">while</span> len(self.heap) &gt; <span class="number">0</span>:</span><br><span class="line">            node = heapq.heappop(self.heap)</span><br><span class="line">            priority, state = node.f, node.position</span><br><span class="line">            <span class="keyword">if</span> self.priorities[state] == self.DONE: </span><br><span class="line">                <span class="keyword">continue</span>  <span class="comment"># Outdated priority, skip</span></span><br><span class="line">            self.priorities[state] = self.DONE</span><br><span class="line">            <span class="keyword">return</span> node</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""A node class for A* Pathfinding"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, parent=None, position=None)</span>:</span></span><br><span class="line">        self.parent = parent</span><br><span class="line">        self.position = position</span><br><span class="line"></span><br><span class="line">        self.g = <span class="number">0</span></span><br><span class="line">        self.h = <span class="number">0</span></span><br><span class="line">        self.f = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__eq__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.position == other.position</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.f &lt; other.f</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Node (position: &#123;&#125;, g: &#123;&#125;, h: &#123;&#125;, f: &#123;&#125;)"</span>.format(self.position, self.g, self.h, self.f)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">astar</span><span class="params">(maze, start, end)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a list of tuples as a path from the given start to the given end in the given maze"""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create start and end node</span></span><br><span class="line">    start_node = Node(<span class="keyword">None</span>, start)</span><br><span class="line">    start_node.g = start_node.h = start_node.f = <span class="number">0</span></span><br><span class="line">    end_node = Node(<span class="keyword">None</span>, end)</span><br><span class="line">    end_node.g = end_node.h = end_node.f = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize both open and closed list</span></span><br><span class="line">    frontier = PriorityQueue()</span><br><span class="line">    frontier.update(start_node)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop until you find the end</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get the current node</span></span><br><span class="line">        current_node = frontier.removeMin()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Found the goal</span></span><br><span class="line">        <span class="keyword">if</span> current_node == end_node:</span><br><span class="line">            path = []</span><br><span class="line">            current = current_node</span><br><span class="line">            <span class="keyword">while</span> current <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                path.append(current.position)</span><br><span class="line">                current = current.parent</span><br><span class="line">            <span class="keyword">return</span> path[::<span class="number">-1</span>] <span class="comment"># Return reversed path</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Generate children</span></span><br><span class="line">        children = []</span><br><span class="line">        <span class="keyword">for</span> new_position <span class="keyword">in</span> [(<span class="number">0</span>, <span class="number">-1</span>), (<span class="number">0</span>, <span class="number">1</span>), (<span class="number">-1</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">0</span>), (<span class="number">-1</span>, <span class="number">-1</span>), (<span class="number">-1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">-1</span>), (<span class="number">1</span>, <span class="number">1</span>)]: <span class="comment"># Adjacent squares</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Get node position</span></span><br><span class="line">            node_position = (current_node.position[<span class="number">0</span>] + new_position[<span class="number">0</span>], current_node.position[<span class="number">1</span>] + new_position[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Make sure within range</span></span><br><span class="line">            <span class="keyword">if</span> node_position[<span class="number">0</span>] &gt; (len(maze) - <span class="number">1</span>) <span class="keyword">or</span> node_position[<span class="number">0</span>] &lt; <span class="number">0</span> <span class="keyword">or</span> node_position[<span class="number">1</span>] &gt; (len(maze[len(maze)<span class="number">-1</span>]) <span class="number">-1</span>) <span class="keyword">or</span> node_position[<span class="number">1</span>] &lt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Make sure walkable terrain</span></span><br><span class="line">            <span class="keyword">if</span> maze[node_position[<span class="number">0</span>]][node_position[<span class="number">1</span>]] != <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Create new node</span></span><br><span class="line">            new_node = Node(current_node, node_position)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Create the f, g, and h values</span></span><br><span class="line">            new_node.g = current_node.g + <span class="number">1</span></span><br><span class="line">            new_node.h = ((new_node.position[<span class="number">0</span>] - end_node.position[<span class="number">0</span>]) ** <span class="number">2</span>) + ((new_node.position[<span class="number">1</span>] - end_node.position[<span class="number">1</span>]) ** <span class="number">2</span>)</span><br><span class="line">            new_node.f = new_node.g + new_node.h</span><br><span class="line"></span><br><span class="line">            frontier.update(new_node)</span><br><span class="line">         </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    maze = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">    start = (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    end = (<span class="number">7</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">    path = astar(maze, start, end)</span><br><span class="line">    print(path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()  </span><br><span class="line"></span><br><span class="line">    <span class="comment"># [(0, 0), (1, 1), (2, 2), (3, 3), (4, 3), (5, 4), (6, 5), (7, 6)]</span></span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://medium.com/@nicholas.w.swift/easy-a-star-pathfinding-7e6689c7f7b2" target="_blank" rel="noopener">https://medium.com/@nicholas.w.swift/easy-a-star-pathfinding-7e6689c7f7b2</a></li><li>CS221 course note and ppt <a href="https://stanford-cs221.github.io/autumn2019/" target="_blank" rel="noopener">https://stanford-cs221.github.io/autumn2019/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Review&quot;&gt;&lt;a href=&quot;#Review&quot; class=&quot;headerlink&quot; title=&quot;Review&quot;&gt;&lt;/a&gt;Review&lt;/h2&gt;&lt;p&gt;Definition: search problem&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S_{start}$: s
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
    
  </entry>
  
  <entry>
    <title>Programming Language: Summary</title>
    <link href="https://zhangruochi.com/Programming-Language-Summary/2020/02/27/"/>
    <id>https://zhangruochi.com/Programming-Language-Summary/2020/02/27/</id>
    <published>2020-02-27T21:51:07.000Z</published>
    <updated>2020-02-27T22:37:59.218Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ML-Expressions-and-Variable-Bindings"><a href="#ML-Expressions-and-Variable-Bindings" class="headerlink" title="ML Expressions and Variable Bindings"></a>ML Expressions and Variable Bindings</h2><p>An ML program is a sequence of <strong>bindings</strong>. Each binding gets type-checked and then (assuming it type-checks) evaluated. What type (if any) a binding has depends on a <strong>static environment</strong>, which is roughly the types of the preceding bindings in the file. How a binding is evaluated depends on a <strong>dynamic</strong> environment, which is roughly the values of the preceding bindings in the file. When we just say environment, we usually mean dynamic environment. Sometimes context is used as a synonym for static environment.</p><p>There are several kinds of bindings, but for now let’s consider only a variable binding, which in ML has this syntax :</p><center> val x = e; </center><p>Here, val is a keyword, x can be any variable, and e can be any expression. We now know a variable binding’s <strong>syntax</strong> (how to write it), but we still need to know its <strong>semantics</strong> (how it type-checks and evaluates). Mostly this depends on the expression e. To type-check a variable binding, we use the current static environment (the types of preceding bindings) to type-check e (which will depend on what kind of expression it is) and produce a <code>new static environment</code> that is the current static environment except with x having type t where t is the type of e.  Evaluation is analogous: To evaluate a variable binding, we use the “current dynamic environment” (the values of preceding bindings) to evaluate e (which will depend on what kind of expression it is) and produce a <code>new dynamic environment</code> that is the current environment except with x having the value v where v is the result of evaluating e.</p><p>Whenever you learn a new construct in a programming language, you should ask these three questions: </p><ul><li>What is the syntax? </li><li>What are the type-checking rules? </li><li>What are the evaluation rules?</li></ul><p>for example:</p><h3 id="Addition"><a href="#Addition" class="headerlink" title="Addition:"></a>Addition:</h3><ul><li>Syntax: e1+e2 where e1 and e2 are expressions</li><li>Type-checking: type int but only if e1 and e2 have type int in the same static environment, else does not type-check</li><li>Evaluation: evaluate e1 to v1 and e2 to v2 in the same dynamic environment and then produce the sum of v1 and v2</li></ul><h2 id="Function-Bindings"><a href="#Function-Bindings" class="headerlink" title="Function Bindings"></a>Function Bindings</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fun pow (x:int, y:int) = (* correct only for y &gt;= 0 *)</span><br><span class="line">    if y=0</span><br><span class="line">    then 1</span><br><span class="line">    else x * pow(x,y-1)</span><br></pre></td></tr></table></figure><h3 id="Syntax"><a href="#Syntax" class="headerlink" title="Syntax:"></a>Syntax:</h3><center> fun x0 (x1 : t1, ..., xn : tn) = e</center><h3 id="Type-checking"><a href="#Type-checking" class="headerlink" title="Type-checking:"></a>Type-checking:</h3><p>To type-check a function binding, we type-check the body e in a static environment that (in addition to alltheearlierbindings)maps x1 to t1,…, xn to tn and x0 to t1 <em> … </em> tn -&gt; t. Because x0 is in the environment, we can make recursive function calls, i.e., a function definition can use itself. The syntax of a function type is <code>argument types -&gt; result type</code> where the argument types are separated by * (which just happens to be the same character used in expressions for multiplication). For the function binding to type-check, the body e must have the type t, i.e., the result type of x0. That makes sense given the evaluation rules below because the result of a function call is the result of evaluating e.</p><p>But what, exactly, is t - we never wrote it down? It can be any type, and it is up to the type-checker (part of the language implementation) to figure out what t should be such that using it for the result type of x0 makes, everything work out.</p><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation:"></a>Evaluation:</h3><p>The evaluation rule for a function binding is trivial: A function is a value — we simply add x0 to the environment as a function that can be called later. As expected for recursion, x0 is in the dynamic environment in the function body and for subsequent bindings (but not, unlike in say Java, for preceding bindings, so the order you define functions is very important).</p><h2 id="Pairs-and-Other-Tuples"><a href="#Pairs-and-Other-Tuples" class="headerlink" title="Pairs and Other Tuples"></a>Pairs and Other Tuples</h2><p>Tuples: <strong>fixed</strong> number of pieces that may have <code>different types</code></p><h3 id="Build"><a href="#Build" class="headerlink" title="Build:"></a>Build:</h3><ul><li>Syntax: <code>(e1,e2)</code></li><li>Evaluation: Evaluate e1 to v1 and e2 to v2; result is (v1,v2)<ul><li>A pair of values is a value</li></ul></li><li>Type-checking: If e1 has type ta and e2 has type tb, then the<br>pair expression has type ta * tb <ul><li>A new kind of type</li></ul></li></ul><h3 id="Access"><a href="#Access" class="headerlink" title="Access:"></a>Access:</h3><ul><li>Syntax: <code>#1 e</code> and <code>#2 e</code></li><li>Evaluation: Evaluate e to a pair of values and return first or<br>second piece<ul><li>Example: If e is a variable x, then look up x in environment</li></ul></li><li>Type-checking: If e has type ta * tb, then #1 e has type ta and #2 e has type tb</li></ul><h2 id="Lists"><a href="#Lists" class="headerlink" title="Lists"></a>Lists</h2><p>Lists can have <strong>any number</strong> of elements,but all list elements have the <strong>same type</strong></p><h3 id="Build-1"><a href="#Build-1" class="headerlink" title="Build"></a>Build</h3><ul><li>The empty list is a value: []</li><li>In general, a list of values is a value; elements separated by commas:<center> [v1,v2,...,vn]</center></li><li>If e1 evaluates to v and e2 evaluates to a list [v1,…,vn], then e1::e2 evaluates to [v,…,vn] <center> e1::e2</center></li></ul><h3 id="Access-1"><a href="#Access-1" class="headerlink" title="Access"></a>Access</h3><p>Until we learn pattern-matching, we will use three standard-library functions</p><ul><li>null e evaluates to true if and only if e evaluates to []</li><li>If e evaluates to [v1,v2,…,vn] then <code>hd</code> e evaluates to v1 (raise exception if e evaluates to [])</li><li>If e evaluates to [v1,v2,…,vn] then <code>tl</code> e evaluates to [v2,…,vn]  (raise exception if e evaluates to [])</li></ul><h2 id="Let-expressions"><a href="#Let-expressions" class="headerlink" title="Let-expressions"></a>Let-expressions</h2><p>Let-expressions are an absolutely crucial feature that allows for local variables in a very simple, general, and flexible way. Let-expressions are crucial for style and for efficiency. A let-expression lets us have <strong>local variables</strong>. In fact, it lets us have local bindings of any sort, including function bindings. Because it is a kind of expression, it can appear anywhere an expression can.</p><ul><li>Syntax: let b1 b2… bn in e end<ul><li>Each bi is any binding and e is any expression</li></ul></li><li>Type-checking: Type-check each bi and e in a static environment that includes the previous bindings. Type of whole let-expression is the type of e.</li><li>Evaluation: Evaluate each bi and e in a dynamic environment that includes the previous bindings.<br>Result of whole let-expression is result of evaluating e.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">fun good_max (xs : int list) = if null xs</span><br><span class="line">    then 0 (* note: bad style; see below *)</span><br><span class="line">    else if null (tl xs)</span><br><span class="line">    then hd xs</span><br><span class="line">    else</span><br><span class="line">(* for style, could also use a let-binding for hd xs *) </span><br><span class="line">    let val tl_ans = good_max(tl xs)</span><br><span class="line">    in</span><br><span class="line">        if hd xs &gt; tl_ans then hd xs</span><br><span class="line">        else tl_ans</span><br><span class="line">    end</span><br></pre></td></tr></table></figure><h2 id="Options"><a href="#Options" class="headerlink" title="Options"></a>Options</h2><p>The previous example does not properly handle the empty list — it returns 0. This is bad style because 0 is really not the maximum value of 0 numbers. There is no good answer, but we should deal with this case reasonably. One possibility is to raise an exception; you can learn about ML exceptions on your own if you are interested before we discuss them later in the course. Instead, let’s change the return type to either return the maximum number or indicate the input list was empty so there is no maximum. Given the constructs we have, we could code this up by return an int list, using [] if the input was the empty list and a list with one integer (the maximum) if the input list was not empty.</p><p>The ML library has <code>options</code> which are a precise description: an option value has either 0 or 1 thing: <code>NONE</code> is an option value carrying nothing whereas SOME e evaluates e to a value v and becomes the option carrying the one value v. The type of NONE is ‘a option and the type of SOME e is t option if e has type t.</p><h3 id="Building"><a href="#Building" class="headerlink" title="Building:"></a>Building:</h3><ul><li><code>NONE</code> has type ‘a option (much like [] has type ‘a list)</li><li><code>SOME</code> e has type t option if e has type t (much like e::[])</li></ul><h3 id="Access-2"><a href="#Access-2" class="headerlink" title="Access:"></a>Access:</h3><ul><li><code>isSome</code> has type ‘a option -&gt; bool</li><li><code>valOf</code> has type ‘a option -&gt; ‘a (exception if given NONE)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fun better_max (xs : int list) = if null xs</span><br><span class="line">    then NONE</span><br><span class="line">    else</span><br><span class="line">        let val tl_ans = better_max(tl xs)</span><br><span class="line">        in </span><br><span class="line">            if isSome tl_ans andalso valOf tl_ans &gt; hd xs</span><br><span class="line">            then tl_ans</span><br><span class="line">            else SOME (hd xs)</span><br><span class="line">        end</span><br></pre></td></tr></table></figure><h2 id="Boolean-operations"><a href="#Boolean-operations" class="headerlink" title="Boolean operations"></a>Boolean operations</h2><ul><li>e1 <code>andalso</code> e2</li><li>e1 <code>orelse</code> e2</li><li><code>not</code> e1</li></ul><h2 id="Comparisons"><a href="#Comparisons" class="headerlink" title="Comparisons"></a>Comparisons</h2><p>= &lt;&gt; &gt; &lt; &gt;= &lt;=</p><ul><li><blockquote><p>&lt; &gt;= &lt;= can be used with real, but not 1 int and 1 real</p></blockquote></li><li>= &lt;&gt; can be used with any equality type but not with real</li></ul><h2 id="Lack-of-Mutation-and-Benefits-Thereof"><a href="#Lack-of-Mutation-and-Benefits-Thereof" class="headerlink" title="Lack of Mutation and Benefits Thereof"></a>Lack of Mutation and Benefits Thereof</h2><p>In ML, there is no way to change the contents of a binding, a tuple, or a list. If x maps to some value like the list of pairs [(3,4),(7,9)] in some environment, then x will forever map to that list in that environment. There is no assignment statement that changes x to map to a different list. (You can introduce a new binding that <strong>shadows</strong> x, but that will not affect any code that looks up the <code>original</code> x in an environment.)</p><p>For a final example, the following Java is the key idea behind an actual security hole in an important (and subsequently fixed) Java library. Suppose we are maintaining permissions for who is allowed to access something like a file on the disk. It is fine to let everyone see who has permission, but clearly only those that do have permission can actually use the resource. Consider this wrong code (some parts omitted if not relevant):</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProtectedResource</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> Resource theResource = ...; </span><br><span class="line"><span class="keyword">private</span> String[] allowedUsers = ...; </span><br><span class="line"><span class="keyword">public</span> String[] getAllowedUsers() &#123;</span><br><span class="line">      <span class="keyword">return</span> allowedUsers;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">currentUser</span><span class="params">()</span> </span>&#123; ... &#125; </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">useTheResource</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; allowedUsers.length; i++) &#123; </span><br><span class="line">        <span class="keyword">if</span>(currentUser().equals(allowedUsers[i])) &#123;</span><br><span class="line">    ... <span class="comment">// access allowed: use it return;</span></span><br><span class="line">        &#125; </span><br><span class="line">    &#125;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IllegalAccessException();</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Can you find the problem? Here it is: </p><p><code>getAllowedUsers</code> returns an alias to the allowedUsers array, so any user can gain access by doing <code>getAllowedUsers()[0] = currentUser().</code></p><h2 id="The-Pieces-of-a-Programming-Language"><a href="#The-Pieces-of-a-Programming-Language" class="headerlink" title="The Pieces of a Programming Language"></a>The Pieces of a Programming Language</h2><p>Now that we have learned enough ML to write some simple functions and programs with it, we can list the essential “pieces” necessary for defining and learning any programming language:</p><ul><li><strong>Syntax</strong>: How do you write the various parts of the language?</li><li><strong>Semantics</strong>: What do the various language features mean? For example, how are expressions evaluated?</li><li><strong>Idioms</strong>: What are the common approaches to using the language features to express computations?</li><li><strong>Libraries</strong>: What has already been written for you? How do you do things you could not do without library support (like access files)?</li><li><strong>Tools</strong>: What is available for manipulating programs in the language (compilers, read-eval-print loops, debuggers, …)</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Course note from <a href="https://www.coursera.org/learn/programming-languages/home/week/2" target="_blank" rel="noopener">Programming Language</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;ML-Expressions-and-Variable-Bindings&quot;&gt;&lt;a href=&quot;#ML-Expressions-and-Variable-Bindings&quot; class=&quot;headerlink&quot; title=&quot;ML Expressions and V
      
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
    
  </entry>
  
  <entry>
    <title>TensorBoard</title>
    <link href="https://zhangruochi.com/TensorBoard/2020/02/22/"/>
    <id>https://zhangruochi.com/TensorBoard/2020/02/22/</id>
    <published>2020-02-22T18:34:48.000Z</published>
    <updated>2020-02-24T21:34:27.301Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TensorBoard-with-Fashion-MNIST"><a href="#TensorBoard-with-Fashion-MNIST" class="headerlink" title="TensorBoard with Fashion MNIST"></a>TensorBoard with Fashion MNIST</h1><p>In this week’s exercise you will train a convolutional neural network to classify images of the Fashion MNIST dataset and you will use TensorBoard to explore how it’s confusion matrix evolves over time.</p><h2 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load the TensorBoard notebook extension.</span></span><br><span class="line">%load_ext tensorboard</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sklearn.metrics</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> getcwd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">"TensorFlow version: "</span>, tf.__version__)</span><br></pre></td></tr></table></figure><pre><code>TensorFlow version:  2.0.0</code></pre><h2 id="Load-the-Fashion-MNIST-Dataset"><a href="#Load-the-Fashion-MNIST-Dataset" class="headerlink" title="Load the Fashion-MNIST Dataset"></a>Load the Fashion-MNIST Dataset</h2><p>We are going to use a CNN to classify images in the the <a href="https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/" target="_blank" rel="noopener">Fashion-MNIST</a> dataset. This dataset consist of 70,000 grayscale images of fashion products from 10 categories, with 7,000 images per category. The images have a size of $28\times28$ pixels.</p><p>First, we load the data. Even though these are really images, we will load them as NumPy arrays and not as binary image objects. The data is already divided into training and testing sets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load the data.</span></span><br><span class="line">train_images = np.load(<span class="string">f"<span class="subst">&#123;getcwd()&#125;</span>/../tmp2/train_images.npy"</span>)</span><br><span class="line">train_labels = np.load(<span class="string">f"<span class="subst">&#123;getcwd()&#125;</span>/../tmp2/train_labels.npy"</span>)</span><br><span class="line"></span><br><span class="line">test_images = np.load(<span class="string">f"<span class="subst">&#123;getcwd()&#125;</span>/../tmp2/test_images.npy"</span>)</span><br><span class="line">test_labels = np.load(<span class="string">f"<span class="subst">&#123;getcwd()&#125;</span>/../tmp2/test_labels.npy"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The labels of the images are integers representing classes.</span></span><br><span class="line"><span class="comment"># Here we set the Names of the integer classes, i.e., 0 -&gt; T-short/top, 1 -&gt; Trouser, etc.</span></span><br><span class="line">class_names = [<span class="string">'T-shirt/top'</span>, <span class="string">'Trouser'</span>, <span class="string">'Pullover'</span>, <span class="string">'Dress'</span>, <span class="string">'Coat'</span>,</span><br><span class="line">               <span class="string">'Sandal'</span>, <span class="string">'Shirt'</span>, <span class="string">'Sneaker'</span>, <span class="string">'Bag'</span>, <span class="string">'Ankle boot'</span>]</span><br></pre></td></tr></table></figure><h2 id="Format-the-Images"><a href="#Format-the-Images" class="headerlink" title="Format the Images"></a>Format the Images</h2><p><code>train_images</code> is a NumPy array with shape <code>(60000, 28, 28)</code> and <code>test_images</code> is a NumPy array with shape <code>(10000, 28, 28)</code>. However, our model expects arrays with shape <code>(batch_size, height, width, channels)</code> . Therefore, we must reshape our NumPy arrays to also include the number of color channels. Since the images are grayscale, we will set <code>channels</code> to <code>1</code>. We will also normalize the values of our NumPy arrays to be in the range <code>[0,1]</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Pre-process images</span></span><br><span class="line">train_images = train_images.reshape(<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line">train_images = train_images / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">test_images = test_images.reshape(<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line">test_images = test_images / <span class="number">255.0</span></span><br></pre></td></tr></table></figure><h2 id="Build-the-Model"><a href="#Build-the-Model" class="headerlink" title="Build the Model"></a>Build the Model</h2><p>We will build a simple CNN and compile it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build the model</span></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">'relu'</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">              loss=<span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure><h2 id="Plot-Confusion-Matrix"><a href="#Plot-Confusion-Matrix" class="headerlink" title="Plot Confusion Matrix"></a>Plot Confusion Matrix</h2><p>When training a classifier, it’s often useful to see the <a href="https://en.wikipedia.org/wiki/Confusion_matrix" target="_blank" rel="noopener">confusion matrix</a>. The confusion matrix gives you detailed knowledge of how your classifier is performing on test data.</p><p>In the cell below, we will define a function that returns a Matplotlib figure containing the plotted confusion matrix.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_confusion_matrix</span><span class="params">(cm, class_names)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns a matplotlib figure containing the plotted confusion matrix.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">       cm (array, shape = [n, n]): a confusion matrix of integer classes</span></span><br><span class="line"><span class="string">       class_names (array, shape = [n]): String names of the integer classes</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    figure = plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">    plt.imshow(cm, interpolation=<span class="string">'nearest'</span>, cmap=plt.cm.Blues)</span><br><span class="line">    plt.title(<span class="string">"Confusion matrix"</span>)</span><br><span class="line">    plt.colorbar()</span><br><span class="line">    tick_marks = np.arange(len(class_names))</span><br><span class="line">    plt.xticks(tick_marks, class_names, rotation=<span class="number">45</span>)</span><br><span class="line">    plt.yticks(tick_marks, class_names)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Normalize the confusion matrix.</span></span><br><span class="line">    cm = np.around(cm.astype(<span class="string">'float'</span>) / cm.sum(axis=<span class="number">1</span>)[:, np.newaxis], decimals=<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use white text if squares are dark; otherwise black.</span></span><br><span class="line">    threshold = cm.max() / <span class="number">2.</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> itertools.product(range(cm.shape[<span class="number">0</span>]), range(cm.shape[<span class="number">1</span>])):</span><br><span class="line">        color = <span class="string">"white"</span> <span class="keyword">if</span> cm[i, j] &gt; threshold <span class="keyword">else</span> <span class="string">"black"</span></span><br><span class="line">        plt.text(j, i, cm[i, j], horizontalalignment=<span class="string">"center"</span>, color=color)</span><br><span class="line">        </span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.ylabel(<span class="string">'True label'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Predicted label'</span>)</span><br><span class="line">    <span class="keyword">return</span> figure</span><br></pre></td></tr></table></figure><h2 id="TensorBoard-Callback"><a href="#TensorBoard-Callback" class="headerlink" title="TensorBoard Callback"></a>TensorBoard Callback</h2><p>We are now ready to train the CNN and regularly log the confusion matrix during the process. In the cell below, you will create a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard" target="_blank" rel="noopener">Keras TensorBoard callback</a> to log basic metrics.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Clear logs prior to logging data.</span></span><br><span class="line">!rm -rf logs/image</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create log directory</span></span><br><span class="line">logdir = <span class="string">"logs/image/"</span> + datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># EXERCISE: Define a TensorBoard callback. Use the log_dir parameter</span></span><br><span class="line"><span class="comment"># to specify the path to the directory where you want to save the</span></span><br><span class="line"><span class="comment"># log files to be parsed by TensorBoard.</span></span><br><span class="line">tensorboard_callback = keras.callbacks.TensorBoard(log_dir = logdir, histogram_freq = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">file_writer_cm = tf.summary.create_file_writer(logdir + <span class="string">'/cm'</span>)</span><br></pre></td></tr></table></figure><pre><code>rm: cannot remove &#39;logs/image/20200222-182126/cm&#39;: Directory not empty</code></pre><h2 id="Convert-Matplotlib-Figure-to-PNG"><a href="#Convert-Matplotlib-Figure-to-PNG" class="headerlink" title="Convert Matplotlib Figure to PNG"></a>Convert Matplotlib Figure to PNG</h2><p>Unfortunately, the Matplotlib file format cannot be logged as an image, but the PNG file format can be logged. So, you will create a helper function that takes a Matplotlib figure and converts it to PNG format so it can be written. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_to_image</span><span class="params">(figure)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts the matplotlib plot specified by 'figure' to a PNG image and</span></span><br><span class="line"><span class="string">    returns it. The supplied figure is closed and inaccessible after this call.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    buf = io.BytesIO()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># EXERCISE: Use plt.savefig to save the plot to a PNG in memory.</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    plt.savefig(buf, format=<span class="string">'png'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Closing the figure prevents it from being displayed directly inside</span></span><br><span class="line">    <span class="comment"># the notebook.</span></span><br><span class="line">    plt.close(figure)</span><br><span class="line">    buf.seek(<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># EXERCISE: Use tf.image.decode_png to convert the PNG buffer</span></span><br><span class="line">    <span class="comment"># to a TF image. Make sure you use 4 channels.</span></span><br><span class="line">    image = tf.image.decode_png(buf.getvalue(), channels=<span class="number">4</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># EXERCISE: Use tf.expand_dims to add the batch dimension</span></span><br><span class="line">    image = tf.expand_dims(image, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure><h2 id="Confusion-Matrix"><a href="#Confusion-Matrix" class="headerlink" title="Confusion Matrix"></a>Confusion Matrix</h2><p>In the cell below, you will define a function that calculates the confusion matrix.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_confusion_matrix</span><span class="params">(epoch, logs)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># EXERCISE: Use the model to predict the values from the test_images.</span></span><br><span class="line">    test_pred_raw = model.predict(test_images)    </span><br><span class="line">    test_pred = np.argmax(test_pred_raw, axis=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># EXERCISE: Calculate the confusion matrix using sklearn.metrics</span></span><br><span class="line">    cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)</span><br><span class="line">    </span><br><span class="line">    figure = plot_confusion_matrix(cm, class_names=class_names)</span><br><span class="line">    cm_image = plot_to_image(figure)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Log the confusion matrix as an image summary.</span></span><br><span class="line">    <span class="keyword">with</span> file_writer_cm.as_default():</span><br><span class="line">        tf.summary.image(<span class="string">"Confusion Matrix"</span>, cm_image, step=epoch)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the per-epoch callback.</span></span><br><span class="line">cm_callback = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)</span><br></pre></td></tr></table></figure><h2 id="Running-TensorBoard"><a href="#Running-TensorBoard" class="headerlink" title="Running TensorBoard"></a>Running TensorBoard</h2><p>The next step will be to run the code shown below to render the TensorBoard. Unfortunately, TensorBoard cannot be rendered within the Coursera environment. Therefore, we won’t run the code below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start TensorBoard.</span></span><br><span class="line">%tensorboard --logdir logs/image</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the classifier.</span></span><br><span class="line">model.fit(train_images,</span><br><span class="line">          train_labels,</span><br><span class="line">          epochs=<span class="number">5</span>,</span><br><span class="line">          verbose=<span class="number">0</span>, <span class="comment"># Suppress chatty output</span></span><br><span class="line">          callbacks=[tensorboard_callback, cm_callback],</span><br><span class="line">          validation_data=(test_images, test_labels))</span><br></pre></td></tr></table></figure><p>However, you are welcome to download the notebook and run the above code locally on your machine or in Google’s Colab to see TensorBoard in action. Below are some example screenshots that you should see when executing the code:</p><table>    <tr>        <td>            <img src="../tmp2/tensorboard_01.png" width="500">        </td>        <td>            <img src="../tmp2/tensorboard_02.png" width="500">        </td>    </tr></table><br><br><table>    <tr>        <td>            <img src="../tmp2/tensorboard_03.png" width="500">        </td>        <td>            <img src="../tmp2/tensorboard_04.png" width="500">        </td>    </tr></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;TensorBoard-with-Fashion-MNIST&quot;&gt;&lt;a href=&quot;#TensorBoard-with-Fashion-MNIST&quot; class=&quot;headerlink&quot; title=&quot;TensorBoard with Fashion MNIST&quot;&gt;
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
      <category term="Development" scheme="https://zhangruochi.com/categories/Development/"/>
    
    
      <category term="Tensorflow" scheme="https://zhangruochi.com/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow hub</title>
    <link href="https://zhangruochi.com/Tensorflow-hub/2020/02/22/"/>
    <id>https://zhangruochi.com/Tensorflow-hub/2020/02/22/</id>
    <published>2020-02-22T07:48:22.000Z</published>
    <updated>2020-02-24T21:34:37.021Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Exporting-an-MNIST-Classifier-in-SavedModel-Format"><a href="#Exporting-an-MNIST-Classifier-in-SavedModel-Format" class="headerlink" title="Exporting an MNIST Classifier in SavedModel Format"></a>Exporting an MNIST Classifier in SavedModel Format</h1><p>In this exercise, we will learn on how to create models for TensorFlow Hub. You will be tasked with performing the following tasks:</p><ul><li>Creating a simple MNIST classifier and evaluating its accuracy.</li><li>Exporting it into SavedModel.</li><li>Hosting the model as TF Hub Module.</li><li>Importing this TF Hub Module to be used with Keras Layers.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow_hub <span class="keyword">as</span> hub</span><br><span class="line"><span class="keyword">import</span> tensorflow_datasets <span class="keyword">as</span> tfds</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> getcwd</span><br></pre></td></tr></table></figure><h2 id="Create-an-MNIST-Classifier"><a href="#Create-an-MNIST-Classifier" class="headerlink" title="Create an MNIST Classifier"></a>Create an MNIST Classifier</h2><p>We will start by creating a class called <code>MNIST</code>. This class will load the MNIST dataset, preprocess the images from the dataset, and build a CNN based classifier. This class will also have some methods to train, test, and save our model. </p><p>In the cell below, fill in the missing code and create the following Keras <code>Sequential</code> model:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Model: &quot;sequential&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">lambda (Lambda)              (None, 28, 28, 1)         0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d (Conv2D)              (None, 28, 28, 8)         80        </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d (MaxPooling2D) (None, 14, 14, 8)         0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_1 (Conv2D)            (None, 14, 14, 16)        1168      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d_1 (MaxPooling2 (None, 7, 7, 16)          0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_2 (Conv2D)            (None, 7, 7, 32)          4640      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten (Flatten)            (None, 1568)              0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                (None, 128)               200832    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (None, 10)                1290      </span><br><span class="line">=================================================================</span><br></pre></td></tr></table></figure><p>Notice that we are using a <code>tf.keras.layers.Lambda</code> layer at the beginning of our model. <code>Lambda</code> layers are used to wrap arbitrary expressions as a <code>Layer</code> object:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.Lambda(expression)</span><br></pre></td></tr></table></figure><p>The <code>Lambda</code> layer exists so that arbitrary TensorFlow functions can be used when constructing <code>Sequential</code> and Functional API models. <code>Lambda</code> layers are best suited for simple operations. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MNIST</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, export_path, buffer_size=<span class="number">1000</span>, batch_size=<span class="number">32</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 learning_rate=<span class="number">1e-3</span>, epochs=<span class="number">10</span>)</span>:</span></span><br><span class="line">        self._export_path = export_path</span><br><span class="line">        self._buffer_size = buffer_size</span><br><span class="line">        self._batch_size = batch_size</span><br><span class="line">        self._learning_rate = learning_rate</span><br><span class="line">        self._epochs = epochs</span><br><span class="line">    </span><br><span class="line">        self._build_model()</span><br><span class="line">        self.train_dataset, self.test_dataset = self._prepare_dataset()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Function to preprocess the images.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocess_fn</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># EXERCISE: Cast x to tf.float32 using the tf.cast() function.</span></span><br><span class="line">        <span class="comment"># You should also normalize the values of x to be in the range [0, 1].</span></span><br><span class="line">        x = tf.cast(x, tf.float32) / <span class="number">255.0</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_model</span><span class="params">(self)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># EXERCISE: Build the model according to the model summary shown above.</span></span><br><span class="line">        self._model = tf.keras.models.Sequential([</span><br><span class="line">            tf.keras.layers.Input(shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>), dtype=tf.uint8),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Use a Lambda layer to use the self.preprocess_fn function</span></span><br><span class="line">            <span class="comment"># defined above to preprocess the images.</span></span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            tf.keras.layers.Lambda(self.preprocess_fn),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Create a Conv2D layer with 8 filters, a kernel size of 3</span></span><br><span class="line">            <span class="comment"># and padding='same'.</span></span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            tf.keras.layers.Conv2D(filters = <span class="number">8</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">"same"</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Create a MaxPool2D() layer. Use default values.</span></span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            tf.keras.layers.MaxPool2D(),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Create a Conv2D layer with 16 filters, a kernel size of 3</span></span><br><span class="line">            <span class="comment"># and padding='same'.</span></span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            tf.keras.layers.Conv2D(filters = <span class="number">16</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">"same"</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Create a MaxPool2D() layer. Use default values.</span></span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            tf.keras.layers.MaxPool2D(),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Create a Conv2D layer with 32 filters, a kernel size of 3</span></span><br><span class="line">            <span class="comment"># and padding='same'.</span></span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            tf.keras.layers.Conv2D(filters = <span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">"same"</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Create the Flatten and Dense layers as described in the </span></span><br><span class="line">            <span class="comment"># model summary shown above.</span></span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            </span><br><span class="line">            tf.keras.layers.Flatten(),</span><br><span class="line">            tf.keras.layers.Dense(<span class="number">128</span>),</span><br><span class="line">            tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">"softmax"</span>)</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># EXERCISE: Define the optimizer, loss function and metrics.</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use the tf.keras.optimizers.Adam optimizer and set the</span></span><br><span class="line">        <span class="comment"># learning rate to self._learning_rate.</span></span><br><span class="line">        optimizer_fn = tf.keras.optimizers.Adam(learning_rate=self._learning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use sparse_categorical_crossentropy as your loss function.</span></span><br><span class="line">        loss_fn = <span class="string">"sparse_categorical_crossentropy"</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set the metrics to accuracy.</span></span><br><span class="line">        metrics_list = [<span class="string">"accuracy"</span>]</span><br><span class="line">     </span><br><span class="line">        <span class="comment"># Compile the model.</span></span><br><span class="line">        self._model.compile(optimizer_fn, loss=loss_fn, metrics=metrics_list)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_prepare_dataset</span><span class="params">(self)</span>:</span></span><br><span class="line">        </span><br><span class="line">        filePath = <span class="string">f"<span class="subst">&#123;getcwd()&#125;</span>/../tmp2"</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># EXERCISE: Load the MNIST dataset using tfds.load(). Make sure to use</span></span><br><span class="line">        <span class="comment"># the argument data_dir=filePath. You should load the images as well</span></span><br><span class="line">        <span class="comment"># as their corresponding labels and load both the test and train splits.</span></span><br><span class="line">        dataset = tfds.load(<span class="string">'mnist'</span>, split=[<span class="string">'train'</span>, <span class="string">'test'</span>], data_dir = filePath, as_supervised=<span class="keyword">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># EXERCISE: Extract the 'train' and 'test' splits from the dataset above.</span></span><br><span class="line">        train_dataset, test_dataset = dataset</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> train_dataset, test_dataset</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># EXERCISE: Shuffle and batch the self.train_dataset. Use self._buffer_size</span></span><br><span class="line">        <span class="comment"># as the shuffling buffer and self._batch_size as the batch size for batching. </span></span><br><span class="line">        dataset_tr = self.train_dataset.shuffle(self._buffer_size).batch(self._batch_size)</span><br><span class="line">        </span><br><span class="line"><span class="comment">#         images, labels = next(iter(dataset_tr)).items()</span></span><br><span class="line"><span class="comment">#         print(images[1].shape) # (32, 28, 28, 1)</span></span><br><span class="line">             </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Train the model for specified number of epochs.</span></span><br><span class="line">        self._model.fit(dataset_tr, epochs=self._epochs)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># EXERCISE: Batch the self.test_dataset. Use a batch size of 32.</span></span><br><span class="line">        dataset_te = self.test_dataset.batch(<span class="number">32</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Evaluate the dataset</span></span><br><span class="line">        results = self._model.evaluate(dataset_te)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Print the metric values on which the model is being evaluated on.</span></span><br><span class="line">        <span class="keyword">for</span> name, value <span class="keyword">in</span> zip(self._model.metrics_names, results):</span><br><span class="line">            print(<span class="string">"%s: %.3f"</span> % (name, value))</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">export_model</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># Save the model.</span></span><br><span class="line">        tf.saved_model.save(self._model, self._export_path)</span><br></pre></td></tr></table></figure><h2 id="Train-Evaluate-and-Save-the-Model"><a href="#Train-Evaluate-and-Save-the-Model" class="headerlink" title="Train, Evaluate, and Save the Model"></a>Train, Evaluate, and Save the Model</h2><p>We will now use the <code>MNIST</code> class we created above to create an <code>mnist</code> object. When creating our <code>mnist</code> object we will use a dictionary to pass our training parameters. We will then call the <code>train</code> and <code>export_model</code> methods to train and save our model, respectively. Finally, we call the <code>test</code> method to evaluate our model after training. </p><p><strong>NOTE:</strong> It will take about 12 minutes to train the model for 5 epochs.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the training parameters.</span></span><br><span class="line">args = &#123;<span class="string">'export_path'</span>: <span class="string">'./saved_model'</span>,</span><br><span class="line">        <span class="string">'buffer_size'</span>: <span class="number">1000</span>,</span><br><span class="line">        <span class="string">'batch_size'</span>: <span class="number">32</span>,</span><br><span class="line">        <span class="string">'learning_rate'</span>: <span class="number">1e-3</span>,</span><br><span class="line">        <span class="string">'epochs'</span>: <span class="number">5</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the mnist object. </span></span><br><span class="line">mnist = MNIST(**args)</span><br><span class="line">print(mnist._model.summary())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model.</span></span><br><span class="line">mnist.train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the model.</span></span><br><span class="line">mnist.export_model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate the trained MNIST model.</span></span><br><span class="line">mnist.test()</span><br></pre></td></tr></table></figure><pre><code>WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /tf/week2/../tmp2. Using currently defined version 1.0.0.Model: &quot;sequential_2&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================lambda_1 (Lambda)            (None, 28, 28, 1)         0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 28, 28, 8)         80        _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 14, 14, 8)         0         _________________________________________________________________conv2d_4 (Conv2D)            (None, 14, 14, 16)        1168      _________________________________________________________________max_pooling2d_3 (MaxPooling2 (None, 7, 7, 16)          0         _________________________________________________________________conv2d_5 (Conv2D)            (None, 7, 7, 32)          4640      _________________________________________________________________flatten_1 (Flatten)          (None, 1568)              0         _________________________________________________________________dense_1 (Dense)              (None, 128)               200832    _________________________________________________________________dense_2 (Dense)              (None, 10)                1290      =================================================================Total params: 208,010Trainable params: 208,010Non-trainable params: 0_________________________________________________________________NoneEpoch 1/51875/1875 [==============================] - 135s 72ms/step - loss: 0.1548 - accuracy: 0.9532548 - accuracy: 0.95Epoch 2/5 563/1875 [========&gt;.....................] - ETA: 1:36 - loss: 0.0868 - accuracy: 0.9733</code></pre><h2 id="Create-a-Tarball"><a href="#Create-a-Tarball" class="headerlink" title="Create a Tarball"></a>Create a Tarball</h2><p>The <code>export_model</code> method saved our model in the TensorFlow SavedModel format in the <code>./saved_model</code> directory. The SavedModel format saves our model and its weights in various files and directories. This makes it difficult to distribute our model. Therefore, it is convenient to create a single compressed file that contains all the files and folders of our model. To do this, we will use the <code>tar</code> archiving program to create a tarball (similar to a Zip file) that contains our SavedModel.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a tarball from the SavedModel.</span></span><br><span class="line">!tar -cz -f module.tar.gz -C ./saved_model .</span><br></pre></td></tr></table></figure><h2 id="Inspect-the-Tarball"><a href="#Inspect-the-Tarball" class="headerlink" title="Inspect the Tarball"></a>Inspect the Tarball</h2><p>We can uncompress our tarball to make sure it has all the files and folders from our SavedModel.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inspect the tarball.</span></span><br><span class="line">!tar -tf module.tar.gz</span><br></pre></td></tr></table></figure><pre><code>././variables/./variables/variables.data-00001-of-00002./variables/variables.data-00000-of-00002./variables/variables.index./saved_model.pb./assets/</code></pre><h2 id="Simulate-Server-Conditions"><a href="#Simulate-Server-Conditions" class="headerlink" title="Simulate Server Conditions"></a>Simulate Server Conditions</h2><p>Once we have verified our tarball, we can now simulate server conditions. In a normal scenario, we will fetch our TF Hub module from a remote server using the module’s handle. However, since this notebook cannot host the server, we will instead point the module handle to the directory where our SavedModel is stored. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!rm -rf ./module</span><br><span class="line">!mkdir -p module</span><br><span class="line">!tar xvzf module.tar.gz -C ./module</span><br></pre></td></tr></table></figure><pre><code>././variables/./variables/variables.data-00001-of-00002./variables/variables.data-00000-of-00002./variables/variables.index./saved_model.pb./assets/</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the module handle.</span></span><br><span class="line">MODULE_HANDLE = <span class="string">'./module'</span></span><br></pre></td></tr></table></figure><h2 id="Load-the-TF-Hub-Module"><a href="#Load-the-TF-Hub-Module" class="headerlink" title="Load the TF Hub Module"></a>Load the TF Hub Module</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Load the TF Hub module using the hub.load API.</span></span><br><span class="line">model = hub.load(MODULE_HANDLE)</span><br></pre></td></tr></table></figure><h2 id="Test-the-TF-Hub-Module"><a href="#Test-the-TF-Hub-Module" class="headerlink" title="Test the TF Hub Module"></a>Test the TF Hub Module</h2><p>We will now test our TF Hub module with images from the <code>test</code> split of the MNIST dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">filePath = <span class="string">f"<span class="subst">&#123;getcwd()&#125;</span>/../tmp2"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># EXERCISE: Load the MNIST 'test' split using tfds.load().</span></span><br><span class="line"><span class="comment"># Make sure to use the argument data_dir=filePath. You</span></span><br><span class="line"><span class="comment"># should load the images along with their corresponding labels.</span></span><br><span class="line"></span><br><span class="line">dataset = tfds.load(<span class="string">'mnist'</span>, split=tfds.Split.TEST, data_dir = filePath, as_supervised=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># EXERCISE: Batch the dataset using a batch size of 32.</span></span><br><span class="line">test_dataset = dataset.batch(<span class="number">32</span>)</span><br></pre></td></tr></table></figure><pre><code>WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /tf/week2/../tmp2. Using currently defined version 1.0.0.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the TF Hub module for a single batch of data</span></span><br><span class="line"><span class="keyword">for</span> batch_data <span class="keyword">in</span> test_dataset.take(<span class="number">1</span>):</span><br><span class="line">    outputs = model(batch_data[<span class="number">0</span>])</span><br><span class="line">    outputs = np.argmax(outputs, axis=<span class="number">-1</span>)</span><br><span class="line">    print(<span class="string">'Predicted Labels:'</span>, outputs)</span><br><span class="line">    print(<span class="string">'True Labels:     '</span>, batch_data[<span class="number">1</span>].numpy())</span><br></pre></td></tr></table></figure><pre><code>Predicted Labels: [6 2 3 7 2 2 3 4 7 6 6 9 2 0 9 6 2 0 6 5 1 4 8 1 9 8 4 0 0 5 8 4]True Labels:      [6 2 3 7 2 2 3 4 7 6 6 9 2 0 9 6 8 0 6 5 1 4 8 1 9 8 4 0 0 5 2 4]</code></pre><p>We can see that the model correctly predicts the labels for most images in the batch. </p><h2 id="Evaluate-the-Model-Using-Keras"><a href="#Evaluate-the-Model-Using-Keras" class="headerlink" title="Evaluate the Model Using Keras"></a>Evaluate the Model Using Keras</h2><p>In the cell below, you will integrate the TensorFlow Hub module into the high level Keras API.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Integrate the TensorFlow Hub module into a Keras</span></span><br><span class="line"><span class="comment"># sequential model. You should use a hub.KerasLayer and you </span></span><br><span class="line"><span class="comment"># should make sure to use the correct values for the output_shape,</span></span><br><span class="line"><span class="comment"># and input_shape parameters. You should also use tf.uint8 for</span></span><br><span class="line"><span class="comment"># the dtype parameter.</span></span><br><span class="line"></span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    hub.KerasLayer(model, output_shape=[<span class="number">10</span>], input_shape=[<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>], </span><br><span class="line">                           dtype=tf.uint8)</span><br><span class="line">]) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compile the model.</span></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">              loss=<span class="string">'sparse_categorical_crossentropy'</span>, </span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Evaluate the model on the test_dataset.</span></span><br><span class="line">results = model.evaluate(test_dataset)</span><br></pre></td></tr></table></figure><pre><code>313/313 [==============================] - 27s 88ms/step - loss: 0.0605 - accuracy: 0.9824</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Print the metric values on which the model is being evaluated on.</span></span><br><span class="line"><span class="keyword">for</span> name, value <span class="keyword">in</span> zip(model.metrics_names, results):</span><br><span class="line">    print(<span class="string">"%s: %.3f"</span> % (name, value))</span><br></pre></td></tr></table></figure><pre><code>loss: 0.061accuracy: 0.982</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Exporting-an-MNIST-Classifier-in-SavedModel-Format&quot;&gt;&lt;a href=&quot;#Exporting-an-MNIST-Classifier-in-SavedModel-Format&quot; class=&quot;headerlink&quot;
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
      <category term="Development" scheme="https://zhangruochi.com/categories/Development/"/>
    
    
      <category term="Tensorflow" scheme="https://zhangruochi.com/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow Serving</title>
    <link href="https://zhangruochi.com/Tensorflow-Serving/2020/02/21/"/>
    <id>https://zhangruochi.com/Tensorflow-Serving/2020/02/21/</id>
    <published>2020-02-22T04:37:47.000Z</published>
    <updated>2020-02-24T21:34:19.233Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Train-Your-Own-Model-and-Serve-It-With-TensorFlow-Serving"><a href="#Train-Your-Own-Model-and-Serve-It-With-TensorFlow-Serving" class="headerlink" title="Train Your Own Model and Serve It With TensorFlow Serving"></a>Train Your Own Model and Serve It With TensorFlow Serving</h1><p>In this notebook, you will train a neural network to classify images of handwritten digits from the <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST</a> dataset. You will then save the trained model, and serve it using <a href="https://www.tensorflow.org/tfx/guide/serving" target="_blank" rel="noopener">TensorFlow Serving</a>.</p><h2 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    %tensorflow_version <span class="number">2.</span>x</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> tempfile</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\u2022 Using TensorFlow Version:"</span>, tf.__version__)</span><br></pre></td></tr></table></figure><p>  Using TensorFlow Version: 2.2.0-dev20200217</p><h2 id="Import-the-MNIST-Dataset"><a href="#Import-the-MNIST-Dataset" class="headerlink" title="Import the MNIST Dataset"></a>Import the MNIST Dataset</h2><p>The <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST</a> dataset contains 70,000 grayscale images of the digits 0 through 9. The images show individual digits at a low resolution (28 by 28 pixels). </p><p>Even though these are really images, we will load them as NumPy arrays and not as binary image objects.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</span><br></pre></td></tr></table></figure><pre><code>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz11493376/11490434 [==============================] - 1s 0us/step</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Scale the values of the arrays below to be between 0.0 and 1.0.</span></span><br><span class="line">train_images = train_images / <span class="number">255.0</span></span><br><span class="line">test_images =  test_images / <span class="number">255.0</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_images.shape, test_images.shape</span><br></pre></td></tr></table></figure><pre><code>((60000, 28, 28), (10000, 28, 28))</code></pre><p>In the cell below use the <code>.reshape</code> method to resize the arrays to the following sizes:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_images.shape: (<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line">test_images.shape: (<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Reshape the arrays below.</span></span><br><span class="line">train_images = train_images.reshape((*train_images.shape,<span class="number">1</span>))</span><br><span class="line">test_images =  test_images.reshape((*test_images.shape,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'\ntrain_images.shape: &#123;&#125;, of &#123;&#125;'</span>.format(train_images.shape, train_images.dtype))</span><br><span class="line">print(<span class="string">'test_images.shape: &#123;&#125;, of &#123;&#125;'</span>.format(test_images.shape, test_images.dtype))</span><br></pre></td></tr></table></figure><pre><code>train_images.shape: (60000, 28, 28, 1), of float64test_images.shape: (10000, 28, 28, 1), of float64</code></pre><h2 id="Look-at-a-Sample-Image"><a href="#Look-at-a-Sample-Image" class="headerlink" title="Look at a Sample Image"></a>Look at a Sample Image</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">idx = <span class="number">42</span></span><br><span class="line"></span><br><span class="line">plt.imshow(test_images[idx].reshape(<span class="number">28</span>,<span class="number">28</span>), cmap=plt.cm.binary)</span><br><span class="line">plt.title(<span class="string">'True Label: &#123;&#125;'</span>.format(test_labels[idx]), fontdict=&#123;<span class="string">'size'</span>: <span class="number">16</span>&#125;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_14_0.png" alt="png"></p><h2 id="Build-a-Model"><a href="#Build-a-Model" class="headerlink" title="Build a Model"></a>Build a Model</h2><p>In the cell below build a <code>tf.keras.Sequential</code> model that can be used to classify the images of the MNIST dataset. Feel free to use the simplest possible CNN. Make sure your model has the correct <code>input_shape</code> and the correct number of output units.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create a model.</span></span><br><span class="line">model =  tf.keras.Sequential([</span><br><span class="line">    </span><br><span class="line">        tf.keras.layers.Conv2D(input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>), filters=<span class="number">8</span>, kernel_size=<span class="number">3</span>,</span><br><span class="line">                               strides=<span class="number">2</span>, activation=<span class="string">'relu'</span>, name=<span class="string">'Conv1'</span>),</span><br><span class="line">        tf.keras.layers.Flatten(),</span><br><span class="line">        tf.keras.layers.Dense(<span class="number">10</span>, activation=tf.nn.softmax, name=<span class="string">'Softmax'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================Conv1 (Conv2D)               (None, 13, 13, 8)         80        _________________________________________________________________flatten (Flatten)            (None, 1352)              0         _________________________________________________________________Softmax (Dense)              (None, 10)                13530     =================================================================Total params: 13,610Trainable params: 13,610Non-trainable params: 0_________________________________________________________________</code></pre><h2 id="Train-the-Model"><a href="#Train-the-Model" class="headerlink" title="Train the Model"></a>Train the Model</h2><p>In the cell below configure your model for training using the <code>adam</code> optimizer, <code>sparse_categorical_crossentropy</code> as the loss, and <code>accuracy</code> for your metrics. Then train the model for the given number of epochs, using the <code>train_images</code> array.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Configure the model for training.</span></span><br><span class="line">model.compile(optimizer = <span class="string">"adam"</span>, loss = <span class="string">"sparse_categorical_crossentropy"</span>, metrics=[<span class="string">"accuracy"</span>])</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># EXERCISE: Train the model.</span></span><br><span class="line">history = model.fit(train_images, train_labels,</span><br><span class="line">                    batch_size = <span class="number">16</span>,</span><br><span class="line">                    epochs = epochs,</span><br><span class="line">                    validation_data= [test_images, test_labels],</span><br><span class="line">                    verbose = <span class="number">1</span></span><br><span class="line">                   )</span><br></pre></td></tr></table></figure><pre><code>Train on 60000 samples, validate on 10000 samplesEpoch 1/560000/60000 [==============================] - 8s 127us/sample - loss: 0.3098 - accuracy: 0.9120 - val_loss: 0.1723 - val_accuracy: 0.9489Epoch 2/560000/60000 [==============================] - 7s 121us/sample - loss: 0.1511 - accuracy: 0.9569 - val_loss: 0.1145 - val_accuracy: 0.9667Epoch 3/560000/60000 [==============================] - 7s 122us/sample - loss: 0.1103 - accuracy: 0.9680 - val_loss: 0.0939 - val_accuracy: 0.9720Epoch 4/560000/60000 [==============================] - 7s 121us/sample - loss: 0.0901 - accuracy: 0.9737 - val_loss: 0.0895 - val_accuracy: 0.9739Epoch 5/560000/60000 [==============================] - 7s 121us/sample - loss: 0.0780 - accuracy: 0.9763 - val_loss: 0.0787 - val_accuracy: 0.9758</code></pre><h2 id="Evaluate-the-Model"><a href="#Evaluate-the-Model" class="headerlink" title="Evaluate the Model"></a>Evaluate the Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Evaluate the model on the test images.</span></span><br><span class="line">results_eval = model.evaluate(test_images, test_labels)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> metric, value <span class="keyword">in</span> zip(model.metrics_names, results_eval):</span><br><span class="line">    print(metric + <span class="string">': &#123;:.3&#125;'</span>.format(value))</span><br></pre></td></tr></table></figure><pre><code>10000/10000 [==============================] - 0s 39us/sample - loss: 0.0787 - accuracy: 0.9758loss: 0.0787accuracy: 0.976</code></pre><h2 id="Save-the-Model"><a href="#Save-the-Model" class="headerlink" title="Save the Model"></a>Save the Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">MODEL_DIR = <span class="string">"digits_model"</span></span><br><span class="line"></span><br><span class="line">version = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">export_path = os.path.join(MODEL_DIR, str(version))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> os.path.isdir(export_path):</span><br><span class="line">    print(<span class="string">'\nAlready saved a model, cleaning up\n'</span>)</span><br><span class="line">    !rm -r &#123;export_path&#125;</span><br><span class="line"></span><br><span class="line">model.save(export_path, save_format=<span class="string">"tf"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\nexport_path = &#123;&#125;'</span>.format(export_path))</span><br><span class="line">!ls -l &#123;export_path&#125;</span><br></pre></td></tr></table></figure><h2 id="Examine-Your-Saved-Model"><a href="#Examine-Your-Saved-Model" class="headerlink" title="Examine Your Saved Model"></a>Examine Your Saved Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!saved_model_cli show --dir &#123;export_path&#125; --all</span><br></pre></td></tr></table></figure><pre><code>MetaGraphDef with tag-set: &#39;serve&#39; contains the following SignatureDefs:signature_def[&#39;__saved_model_init_op&#39;]:  The given SavedModel SignatureDef contains the following input(s):  The given SavedModel SignatureDef contains the following output(s):    outputs[&#39;__saved_model_init_op&#39;] tensor_info:        dtype: DT_INVALID        shape: unknown_rank        name: NoOp  Method name is: signature_def[&#39;serving_default&#39;]:  The given SavedModel SignatureDef contains the following input(s):    inputs[&#39;Conv1_input&#39;] tensor_info:        dtype: DT_FLOAT        shape: (-1, 28, 28, 1)        name: serving_default_Conv1_input:0  The given SavedModel SignatureDef contains the following output(s):    outputs[&#39;Softmax&#39;] tensor_info:        dtype: DT_FLOAT        shape: (-1, 10)        name: StatefulPartitionedCall:0  Method name is: tensorflow/serving/predictWARNING:tensorflow:From /Users/ZRC/miniconda3/envs/tryit/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1809: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.Instructions for updating:If using Keras pass *_constraint arguments to layers.Defined Functions:  Function Name: &#39;__call__&#39;    Option #1      Callable with:        Argument #1          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;inputs&#39;)        Argument #2          DType: bool          Value: False        Argument #3          DType: NoneType          Value: None    Option #2      Callable with:        Argument #1          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;Conv1_input&#39;)        Argument #2          DType: bool          Value: False        Argument #3          DType: NoneType          Value: None    Option #3      Callable with:        Argument #1          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;Conv1_input&#39;)        Argument #2          DType: bool          Value: True        Argument #3          DType: NoneType          Value: None    Option #4      Callable with:        Argument #1          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;inputs&#39;)        Argument #2          DType: bool          Value: True        Argument #3          DType: NoneType          Value: None  Function Name: &#39;_default_save_signature&#39;    Option #1      Callable with:        Argument #1          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;Conv1_input&#39;)  Function Name: &#39;call_and_return_all_conditional_losses&#39;    Option #1      Callable with:        Argument #1          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;Conv1_input&#39;)        Argument #2          DType: bool          Value: False        Argument #3          DType: NoneType          Value: None    Option #2      Callable with:        Argument #1          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;Conv1_input&#39;)        Argument #2          DType: bool          Value: True        Argument #3          DType: NoneType          Value: None    Option #3      Callable with:        Argument #1          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;inputs&#39;)        Argument #2          DType: bool          Value: True        Argument #3          DType: NoneType          Value: None    Option #4      Callable with:        Argument #1          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;inputs&#39;)        Argument #2          DType: bool          Value: False        Argument #3          DType: NoneType          Value: None</code></pre><h2 id="Add-TensorFlow-Serving-Distribution-URI-as-a-Package-Source"><a href="#Add-TensorFlow-Serving-Distribution-URI-as-a-Package-Source" class="headerlink" title="Add TensorFlow Serving Distribution URI as a Package Source"></a>Add TensorFlow Serving Distribution URI as a Package Source</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is the same as you would do from your command line, but without the [arch=amd64], and no sudo</span></span><br><span class="line"><span class="comment"># You would instead do:</span></span><br><span class="line"><span class="comment"># echo "deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list &amp;&amp; \</span></span><br><span class="line"><span class="comment"># curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -</span></span><br><span class="line"></span><br><span class="line">!echo <span class="string">"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal"</span> | tee /etc/apt/sources.list.d/tensorflow-serving.list &amp;&amp; \</span><br><span class="line">curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -</span><br><span class="line">!apt update</span><br></pre></td></tr></table></figure><pre><code>tee: /etc/apt/sources.list.d/tensorflow-serving.list: No such file or directorydeb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universalUnable to locate an executable at &quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_73.jdk/Contents/Home/bin/apt&quot; (-1)</code></pre><h2 id="Install-TensorFlow-Serving"><a href="#Install-TensorFlow-Serving" class="headerlink" title="Install TensorFlow Serving"></a>Install TensorFlow Serving</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!apt-get install tensorflow-model-server</span><br></pre></td></tr></table></figure><pre><code>/bin/sh: apt-get: command not found</code></pre><h2 id="Run-the-TensorFlow-Model-Server"><a href="#Run-the-TensorFlow-Model-Server" class="headerlink" title="Run the TensorFlow Model Server"></a>Run the TensorFlow Model Server</h2><p>You will now launch the TensorFlow model server with a bash script. In the cell below use the following parameters when running the TensorFlow model server:</p><ul><li><code>rest_api_port</code>: Use port <code>8501</code> for your requests.</li></ul><ul><li><code>model_name</code>: Use <code>digits_model</code> as your model name. </li></ul><ul><li><code>model_base_path</code>: Use the environment variable <code>MODEL_DIR</code> defined below as the base path to the saved model.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.environ[<span class="string">"MODEL_DIR"</span>] = MODEL_DIR</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MODEL_DIR</span><br></pre></td></tr></table></figure><pre><code>&#39;digits_model&#39;</code></pre><ul><li>-p 8501:8501 : Publishing the container’s port 8501 (where TF Serving responds to REST API requests) to the host’s port 8501</li><li>—mount type=bind,source=/tmp/resnet,target=/models/resnet : Mounting the host’s local directory (/tmp/resnet) on the container (/models/resnet) so TF Serving can read the model from inside the container.</li><li>-e MODEL_NAME=digits_model : Telling TensorFlow Serving to load the model named “digits_model”</li><li>-t tensorflow/serving : Running a Docker container based on the serving image “tensorflow/serving”</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%%bash</span><br><span class="line"></span><br><span class="line">nohup docker run -p 8501:8501 \</span><br><span class="line">  --mount <span class="built_in">type</span>=<span class="built_in">bind</span>,<span class="built_in">source</span>=<span class="string">"/Users/ZRC/Desktop/TensorFlow Data and Deployment/week13/Exercises/digits_model,target=/models/digits_model"</span> \</span><br><span class="line">  -e MODEL_NAME=digits_model -t tensorflow/serving  &amp;</span><br></pre></td></tr></table></figure><pre><code>docker: Error response from daemon: driver failed programming external connectivity on endpoint wonderful_ganguly (32049ac8fc320931031b817d6269004dcac5878b1e8c8addceb79fb1cd5dca24): Bind for 0.0.0.0:8501 failed: port is already allocated.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># # EXERCISE: Fill in the missing code below.</span></span><br><span class="line"><span class="comment"># %%bash --bg </span></span><br><span class="line"><span class="comment"># nohup tensorflow_model_server \</span></span><br><span class="line"><span class="comment">#   --rest_api_port=8501 \</span></span><br><span class="line"><span class="comment">#   --model_name=digits_model \</span></span><br><span class="line"><span class="comment">#   --model_base_path="$&#123;MODEL_DIR&#125;" &gt;server.log 2&gt;&amp;1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!tail server.log</span><br></pre></td></tr></table></figure><pre><code>docker: Error response from daemon: driver failed programming external connectivity on endpoint determined_antonelli (6a4d67f3751bc7f9fed7821f6df430cbf368992e00c710738cb6f1d9f1e647d2): Bind for 0.0.0.0:8501 failed: port is already allocated.</code></pre><h2 id="Create-JSON-Object-with-Test-Images"><a href="#Create-JSON-Object-with-Test-Images" class="headerlink" title="Create JSON Object with Test Images"></a>Create JSON Object with Test Images</h2><p>In the cell below construct a JSON object and use the first three images of the testing set (<code>test_images</code>) as your data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create JSON Object</span></span><br><span class="line">data = data = json.dumps(&#123;<span class="string">"signature_name"</span>: <span class="string">"serving_default"</span>, <span class="string">"instances"</span>: test_images[<span class="number">0</span>:<span class="number">3</span>].tolist()&#125;)</span><br></pre></td></tr></table></figure><h2 id="Make-Inference-Request"><a href="#Make-Inference-Request" class="headerlink" title="Make Inference Request"></a>Make Inference Request</h2><p>In the cell below, send a predict request as a POST to the server’s REST endpoint, and pass it your test data. You should ask the server to give you the latest version of your model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Fill in the code below</span></span><br><span class="line">headers = &#123;<span class="string">"content-type"</span>: <span class="string">"application/json"</span>&#125;</span><br><span class="line">json_response = requests.post(<span class="string">'http://localhost:8501/v1/models/digits_model:predict'</span>, data=data, headers=headers)</span><br><span class="line">    </span><br><span class="line">predictions = json.loads(json_response.text)[<span class="string">'predictions'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions</span><br></pre></td></tr></table></figure><pre><code>[[1.68959902e-09,  3.5654768e-10,  4.89267848e-07,  6.09665512e-05,  5.58686e-10,  2.27727126e-09,  8.49646383e-15,  0.999936819,  1.21679037e-07,  1.63355e-06], [1.37625943e-06,  6.47248962e-05,  0.99961108,  4.23558049e-06,  5.8764843e-10,  7.63888067e-07,  0.000306190021,  2.43645703e-15,  1.15736648e-05,  8.24755108e-12], [2.70507389e-05,  0.996317267,  0.00121238839,  1.1719947e-05,  0.00065574795,  2.86702857e-06,  0.000181563752,  0.000955980679,  0.000630841067,  4.60029833e-06]]</code></pre><h2 id="Plot-Predictions"><a href="#Plot-Predictions" class="headerlink" title="Plot Predictions"></a>Plot Predictions</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">15</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(test_images[i].reshape(<span class="number">28</span>,<span class="number">28</span>), cmap = plt.cm.binary)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    color = <span class="string">'green'</span> <span class="keyword">if</span> np.argmax(predictions[i]) == test_labels[i] <span class="keyword">else</span> <span class="string">'red'</span></span><br><span class="line">    plt.title(<span class="string">'Prediction: &#123;&#125;\nTrue Label: &#123;&#125;'</span>.format(np.argmax(predictions[i]), test_labels[i]), color=color)</span><br><span class="line">    </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_42_0.png" alt="png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Train-Your-Own-Model-and-Serve-It-With-TensorFlow-Serving&quot;&gt;&lt;a href=&quot;#Train-Your-Own-Model-and-Serve-It-With-TensorFlow-Serving&quot; clas
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
      <category term="Development" scheme="https://zhangruochi.com/categories/Development/"/>
    
    
      <category term="Tensorflow" scheme="https://zhangruochi.com/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Improving Performance of Tensorflow ETL Pipeline</title>
    <link href="https://zhangruochi.com/Improving-Performance-of-Tensorflow-ETL-pipeline/2020/02/20/"/>
    <id>https://zhangruochi.com/Improving-Performance-of-Tensorflow-ETL-pipeline/2020/02/20/</id>
    <published>2020-02-20T07:17:20.000Z</published>
    <updated>2020-02-24T21:34:43.093Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Parallelization-with-TFDS"><a href="#Parallelization-with-TFDS" class="headerlink" title="Parallelization with TFDS"></a>Parallelization with TFDS</h1><p>In this week’s exercise, we’ll go back to the classic cats versus dogs example, but instead of just naively loading the data to train a model, you will be parallelizing various stages of the Extract, Transform and Load processes. In particular, you will be performing following tasks:   </p><ol><li>Parallelize the extraction of the stored TFRecords of the cats_vs_dogs dataset by using the interleave operation.</li><li>Parallelize the transformation during the preprocessing of the raw dataset by using the map operation.</li><li>Cache the processed dataset in memory by using the cache operation for faster retrieval.</li><li>Parallelize the loading of the cached dataset during the training cycle by using the prefetch operation.</li></ol><h2 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow_datasets <span class="keyword">as</span> tfds</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> getcwd</span><br></pre></td></tr></table></figure><h2 id="Create-and-Compile-the-Model"><a href="#Create-and-Compile-the-Model" class="headerlink" title="Create and Compile the Model"></a>Create and Compile the Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    input_layer = tf.keras.layers.Input(shape=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br><span class="line">    base_model = tf.keras.applications.MobileNetV2(input_tensor=input_layer,</span><br><span class="line">                                                   weights=<span class="string">'imagenet'</span>,</span><br><span class="line">                                                   include_top=<span class="keyword">False</span>)</span><br><span class="line">    base_model.trainable = <span class="keyword">False</span></span><br><span class="line">    x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)</span><br><span class="line">    x = tf.keras.layers.Dense(<span class="number">2</span>, activation=<span class="string">'softmax'</span>)(x)</span><br><span class="line">    </span><br><span class="line">    model = tf.keras.models.Model(inputs=input_layer, outputs=x)</span><br><span class="line">    model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">                  loss=<span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">                  metrics=[<span class="string">'acc'</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h2 id="Naive-Approach"><a href="#Naive-Approach" class="headerlink" title="Naive Approach"></a>Naive Approach</h2><p>Just for comparison, let’s start by using the naive approach to Extract, Transform, and Load the data to train the model defined above. By naive approach we mean that we won’t apply any of the new concepts of parallelization that we learned about in this module.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset_name = <span class="string">'cats_vs_dogs'</span></span><br><span class="line">filePath = <span class="string">f"<span class="subst">&#123;getcwd()&#125;</span>/../tmp2"</span></span><br><span class="line">dataset, info = tfds.load(name=dataset_name, split=tfds.Split.TRAIN, with_info=<span class="keyword">True</span>, data_dir=filePath)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(info.version)</span><br></pre></td></tr></table></figure><pre><code>2.0.1</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(features)</span>:</span></span><br><span class="line">    image = features[<span class="string">'image'</span>]</span><br><span class="line">    image = tf.image.resize(image, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    image = image / <span class="number">255.0</span></span><br><span class="line">    <span class="keyword">return</span> image, features[<span class="string">'label'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = dataset.map(preprocess).batch(<span class="number">32</span>)</span><br></pre></td></tr></table></figure><p>The next step will be to train the model using the following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = create_model()</span><br><span class="line">model.fit(train_dataset, epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>Since we want to focus on the parallelization techniques, we won’t go through the training process here, as this can take some time. </p><h1 id="Parallelize-Various-Stages-of-the-ETL-Processes"><a href="#Parallelize-Various-Stages-of-the-ETL-Processes" class="headerlink" title="Parallelize Various Stages of the ETL Processes"></a>Parallelize Various Stages of the ETL Processes</h1><p>The following exercises are about parallelizing various stages of Extract, Transform and Load processes. In particular, you will be tasked with performing following tasks:   </p><ol><li>Parallelize the extraction of the stored TFRecords of the cats_vs_dogs dataset by using the interleave operation.</li><li>Parallelize the transformation during the preprocessing of the raw dataset by using the map operation.</li><li>Cache the processed dataset in memory by using the cache operation for faster retrieval.</li><li>Parallelize the loading of the cached dataset during the training cycle by using the prefetch operation.</li></ol><p>We start by creating a dataset of strings corresponding to the <code>file_pattern</code> of the TFRecords of the cats_vs_dogs dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file_pattern = <span class="string">f'<span class="subst">&#123;getcwd()&#125;</span>/../tmp2/<span class="subst">&#123;dataset_name&#125;</span>/<span class="subst">&#123;info.version&#125;</span>/<span class="subst">&#123;dataset_name&#125;</span>-train.tfrecord*'</span></span><br><span class="line">files = tf.data.Dataset.list_files(file_pattern)</span><br></pre></td></tr></table></figure><p>Let’s recall that the TFRecord format is a simple format for storing a sequence of binary records. This is very useful because by serializing the data and storing it in a set of files (100-200MB each) that can each be read linearly greatly increases the efficiency when reading the data.</p><p>Since we will use it later, we should also recall that a <code>tf.Example</code> message (or protobuf) is a flexible message type that represents a <code>{&quot;string&quot;: tf.train.Feature}</code> mapping.</p><h2 id="Parallelize-Extraction"><a href="#Parallelize-Extraction" class="headerlink" title="Parallelize Extraction"></a>Parallelize Extraction</h2><p>In the cell below you will use the <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave" target="_blank" rel="noopener">interleave</a> operation with certain <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#args_38" target="_blank" rel="noopener">arguments</a> to parallelize the extraction of the stored TFRecords of the cats_vs_dogs dataset.</p><p>Recall that <code>tf.data.experimental.AUTOTUNE</code> will delegate the decision about what level of parallelism to use to the <code>tf.data</code> runtime.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Parallelize the extraction of the stored TFRecords of</span></span><br><span class="line"><span class="comment"># the cats_vs_dogs dataset by using the interleave operation with</span></span><br><span class="line"><span class="comment"># cycle_length = 4 and the number of parallel calls set to tf.data.experimental.AUTOTUNE.</span></span><br><span class="line">train_dataset = files.interleave(tf.data.TFRecordDataset,</span><br><span class="line">                                cycle_length=<span class="number">4</span>,</span><br><span class="line">                                block_length=<span class="number">1</span>,</span><br><span class="line">                                num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure><h2 id="Parse-and-Decode"><a href="#Parse-and-Decode" class="headerlink" title="Parse and Decode"></a>Parse and Decode</h2><p>At this point the <code>train_dataset</code> contains serialized <code>tf.train.Example</code> messages. When iterated over, it returns these as scalar string tensors. The sample output for one record is given below:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: id=189, shape=(), dtype=string, numpy=b&apos;\n\x8f\xc4\x01\n\x0e\n\x05label\x12\x05\x1a\x03\n\x01\x00\n,\n\x0eimage/filename\x12\x1a\n\x18\n\x16PetImages/Cat/4159.jpg\n\xcd\xc3\x01\n\x05image\x12...\xff\xd9&apos;&gt;</span><br></pre></td></tr></table></figure><p>In order to be able to use these tensors to train our model, we must first parse them and decode them. We can parse and decode these string tensors by using a function. In the cell below you will create a <code>read_tfrecord</code> function that will read the serialized <code>tf.train.Example</code> messages and decode them. The function will also normalize and resize the images after they have been decoded. </p><p>In order to parse the <code>tf.train.Example</code> messages we need to create a <code>feature_description</code> dictionary. We need the <code>feature_description</code> dictionary because TFDS uses graph-execution and therefore, needs this description to build their shape and type signature. The basic structure of the <code>feature_description</code> dictionary looks like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">feature_description = &#123;<span class="string">'feature'</span>: tf.io.FixedLenFeature([], tf.Dtype, default_value)&#125;</span><br></pre></td></tr></table></figure><p>The number of features in your <code>feature_description</code> dictionary will vary depending on your dataset. In our particular case, the features are <code>&#39;image&#39;</code> and <code>&#39;label&#39;</code> and can be seen in the sample output of the string tensor above. Therefore, our <code>feature_description</code> dictionary will look like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">feature_description = &#123;</span><br><span class="line">    <span class="string">'image'</span>: tf.io.FixedLenFeature((), tf.string, <span class="string">""</span>),</span><br><span class="line">    <span class="string">'label'</span>: tf.io.FixedLenFeature((), tf.int64, <span class="number">-1</span>),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>where we have given the default values of <code>&quot;&quot;</code> and <code>-1</code> to the <code>&#39;image&#39;</code> and <code>&#39;label&#39;</code> respectively.</p><p>The next step will be to parse the serialized <code>tf.train.Example</code> message using the <code>feature_description</code> dictionary given above. This can be done with the following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">example = tf.io.parse_single_example(serialized_example, feature_description)</span><br></pre></td></tr></table></figure><p>Finally, we can decode the image by using:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image = tf.io.decode_jpeg(example[<span class="string">'image'</span>], channels=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>Use the code given above to complete the exercise below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Fill in the missing code below.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_tfrecord</span><span class="params">(serialized_example)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create the feature description dictionary</span></span><br><span class="line">    feature_description = &#123;</span><br><span class="line">        <span class="string">'image'</span>: tf.io.FixedLenFeature((), tf.string, <span class="string">""</span>),</span><br><span class="line">        <span class="string">'label'</span>: tf.io.FixedLenFeature((), tf.int64, <span class="number">-1</span>),</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># Parse the serialized_example and decode the image</span></span><br><span class="line">    example = tf.io.parse_single_example(serialized_example, feature_description)</span><br><span class="line">    image = tf.io.decode_jpeg(example[<span class="string">'image'</span>], channels=<span class="number">3</span>)</span><br><span class="line">    </span><br><span class="line">    image = tf.cast(image, tf.float32)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Normalize the pixels in the image</span></span><br><span class="line">    image = image / <span class="number">255.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Resize the image to (224, 224) using tf.image.resize</span></span><br><span class="line">    image = tf.image.resize(image, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> image, example[<span class="string">'label'</span>]</span><br></pre></td></tr></table></figure><h2 id="Parallelize-Transformation"><a href="#Parallelize-Transformation" class="headerlink" title="Parallelize Transformation"></a>Parallelize Transformation</h2><p>You can now apply the <code>read_tfrecord</code> function to each item in the <code>train_dataset</code> by using the <code>map</code> method. You can parallelize the transformation of the <code>train_dataset</code> by using the <code>map</code> method with the <code>num_parallel_calls</code> set to the number of CPU cores.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Fill in the missing code below.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the number of CPU cores. </span></span><br><span class="line">cores = multiprocessing.cpu_count()</span><br><span class="line"></span><br><span class="line">print(cores)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parallelize the transformation of the train_dataset by using</span></span><br><span class="line"><span class="comment"># the map operation with the number of parallel calls set to</span></span><br><span class="line"><span class="comment"># the number of CPU cores.</span></span><br><span class="line">train_dataset = train_dataset.map(read_tfrecord, num_parallel_calls=cores)</span><br></pre></td></tr></table></figure><pre><code>8</code></pre><h2 id="Cache-the-Dataset"><a href="#Cache-the-Dataset" class="headerlink" title="Cache the Dataset"></a>Cache the Dataset</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Cache the train_dataset in-memory.</span></span><br><span class="line">train_dataset = train_dataset.cache()</span><br></pre></td></tr></table></figure><h2 id="Parallelize-Loading"><a href="#Parallelize-Loading" class="headerlink" title="Parallelize Loading"></a>Parallelize Loading</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Fill in the missing code below.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Shuffle and batch the train_dataset. Use a buffer size of 1024</span></span><br><span class="line"><span class="comment"># for shuffling and a batch size 32 for batching. </span></span><br><span class="line">train_dataset = train_dataset.shuffle(buffer_size=<span class="number">1024</span>).batch(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parallelize the loading by prefetching the train_dataset.</span></span><br><span class="line"><span class="comment"># Set the prefetching buffer size to tf.data.experimental.AUTOTUNE.</span></span><br><span class="line">train_dataset = train_dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure><p>The next step will be to train your model using the following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = create_model()</span><br><span class="line">model.fit(train_dataset, epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>We won’t go through the training process here as this can take some time. However, due to the parallelization of the various stages of the ETL processes, you should see a decrease in training time as compared to the naive approach depicted at beginning of the notebook.<br>```</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Parallelization-with-TFDS&quot;&gt;&lt;a href=&quot;#Parallelization-with-TFDS&quot; class=&quot;headerlink&quot; title=&quot;Parallelization with TFDS&quot;&gt;&lt;/a&gt;Paralleliza
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
      <category term="Development" scheme="https://zhangruochi.com/categories/Development/"/>
    
    
      <category term="Tensorflow" scheme="https://zhangruochi.com/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow input pipeline</title>
    <link href="https://zhangruochi.com/Tensorflow-input-pipeline/2020/02/20/"/>
    <id>https://zhangruochi.com/Tensorflow-input-pipeline/2020/02/20/</id>
    <published>2020-02-20T07:16:41.000Z</published>
    <updated>2020-02-24T21:34:32.334Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Classify-Structured-Data"><a href="#Classify-Structured-Data" class="headerlink" title="Classify Structured Data"></a>Classify Structured Data</h1><h2 id="Import-TensorFlow-and-Other-Libraries"><a href="#Import-TensorFlow-and-Other-Libraries" class="headerlink" title="Import TensorFlow and Other Libraries"></a>Import TensorFlow and Other Libraries</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> feature_column</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> getcwd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure><h2 id="Use-Pandas-to-Create-a-Dataframe"><a href="#Use-Pandas-to-Create-a-Dataframe" class="headerlink" title="Use Pandas to Create a Dataframe"></a>Use Pandas to Create a Dataframe</h2><p><a href="https://pandas.pydata.org/" target="_blank" rel="noopener">Pandas</a> is a Python library with many helpful utilities for loading and working with structured data. We will use Pandas to download the dataset and load it into a dataframe.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">filePath = <span class="string">f"<span class="subst">&#123;getcwd()&#125;</span>/../tmp2/heart.csv"</span></span><br><span class="line">dataframe = pd.read_csv(filePath)</span><br><span class="line">dataframe.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>age</th>      <th>sex</th>      <th>cp</th>      <th>trestbps</th>      <th>chol</th>      <th>fbs</th>      <th>restecg</th>      <th>thalach</th>      <th>exang</th>      <th>oldpeak</th>      <th>slope</th>      <th>ca</th>      <th>thal</th>      <th>target</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>63</td>      <td>1</td>      <td>1</td>      <td>145</td>      <td>233</td>      <td>1</td>      <td>2</td>      <td>150</td>      <td>0</td>      <td>2.3</td>      <td>3</td>      <td>0</td>      <td>fixed</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>67</td>      <td>1</td>      <td>4</td>      <td>160</td>      <td>286</td>      <td>0</td>      <td>2</td>      <td>108</td>      <td>1</td>      <td>1.5</td>      <td>2</td>      <td>3</td>      <td>normal</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>67</td>      <td>1</td>      <td>4</td>      <td>120</td>      <td>229</td>      <td>0</td>      <td>2</td>      <td>129</td>      <td>1</td>      <td>2.6</td>      <td>2</td>      <td>2</td>      <td>reversible</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>37</td>      <td>1</td>      <td>3</td>      <td>130</td>      <td>250</td>      <td>0</td>      <td>0</td>      <td>187</td>      <td>0</td>      <td>3.5</td>      <td>3</td>      <td>0</td>      <td>normal</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>41</td>      <td>0</td>      <td>2</td>      <td>130</td>      <td>204</td>      <td>0</td>      <td>2</td>      <td>172</td>      <td>0</td>      <td>1.4</td>      <td>1</td>      <td>0</td>      <td>normal</td>      <td>0</td>    </tr>  </tbody></table></div><h2 id="Split-the-Dataframe-Into-Train-Validation-and-Test-Sets"><a href="#Split-the-Dataframe-Into-Train-Validation-and-Test-Sets" class="headerlink" title="Split the Dataframe Into Train, Validation, and Test Sets"></a>Split the Dataframe Into Train, Validation, and Test Sets</h2><p>The dataset we downloaded was a single CSV file. We will split this into train, validation, and test sets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train, test = train_test_split(dataframe, test_size=<span class="number">0.2</span>)</span><br><span class="line">train, val = train_test_split(train, test_size=<span class="number">0.2</span>)</span><br><span class="line">print(len(train), <span class="string">'train examples'</span>)</span><br><span class="line">print(len(val), <span class="string">'validation examples'</span>)</span><br><span class="line">print(len(test), <span class="string">'test examples'</span>)</span><br></pre></td></tr></table></figure><pre><code>193 train examples49 validation examples61 test examples</code></pre><h2 id="Create-an-Input-Pipeline-Using-tf-data"><a href="#Create-an-Input-Pipeline-Using-tf-data" class="headerlink" title="Create an Input Pipeline Using tf.data"></a>Create an Input Pipeline Using <code>tf.data</code></h2><p>Next, we will wrap the dataframes with <a href="https://www.tensorflow.org/guide/datasets" target="_blank" rel="noopener">tf.data</a>. This will enable us  to use feature columns as a bridge to map from the columns in the Pandas dataframe to features used to train the model. If we were working with a very large CSV file (so large that it does not fit into memory), we would use tf.data to read it from disk directly.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: A utility method to create a tf.data dataset from a Pandas Dataframe.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">df_to_dataset</span><span class="params">(dataframe, shuffle=True, batch_size=<span class="number">32</span>)</span>:</span></span><br><span class="line">    dataframe = dataframe.copy()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use Pandas dataframe's pop method to get the list of targets.</span></span><br><span class="line">    labels = dataframe[<span class="string">"target"</span>].values</span><br><span class="line">    dataframe.drop(<span class="string">"target"</span>, axis = <span class="number">1</span>, inplace = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a tf.data.Dataset from the dataframe and labels.</span></span><br><span class="line">    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe),labels))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        <span class="comment"># Shuffle dataset.</span></span><br><span class="line">        ds = ds.shuffle(<span class="number">3</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Batch dataset with specified batch_size parameter.</span></span><br><span class="line">    ds = ds.batch(batch_size)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">5</span> <span class="comment"># A small batch sized is used for demonstration purposes</span></span><br><span class="line">train_ds = df_to_dataset(train, batch_size=batch_size)</span><br><span class="line">val_ds = df_to_dataset(val, shuffle=<span class="keyword">False</span>, batch_size=batch_size)</span><br><span class="line">test_ds = df_to_dataset(test, shuffle=<span class="keyword">False</span>, batch_size=batch_size)</span><br></pre></td></tr></table></figure><h2 id="Understand-the-Input-Pipeline"><a href="#Understand-the-Input-Pipeline" class="headerlink" title="Understand the Input Pipeline"></a>Understand the Input Pipeline</h2><p>Now that we have created the input pipeline, let’s call it to see the format of the data it returns. We have used a small batch size to keep the output readable.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> feature_batch, label_batch <span class="keyword">in</span> train_ds.take(<span class="number">1</span>):</span><br><span class="line">    print(<span class="string">'Every feature:'</span>, list(feature_batch.keys()))</span><br><span class="line">    print(<span class="string">'A batch of ages:'</span>, feature_batch[<span class="string">'age'</span>])</span><br><span class="line">    print(<span class="string">'A batch of targets:'</span>, label_batch )</span><br></pre></td></tr></table></figure><pre><code>Every feature: [&#39;age&#39;, &#39;sex&#39;, &#39;cp&#39;, &#39;trestbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalach&#39;, &#39;exang&#39;, &#39;oldpeak&#39;, &#39;slope&#39;, &#39;ca&#39;, &#39;thal&#39;]A batch of ages: tf.Tensor([51 63 64 58 57], shape=(5,), dtype=int32)A batch of targets: tf.Tensor([0 1 0 0 0], shape=(5,), dtype=int64)</code></pre><p>We can see that the dataset returns a dictionary of column names (from the dataframe) that map to column values from rows in the dataframe.</p><h2 id="Create-Several-Types-of-Feature-Columns"><a href="#Create-Several-Types-of-Feature-Columns" class="headerlink" title="Create Several Types of Feature Columns"></a>Create Several Types of Feature Columns</h2><p>TensorFlow provides many types of feature columns. In this section, we will create several types of feature columns, and demonstrate how they transform a column from the dataframe.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Try to demonstrate several types of feature columns by getting an example.</span></span><br><span class="line">example_batch = next(iter(train_ds))[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># A utility method to create a feature column and to transform a batch of data.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">(feature_column)</span>:</span></span><br><span class="line">    feature_layer = layers.DenseFeatures(feature_column, dtype=<span class="string">'float64'</span>)</span><br><span class="line">    print(feature_layer(example_batch).numpy())</span><br></pre></td></tr></table></figure><h3 id="Numeric-Columns"><a href="#Numeric-Columns" class="headerlink" title="Numeric Columns"></a>Numeric Columns</h3><p>The output of a feature column becomes the input to the model (using the demo function defined above, we will be able to see exactly how each column from the dataframe is transformed). A <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column" target="_blank" rel="noopener">numeric column</a> is the simplest type of column. It is used to represent real valued features. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create a numeric feature column out of 'age' and demo it.</span></span><br><span class="line">age = tf.feature_column.numeric_column(<span class="string">"age"</span>)</span><br><span class="line"></span><br><span class="line">demo(age)</span><br></pre></td></tr></table></figure><pre><code>[[51.] [58.] [63.] [64.] [60.]]</code></pre><p>In the heart disease dataset, most columns from the dataframe are numeric.</p><h3 id="Bucketized-Columns"><a href="#Bucketized-Columns" class="headerlink" title="Bucketized Columns"></a>Bucketized Columns</h3><p>Often, you don’t want to feed a number directly into the model, but instead split its value into different categories based on numerical ranges. Consider raw data that represents a person’s age. Instead of representing age as a numeric column, we could split the age into several buckets using a <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column" target="_blank" rel="noopener">bucketized column</a>. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create a bucketized feature column out of 'age' with</span></span><br><span class="line"><span class="comment"># the following boundaries and demo it.</span></span><br><span class="line">boundaries = [<span class="number">18</span>, <span class="number">25</span>, <span class="number">30</span>, <span class="number">35</span>, <span class="number">40</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">55</span>, <span class="number">60</span>, <span class="number">65</span>]</span><br><span class="line"></span><br><span class="line">age_buckets = tf.feature_column.bucketized_column(age, boundaries = boundaries)</span><br><span class="line"></span><br><span class="line">demo(age_buckets)</span><br></pre></td></tr></table></figure><pre><code>[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]</code></pre><p>Notice the one-hot values above describe which age range each row matches.</p><h3 id="Categorical-Columns"><a href="#Categorical-Columns" class="headerlink" title="Categorical Columns"></a>Categorical Columns</h3><p>In this dataset, thal is represented as a string (e.g. ‘fixed’, ‘normal’, or ‘reversible’). We cannot feed strings directly to a model. Instead, we must first map them to numeric values. The categorical vocabulary columns provide a way to represent strings as a one-hot vector (much like you have seen above with age buckets). </p><p><strong>Note</strong>: You will probably see some warning messages when running some of the code cell below. These warnings have to do with software updates and should not cause any errors or prevent your code from running.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create a categorical vocabulary column out of the</span></span><br><span class="line"><span class="comment"># above mentioned categories with the key specified as 'thal'.</span></span><br><span class="line">thal = tf.feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">      <span class="string">'thal'</span>, [<span class="string">'fixed'</span>, <span class="string">'normal'</span>, <span class="string">'reversible'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># EXERCISE: Create an indicator column out of the created categorical column.</span></span><br><span class="line">thal_one_hot = tf.feature_column.indicator_column(thal)</span><br><span class="line"></span><br><span class="line">demo(thal_one_hot)</span><br></pre></td></tr></table></figure><pre><code>[[0. 1. 0.] [0. 1. 0.] [0. 0. 1.] [0. 0. 1.] [0. 1. 0.]]</code></pre><p>The vocabulary can be passed as a list using <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list" target="_blank" rel="noopener">categorical_column_with_vocabulary_list</a>, or loaded from a file using <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file" target="_blank" rel="noopener">categorical_column_with_vocabulary_file</a>.</p><h3 id="Embedding-Columns"><a href="#Embedding-Columns" class="headerlink" title="Embedding Columns"></a>Embedding Columns</h3><p>Suppose instead of having just a few possible strings, we have thousands (or more) values per category. For a number of reasons, as the number of categories grow large, it becomes infeasible to train a neural network using one-hot encodings. We can use an embedding column to overcome this limitation. Instead of representing the data as a one-hot vector of many dimensions, an <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column" target="_blank" rel="noopener">embedding column</a> represents that data as a lower-dimensional, dense vector in which each cell can contain any number, not just 0 or 1. You can tune the size of the embedding with the <code>dimension</code> parameter.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create an embedding column out of the categorical</span></span><br><span class="line"><span class="comment"># vocabulary you just created (thal). Set the size of the </span></span><br><span class="line"><span class="comment"># embedding to 8, by using the dimension parameter.</span></span><br><span class="line"></span><br><span class="line">thal_embedding = tf.feature_column.embedding_column(thal, dimension=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">demo(thal_embedding)</span><br></pre></td></tr></table></figure><pre><code>[[-1.4254066e-01 -1.0374661e-01  3.4352791e-01 -3.3996427e-01  -3.2193713e-02 -1.8381193e-01 -1.8051244e-01  3.2638407e-01] [-1.4254066e-01 -1.0374661e-01  3.4352791e-01 -3.3996427e-01  -3.2193713e-02 -1.8381193e-01 -1.8051244e-01  3.2638407e-01] [-6.5549983e-05  2.7680036e-01  4.1849682e-01  5.3418136e-01  -1.6281548e-01  2.5406811e-01  8.8969752e-02  1.8004593e-01] [-6.5549983e-05  2.7680036e-01  4.1849682e-01  5.3418136e-01  -1.6281548e-01  2.5406811e-01  8.8969752e-02  1.8004593e-01] [-1.4254066e-01 -1.0374661e-01  3.4352791e-01 -3.3996427e-01  -3.2193713e-02 -1.8381193e-01 -1.8051244e-01  3.2638407e-01]]</code></pre><h3 id="Hashed-Feature-Columns"><a href="#Hashed-Feature-Columns" class="headerlink" title="Hashed Feature Columns"></a>Hashed Feature Columns</h3><p>Another way to represent a categorical column with a large number of values is to use a <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket" target="_blank" rel="noopener">categorical_column_with_hash_bucket</a>. This feature column calculates a hash value of the input, then selects one of the <code>hash_bucket_size</code> buckets to encode a string. When using this column, you do not need to provide the vocabulary, and you can choose to make the number of hash buckets significantly smaller than the number of actual categories to save space.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create a hashed feature column with 'thal' as the key and </span></span><br><span class="line"><span class="comment"># 1000 hash buckets.</span></span><br><span class="line">thal_hashed = tf.feature_column.categorical_column_with_hash_bucket(</span><br><span class="line">      <span class="string">'thal'</span>, hash_bucket_size=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">demo(feature_column.indicator_column(thal_hashed))</span><br></pre></td></tr></table></figure><pre><code>[[0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]]</code></pre><h3 id="Crossed-Feature-Columns"><a href="#Crossed-Feature-Columns" class="headerlink" title="Crossed Feature Columns"></a>Crossed Feature Columns</h3><p>Combining features into a single feature, better known as <a href="https://developers.google.com/machine-learning/glossary/#feature_cross" target="_blank" rel="noopener">feature crosses</a>, enables a model to learn separate weights for each combination of features. Here, we will create a new feature that is the cross of age and thal. Note that <code>crossed_column</code> does not build the full table of all possible combinations (which could be very large). Instead, it is backed by a <code>hashed_column</code>, so you can choose how large the table is.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create a crossed column using the bucketized column (age_buckets),</span></span><br><span class="line"><span class="comment"># the categorical vocabulary column (thal) previously created, and 1000 hash buckets.</span></span><br><span class="line">crossed_feature = tf.feature_column.crossed_column([age_buckets, thal], hash_bucket_size=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">demo(feature_column.indicator_column(crossed_feature))</span><br></pre></td></tr></table></figure><pre><code>[[0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]]</code></pre><h2 id="Choose-Which-Columns-to-Use"><a href="#Choose-Which-Columns-to-Use" class="headerlink" title="Choose Which Columns to Use"></a>Choose Which Columns to Use</h2><p>We have seen how to use several types of feature columns. Now we will use them to train a model. The goal of this exercise is to show you the complete code needed to work with feature columns. We have selected a few columns to train our model below arbitrarily.</p><p>If your aim is to build an accurate model, try a larger dataset of your own, and think carefully about which features are the most meaningful to include, and how they should be represented.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataframe.dtypes</span><br></pre></td></tr></table></figure><pre><code>age           int64sex           int64cp            int64trestbps      int64chol          int64fbs           int64restecg       int64thalach       int64exang         int64oldpeak     float64slope         int64ca            int64thal         objecttarget        int64dtype: object</code></pre><p>You can use the above list of column datatypes to map the appropriate feature column to every column in the dataframe.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Fill in the missing code below</span></span><br><span class="line">feature_columns = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Numeric Cols.</span></span><br><span class="line"><span class="comment"># Create a list of numeric columns. Use the following list of columns</span></span><br><span class="line"><span class="comment"># that have a numeric datatype: ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca'].</span></span><br><span class="line">numeric_columns = [<span class="string">'age'</span>, <span class="string">'trestbps'</span>, <span class="string">'chol'</span>, <span class="string">'thalach'</span>, <span class="string">'oldpeak'</span>, <span class="string">'slope'</span>, <span class="string">'ca'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> header <span class="keyword">in</span> numeric_columns:</span><br><span class="line">    <span class="comment"># Create a numeric feature column  out of the header.</span></span><br><span class="line">    numeric_feature_column = tf.feature_column.numeric_column(header)</span><br><span class="line">    </span><br><span class="line">    feature_columns.append(numeric_feature_column)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bucketized Cols.</span></span><br><span class="line"><span class="comment"># Create a bucketized feature column out of the age column (numeric column)</span></span><br><span class="line"><span class="comment"># that you've already created. Use the following boundaries:</span></span><br><span class="line"><span class="comment"># [18, 25, 30, 35, 40, 45, 50, 55, 60, 65]</span></span><br><span class="line">age_buckets = tf.feature_column.bucketized_column(age, boundaries = [<span class="number">18</span>, <span class="number">25</span>, <span class="number">30</span>, <span class="number">35</span>, <span class="number">40</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">55</span>, <span class="number">60</span>, <span class="number">65</span>] )</span><br><span class="line"></span><br><span class="line">feature_columns.append(age_buckets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Indicator Cols.</span></span><br><span class="line"><span class="comment"># Create a categorical vocabulary column out of the categories</span></span><br><span class="line"><span class="comment"># ['fixed', 'normal', 'reversible'] with the key specified as 'thal'.</span></span><br><span class="line">thal = feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">      <span class="string">'thal'</span>, [<span class="string">'fixed'</span>, <span class="string">'normal'</span>, <span class="string">'reversible'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an indicator column out of the created thal categorical column</span></span><br><span class="line">thal_one_hot = feature_column.indicator_column(thal)</span><br><span class="line"></span><br><span class="line">feature_columns.append(thal_one_hot)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Embedding Cols.</span></span><br><span class="line"><span class="comment"># Create an embedding column out of the categorical vocabulary you</span></span><br><span class="line"><span class="comment"># just created (thal). Set the size of the embedding to 8, by using</span></span><br><span class="line"><span class="comment"># the dimension parameter.</span></span><br><span class="line">thal_embedding = tf.feature_column.embedding_column(thal, dimension=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">feature_columns.append(thal_embedding)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Crossed Cols.</span></span><br><span class="line"><span class="comment"># Create a crossed column using the bucketized column (age_buckets),</span></span><br><span class="line"><span class="comment"># the categorical vocabulary column (thal) previously created, and 1000 hash buckets.</span></span><br><span class="line">crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an indicator column out of the crossed column created above to one-hot encode it.</span></span><br><span class="line">crossed_feature = feature_column.indicator_column(crossed_feature)</span><br><span class="line"></span><br><span class="line">feature_columns.append(crossed_feature)</span><br></pre></td></tr></table></figure><h3 id="Create-a-Feature-Layer"><a href="#Create-a-Feature-Layer" class="headerlink" title="Create a Feature Layer"></a>Create a Feature Layer</h3><p>Now that we have defined our feature columns, we will use a <a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures" target="_blank" rel="noopener">DenseFeatures</a> layer to input them to our Keras model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create a Keras DenseFeatures layer and pass the feature_columns you just created.</span></span><br><span class="line">feature_layer = tf.keras.layers.DenseFeatures(feature_columns)</span><br></pre></td></tr></table></figure><p>Earlier, we used a small batch size to demonstrate how feature columns worked. We create a new input pipeline with a larger batch size.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">train_ds = df_to_dataset(train, batch_size=batch_size)</span><br><span class="line">val_ds = df_to_dataset(val, shuffle=<span class="keyword">False</span>, batch_size=batch_size)</span><br><span class="line">test_ds = df_to_dataset(test, shuffle=<span class="keyword">False</span>, batch_size=batch_size)</span><br></pre></td></tr></table></figure><h2 id="Create-Compile-and-Train-the-Model"><a href="#Create-Compile-and-Train-the-Model" class="headerlink" title="Create, Compile, and Train the Model"></a>Create, Compile, and Train the Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.Sequential([</span><br><span class="line">        feature_layer,</span><br><span class="line">        layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(train_ds,</span><br><span class="line">          validation_data=val_ds,</span><br><span class="line">          epochs=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/1007/7 [==============================] - 4s 609ms/step - loss: 1.5455 - accuracy: 0.6321 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 2/1007/7 [==============================] - 0s 45ms/step - loss: 1.6424 - accuracy: 0.5803 - val_loss: 1.7392 - val_accuracy: 0.7143Epoch 3/1007/7 [==============================] - 0s 44ms/step - loss: 1.2255 - accuracy: 0.6995 - val_loss: 0.7653 - val_accuracy: 0.5714Epoch 4/1007/7 [==============================] - 0s 44ms/step - loss: 0.7326 - accuracy: 0.6891 - val_loss: 0.5689 - val_accuracy: 0.6939Epoch 5/1007/7 [==============================] - 0s 43ms/step - loss: 0.5230 - accuracy: 0.7358 - val_loss: 0.5406 - val_accuracy: 0.7143Epoch 6/1007/7 [==============================] - 0s 44ms/step - loss: 0.4348 - accuracy: 0.8083 - val_loss: 0.5609 - val_accuracy: 0.7143Epoch 7/1007/7 [==============================] - 0s 56ms/step - loss: 0.4592 - accuracy: 0.7824 - val_loss: 0.5710 - val_accuracy: 0.7347Epoch 8/1007/7 [==============================] - 0s 45ms/step - loss: 0.4996 - accuracy: 0.7461 - val_loss: 0.5585 - val_accuracy: 0.7143Epoch 9/1007/7 [==============================] - 0s 44ms/step - loss: 0.4389 - accuracy: 0.7927 - val_loss: 0.5297 - val_accuracy: 0.6735Epoch 10/1007/7 [==============================] - 0s 55ms/step - loss: 0.3914 - accuracy: 0.8446 - val_loss: 0.5216 - val_accuracy: 0.6531Epoch 11/1007/7 [==============================] - 0s 45ms/step - loss: 0.4022 - accuracy: 0.7979 - val_loss: 0.5331 - val_accuracy: 0.7347Epoch 12/1007/7 [==============================] - 0s 54ms/step - loss: 0.3811 - accuracy: 0.8238 - val_loss: 0.6522 - val_accuracy: 0.6735Epoch 13/1007/7 [==============================] - 0s 44ms/step - loss: 0.4173 - accuracy: 0.7927 - val_loss: 0.5219 - val_accuracy: 0.7347Epoch 14/1007/7 [==============================] - 0s 44ms/step - loss: 0.4235 - accuracy: 0.7513 - val_loss: 0.5027 - val_accuracy: 0.6531Epoch 15/1007/7 [==============================] - 0s 44ms/step - loss: 0.3789 - accuracy: 0.7979 - val_loss: 0.7249 - val_accuracy: 0.6531Epoch 16/1007/7 [==============================] - 0s 45ms/step - loss: 0.3972 - accuracy: 0.8342 - val_loss: 0.4830 - val_accuracy: 0.6939Epoch 17/1007/7 [==============================] - 0s 55ms/step - loss: 0.3339 - accuracy: 0.8601 - val_loss: 0.4912 - val_accuracy: 0.6531Epoch 18/1007/7 [==============================] - 0s 44ms/step - loss: 0.3555 - accuracy: 0.7927 - val_loss: 0.6399 - val_accuracy: 0.6939Epoch 19/1007/7 [==============================] - 0s 43ms/step - loss: 0.3531 - accuracy: 0.8601 - val_loss: 0.5526 - val_accuracy: 0.6735Epoch 20/1007/7 [==============================] - 0s 44ms/step - loss: 0.3810 - accuracy: 0.7876 - val_loss: 0.5751 - val_accuracy: 0.7143Epoch 21/1007/7 [==============================] - 0s 56ms/step - loss: 0.3409 - accuracy: 0.8549 - val_loss: 0.5524 - val_accuracy: 0.7551Epoch 22/1007/7 [==============================] - 0s 44ms/step - loss: 0.3167 - accuracy: 0.8756 - val_loss: 0.6607 - val_accuracy: 0.7143Epoch 23/1007/7 [==============================] - 0s 45ms/step - loss: 0.3732 - accuracy: 0.8601 - val_loss: 0.5993 - val_accuracy: 0.6939Epoch 24/1007/7 [==============================] - 0s 55ms/step - loss: 0.3918 - accuracy: 0.7979 - val_loss: 0.5646 - val_accuracy: 0.6735Epoch 25/1007/7 [==============================] - 0s 44ms/step - loss: 0.3624 - accuracy: 0.8187 - val_loss: 0.7324 - val_accuracy: 0.6735Epoch 26/1007/7 [==============================] - 0s 56ms/step - loss: 0.3531 - accuracy: 0.8446 - val_loss: 0.4501 - val_accuracy: 0.6939Epoch 27/1007/7 [==============================] - 0s 45ms/step - loss: 0.3164 - accuracy: 0.8653 - val_loss: 0.4770 - val_accuracy: 0.6735Epoch 28/1007/7 [==============================] - 0s 55ms/step - loss: 0.3557 - accuracy: 0.8290 - val_loss: 0.5188 - val_accuracy: 0.7551Epoch 29/1007/7 [==============================] - 0s 45ms/step - loss: 0.3193 - accuracy: 0.8446 - val_loss: 0.5949 - val_accuracy: 0.7347Epoch 30/1007/7 [==============================] - 0s 55ms/step - loss: 0.3049 - accuracy: 0.8601 - val_loss: 0.5904 - val_accuracy: 0.7347Epoch 31/1007/7 [==============================] - 0s 44ms/step - loss: 0.3150 - accuracy: 0.8705 - val_loss: 0.4901 - val_accuracy: 0.6531Epoch 32/1007/7 [==============================] - 0s 55ms/step - loss: 0.3223 - accuracy: 0.8446 - val_loss: 0.5034 - val_accuracy: 0.6939Epoch 33/1007/7 [==============================] - 0s 43ms/step - loss: 0.3178 - accuracy: 0.8394 - val_loss: 0.6359 - val_accuracy: 0.7347Epoch 34/1007/7 [==============================] - 0s 43ms/step - loss: 0.3041 - accuracy: 0.8549 - val_loss: 0.5558 - val_accuracy: 0.7551Epoch 35/1007/7 [==============================] - 0s 44ms/step - loss: 0.2853 - accuracy: 0.8808 - val_loss: 0.5089 - val_accuracy: 0.6939Epoch 36/1007/7 [==============================] - 0s 55ms/step - loss: 0.2905 - accuracy: 0.8653 - val_loss: 0.5989 - val_accuracy: 0.7347Epoch 37/1007/7 [==============================] - 0s 45ms/step - loss: 0.2885 - accuracy: 0.8705 - val_loss: 0.5644 - val_accuracy: 0.7551Epoch 38/1007/7 [==============================] - 0s 55ms/step - loss: 0.2890 - accuracy: 0.8601 - val_loss: 0.5590 - val_accuracy: 0.7755Epoch 39/1007/7 [==============================] - 0s 45ms/step - loss: 0.2792 - accuracy: 0.8808 - val_loss: 0.4820 - val_accuracy: 0.7551Epoch 40/1007/7 [==============================] - 0s 55ms/step - loss: 0.2781 - accuracy: 0.8653 - val_loss: 0.4974 - val_accuracy: 0.7551Epoch 41/1007/7 [==============================] - 0s 45ms/step - loss: 0.2873 - accuracy: 0.8705 - val_loss: 0.5550 - val_accuracy: 0.7551Epoch 42/1007/7 [==============================] - 0s 55ms/step - loss: 0.2737 - accuracy: 0.8808 - val_loss: 0.5356 - val_accuracy: 0.7347Epoch 43/1007/7 [==============================] - 0s 44ms/step - loss: 0.2677 - accuracy: 0.8860 - val_loss: 0.5071 - val_accuracy: 0.7551Epoch 44/1007/7 [==============================] - 0s 44ms/step - loss: 0.2794 - accuracy: 0.8756 - val_loss: 0.5320 - val_accuracy: 0.6939Epoch 45/1007/7 [==============================] - 0s 55ms/step - loss: 0.2932 - accuracy: 0.8394 - val_loss: 0.5533 - val_accuracy: 0.7755Epoch 46/1007/7 [==============================] - 0s 45ms/step - loss: 0.2750 - accuracy: 0.8705 - val_loss: 0.5723 - val_accuracy: 0.7347Epoch 47/1007/7 [==============================] - 0s 55ms/step - loss: 0.2694 - accuracy: 0.8808 - val_loss: 0.5347 - val_accuracy: 0.7551Epoch 48/1007/7 [==============================] - 0s 44ms/step - loss: 0.2632 - accuracy: 0.8912 - val_loss: 0.5369 - val_accuracy: 0.7755Epoch 49/1007/7 [==============================] - 0s 44ms/step - loss: 0.2677 - accuracy: 0.8860 - val_loss: 0.5837 - val_accuracy: 0.7143Epoch 50/1007/7 [==============================] - 0s 55ms/step - loss: 0.2635 - accuracy: 0.8808 - val_loss: 0.5337 - val_accuracy: 0.7755Epoch 51/1007/7 [==============================] - 0s 45ms/step - loss: 0.2592 - accuracy: 0.8912 - val_loss: 0.5533 - val_accuracy: 0.7755Epoch 52/1007/7 [==============================] - 0s 55ms/step - loss: 0.2536 - accuracy: 0.8912 - val_loss: 0.5743 - val_accuracy: 0.7347Epoch 53/1007/7 [==============================] - 0s 45ms/step - loss: 0.2511 - accuracy: 0.9016 - val_loss: 0.5451 - val_accuracy: 0.7551Epoch 54/1007/7 [==============================] - 0s 55ms/step - loss: 0.2650 - accuracy: 0.8860 - val_loss: 0.5864 - val_accuracy: 0.6531Epoch 55/1007/7 [==============================] - 0s 45ms/step - loss: 0.3354 - accuracy: 0.8290 - val_loss: 0.5772 - val_accuracy: 0.7347Epoch 56/1007/7 [==============================] - 0s 54ms/step - loss: 0.2759 - accuracy: 0.8653 - val_loss: 0.5857 - val_accuracy: 0.7347Epoch 57/1007/7 [==============================] - 0s 44ms/step - loss: 0.2522 - accuracy: 0.8860 - val_loss: 0.5930 - val_accuracy: 0.7347Epoch 58/1007/7 [==============================] - 0s 44ms/step - loss: 0.2488 - accuracy: 0.8808 - val_loss: 0.5814 - val_accuracy: 0.7551Epoch 59/1007/7 [==============================] - 0s 55ms/step - loss: 0.2428 - accuracy: 0.8964 - val_loss: 0.5805 - val_accuracy: 0.7551Epoch 60/1007/7 [==============================] - 0s 44ms/step - loss: 0.2555 - accuracy: 0.8912 - val_loss: 0.5903 - val_accuracy: 0.7347Epoch 61/1007/7 [==============================] - 0s 44ms/step - loss: 0.2391 - accuracy: 0.9016 - val_loss: 0.5721 - val_accuracy: 0.7755Epoch 62/1007/7 [==============================] - 0s 44ms/step - loss: 0.2423 - accuracy: 0.8860 - val_loss: 0.5911 - val_accuracy: 0.7551Epoch 63/1007/7 [==============================] - 0s 44ms/step - loss: 0.2450 - accuracy: 0.8756 - val_loss: 0.5845 - val_accuracy: 0.7755Epoch 64/1007/7 [==============================] - 0s 55ms/step - loss: 0.2447 - accuracy: 0.8912 - val_loss: 0.5883 - val_accuracy: 0.7551Epoch 65/1007/7 [==============================] - 0s 44ms/step - loss: 0.2386 - accuracy: 0.8964 - val_loss: 0.6093 - val_accuracy: 0.7551Epoch 66/1007/7 [==============================] - 0s 56ms/step - loss: 0.2278 - accuracy: 0.9067 - val_loss: 0.6654 - val_accuracy: 0.7347Epoch 67/1007/7 [==============================] - 0s 44ms/step - loss: 0.2474 - accuracy: 0.8912 - val_loss: 0.6545 - val_accuracy: 0.7143Epoch 68/1007/7 [==============================] - 0s 45ms/step - loss: 0.2509 - accuracy: 0.8808 - val_loss: 0.6298 - val_accuracy: 0.6735Epoch 69/1007/7 [==============================] - 0s 44ms/step - loss: 0.2931 - accuracy: 0.8549 - val_loss: 0.6237 - val_accuracy: 0.7347Epoch 70/1007/7 [==============================] - 0s 44ms/step - loss: 0.2653 - accuracy: 0.8808 - val_loss: 0.6296 - val_accuracy: 0.7143Epoch 71/1007/7 [==============================] - 0s 43ms/step - loss: 0.2649 - accuracy: 0.8549 - val_loss: 0.5915 - val_accuracy: 0.6531Epoch 72/1007/7 [==============================] - 0s 44ms/step - loss: 0.3141 - accuracy: 0.8394 - val_loss: 0.6017 - val_accuracy: 0.7755Epoch 73/1007/7 [==============================] - 0s 45ms/step - loss: 0.2557 - accuracy: 0.8756 - val_loss: 0.6444 - val_accuracy: 0.7347Epoch 74/1007/7 [==============================] - 0s 55ms/step - loss: 0.2220 - accuracy: 0.9067 - val_loss: 0.6380 - val_accuracy: 0.7347Epoch 75/1007/7 [==============================] - 0s 44ms/step - loss: 0.2209 - accuracy: 0.9016 - val_loss: 0.6977 - val_accuracy: 0.7347Epoch 76/1007/7 [==============================] - 0s 55ms/step - loss: 0.2318 - accuracy: 0.8964 - val_loss: 0.6422 - val_accuracy: 0.7347Epoch 77/1007/7 [==============================] - 0s 44ms/step - loss: 0.2183 - accuracy: 0.9067 - val_loss: 0.6183 - val_accuracy: 0.7143Epoch 78/1007/7 [==============================] - 0s 55ms/step - loss: 0.2304 - accuracy: 0.8860 - val_loss: 0.6522 - val_accuracy: 0.7143Epoch 79/1007/7 [==============================] - 0s 44ms/step - loss: 0.2338 - accuracy: 0.8756 - val_loss: 0.5959 - val_accuracy: 0.7551Epoch 80/1007/7 [==============================] - 0s 44ms/step - loss: 0.2250 - accuracy: 0.8964 - val_loss: 0.6232 - val_accuracy: 0.7551Epoch 81/1007/7 [==============================] - 0s 55ms/step - loss: 0.2275 - accuracy: 0.8912 - val_loss: 0.6500 - val_accuracy: 0.7551Epoch 82/1007/7 [==============================] - 0s 44ms/step - loss: 0.2053 - accuracy: 0.9016 - val_loss: 0.6249 - val_accuracy: 0.7347Epoch 83/1007/7 [==============================] - 0s 45ms/step - loss: 0.2250 - accuracy: 0.8964 - val_loss: 0.6744 - val_accuracy: 0.7347Epoch 84/1007/7 [==============================] - 0s 44ms/step - loss: 0.2109 - accuracy: 0.9067 - val_loss: 0.7039 - val_accuracy: 0.7347Epoch 85/1007/7 [==============================] - 0s 45ms/step - loss: 0.2171 - accuracy: 0.9016 - val_loss: 0.6693 - val_accuracy: 0.7347Epoch 86/1007/7 [==============================] - 0s 44ms/step - loss: 0.2187 - accuracy: 0.9067 - val_loss: 0.6765 - val_accuracy: 0.7143Epoch 87/1007/7 [==============================] - 0s 45ms/step - loss: 0.2225 - accuracy: 0.9067 - val_loss: 0.6637 - val_accuracy: 0.6939Epoch 88/1007/7 [==============================] - 0s 44ms/step - loss: 0.2193 - accuracy: 0.8808 - val_loss: 0.7029 - val_accuracy: 0.6735Epoch 89/1007/7 [==============================] - 0s 45ms/step - loss: 0.2644 - accuracy: 0.8653 - val_loss: 0.6829 - val_accuracy: 0.6939Epoch 90/1007/7 [==============================] - 0s 55ms/step - loss: 0.2625 - accuracy: 0.8601 - val_loss: 0.6617 - val_accuracy: 0.7347Epoch 91/1007/7 [==============================] - 0s 45ms/step - loss: 0.2206 - accuracy: 0.8860 - val_loss: 0.6889 - val_accuracy: 0.7551Epoch 92/1007/7 [==============================] - 0s 55ms/step - loss: 0.2090 - accuracy: 0.8964 - val_loss: 0.7322 - val_accuracy: 0.7347Epoch 93/1007/7 [==============================] - 0s 45ms/step - loss: 0.2016 - accuracy: 0.9119 - val_loss: 0.7244 - val_accuracy: 0.7347Epoch 94/1007/7 [==============================] - 0s 55ms/step - loss: 0.1933 - accuracy: 0.9067 - val_loss: 0.6788 - val_accuracy: 0.7347Epoch 95/1007/7 [==============================] - 0s 45ms/step - loss: 0.2002 - accuracy: 0.9171 - val_loss: 0.6849 - val_accuracy: 0.7143Epoch 96/1007/7 [==============================] - 0s 55ms/step - loss: 0.2138 - accuracy: 0.8964 - val_loss: 0.7610 - val_accuracy: 0.6939Epoch 97/1007/7 [==============================] - 0s 45ms/step - loss: 0.2225 - accuracy: 0.8912 - val_loss: 0.6998 - val_accuracy: 0.6939Epoch 98/1007/7 [==============================] - 0s 55ms/step - loss: 0.2089 - accuracy: 0.9067 - val_loss: 0.6846 - val_accuracy: 0.7143Epoch 99/1007/7 [==============================] - 0s 44ms/step - loss: 0.2043 - accuracy: 0.8964 - val_loss: 0.7292 - val_accuracy: 0.7347Epoch 100/1007/7 [==============================] - 0s 55ms/step - loss: 0.2008 - accuracy: 0.9016 - val_loss: 0.7064 - val_accuracy: 0.7143&lt;tensorflow.python.keras.callbacks.History at 0x7f33184937b8&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss, accuracy = model.evaluate(test_ds)</span><br><span class="line">print(<span class="string">"Accuracy"</span>, accuracy)</span><br></pre></td></tr></table></figure><pre><code>2/2 [==============================] - 1s 329ms/step - loss: 0.5511 - accuracy: 0.8197Accuracy 0.8196721</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Classify-Structured-Data&quot;&gt;&lt;a href=&quot;#Classify-Structured-Data&quot; class=&quot;headerlink&quot; title=&quot;Classify Structured Data&quot;&gt;&lt;/a&gt;Classify Struc
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
      <category term="Development" scheme="https://zhangruochi.com/categories/Development/"/>
    
    
      <category term="Tensorflow" scheme="https://zhangruochi.com/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow Data Service</title>
    <link href="https://zhangruochi.com/TensorFlow-Data-Service/2020/02/17/"/>
    <id>https://zhangruochi.com/TensorFlow-Data-Service/2020/02/17/</id>
    <published>2020-02-18T04:58:32.000Z</published>
    <updated>2020-02-24T21:34:49.429Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Extract-Transform-Load-ETL"><a href="#Extract-Transform-Load-ETL" class="headerlink" title="Extract - Transform - Load (ETL)"></a>Extract - Transform - Load (ETL)</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="ETL.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">ETL Pipeline</div></center><p>First, to perform the Extract process we use tfts.load. This handles everything from downloading the raw data to parsing and splitting it, giving us a dataset. Next, we perform the Transform process. In this simple example, our transform process will just consist of shuffling the dataset. Finally, we Load one record by using the take(1) method. In this case, each record consists of an image and its corresponding label. After loading the record we proceed to plot the image and print its corresponding label.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXTRACT</span></span><br><span class="line">dataset = tfds.load(name=<span class="string">"mnist"</span>, split=<span class="string">"train"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TRANSFORM</span></span><br><span class="line">dataset.shuffle(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># LOAD</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataset.take(<span class="number">1</span>):</span><br><span class="line">    image = data[<span class="string">"image"</span>].numpy().squeeze()</span><br><span class="line">    label = data[<span class="string">"label"</span>].numpy()</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"Label: &#123;&#125;"</span>.format(label))</span><br><span class="line">    plt.imshow(image, cmap=plt.cm.binary)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h2 id="US3-API-for-TensorFlow-datasets"><a href="#US3-API-for-TensorFlow-datasets" class="headerlink" title="US3 API for TensorFlow datasets."></a>US3 API for TensorFlow datasets.</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="s3.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">S3 API</div></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow_datasets <span class="keyword">as</span> tfds</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\u2022 Using TensorFlow Version:"</span>, tf.__version__)</span><br></pre></td></tr></table></figure><p>Before using the new S3 API, we must first find out whether the MNIST dataset implements the new S3 API. In the cell below we indicate that we want to use version <code>3.*.*</code> of the MNIST dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mnist_builder = tfds.builder(<span class="string">"mnist:3.*.*"</span>)</span><br><span class="line"></span><br><span class="line">print(mnist_builder.version.implements(tfds.core.Experiment.S3))</span><br></pre></td></tr></table></figure><p>We can see that the code above printed <code>True</code>, which means that version <code>3.*.*</code> of the MNIST dataset supports the new S3 API.</p><p>Now, let’s see how we can use the S3 API to download the MNIST dataset and specify the splits we want use. In the code below we download the <code>train</code> and <code>test</code> splits of the MNIST dataset and then we print their size. We will see that there are 60,000 records in the training set and 10,000 in the test set.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_ds, test_ds = tfds.load(<span class="string">'mnist:3.*.*'</span>, split=[<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line"></span><br><span class="line">print(len(list(train_ds)))</span><br><span class="line">print(len(list(test_ds)))</span><br></pre></td></tr></table></figure><p>In the S3 API we can use strings to specify the slicing instructions. For example, in the cell below we will merge the training and test sets by passing the string <code>’train+test&#39;</code> to the <code>split</code> argument.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">combined = tfds.load(<span class="string">'mnist:3.*.*'</span>, split=<span class="string">'train+test'</span>)</span><br><span class="line"></span><br><span class="line">print(len(list(combined)))</span><br></pre></td></tr></table></figure><p>We can also use Python style list slicers to specify the data we want. For example, we can specify that we want to take the first 10,000 records of the <code>train</code> split with the string <code>&#39;train[:10000]&#39;</code>, as shown below:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">first10k = tfds.load(<span class="string">'mnist:3.*.*'</span>, split=<span class="string">'train[:10000]'</span>)</span><br><span class="line"></span><br><span class="line">print(len(list(first10k)))</span><br></pre></td></tr></table></figure><p>The S3 API, also allows us to specify the percentage of the data we want to use. For example, we can select the first 20\% of the training set with the string <code>&#39;train[:20%]&#39;</code>, as shown below:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">first20p = tfds.load(<span class="string">'mnist:3.*.*'</span>, split=<span class="string">'train[:20%]'</span>)</span><br><span class="line"></span><br><span class="line">print(len(list(first20p)))</span><br></pre></td></tr></table></figure><p>We can see that <code>first20p</code> contains 12,000 records, which is indeed 20\% the total number of records in the training set. Recall that the training set contains 60,000 records. </p><p>Because the slices are string-based we can use loops, like the ones shown below, to slice up the dataset and make some pretty complex splits. For example, the loops below create 10 complimentary validation and training sets (each loop returns a list with 5 data sets).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val_ds = tfds.load(<span class="string">'mnist:3.*.*'</span>, split=[<span class="string">'train[&#123;&#125;%:&#123;&#125;%]'</span>.format(k, k+<span class="number">20</span>) <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">100</span>, <span class="number">20</span>)])</span><br><span class="line"></span><br><span class="line">train_ds = tfds.load(<span class="string">'mnist:3.*.*'</span>, split=[<span class="string">'train[:&#123;&#125;%]+train[&#123;&#125;%:]'</span>.format(k, k+<span class="number">20</span>) <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">100</span>, <span class="number">20</span>)])</span><br></pre></td></tr></table></figure><p>The S3 API also allows us to compose new datasets by using pieces from different splits. For example, we can create a new dataset from the first 10\% of the test set and the last 80\% of the training set, as shown below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">composed_ds = tfds.load(<span class="string">'mnist:3.*.*'</span>, split=<span class="string">'test[:10%]+train[-80%:]'</span>)</span><br><span class="line"></span><br><span class="line">print(len(list(composed_ds)))</span><br></pre></td></tr></table></figure><h1 id="Pipeline-for-Classifing-Structured-Data"><a href="#Pipeline-for-Classifing-Structured-Data" class="headerlink" title="Pipeline for Classifing Structured Data"></a>Pipeline for Classifing Structured Data</h1><h2 id="Import-TensorFlow-and-Other-Libraries"><a href="#Import-TensorFlow-and-Other-Libraries" class="headerlink" title="Import TensorFlow and Other Libraries"></a>Import TensorFlow and Other Libraries</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> feature_column</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> getcwd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure><h2 id="Use-Pandas-to-Create-a-Dataframe"><a href="#Use-Pandas-to-Create-a-Dataframe" class="headerlink" title="Use Pandas to Create a Dataframe"></a>Use Pandas to Create a Dataframe</h2><p><a href="https://pandas.pydata.org/" target="_blank" rel="noopener">Pandas</a> is a Python library with many helpful utilities for loading and working with structured data. We will use Pandas to download the dataset and load it into a dataframe.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">filePath = <span class="string">f"<span class="subst">&#123;getcwd()&#125;</span>/../tmp2/heart.csv"</span></span><br><span class="line">dataframe = pd.read_csv(filePath)</span><br><span class="line">dataframe.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>age</th>      <th>sex</th>      <th>cp</th>      <th>trestbps</th>      <th>chol</th>      <th>fbs</th>      <th>restecg</th>      <th>thalach</th>      <th>exang</th>      <th>oldpeak</th>      <th>slope</th>      <th>ca</th>      <th>thal</th>      <th>target</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>63</td>      <td>1</td>      <td>1</td>      <td>145</td>      <td>233</td>      <td>1</td>      <td>2</td>      <td>150</td>      <td>0</td>      <td>2.3</td>      <td>3</td>      <td>0</td>      <td>fixed</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>67</td>      <td>1</td>      <td>4</td>      <td>160</td>      <td>286</td>      <td>0</td>      <td>2</td>      <td>108</td>      <td>1</td>      <td>1.5</td>      <td>2</td>      <td>3</td>      <td>normal</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>67</td>      <td>1</td>      <td>4</td>      <td>120</td>      <td>229</td>      <td>0</td>      <td>2</td>      <td>129</td>      <td>1</td>      <td>2.6</td>      <td>2</td>      <td>2</td>      <td>reversible</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>37</td>      <td>1</td>      <td>3</td>      <td>130</td>      <td>250</td>      <td>0</td>      <td>0</td>      <td>187</td>      <td>0</td>      <td>3.5</td>      <td>3</td>      <td>0</td>      <td>normal</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>41</td>      <td>0</td>      <td>2</td>      <td>130</td>      <td>204</td>      <td>0</td>      <td>2</td>      <td>172</td>      <td>0</td>      <td>1.4</td>      <td>1</td>      <td>0</td>      <td>normal</td>      <td>0</td>    </tr>  </tbody></table></div><h2 id="Split-the-Dataframe-Into-Train-Validation-and-Test-Sets"><a href="#Split-the-Dataframe-Into-Train-Validation-and-Test-Sets" class="headerlink" title="Split the Dataframe Into Train, Validation, and Test Sets"></a>Split the Dataframe Into Train, Validation, and Test Sets</h2><p>The dataset we downloaded was a single CSV file. We will split this into train, validation, and test sets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train, test = train_test_split(dataframe, test_size=<span class="number">0.2</span>)</span><br><span class="line">train, val = train_test_split(train, test_size=<span class="number">0.2</span>)</span><br><span class="line">print(len(train), <span class="string">'train examples'</span>)</span><br><span class="line">print(len(val), <span class="string">'validation examples'</span>)</span><br><span class="line">print(len(test), <span class="string">'test examples'</span>)</span><br></pre></td></tr></table></figure><pre><code>193 train examples49 validation examples61 test examples</code></pre><h2 id="Create-an-Input-Pipeline-Using-tf-data"><a href="#Create-an-Input-Pipeline-Using-tf-data" class="headerlink" title="Create an Input Pipeline Using tf.data"></a>Create an Input Pipeline Using <code>tf.data</code></h2><p>Next, we will wrap the dataframes with <a href="https://www.tensorflow.org/guide/datasets" target="_blank" rel="noopener">tf.data</a>. This will enable us  to use feature columns as a bridge to map from the columns in the Pandas dataframe to features used to train the model. If we were working with a very large CSV file (so large that it does not fit into memory), we would use tf.data to read it from disk directly.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: A utility method to create a tf.data dataset from a Pandas Dataframe.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">df_to_dataset</span><span class="params">(dataframe, shuffle=True, batch_size=<span class="number">32</span>)</span>:</span></span><br><span class="line">    dataframe = dataframe.copy()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use Pandas dataframe's pop method to get the list of targets.</span></span><br><span class="line">    labels = dataframe[<span class="string">"target"</span>].values</span><br><span class="line">    dataframe.drop(<span class="string">"target"</span>, axis = <span class="number">1</span>, inplace = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a tf.data.Dataset from the dataframe and labels.</span></span><br><span class="line">    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe),labels))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        <span class="comment"># Shuffle dataset.</span></span><br><span class="line">        ds = ds.shuffle(<span class="number">3</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Batch dataset with specified batch_size parameter.</span></span><br><span class="line">    ds = ds.batch(batch_size)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">5</span> <span class="comment"># A small batch sized is used for demonstration purposes</span></span><br><span class="line">train_ds = df_to_dataset(train, batch_size=batch_size)</span><br><span class="line">val_ds = df_to_dataset(val, shuffle=<span class="keyword">False</span>, batch_size=batch_size)</span><br><span class="line">test_ds = df_to_dataset(test, shuffle=<span class="keyword">False</span>, batch_size=batch_size)</span><br></pre></td></tr></table></figure><h2 id="Understand-the-Input-Pipeline"><a href="#Understand-the-Input-Pipeline" class="headerlink" title="Understand the Input Pipeline"></a>Understand the Input Pipeline</h2><p>Now that we have created the input pipeline, let’s call it to see the format of the data it returns. We have used a small batch size to keep the output readable.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> feature_batch, label_batch <span class="keyword">in</span> train_ds.take(<span class="number">1</span>):</span><br><span class="line">    print(<span class="string">'Every feature:'</span>, list(feature_batch.keys()))</span><br><span class="line">    print(<span class="string">'A batch of ages:'</span>, feature_batch[<span class="string">'age'</span>])</span><br><span class="line">    print(<span class="string">'A batch of targets:'</span>, label_batch )</span><br></pre></td></tr></table></figure><pre><code>Every feature: [&#39;age&#39;, &#39;sex&#39;, &#39;cp&#39;, &#39;trestbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalach&#39;, &#39;exang&#39;, &#39;oldpeak&#39;, &#39;slope&#39;, &#39;ca&#39;, &#39;thal&#39;]A batch of ages: tf.Tensor([51 63 64 58 57], shape=(5,), dtype=int32)A batch of targets: tf.Tensor([0 1 0 0 0], shape=(5,), dtype=int64)</code></pre><p>We can see that the dataset returns a dictionary of column names (from the dataframe) that map to column values from rows in the dataframe.</p><h2 id="Create-Several-Types-of-Feature-Columns"><a href="#Create-Several-Types-of-Feature-Columns" class="headerlink" title="Create Several Types of Feature Columns"></a>Create Several Types of Feature Columns</h2><p>TensorFlow provides many types of feature columns. In this section, we will create several types of feature columns, and demonstrate how they transform a column from the dataframe.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Try to demonstrate several types of feature columns by getting an example.</span></span><br><span class="line">example_batch = next(iter(train_ds))[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># A utility method to create a feature column and to transform a batch of data.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">(feature_column)</span>:</span></span><br><span class="line">    feature_layer = layers.DenseFeatures(feature_column, dtype=<span class="string">'float64'</span>)</span><br><span class="line">    print(feature_layer(example_batch).numpy())</span><br></pre></td></tr></table></figure><h3 id="Numeric-Columns"><a href="#Numeric-Columns" class="headerlink" title="Numeric Columns"></a>Numeric Columns</h3><p>The output of a feature column becomes the input to the model (using the demo function defined above, we will be able to see exactly how each column from the dataframe is transformed). A <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column" target="_blank" rel="noopener">numeric column</a> is the simplest type of column. It is used to represent real valued features. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create a numeric feature column out of 'age' and demo it.</span></span><br><span class="line">age = tf.feature_column.numeric_column(<span class="string">"age"</span>)</span><br><span class="line"></span><br><span class="line">demo(age)</span><br></pre></td></tr></table></figure><pre><code>[[51.] [58.] [63.] [64.] [60.]]</code></pre><p>In the heart disease dataset, most columns from the dataframe are numeric.</p><h3 id="Bucketized-Columns"><a href="#Bucketized-Columns" class="headerlink" title="Bucketized Columns"></a>Bucketized Columns</h3><p>Often, you don’t want to feed a number directly into the model, but instead split its value into different categories based on numerical ranges. Consider raw data that represents a person’s age. Instead of representing age as a numeric column, we could split the age into several buckets using a <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column" target="_blank" rel="noopener">bucketized column</a>. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create a bucketized feature column out of 'age' with</span></span><br><span class="line"><span class="comment"># the following boundaries and demo it.</span></span><br><span class="line">boundaries = [<span class="number">18</span>, <span class="number">25</span>, <span class="number">30</span>, <span class="number">35</span>, <span class="number">40</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">55</span>, <span class="number">60</span>, <span class="number">65</span>]</span><br><span class="line"></span><br><span class="line">age_buckets = tf.feature_column.bucketized_column(age, boundaries = boundaries)</span><br><span class="line"></span><br><span class="line">demo(age_buckets)</span><br></pre></td></tr></table></figure><pre><code>[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]</code></pre><p>Notice the one-hot values above describe which age range each row matches.</p><h3 id="Categorical-Columns"><a href="#Categorical-Columns" class="headerlink" title="Categorical Columns"></a>Categorical Columns</h3><p>In this dataset, thal is represented as a string (e.g. ‘fixed’, ‘normal’, or ‘reversible’). We cannot feed strings directly to a model. Instead, we must first map them to numeric values. The categorical vocabulary columns provide a way to represent strings as a one-hot vector (much like you have seen above with age buckets). </p><p><strong>Note</strong>: You will probably see some warning messages when running some of the code cell below. These warnings have to do with software updates and should not cause any errors or prevent your code from running.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create a categorical vocabulary column out of the</span></span><br><span class="line"><span class="comment"># above mentioned categories with the key specified as 'thal'.</span></span><br><span class="line">thal = tf.feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">      <span class="string">'thal'</span>, [<span class="string">'fixed'</span>, <span class="string">'normal'</span>, <span class="string">'reversible'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># EXERCISE: Create an indicator column out of the created categorical column.</span></span><br><span class="line">thal_one_hot = tf.feature_column.indicator_column(thal)</span><br><span class="line"></span><br><span class="line">demo(thal_one_hot)</span><br></pre></td></tr></table></figure><pre><code>[[0. 1. 0.] [0. 1. 0.] [0. 0. 1.] [0. 0. 1.] [0. 1. 0.]]</code></pre><p>The vocabulary can be passed as a list using <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list" target="_blank" rel="noopener">categorical_column_with_vocabulary_list</a>, or loaded from a file using <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file" target="_blank" rel="noopener">categorical_column_with_vocabulary_file</a>.</p><h3 id="Embedding-Columns"><a href="#Embedding-Columns" class="headerlink" title="Embedding Columns"></a>Embedding Columns</h3><p>Suppose instead of having just a few possible strings, we have thousands (or more) values per category. For a number of reasons, as the number of categories grow large, it becomes infeasible to train a neural network using one-hot encodings. We can use an embedding column to overcome this limitation. Instead of representing the data as a one-hot vector of many dimensions, an <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column" target="_blank" rel="noopener">embedding column</a> represents that data as a lower-dimensional, dense vector in which each cell can contain any number, not just 0 or 1. You can tune the size of the embedding with the <code>dimension</code> parameter.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create an embedding column out of the categorical</span></span><br><span class="line"><span class="comment"># vocabulary you just created (thal). Set the size of the </span></span><br><span class="line"><span class="comment"># embedding to 8, by using the dimension parameter.</span></span><br><span class="line"></span><br><span class="line">thal_embedding = tf.feature_column.embedding_column(thal, dimension=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">demo(thal_embedding)</span><br></pre></td></tr></table></figure><pre><code>[[-1.4254066e-01 -1.0374661e-01  3.4352791e-01 -3.3996427e-01  -3.2193713e-02 -1.8381193e-01 -1.8051244e-01  3.2638407e-01] [-1.4254066e-01 -1.0374661e-01  3.4352791e-01 -3.3996427e-01  -3.2193713e-02 -1.8381193e-01 -1.8051244e-01  3.2638407e-01] [-6.5549983e-05  2.7680036e-01  4.1849682e-01  5.3418136e-01  -1.6281548e-01  2.5406811e-01  8.8969752e-02  1.8004593e-01] [-6.5549983e-05  2.7680036e-01  4.1849682e-01  5.3418136e-01  -1.6281548e-01  2.5406811e-01  8.8969752e-02  1.8004593e-01] [-1.4254066e-01 -1.0374661e-01  3.4352791e-01 -3.3996427e-01  -3.2193713e-02 -1.8381193e-01 -1.8051244e-01  3.2638407e-01]]</code></pre><h3 id="Hashed-Feature-Columns"><a href="#Hashed-Feature-Columns" class="headerlink" title="Hashed Feature Columns"></a>Hashed Feature Columns</h3><p>Another way to represent a categorical column with a large number of values is to use a <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket" target="_blank" rel="noopener">categorical_column_with_hash_bucket</a>. This feature column calculates a hash value of the input, then selects one of the <code>hash_bucket_size</code> buckets to encode a string. When using this column, you do not need to provide the vocabulary, and you can choose to make the number of hash buckets significantly smaller than the number of actual categories to save space.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create a hashed feature column with 'thal' as the key and </span></span><br><span class="line"><span class="comment"># 1000 hash buckets.</span></span><br><span class="line">thal_hashed = tf.feature_column.categorical_column_with_hash_bucket(</span><br><span class="line">      <span class="string">'thal'</span>, hash_bucket_size=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">demo(feature_column.indicator_column(thal_hashed))</span><br></pre></td></tr></table></figure><pre><code>[[0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]]</code></pre><h3 id="Crossed-Feature-Columns"><a href="#Crossed-Feature-Columns" class="headerlink" title="Crossed Feature Columns"></a>Crossed Feature Columns</h3><p>Combining features into a single feature, better known as <a href="https://developers.google.com/machine-learning/glossary/#feature_cross" target="_blank" rel="noopener">feature crosses</a>, enables a model to learn separate weights for each combination of features. Here, we will create a new feature that is the cross of age and thal. Note that <code>crossed_column</code> does not build the full table of all possible combinations (which could be very large). Instead, it is backed by a <code>hashed_column</code>, so you can choose how large the table is.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create a crossed column using the bucketized column (age_buckets),</span></span><br><span class="line"><span class="comment"># the categorical vocabulary column (thal) previously created, and 1000 hash buckets.</span></span><br><span class="line">crossed_feature = tf.feature_column.crossed_column([age_buckets, thal], hash_bucket_size=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">demo(feature_column.indicator_column(crossed_feature))</span><br></pre></td></tr></table></figure><pre><code>[[0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]]</code></pre><h2 id="Choose-Which-Columns-to-Use"><a href="#Choose-Which-Columns-to-Use" class="headerlink" title="Choose Which Columns to Use"></a>Choose Which Columns to Use</h2><p>We have seen how to use several types of feature columns. Now we will use them to train a model. The goal of this exercise is to show you the complete code needed to work with feature columns. We have selected a few columns to train our model below arbitrarily.</p><p>If your aim is to build an accurate model, try a larger dataset of your own, and think carefully about which features are the most meaningful to include, and how they should be represented.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataframe.dtypes</span><br></pre></td></tr></table></figure><pre><code>age           int64sex           int64cp            int64trestbps      int64chol          int64fbs           int64restecg       int64thalach       int64exang         int64oldpeak     float64slope         int64ca            int64thal         objecttarget        int64dtype: object</code></pre><p>You can use the above list of column datatypes to map the appropriate feature column to every column in the dataframe.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Fill in the missing code below</span></span><br><span class="line">feature_columns = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Numeric Cols.</span></span><br><span class="line"><span class="comment"># Create a list of numeric columns. Use the following list of columns</span></span><br><span class="line"><span class="comment"># that have a numeric datatype: ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca'].</span></span><br><span class="line">numeric_columns = [<span class="string">'age'</span>, <span class="string">'trestbps'</span>, <span class="string">'chol'</span>, <span class="string">'thalach'</span>, <span class="string">'oldpeak'</span>, <span class="string">'slope'</span>, <span class="string">'ca'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> header <span class="keyword">in</span> numeric_columns:</span><br><span class="line">    <span class="comment"># Create a numeric feature column  out of the header.</span></span><br><span class="line">    numeric_feature_column = tf.feature_column.numeric_column(header)</span><br><span class="line">    </span><br><span class="line">    feature_columns.append(numeric_feature_column)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bucketized Cols.</span></span><br><span class="line"><span class="comment"># Create a bucketized feature column out of the age column (numeric column)</span></span><br><span class="line"><span class="comment"># that you've already created. Use the following boundaries:</span></span><br><span class="line"><span class="comment"># [18, 25, 30, 35, 40, 45, 50, 55, 60, 65]</span></span><br><span class="line">age_buckets = tf.feature_column.bucketized_column(age, boundaries = [<span class="number">18</span>, <span class="number">25</span>, <span class="number">30</span>, <span class="number">35</span>, <span class="number">40</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">55</span>, <span class="number">60</span>, <span class="number">65</span>] )</span><br><span class="line"></span><br><span class="line">feature_columns.append(age_buckets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Indicator Cols.</span></span><br><span class="line"><span class="comment"># Create a categorical vocabulary column out of the categories</span></span><br><span class="line"><span class="comment"># ['fixed', 'normal', 'reversible'] with the key specified as 'thal'.</span></span><br><span class="line">thal = feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">      <span class="string">'thal'</span>, [<span class="string">'fixed'</span>, <span class="string">'normal'</span>, <span class="string">'reversible'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an indicator column out of the created thal categorical column</span></span><br><span class="line">thal_one_hot = feature_column.indicator_column(thal)</span><br><span class="line"></span><br><span class="line">feature_columns.append(thal_one_hot)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Embedding Cols.</span></span><br><span class="line"><span class="comment"># Create an embedding column out of the categorical vocabulary you</span></span><br><span class="line"><span class="comment"># just created (thal). Set the size of the embedding to 8, by using</span></span><br><span class="line"><span class="comment"># the dimension parameter.</span></span><br><span class="line">thal_embedding = tf.feature_column.embedding_column(thal, dimension=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">feature_columns.append(thal_embedding)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Crossed Cols.</span></span><br><span class="line"><span class="comment"># Create a crossed column using the bucketized column (age_buckets),</span></span><br><span class="line"><span class="comment"># the categorical vocabulary column (thal) previously created, and 1000 hash buckets.</span></span><br><span class="line">crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an indicator column out of the crossed column created above to one-hot encode it.</span></span><br><span class="line">crossed_feature = feature_column.indicator_column(crossed_feature)</span><br><span class="line"></span><br><span class="line">feature_columns.append(crossed_feature)</span><br></pre></td></tr></table></figure><h3 id="Create-a-Feature-Layer"><a href="#Create-a-Feature-Layer" class="headerlink" title="Create a Feature Layer"></a>Create a Feature Layer</h3><p>Now that we have defined our feature columns, we will use a <a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures" target="_blank" rel="noopener">DenseFeatures</a> layer to input them to our Keras model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Create a Keras DenseFeatures layer and pass the feature_columns you just created.</span></span><br><span class="line">feature_layer = tf.keras.layers.DenseFeatures(feature_columns)</span><br></pre></td></tr></table></figure><p>Earlier, we used a small batch size to demonstrate how feature columns worked. We create a new input pipeline with a larger batch size.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">train_ds = df_to_dataset(train, batch_size=batch_size)</span><br><span class="line">val_ds = df_to_dataset(val, shuffle=<span class="keyword">False</span>, batch_size=batch_size)</span><br><span class="line">test_ds = df_to_dataset(test, shuffle=<span class="keyword">False</span>, batch_size=batch_size)</span><br></pre></td></tr></table></figure><h2 id="Create-Compile-and-Train-the-Model"><a href="#Create-Compile-and-Train-the-Model" class="headerlink" title="Create, Compile, and Train the Model"></a>Create, Compile, and Train the Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.Sequential([</span><br><span class="line">        feature_layer,</span><br><span class="line">        layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(train_ds,</span><br><span class="line">          validation_data=val_ds,</span><br><span class="line">          epochs=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><pre><code>......7/7 [==============================] - 0s 45ms/step - loss: 0.2225 - accuracy: 0.8912 - val_loss: 0.6998 - val_accuracy: 0.6939Epoch 98/1007/7 [==============================] - 0s 55ms/step - loss: 0.2089 - accuracy: 0.9067 - val_loss: 0.6846 - val_accuracy: 0.7143Epoch 99/1007/7 [==============================] - 0s 44ms/step - loss: 0.2043 - accuracy: 0.8964 - val_loss: 0.7292 - val_accuracy: 0.7347Epoch 100/1007/7 [==============================] - 0s 55ms/step - loss: 0.2008 - accuracy: 0.9016 - val_loss: 0.7064 - val_accuracy: 0.7143&lt;tensorflow.python.keras.callbacks.History at 0x7f33184937b8&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss, accuracy = model.evaluate(test_ds)</span><br><span class="line">print(<span class="string">"Accuracy"</span>, accuracy)</span><br></pre></td></tr></table></figure><pre><code>2/2 [==============================] - 1s 329ms/step - loss: 0.5511 - accuracy: 0.8197Accuracy 0.8196721</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Extract-Transform-Load-ETL&quot;&gt;&lt;a href=&quot;#Extract-Transform-Load-ETL&quot; class=&quot;headerlink&quot; title=&quot;Extract - Transform - Load (ETL)&quot;&gt;&lt;/a&gt;Ex
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Tensorflow" scheme="https://zhangruochi.com/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Use Tensorflow Lite to do Inference on Raspberry Pi</title>
    <link href="https://zhangruochi.com/Use-Tensorflow-Lite-to-do-Inference-on-Raspberry-Pi/2020/02/17/"/>
    <id>https://zhangruochi.com/Use-Tensorflow-Lite-to-do-Inference-on-Raspberry-Pi/2020/02/17/</id>
    <published>2020-02-18T01:30:15.000Z</published>
    <updated>2020-02-24T21:34:55.859Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Image-Classification-in-Raspberry-Pi"><a href="#Image-Classification-in-Raspberry-Pi" class="headerlink" title="Image Classification in Raspberry Pi"></a>Image Classification in Raspberry Pi</h1><h2 id="Building-TensorFlow-Lite"><a href="#Building-TensorFlow-Lite" class="headerlink" title="Building TensorFlow Lite"></a>Building TensorFlow Lite</h2><h3 id="Cross-Compile"><a href="#Cross-Compile" class="headerlink" title="Cross Compile"></a>Cross Compile</h3><p>We recommend cross-compiling the TensorFlow Raspbian package. Cross-compilation is using a different platform to build the package than deploy to. Instead of using the Raspberry Pi’s limited RAM and comparatively slow processor, it’s easier to build TensorFlow on a more powerful host machine running Linux, macOS, or Windows. You can see detailed instructions <a href="https://www.tensorflow.org/install/source_rpi" target="_blank" rel="noopener">here</a>.</p><h2 id="Install-just-the-TensorFlow-Lite-interpreter"><a href="#Install-just-the-TensorFlow-Lite-interpreter" class="headerlink" title="Install just the TensorFlow Lite interpreter"></a>Install just the TensorFlow Lite interpreter</h2><p>To quickly start executing TensorFlow Lite models with Python, you can install just the TensorFlow Lite interpreter, instead of all TensorFlow packages.</p><p>This interpreter-only package is a fraction the size of the full TensorFlow package and includes the bare minimum code required to run inferences with TensorFlow Lite—it includes only the <code>tf.lite.Interpreter</code> Python class. This small package is ideal when all you want to do is execute .tflite models and avoid wasting disk space with the large TensorFlow library.</p><h3 id="Install-from-pip"><a href="#Install-from-pip" class="headerlink" title="Install from pip"></a>Install from pip</h3><p>To install just the interpreter, download the appropriate Python wheel for your system from the following <a href="https://www.tensorflow.org/lite/guide/python" target="_blank" rel="noopener">link</a>, and then install it with the <code>pip install</code> command.</p><p>For example, if you’re setting up a Raspberry Pi Model B (using Raspbian Stretch, which has Python 3.5), install the Python wheel as follows (after you click to download the <code>.whl</code> file in the provided link):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tflite_runtime-1.14.0-cp35-cp35m-linux_armv7l.whl</span><br></pre></td></tr></table></figure><h3 id="Running-inference"><a href="#Running-inference" class="headerlink" title="Running inference"></a>Running inference</h3><p>So instead of importing Interpreter from the tensorflow module, you need to import it from tflite_runtime.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from tflite_runtime.interpreter import Interpreter</span><br></pre></td></tr></table></figure></p><h2 id="Additional-notes"><a href="#Additional-notes" class="headerlink" title="Additional notes"></a>Additional notes</h2><p>In case you have built TensorFlow from source, you need to import the Interpreter as follows:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.lite.python.interpreter import Interpreter</span><br></pre></td></tr></table></figure></p><h2 id="Inference-on-Pi"><a href="#Inference-on-Pi" class="headerlink" title="Inference on Pi"></a>Inference on Pi</h2><p>To get started, download the pretrained model along with its label file.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_quant_and_labels.zip</span><br><span class="line">unzip mobilenet_v1_1.0_224_quant_and_labels</span><br></pre></td></tr></table></figure><h3 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h3><p>To install the Python dependencies, run:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install numpy</span><br><span class="line">pip install Pillow</span><br></pre></td></tr></table></figure><p>Next, to run the code on Raspberry Pi, use <code>classify.py</code> as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 classify.py --filename dog.jpg --model_path mobilenet_v1_1.0_224_quant.tflite --label_path labels_mobilenet_quant_v1_224.txt</span><br></pre></td></tr></table></figure><blockquote><p>classify.py</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tflite_runtime.interpreter <span class="keyword">import</span> Interpreter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'Image Classification'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--filename'</span>, type=str, help=<span class="string">'Specify the filename'</span>, required=<span class="keyword">True</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--model_path'</span>, type=str, help=<span class="string">'Specify the model path'</span>, required=<span class="keyword">True</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--label_path'</span>, type=str, help=<span class="string">'Specify the label map'</span>, required=<span class="keyword">True</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--top_k'</span>, type=int, help=<span class="string">'How many top results'</span>, default=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">filename = args.filename</span><br><span class="line">model_path = args.model_path </span><br><span class="line">label_path = args.label_path </span><br><span class="line">top_k_results = args.top_k</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(label_path, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    labels = list(map(str.strip, f.readlines()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load TFLite model and allocate tensors</span></span><br><span class="line">interpreter = Interpreter(model_path=model_path)</span><br><span class="line">interpreter.allocate_tensors()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get input and output tensors.</span></span><br><span class="line">input_details = interpreter.get_input_details()</span><br><span class="line">output_details = interpreter.get_output_details()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read image</span></span><br><span class="line">img = Image.open(filename).convert(<span class="string">'RGB'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get input size</span></span><br><span class="line">input_shape = input_details[<span class="number">0</span>][<span class="string">'shape'</span>]</span><br><span class="line">size = input_shape[:<span class="number">2</span>] <span class="keyword">if</span> len(input_shape) == <span class="number">3</span> <span class="keyword">else</span> input_shape[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocess image</span></span><br><span class="line">img = img.resize(size)</span><br><span class="line">img = np.array(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add a batch dimension</span></span><br><span class="line">input_data = np.expand_dims(img, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Point the data to be used for testing and run the interpreter</span></span><br><span class="line">interpreter.set_tensor(input_details[<span class="number">0</span>][<span class="string">'index'</span>], input_data)</span><br><span class="line">interpreter.invoke()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Obtain results and map them to the classes</span></span><br><span class="line">predictions = interpreter.get_tensor(output_details[<span class="number">0</span>][<span class="string">'index'</span>])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get indices of the top k results</span></span><br><span class="line">top_k_indices = np.argsort(predictions)[::<span class="number">-1</span>][:top_k_results]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(top_k_results):</span><br><span class="line">    print(labels[top_k_indices[i]], predictions[top_k_indices[i]] / <span class="number">255.0</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Image-Classification-in-Raspberry-Pi&quot;&gt;&lt;a href=&quot;#Image-Classification-in-Raspberry-Pi&quot; class=&quot;headerlink&quot; title=&quot;Image Classification
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
      <category term="Development" scheme="https://zhangruochi.com/categories/Development/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Tensorflow" scheme="https://zhangruochi.com/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow Lite: Model Converter</title>
    <link href="https://zhangruochi.com/Tensorflow-Lite-Model-Converter/2020/02/16/"/>
    <id>https://zhangruochi.com/Tensorflow-Lite-Model-Converter/2020/02/16/</id>
    <published>2020-02-16T09:39:52.000Z</published>
    <updated>2020-02-24T21:35:03.325Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Copyright-2018-The-TensorFlow-Authors"><a href="#Copyright-2018-The-TensorFlow-Authors" class="headerlink" title="Copyright 2018 The TensorFlow Authors."></a>Copyright 2018 The TensorFlow Authors.</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ATTENTION: Please do not alter any of the provided code in the exercise. Only add your own code where indicated</span></span><br><span class="line"><span class="comment"># ATTENTION: Please do not add or remove any cells in the exercise. The grader will check specific cells based on the cell position.</span></span><br><span class="line"><span class="comment"># ATTENTION: Please use the provided epoch values when training.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Licensed under the Apache License, Version 2.0 (the "License");</span></span><br><span class="line"><span class="comment"># you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment"># You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># https://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br></pre></td></tr></table></figure><h1 id="Train-Your-Own-Model-and-Convert-It-to-TFLite"><a href="#Train-Your-Own-Model-and-Convert-It-to-TFLite" class="headerlink" title="Train Your Own Model and Convert It to TFLite"></a>Train Your Own Model and Convert It to TFLite</h1><p>This notebook uses the <a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank" rel="noopener">Fashion MNIST</a> dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:</p><table>  <tr><td>    <img src="https://tensorflow.org/images/fashion-mnist-sprite.png" alt="Fashion MNIST sprite" width="600">  </td></tr>  <tr><td align="center">    <b>Figure 1.</b> <a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank" rel="noopener">Fashion-MNIST samples</a> (by Zalando, MIT License).<br>&nbsp;  </td></tr></table><p>Fashion MNIST is intended as a drop-in replacement for the classic <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST</a> dataset—often used as the “Hello, World” of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc.) in a format identical to that of the articles of clothing we’ll use here.</p><p>This uses Fashion MNIST for variety, and because it’s a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They’re good starting points to test and debug code.</p><p>We will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow. Import and load the Fashion MNIST data directly from TensorFlow:</p><h1 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TensorFlow</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorFlow Datsets</span></span><br><span class="line"><span class="keyword">import</span> tensorflow_datasets <span class="keyword">as</span> tfds</span><br><span class="line">tfds.disable_progress_bar()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Helper Libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pathlib</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> getcwd</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\u2022 Using TensorFlow Version:'</span>, tf.__version__)</span><br><span class="line">print(<span class="string">'\u2022 GPU Device Found.'</span> <span class="keyword">if</span> tf.test.is_gpu_available() <span class="keyword">else</span> <span class="string">'\u2022 GPU Device Not Found. Running on CPU'</span>)</span><br></pre></td></tr></table></figure><pre><code>• Using TensorFlow Version: 2.0.0• GPU Device Found.</code></pre><h1 id="Download-Fashion-MNIST-Dataset"><a href="#Download-Fashion-MNIST-Dataset" class="headerlink" title="Download Fashion MNIST Dataset"></a>Download Fashion MNIST Dataset</h1><p>We will use TensorFlow Datasets to load the Fashion MNIST dataset. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">splits = tfds.Split.ALL.subsplit(weighted=(<span class="number">80</span>, <span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">filePath = <span class="string">f"<span class="subst">&#123;getcwd()&#125;</span>/../tmp2/"</span></span><br><span class="line">splits, info = tfds.load(<span class="string">'fashion_mnist'</span>, with_info=<span class="keyword">True</span>, as_supervised=<span class="keyword">True</span>, split=splits, data_dir=filePath)</span><br><span class="line"></span><br><span class="line">(train_examples, validation_examples, test_examples) = splits</span><br><span class="line"></span><br><span class="line">num_examples = info.splits[<span class="string">'train'</span>].num_examples</span><br><span class="line">num_classes = info.features[<span class="string">'label'</span>].num_classes</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample = next(iter(train_examples))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample[<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure><pre><code>TensorShape([28, 28, 1])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><pre><code>&lt;tf.Tensor: id=490, shape=(), dtype=int64, numpy=6&gt;</code></pre><p>The class names are not included with the dataset, so we will specify them here.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class_names = [<span class="string">'T-shirt_top'</span>, <span class="string">'Trouser'</span>, <span class="string">'Pullover'</span>, <span class="string">'Dress'</span>, <span class="string">'Coat'</span>,</span><br><span class="line">               <span class="string">'Sandal'</span>, <span class="string">'Shirt'</span>, <span class="string">'Sneaker'</span>, <span class="string">'Bag'</span>, <span class="string">'Ankle boot'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a labels.txt file with the class names</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'labels.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'\n'</span>.join(class_names))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The images in the dataset are 28 by 28 pixels.</span></span><br><span class="line">IMG_SIZE = <span class="number">28</span></span><br></pre></td></tr></table></figure><h1 id="Preprocessing-Data"><a href="#Preprocessing-Data" class="headerlink" title="Preprocessing Data"></a>Preprocessing Data</h1><h2 id="Preprocess"><a href="#Preprocess" class="headerlink" title="Preprocess"></a>Preprocess</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Write a function to normalize the images.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">format_example</span><span class="params">(image, label)</span>:</span></span><br><span class="line">    <span class="comment"># Cast image to float32</span></span><br><span class="line">    image = tf.cast(image, tf.float32)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Normalize the image in the range [0, 1]</span></span><br><span class="line">    image = image / <span class="number">255.0</span></span><br><span class="line">    </span><br><span class="line">    label = tf.one_hot(label, num_classes)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Specify the batch size</span></span><br><span class="line">BATCH_SIZE = <span class="number">256</span></span><br></pre></td></tr></table></figure><h2 id="Create-Datasets-From-Images-and-Labels"><a href="#Create-Datasets-From-Images-and-Labels" class="headerlink" title="Create Datasets From Images and Labels"></a>Create Datasets From Images and Labels</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create Datasets</span></span><br><span class="line">train_batches = train_examples.cache().shuffle(num_examples//<span class="number">4</span>).batch(BATCH_SIZE).map(format_example).prefetch(<span class="number">1</span>)</span><br><span class="line">validation_batches = validation_examples.cache().batch(BATCH_SIZE).map(format_example)</span><br><span class="line">test_batches = test_examples.map(format_example).batch(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch_sample = next(iter(train_batches))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.squeeze(batch_sample[<span class="number">0</span>][<span class="number">0</span>]).numpy()</span><br></pre></td></tr></table></figure><pre><code>array([[0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.16078432, 0.47843137, 0.3019608 ,        0.10588235, 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.46666667, 0.8509804 , 0.6666667 ,        0.63529414, 0.11372549, 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.13333334, 0.74509805, 0.98039216, 0.8117647 , 0.6156863 ,        0.62352943, 0.87058824, 0.40392157, 0.01568628, 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.4509804 , 0.99215686, 0.87058824, 0.7019608 , 0.57254905,        0.6666667 , 0.56078434, 0.49411765, 0.47058824, 0.11372549,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.3372549 , 0.8666667 , 0.76862746, 0.7019608 , 0.6901961 ,        0.57254905, 0.33333334, 0.49411765, 0.6039216 , 0.24705882,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.40392157, 0.78431374, 0.65882355, 0.7137255 , 0.58431375,        0.39215687, 0.29411766, 0.78431374, 0.39215687, 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.6039216 , 0.8       , 0.73333335, 0.78431374, 0.49019608,        0.3137255 , 0.38039216, 0.17254902, 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.26666668, 0.9019608 , 0.8117647 , 0.8392157 , 0.49019608,        0.38039216, 0.37254903, 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.7058824 , 0.58431375, 0.49019608, 0.4       ,        0.4509804 , 0.23529412, 0.        , 0.00392157, 0.00392157,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.3137255 , 0.7176471 , 0.42745098, 0.42352942, 0.38039216,        0.45882353, 0.15686275, 0.        , 0.        , 0.01176471,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.62352943, 0.7372549 , 0.44705883, 0.40392157, 0.38039216,        0.44705883, 0.23921569, 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.85882354, 0.78039217, 0.44705883, 0.4117647 , 0.4       ,        0.4117647 , 0.4117647 , 0.03529412, 0.        , 0.00392157,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.67058825, 0.8666667 , 0.54509807, 0.48235294, 0.43529412,        0.4117647 , 0.4       , 0.1254902 , 0.        , 0.00392157,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.37254903, 0.9137255 , 0.6039216 , 0.54509807, 0.5058824 ,        0.53333336, 0.5372549 , 0.11372549, 0.        , 0.00392157,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.4       , 0.89411765, 0.5254902 , 0.54509807, 0.5176471 ,        0.57254905, 0.65882355, 0.16078432, 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.53333336, 0.8901961 , 0.5254902 , 0.54509807, 0.5176471 ,        0.57254905, 0.56078434, 0.16078432, 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.64705884, 0.8784314 , 0.49411765, 0.56078434, 0.48235294,        0.6039216 , 0.6156863 , 0.14509805, 0.        , 0.00392157,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.8156863 , 0.8784314 , 0.5058824 , 0.6156863 , 0.48235294,        0.6039216 , 0.6039216 , 0.16078432, 0.        , 0.00392157,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.9372549 , 0.84705883, 0.49411765, 0.6784314 , 0.45882353,        0.68235296, 0.627451  , 0.21176471, 0.        , 0.00392157,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.95686275, 0.8392157 , 0.49411765, 0.7176471 , 0.47058824,        0.68235296, 0.6156863 , 0.23529412, 0.        , 0.00392157,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.06666667,        0.9019608 , 0.8392157 , 0.47058824, 0.7490196 , 0.49019608,        0.75686276, 0.6039216 , 0.27058825, 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.14509805,        0.8352941 , 0.8784314 , 0.46666667, 0.78039217, 0.54901963,        0.78039217, 0.57254905, 0.30588236, 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.22352941,        0.8235294 , 0.99215686, 0.47058824, 0.8       , 0.58431375,        0.7921569 , 0.54901963, 0.34901962, 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.28235295,        0.75686276, 1.        , 0.48235294, 0.76862746, 0.6156863 ,        0.7921569 , 0.5568628 , 0.36078432, 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.36862746,        0.73333335, 0.8901961 , 0.5058824 , 0.7254902 , 0.68235296,        0.77254903, 0.54509807, 0.4117647 , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.49019608,        0.6901961 , 0.90588236, 0.5372549 , 0.6392157 , 0.7058824 ,        0.85882354, 0.56078434, 0.42745098, 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.627451  ,        0.78039217, 0.9137255 , 0.827451  , 0.6509804 , 0.91764706,        0.69411767, 0.6       , 0.48235294, 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ],       [0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.04705882, 0.34901962, 0.24705882, 0.11764706,        0.18431373, 0.77254903, 0.29411766, 0.        , 0.        ,        0.        , 0.        , 0.        , 0.        , 0.        ,        0.        , 0.        , 0.        ]], dtype=float32)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch_sample[<span class="number">1</span>][<span class="number">0</span>].numpy().argmax()</span><br></pre></td></tr></table></figure><pre><code>3</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_batch</span><span class="params">(x,y,shape = None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    input: </span></span><br><span class="line"><span class="string">        x(Tensor[num_images, rows, columns]): images tensor</span></span><br><span class="line"><span class="string">        y(array): labels</span></span><br><span class="line"><span class="string">        shape(tuple): (rows,col) </span></span><br><span class="line"><span class="string">    output:</span></span><br><span class="line"><span class="string">        grid of smaple images</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> shape:</span><br><span class="line">        shape = (int(x.shape[<span class="number">0</span>]**<span class="number">0.5</span>), int(x.shape[<span class="number">0</span>]**<span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line">    fig, axs = plt.subplots(nrows= shape[<span class="number">0</span>], ncols=shape[<span class="number">1</span>], figsize = (<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> axs:</span><br><span class="line">        <span class="keyword">for</span> ax <span class="keyword">in</span> row:</span><br><span class="line">            ax.imshow(tf.squeeze(x[index]).numpy())</span><br><span class="line">            ax.set_xlabel(class_names[y[index].numpy().argmax()])</span><br><span class="line">            index+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># plt.subplots_adjust(wspace = 0.2, hspace = 0.5) </span></span><br><span class="line">    fig.tight_layout()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_batch(batch_sample[<span class="number">0</span>],batch_sample[<span class="number">1</span>], (<span class="number">4</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure><p><img src="output_24_0.png" alt="png"></p><h1 id="Building-the-Model"><a href="#Building-the-Model" class="headerlink" title="Building the Model"></a>Building the Model</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Model: &quot;sequential&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">conv2d (Conv2D)              (None, 26, 26, 16)        160       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten (Flatten)            (None, 3872)              0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                (None, 64)                247872    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (None, 10)                650       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 253,322</span><br><span class="line">Trainable params: 253,322</span><br><span class="line">Non-trainable params: 0</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Build and compile the model shown in the previous cell.</span></span><br><span class="line"></span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    <span class="comment"># Set the input shape to (28, 28, 1), kernel size=3, filters=16 and use ReLU activation,</span></span><br><span class="line">    tf.keras.layers.Conv2D(input_shape = (<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>), kernel_size=<span class="number">3</span>, filters=<span class="number">16</span>,activation = <span class="string">"relu"</span>),</span><br><span class="line">      </span><br><span class="line">    tf.keras.layers.MaxPooling2D(),</span><br><span class="line">      </span><br><span class="line">    <span class="comment"># Set the number of filters to 32, kernel size to 3 and use ReLU activation </span></span><br><span class="line">    tf.keras.layers.Conv2D(filters=<span class="number">32</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">      </span><br><span class="line">    <span class="comment"># Flatten the output layer to 1 dimension</span></span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">      </span><br><span class="line">    <span class="comment"># Add a fully connected layer with 64 hidden units and ReLU activation</span></span><br><span class="line">    tf.keras.layers.Dense(units=<span class="number">64</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">      </span><br><span class="line">    <span class="comment"># Attach a final softmax classification head</span></span><br><span class="line">    tf.keras.layers.Dense(units = num_classes, activation=<span class="string">"softmax"</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the appropriate loss function and use accuracy as your metric</span></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">              loss= tf.keras.losses.CategoricalCrossentropy(),</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>] )</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential_1&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_2 (Conv2D)            (None, 26, 26, 16)        160       _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 13, 13, 16)        0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 11, 11, 32)        4640      _________________________________________________________________flatten_1 (Flatten)          (None, 3872)              0         _________________________________________________________________dense_2 (Dense)              (None, 64)                247872    _________________________________________________________________dense_3 (Dense)              (None, 10)                650       =================================================================Total params: 253,322Trainable params: 253,322Non-trainable params: 0_________________________________________________________________</code></pre><h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(train_batches, epochs=<span class="number">10</span>, validation_data=validation_batches)</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/10219/219 [==============================] - 148s 675ms/step - loss: 0.5912 - accuracy: 0.7919 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 2/10219/219 [==============================] - 4s 20ms/step - loss: 0.3837 - accuracy: 0.8648 - val_loss: 0.3390 - val_accuracy: 0.8796Epoch 3/10219/219 [==============================] - 4s 20ms/step - loss: 0.3319 - accuracy: 0.8819 - val_loss: 0.3046 - val_accuracy: 0.8914Epoch 4/10219/219 [==============================] - 4s 20ms/step - loss: 0.3014 - accuracy: 0.8925 - val_loss: 0.2903 - val_accuracy: 0.8957Epoch 5/10219/219 [==============================] - 4s 20ms/step - loss: 0.2805 - accuracy: 0.8993 - val_loss: 0.2841 - val_accuracy: 0.9011Epoch 6/10219/219 [==============================] - 4s 20ms/step - loss: 0.2602 - accuracy: 0.9054 - val_loss: 0.2777 - val_accuracy: 0.9009Epoch 7/10219/219 [==============================] - 4s 20ms/step - loss: 0.2477 - accuracy: 0.9101 - val_loss: 0.2548 - val_accuracy: 0.9091Epoch 8/10219/219 [==============================] - 4s 20ms/step - loss: 0.2351 - accuracy: 0.9144 - val_loss: 0.2703 - val_accuracy: 0.9000Epoch 9/10219/219 [==============================] - 4s 20ms/step - loss: 0.2209 - accuracy: 0.9198 - val_loss: 0.2462 - val_accuracy: 0.9126Epoch 10/10219/219 [==============================] - 4s 20ms/step - loss: 0.2108 - accuracy: 0.9243 - val_loss: 0.2566 - val_accuracy: 0.9089</code></pre><h1 id="Exporting-to-TFLite"><a href="#Exporting-to-TFLite" class="headerlink" title="Exporting to TFLite"></a>Exporting to TFLite</h1><p>You will now save the model to TFLite. We should note, that you will probably see some warning messages when running the code below. These warnings have to do with software updates and should not cause any errors or prevent your code from running. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Use the tf.saved_model API to save your model in the SavedModel format. </span></span><br><span class="line">export_dir = <span class="string">'saved_model/1'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># YOUR CODE HERE</span></span><br><span class="line">tf.saved_model.save(model, export_dir)</span><br></pre></td></tr></table></figure><pre><code>WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.Instructions for updating:If using Keras pass *_constraint arguments to layers.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.Instructions for updating:If using Keras pass *_constraint arguments to layers.INFO:tensorflow:Assets written to: saved_model/1/assetsINFO:tensorflow:Assets written to: saved_model/1/assets</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Select mode of optimization</span></span><br><span class="line">mode = <span class="string">"Speed"</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> mode == <span class="string">'Storage'</span>:</span><br><span class="line">    optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE</span><br><span class="line"><span class="keyword">elif</span> mode == <span class="string">'Speed'</span>:</span><br><span class="line">    optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    optimization = tf.lite.Optimize.DEFAULT</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Use the TFLiteConverter SavedModel API to initialize the converter</span></span><br><span class="line"></span><br><span class="line">converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the optimzations</span></span><br><span class="line">converter.optimizations = [optimization]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Invoke the converter to finally generate the TFLite model</span></span><br><span class="line">tflite_model = converter.convert()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tflite_model_file = pathlib.Path(<span class="string">'./model.tflite'</span>)</span><br><span class="line">tflite_model_file.write_bytes(tflite_model)</span><br></pre></td></tr></table></figure><pre><code>258704</code></pre><h1 id="Test-the-Model-with-TFLite-Interpreter"><a href="#Test-the-Model-with-TFLite-Interpreter" class="headerlink" title="Test the Model with TFLite Interpreter"></a>Test the Model with TFLite Interpreter</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load TFLite model and allocate tensors.</span></span><br><span class="line">interpreter = tf.lite.Interpreter(model_content=tflite_model)</span><br><span class="line">interpreter.allocate_tensors()</span><br><span class="line"></span><br><span class="line">input_index = interpreter.get_input_details()[<span class="number">0</span>][<span class="string">"index"</span>]</span><br><span class="line">output_index = interpreter.get_output_details()[<span class="number">0</span>][<span class="string">"index"</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Gather results for the randomly sampled test images</span></span><br><span class="line">predictions = []</span><br><span class="line">test_labels = []</span><br><span class="line">test_images = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> img, label <span class="keyword">in</span> test_batches.take(<span class="number">50</span>):</span><br><span class="line">    interpreter.set_tensor(input_index, img)</span><br><span class="line">    interpreter.invoke()</span><br><span class="line">    predictions.append(interpreter.get_tensor(output_index))</span><br><span class="line">    test_labels.append(label[<span class="number">0</span>])</span><br><span class="line">    test_images.append(np.array(img))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class_names</span><br></pre></td></tr></table></figure><pre><code>[&#39;T-shirt_top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Utilities functions for plotting</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_image</span><span class="params">(i, predictions_array, true_label, img)</span>:</span></span><br><span class="line">    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]</span><br><span class="line">    plt.grid(<span class="keyword">False</span>)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    </span><br><span class="line">    img = np.squeeze(img)</span><br><span class="line">    </span><br><span class="line">    plt.imshow(img, cmap=plt.cm.binary)</span><br><span class="line">    </span><br><span class="line">    predicted_label = np.argmax(predictions_array)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#     print(predicted_label)</span></span><br><span class="line"><span class="comment">#     print(true_label.numpy().argmax())</span></span><br><span class="line">    <span class="keyword">if</span> predicted_label == true_label.numpy().argmax():</span><br><span class="line">        color = <span class="string">'green'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        color = <span class="string">'red'</span></span><br><span class="line">        </span><br><span class="line">    plt.xlabel(<span class="string">"&#123;&#125; &#123;:2.0f&#125;% (&#123;&#125;)"</span>.format(class_names[predicted_label],</span><br><span class="line">                                         <span class="number">100</span>*np.max(predictions_array),</span><br><span class="line">                                         class_names[true_label.numpy().argmax()]),</span><br><span class="line">                                         color=color)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_value_array</span><span class="params">(i, predictions_array, true_label)</span>:</span></span><br><span class="line">    predictions_array, true_label = predictions_array[i], true_label[i]</span><br><span class="line">    plt.grid(<span class="keyword">False</span>)</span><br><span class="line">    plt.xticks(list(range(<span class="number">10</span>)))</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    thisplot = plt.bar(range(<span class="number">10</span>), predictions_array[<span class="number">0</span>], color=<span class="string">"#777777"</span>)</span><br><span class="line">    plt.ylim([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    predicted_label = np.argmax(predictions_array[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    thisplot[predicted_label].set_color(<span class="string">'red'</span>)</span><br><span class="line">    thisplot[true_label.numpy().argmax()].set_color(<span class="string">'blue'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Visualize the outputs</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Select index of image to display. Minimum index value is 1 and max index value is 50. </span></span><br><span class="line">index = <span class="number">49</span> </span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">3</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plot_image(index, predictions, test_labels, test_images)</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plot_value_array(index, predictions, test_labels)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_41_0.png" alt="png"></p><h2 id="Other-Comfiguration-example"><a href="#Other-Comfiguration-example" class="headerlink" title="Other Comfiguration example"></a>Other Comfiguration example</h2><h3 id="Post-Training-Quantization"><a href="#Post-Training-Quantization" class="headerlink" title="Post-Training Quantization"></a>Post-Training Quantization</h3><p>The simplest form of post-training quantization quantizes weights from floating point to 8-bits of precision. This technique is enabled as an option in the TensorFlow Lite converter. At inference, weights are converted from 8-bits of precision to floating point and computed using floating-point kernels. This conversion is done once and cached to reduce latency.</p><p>To further improve latency, hybrid operators dynamically quantize activations to 8-bits and perform computations with 8-bit weights and activations. This optimization provides latencies close to fully fixed-point inference. However, the outputs are still stored using floating point, so that the speedup with hybrid ops is less than a full fixed-point computation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">converter.optimizations = [tf.lite.Optimize.DEFAULT]</span><br></pre></td></tr></table></figure><h3 id="Post-Training-Integer-Quantization"><a href="#Post-Training-Integer-Quantization" class="headerlink" title="Post-Training Integer Quantization"></a>Post-Training Integer Quantization</h3><p>We can get further latency improvements, reductions in peak memory usage, and access to integer only hardware accelerators by making sure all model math is quantized. To do this, we need to measure the dynamic range of activations and inputs with a representative data set. You can simply create an input data generator and provide it to our converter.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">representative_data_gen</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> input_value, _ <span class="keyword">in</span> test_batches.take(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">yield</span> [input_value]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">converter.representative_dataset = representative_data_gen</span><br></pre></td></tr></table></figure><p>The resulting model will be fully quantized but still take float input and output for convenience.</p><p>Ops that do not have quantized implementations will automatically be left in floating point. This allows conversion to occur smoothly but may restrict deployment to accelerators that support float. </p><h3 id="Full-Integer-Quantization"><a href="#Full-Integer-Quantization" class="headerlink" title="Full Integer Quantization"></a>Full Integer Quantization</h3><p>To require the converter to only output integer operations, one can specify:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;Copyright-2018-The-TensorFlow-Authors&quot;&gt;&lt;a href=&quot;#Copyright-2018-The-TensorFlow-Authors&quot; class=&quot;headerlink&quot; title=&quot;Copyright 2018 The
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
      <category term="Development" scheme="https://zhangruochi.com/categories/Development/"/>
    
    
  </entry>
  
  <entry>
    <title>Promise</title>
    <link href="https://zhangruochi.com/Promise/2020/02/13/"/>
    <id>https://zhangruochi.com/Promise/2020/02/13/</id>
    <published>2020-02-13T16:22:12.000Z</published>
    <updated>2020-02-13T17:07:26.743Z</updated>
    
    <content type="html"><![CDATA[<h2 id="What-is-Promise"><a href="#What-is-Promise" class="headerlink" title="What is Promise"></a>What is Promise</h2><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> p = <span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function">(<span class="params">resolve, reject</span>) =&gt;</span> &#123;</span><br><span class="line">  <span class="comment">// 做一些事情</span></span><br><span class="line">  <span class="comment">// 然后在某些条件下resolve，或者reject</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="comment">/* 条件随便写^_^ */</span>) &#123;</span><br><span class="line">    resolve()</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    reject()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">p.then(<span class="function"><span class="params">()</span> =&gt;</span> &#123;</span><br><span class="line">    <span class="comment">// 如果p的状态被resolve了，就进入这里</span></span><br><span class="line">&#125;, () =&gt; &#123;</span><br><span class="line">    <span class="comment">// 如果p的状态被reject</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h3 id="构造实例"><a href="#构造实例" class="headerlink" title="构造实例"></a>构造实例</h3><ul><li>构造函数接受一个函数作为参数</li><li>调用构造函数得到实例p的同时，作为参数的函数会立即执行</li><li>参数函数接受两个回调函数参数resolve和reject</li><li>在参数函数被执行的过程中，如果在其内部调用resolve，会将p的状态变成<code>fulfilled</code>，或者调用reject，会将p的状态变成<code>rejected</code>.</li></ul><h3 id="调用-then"><a href="#调用-then" class="headerlink" title="调用.then"></a>调用<code>.then</code></h3><ul><li>调用.then可以为实例p注册两种状态回调函数</li><li>当实例p的状态为fulfilled，会触发第一个函数执行</li><li>当实例p的状态为rejected，则触发第二个函数执行</li><li>then方法返回的是一个新的Promise实例（注意，不是原来那个Promise实例）。因此可以采用链式写法，即then方法后面再调用另一个then方法。</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>将异步过程转化成promise对象</li><li>对象有3种状态</li><li>通过<code>.then</code>注册状态的回调</li><li>已完成的状态能触发回调</li></ul><h2 id="How-to-use-Promise"><a href="#How-to-use-Promise" class="headerlink" title="How to use Promise"></a>How to use Promise</h2><h3 id="3种状态"><a href="#3种状态" class="headerlink" title="3种状态"></a>3种状态</h3><p>首先，promise实例有三种状态：</p><ul><li>pending（待定）</li><li>fulfilled（已执行）</li><li>rejected（已拒绝）</li></ul><p>fulfilled和rejected有可以说是已成功和已失败，这两种状态又归为已完成状态</p><h3 id="resolve和reject"><a href="#resolve和reject" class="headerlink" title="resolve和reject"></a><code>resolve</code>和<code>reject</code></h3><p>调用resolve和reject能将分别将promise实例的状态变成fulfilled和rejected，只有状态变成已完成（即fulfilled和rejected之一），才能触发状态的回调</p><h3 id="promise的内容分为构造函数、实例方法和静态方法"><a href="#promise的内容分为构造函数、实例方法和静态方法" class="headerlink" title="promise的内容分为构造函数、实例方法和静态方法"></a>promise的内容分为构造函数、实例方法和静态方法</h3><h4 id="1个构造函数：-new-Promise"><a href="#1个构造函数：-new-Promise" class="headerlink" title="1个构造函数： new Promise"></a>1个构造函数： new Promise</h4><ul><li>new Promise能将一个异步过程转化成promise对象。先有了promise对象，然后才有promise编程方式。</li></ul><h4 id="2个实例方法：-then-和-catch"><a href="#2个实例方法：-then-和-catch" class="headerlink" title="2个实例方法：.then 和 .catch"></a>2个实例方法：.then 和 .catch</h4><ul><li><code>.then</code>用于为promise对象的状态注册回调函数。它会返回一个promise对象，所以可以进行链式调用，也就是.then后面可以继续.then。在注册的状态回调函数中，可以通过<code>return</code>语句改变.then返回的promise对象的状态，以及向后面.then注册的状态回调传递数据；也可以不使用return语句，那样默认就是将返回的promise对象resolve。</li><li><code>.catch</code>用于注册rejected状态的回调函数，同时该回调也是程序出错的回调，即如果前面的程序运行过程中出错，也会进入执行该回调函数。同.then一样，也会返回新的promise对象。</li></ul><h4 id="4个静态方法：Promise-all、Promise-race、Promise-resolve和Promise-reject"><a href="#4个静态方法：Promise-all、Promise-race、Promise-resolve和Promise-reject" class="headerlink" title="4个静态方法：Promise.all、Promise.race、Promise.resolve和Promise.reject"></a>4个静态方法：Promise.all、Promise.race、Promise.resolve和Promise.reject</h4><p>调用Promise.resolve会返回一个状态为fulfilled状态的promise对象，参数会作为数据传递给后面的状态回调函数; Promise.reject与Promise.resolve同理，区别在于返回的promise对象状态为rejected</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Promise</span>.resolve(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 等价于</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function">(<span class="params">resolve</span>) =&gt;</span> resolve(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line"><span class="built_in">Promise</span>.reject(<span class="keyword">new</span> <span class="built_in">Error</span>(<span class="string">"BOOM"</span>)) </span><br><span class="line"></span><br><span class="line"><span class="comment">// 等价于</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">new</span> <span class="built_in">Promise</span>((resolve, reject) </span><br><span class="line">    =&gt; reject(<span class="keyword">new</span> <span class="built_in">Error</span>(<span class="string">"BOOM"</span>)));</span><br></pre></td></tr></table></figure><h2 id="Practice"><a href="#Practice" class="headerlink" title="Practice"></a>Practice</h2><blockquote><p>题目：红灯三秒亮一次，绿灯一秒亮一次，黄灯2秒亮一次；如何让三个灯不断交替重复亮灯？</p></blockquote><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">red</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'red'</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">green</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'green'</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">yellow</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'yellow'</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> light = <span class="function">(<span class="params">fn, timer</span>) =&gt;</span> <span class="keyword">new</span> <span class="built_in">Promise</span>( <span class="function"><span class="params">resolve</span> =&gt;</span> &#123;</span><br><span class="line"> setTimeout(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    fn();</span><br><span class="line">    resolve();</span><br><span class="line"> &#125;, timer)</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">start</span>(<span class="params">times</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (!times) &#123;</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  times--</span><br><span class="line">  <span class="built_in">Promise</span>.resolve()</span><br><span class="line">    .then(<span class="function"><span class="params">()</span> =&gt;</span> light(red, <span class="number">3000</span>))</span><br><span class="line">    .then(<span class="function"><span class="params">()</span> =&gt;</span> light(green, <span class="number">1000</span>))</span><br><span class="line">    .then(<span class="function"><span class="params">()</span> =&gt;</span> light(yellow, <span class="number">2000</span>))</span><br><span class="line">    .then(<span class="function"><span class="params">()</span> =&gt;</span> start(times))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">start(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><h2 id="async-await"><a href="#async-await" class="headerlink" title="async/await"></a>async/await</h2><h3 id="async"><a href="#async" class="headerlink" title="async"></a>async</h3><p>async关键字的意思很简单，就是函数返回的是一个promise</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">function</span> <span class="title">f</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line">f().then(<span class="function">(<span class="params">res</span>) =&gt;</span> &#123;</span><br><span class="line">    <span class="built_in">console</span>.log(res)</span><br><span class="line">&#125;) <span class="comment">// 1</span></span><br></pre></td></tr></table></figure><p>async函数会返回一个promise对象，如果function中返回的是一个值，async直接会用<code>Promise.resolve()</code>包裹一下返回。</p><h3 id="await"><a href="#await" class="headerlink" title="await"></a>await</h3><blockquote><p>[return_value] = await expression;</p></blockquote><p>关键词await是等待的意思，那么他在等什么呢？ 等的是一个表达式，那么表达式，可以是一个常量，变量，promise，函数等。</p><p>await操作符等的是一个返回的结果，那么如果是同步的情况，那就直接返回了。 异步的情况下，await会阻塞整一个流程，直到结果返回之后，才会继续下面的代码。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">getProvinces</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function"><span class="params">resolve</span> =&gt;</span> &#123;</span><br><span class="line">        setTimeout(resolve, <span class="number">1000</span>)</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">function</span> <span class="title">asyncFn</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">await</span> getProvinces()</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'hello async'</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">asyncFn()</span><br></pre></td></tr></table></figure><p>上面代码先定义了一个获取省份数据的getProvinces函数，其中用setTimeout模拟数据请求的异步操作。当我们在asyncFn 函数前面使用关键字 async 就表明该函数内存在异步操作。当遇到 await 关键字时，会等待异步操作完成后再接着执行接下去的代码。所以代码的执行结果为等待1000毫秒之后才会在控制台中打印出 ‘hello async’。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/26523836" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26523836</a></li><li><a href="https://juejin.im/post/5d9e8539f265da5b8a515e63" target="_blank" rel="noopener">https://juejin.im/post/5d9e8539f265da5b8a515e63</a></li><li><a href="https://juejin.im/post/5c553b71f265da2d8532b351" target="_blank" rel="noopener">https://juejin.im/post/5c553b71f265da2d8532b351</a></li></ul>]]></content>
    
    <summary type="html">
    
      concepts of Promise, Async, Await
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
      <category term="JavaScript" scheme="https://zhangruochi.com/categories/Programming-Language/JavaScript/"/>
    
    
  </entry>
  
  <entry>
    <title>Introduction to Operating Systems</title>
    <link href="https://zhangruochi.com/Introduction-to-Operating-Systems/2020/02/07/"/>
    <id>https://zhangruochi.com/Introduction-to-Operating-Systems/2020/02/07/</id>
    <published>2020-02-07T05:29:25.000Z</published>
    <updated>2020-02-07T05:43:31.624Z</updated>
    
    <content type="html"><![CDATA[<p>Introduction to Operating Systems</p><ul><li>Topics to be covered in this lesson:<ul><li>What is an OS (operating system)?</li><li>What are key components of an OS?</li><li>Design and implementation considerations of OSs</li></ul></li></ul><h2 id="What-is-an-Operating-System"><a href="#What-is-an-Operating-System" class="headerlink" title="What is an Operating System?"></a>What is an Operating System?</h2><ul><li>An OS is a special piece of software that abstracts and arbitrates the use of a computer system</li><li>An <strong>OS</strong> is like <strong>a toy shop manager</strong> in that an OS:<ul><li>Directs operational resources</li><li>Enforces working policies</li><li>Mitigates difficulty of complex tasks</li></ul></li><li>By definition, an OS is a layer of systems software that:<ul><li>Directly has privileged access to the underlying hardware</li><li>Hides the hardware complexity</li><li>Manages hardware on behalf of one of more applications according to some predefined polices</li><li>In addition, it ensures that applications are isolated and protected from one another</li></ul></li></ul><h2 id="OS-Elements"><a href="#OS-Elements" class="headerlink" title="OS Elements"></a>OS Elements</h2><ul><li>Abstractions:<ul><li>Process, thread, file, socket, memory page</li></ul></li><li>Mechanisms<ul><li>Create, schedule, open, write, allocate</li></ul></li><li>Policies<ul><li>Least recently used (LRU), earliest deadline first (EDF)</li></ul></li></ul><h2 id="Design-Principles"><a href="#Design-Principles" class="headerlink" title="Design Principles"></a>Design Principles</h2><ul><li>Separation of mechanisms to policy:<ul><li>Implement flexible mechanisms to support many policies</li></ul></li><li>Optimize for common case:<ul><li>Where will the OS be used?</li><li>What will the user want to execute on that machine?</li><li>What are the workload requirements?</li></ul></li></ul><h2 id="OS-Protection-Boundary"><a href="#OS-Protection-Boundary" class="headerlink" title="OS Protection Boundary"></a>OS Protection Boundary</h2><ul><li>Generally, applications operate in unprivileged mode (user level) while operating systems operate in privileged mode (kernel level)</li><li>Kernel level software is able to access hardware directly</li><li>User-kernel switch is supported by hardware<ul><li>trap instructions</li><li>system call(open send malloc …)</li><li>signals</li></ul></li></ul><h2 id="Crossing-The-OS-Boundary"><a href="#Crossing-The-OS-Boundary" class="headerlink" title="Crossing The OS Boundary"></a>Crossing The OS Boundary</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="system_call.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">System Call</div></center><ul><li>Applications will need to utilize user-kernel transitions which is accomplished by hardware, this involves a number of instructions and switches locality</li><li>Switching locality will affect hardware cache (transitions are costly)</li><li>Hardware will set _traps_ on illegal instructions and os can check what cause the trap</li><li>Cache <ul><li>Because context switches will swap the data/addresses currently in cache, the performance of applications can benefit or suffer based on how a context switch changes what is in cache at the time they are accessing it.</li><li>A cache would be considered hot (fire) if an application is accessing the cache when it contains the data/addresses it needs.</li><li>Likewise, a cache would be considered cold (ice) if an application is accessing the cache when it does not contain the data/addresses it needs — forcing it to retrieve data/addresses from main memory.</li></ul></li></ul><h2 id="OS-Services"><a href="#OS-Services" class="headerlink" title="OS Services"></a>OS Services</h2><p>An operating system provide applications with access to the underlying hardware by exporting a number of services. Thses services are directly linked to some of the components of the hardware.</p><ul><li>process management</li><li>file management</li><li>device management</li><li>memory management</li><li>storage management</li><li>security management</li></ul><h2 id="Monolithic-OS"><a href="#Monolithic-OS" class="headerlink" title="Monolithic OS"></a>Monolithic OS</h2><ul><li>Pros:<ul><li>Everything included</li><li>Inlining, compile-time optimizations</li></ul></li><li>Cons:<ul><li>Customization, portability, manageability</li><li>Memory footprint</li><li>Performance</li></ul></li></ul><h2 id="Modular-OS"><a href="#Modular-OS" class="headerlink" title="Modular OS"></a>Modular OS</h2><ul><li>Pros:<ul><li>Maintainability</li><li>Smaller footprint</li><li>Less resource needs</li></ul></li><li>Cons:<ul><li>Indirection can impact performance</li><li>Maintenance can still be an issue</li></ul></li></ul><h2 id="Microkernel"><a href="#Microkernel" class="headerlink" title="Microkernel"></a>Microkernel</h2><ul><li>Pros:<ul><li>Size</li><li>Verifiability</li></ul></li><li>Cons:<ul><li>Portability</li><li>Complexity of software development</li><li>Cost of user/kernel crossing</li></ul></li></ul><h2 id="Operating-System-Architecture"><a href="#Operating-System-Architecture" class="headerlink" title="Operating System Architecture"></a>Operating System Architecture</h2><h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="linux.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Linux Architecture</div></center><h3 id="MacOS"><a href="#MacOS" class="headerlink" title="MacOS"></a>MacOS</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="macos.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Macos Architecture</div></center><h2 id="Quiz"><a href="#Quiz" class="headerlink" title="Quiz"></a>Quiz</h2><h4 id="Quiz-1-Operating-Systems-Components"><a href="#Quiz-1-Operating-Systems-Components" class="headerlink" title="Quiz 1: Operating Systems Components"></a>Quiz 1: Operating Systems Components</h4><p>Which of the following are likely components of an operating system?</p><p>A: </p><p>file system (hides hardware complexity), device driver (makes decisions), scheduler (distributes processes).</p><h4 id="Quiz-2-Operating-Systems-Components"><a href="#Quiz-2-Operating-Systems-Components" class="headerlink" title="Quiz 2: Operating Systems Components"></a>Quiz 2: Operating Systems Components</h4><p>For the following options, indicate if they are examples of abstraction or arbitration.</p><p>A: </p><ul><li>Distributing memory between multiple processes (arbitration)</li><li>Supporting different types of speakers (abstraction)</li><li>Interchangeable access of hard disk or SSD (abstraction)</li></ul><h4 id="Quiz-3-System-Calls"><a href="#Quiz-3-System-Calls" class="headerlink" title="# Quiz 3: System Calls"></a># Quiz 3: System Calls</h4><p>On a 64-bit Linux-based OS, which system call is used to:</p><ul><li>Send a signal to a process?</li><li>Set the group identity of a process?</li><li>Mount a file system?</li><li>Read/write system parameters</li></ul><p>A: </p><p>The answers are respectively,</p><ul><li><code>kill</code></li><li><code>setgid</code></li><li><code>mount</code></li><li><code>sysctl</code></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/stevenxchung/Introduction-to-Operating-Systems" target="_blank" rel="noopener">https://github.com/stevenxchung/Introduction-to-Operating-Systems</a></li></ul>]]></content>
    
    <summary type="html">
    
      This is the course note from Introduction to Operating Systems of Udacity
    
    </summary>
    
    
      <category term="Operating System" scheme="https://zhangruochi.com/categories/Operating-System/"/>
    
    
  </entry>
  
  <entry>
    <title>AI - Search</title>
    <link href="https://zhangruochi.com/AI-Search/2020/02/06/"/>
    <id>https://zhangruochi.com/AI-Search/2020/02/06/</id>
    <published>2020-02-07T01:56:08.000Z</published>
    <updated>2020-03-05T21:34:38.412Z</updated>
    
    <content type="html"><![CDATA[<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="AI.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">AI</div></center><h2 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h2><h3 id="Route-finding"><a href="#Route-finding" class="headerlink" title="Route finding"></a>Route finding</h3><ul><li>Route finding is perhaps the most canonical example of a search problem. We are given as the input a map, a source point and a destination point. The goal is to output a sequence of actions (e.g., go straight, turn left, or turn right) that will take us from the source to the destination.</li><li>We might evaluate action sequences based on an objective (distance, time, or pleasantness).</li></ul><h3 id="Robot-motion-planning"><a href="#Robot-motion-planning" class="headerlink" title="Robot motion planning"></a>Robot motion planning</h3><ul><li>In robot motion planning, the goal is get a robot to move from one position/pose to another. The desired output trajectory consists of individual actions, each action corresponding to moving or rotating the joints by a small amount.</li><li>Again, we might evaluate action sequences based on various resources like time or energy.</li></ul><h2 id="Search-Problems"><a href="#Search-Problems" class="headerlink" title="Search Problems"></a>Search Problems</h2><ul><li>Recall the modeling-inference-learning paradigm. For reflex-based classifiers, modeling consisted of choos- ing the features and the neural network architecture; inference was trivial forward computation of the output given the input; and learning involved using stochastic gradient descent on the gradient of the loss function, which might involve backpropagation.</li><li>Today, we will focus on the modeling and inference part of search problems. The next lecture will cover learning.</li></ul><h3 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h3><ul><li>$S_{start}$: staring state$</li><li>Actions(s): possible actions</li><li>Cons(s,a): action cost</li><li>Succ(s,a): successor</li><li>IsEnd(s): reached end state?</li></ul><p>We will build what we will call a <strong>search tree</strong>. The root of the tree is the start state <script type="math/tex">S_{start}</script>, and the leaves are the end states (IsEnd(s) is true). Each edge leaving a node s corresponds to a possible action <script type="math/tex">a \in \text{Actions(s)}</script>  that could be performed in state s. The edge is labeled with the action and its cost, written <code>a : Cost(s,a)</code>. The action leads deterministically to the successor state <code>Succ(s,a)</code>, represented by the child node.</p><p>In summary, each root-to-leaf path represents a possible action sequence, and the sum of the costs of the edges is the cost of that path. The goal is to find the root-to-leaf path that ends in a valid end state with minimum cost.</p><p>Note that in code, we usually do not build the search tree as a concrete data structure. The search tree is used merely to visualize the computation of the search algorithms and study the structure of the search problem.</p><h2 id="Transportation-example"><a href="#Transportation-example" class="headerlink" title="Transportation example"></a>Transportation example</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Street with blocks numbered 1 to n.</span><br><span class="line"></span><br><span class="line">Walking from s to s + 1 takes 1 minute.</span><br><span class="line"></span><br><span class="line">Taking a magic tram from s to 2s takes 2 minutes. </span><br><span class="line"></span><br><span class="line">How to travel from 1 to n in the least time?</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransportationProblem</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, N)</span>:</span></span><br><span class="line">        <span class="comment"># N = number of blocks</span></span><br><span class="line">        self.N = N</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">startState</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isEnd</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> state == self.N</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">succAndCost</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="comment"># return list of (action, newState, cost) triples</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">if</span> state+<span class="number">1</span>&lt;=self.N:</span><br><span class="line">            result.append((<span class="string">'walk'</span>, state+<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">if</span> state*<span class="number">2</span>&lt;=self.N:</span><br><span class="line">            result.append((<span class="string">'tram'</span>, state*<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><p>Now let’s put modeling aside and suppose we are handed a search problem. How do we construct an algorithm for finding a minimum cost path (not necessarily unique)?</p><h2 id="Tree-search"><a href="#Tree-search" class="headerlink" title="Tree search"></a>Tree search</h2><h3 id="Backtracking-search"><a href="#Backtracking-search" class="headerlink" title="Backtracking search"></a>Backtracking search</h3><p>If <code>b</code> actions per state, maximum depth is <code>D</code> actions:</p><ul><li>Memory: $O(D)$ (small)</li><li>Time: $O(b^D)$ (huge) [$2^{50}$ = 1125899906842624]</li></ul><p>We will start with backtracking search, the simplest algorithm which just tries all paths. The algorithm is called recursively on the <code>current state s</code> and the path leading up to that state. If we have reached a goal, then we can update the minimum cost path with the current path. Otherwise, we consider <code>all possible actions a from state s</code>, and recursively search each of the possibilities.</p><p>Graphically, backtracking search performs a depth-first traversal of the search tree. What is the time and memory complexity of this algorithm? To get a simple characterization, assume that the search tree has maximum depth D (each path consists of D actions/edges) and that there are b available actions per state (the branching factor is b). It is easy to see that backtracking search only requires <strong>O(D) memory</strong> (to maintain the stack for the recurrence), which is as good as it gets.</p><p>However, the running time is proportional to the number of nodes in the tree, since the algorithm needs to check each of them. The number of nodes is</p><script type="math/tex; mode=display">1 + b + b^{2} + \cdots + b^{D} = \frac{b^{D+1} - 1}{B-1} = O(b^{D})</script><p>Note that the total number of nodes in the search tree is on the same order as the number of leaves, so the cost is always dominated by the last level.</p><h3 id="Depth-first-search"><a href="#Depth-first-search" class="headerlink" title="Depth-first search"></a>Depth-first search</h3><ul><li>Assume action costs Cost(s, a) = 0.</li><li><strong>Idea</strong>: Backtracking search + stop when find the first end state.</li><li>If b actions per state, maximum depth is D actions:</li><li>Memory: $O(D)$ (small)</li><li>Time: $O(b^D)$ worst case but could be much better if solutions<br>are easy to find.</li></ul><p>Backtracking search will always work (i.e., find a minimum cost path), but there are cases where we can do it faster. But in order to do that, we need some additional assumptions — there is no free lunch. Suppose we make the assumption that all the action costs are zero. In other words, all we care about is finding a valid action sequence that reaches the goal. Any such sequence will have the minimum cost: zero.</p><p>In this case, we can just modify backtracking search to not keep track of costs and then stop searching as soon as we reach a goal. The resulting algorithm is depth-first search (DFS), which should be familiar to you. The worst time and space complexity are of the same order as backtracking search. In particular, if there is no path to an end state, then we have to search the entire tree.</p><h3 id="Breadth-first-search"><a href="#Breadth-first-search" class="headerlink" title="Breadth-first search"></a>Breadth-first search</h3><ul><li>Assume action costs Cost(s, a) = c for some c &gt;= 0.</li><li><strong>Idea</strong>: explore all nodes in order of increasing depth.</li><li>Legend: b actions per state, solution has d actions</li><li>Memory: $O(b^{d})$ (small)</li><li>Time: $O(b^d)$ (better, depends on d, not D).</li></ul><p>Breadth-first search (BFS), which should also be familiar, makes a less stringent assumption, that all the action costs are the same non-negative number. This effectively means that all the paths of a given length have the same cost. BFS maintains a <code>queue</code> of states to be explored. It pops a state off the queue, then pushes its successors back on the queue.</p><h3 id="DFS-with-iterative-deepening"><a href="#DFS-with-iterative-deepening" class="headerlink" title="DFS with iterative deepening"></a>DFS with iterative deepening</h3><ul><li>Assume action costs Cost(s, a) = c for some c ≥ 0.</li><li>Modify DFS to stop at a maximum depth.</li><li>Call DFS for maximum depths 1,2,….</li><li>Memory: $O(d)$ (sabed!)</li><li>Time: $O(b^d)$ (same as BFS).</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backtrackingSearch</span><span class="params">(problem)</span>:</span></span><br><span class="line">    <span class="comment"># Best solution found so far (dictionary because of python scoping technicality)</span></span><br><span class="line">    best = &#123;</span><br><span class="line">        <span class="string">'cost'</span>: float(<span class="string">'+inf'</span>),</span><br><span class="line">        <span class="string">'history'</span>: <span class="keyword">None</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recurse</span><span class="params">(state, history, totalCost)</span>:</span></span><br><span class="line">        <span class="comment"># At state, having undergone history, accumulated</span></span><br><span class="line">        <span class="comment"># totalCost.</span></span><br><span class="line">        <span class="comment"># Explore the rest of the subtree under state.</span></span><br><span class="line">        <span class="keyword">if</span> problem.isEnd(state):</span><br><span class="line">            <span class="comment"># Update the best solution so far</span></span><br><span class="line">            <span class="keyword">if</span> totalCost&lt;best[<span class="string">'cost'</span>]:</span><br><span class="line">                best[<span class="string">'cost'</span>] = totalCost</span><br><span class="line">                best[<span class="string">'history'</span>] = history</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="comment"># Recurse on children</span></span><br><span class="line">        <span class="keyword">for</span> action, newState, cost <span class="keyword">in</span> problem.succAndCost(state):</span><br><span class="line">            recurse(newState, history+[(action, newState, cost)], totalCost+cost)</span><br><span class="line">    recurse(problem.startState(), history=[], totalCost=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> (best[<span class="string">'cost'</span>], best[<span class="string">'history'</span>])</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="Search.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;"> Tree search algorithms</div></center><h2 id="Dynamic-programming"><a href="#Dynamic-programming" class="headerlink" title="Dynamic programming"></a>Dynamic programming</h2><p>backtracking search with memoization — potentially exponential savings</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="dp.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;"> Dynamic programming</div></center><ul><li>Now let us see if we can avoid the exponential time. If we consider the simple route finding problem of traveling from city 1 to city n, the search tree grows exponentially with n.</li><li>However, upon closer inspection, we note that this search tree has a lot of <strong>repeated structures</strong>. Moreover (and this is important), <strong>the future costs (the minimum cost of reaching a end state) of a state only depends on the current city!</strong> So therefore, all the subtrees rooted at city 5, for example, have the same minimum cost!</li><li>If we can just do that computation once, then we will have saved big time. This is the central idea of dynamic programming.</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="dp1.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;"> Dynamic programming</div></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="dp2.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;"> Dynamic programming</div></center><ul><li>The dynamic programming algorithm is exactly backtracking search with one twist. At the beginning of the function, we check to see if we’ve already computed the future cost for s. If we have, then we simply return it (which takes constant time if we use a hash map). Otherwise, we compute it and save it in the cache so we don’t have to recompute it again. In this way, for every state, we are only computing its value once.</li><li>One important point is that the graph must be acyclic for dynamic programming to work. If there are cycles, the computation of a future cost for s might depend on $s^{\prime}$ which might depend on s. We will infinite loop in this case. To deal with cycles, we need uniform cost search, which we will describe later.</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="dp3.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Dynamic programming</div></center><ul><li>This is perhaps the most important idea of this lecture: <strong>state</strong>. A state is a summary of all the past actions sufficient to choose future actions optimally.</li><li>What state is really about is forgetting the past. We can’t forget everything because the action costs in the future might depend on what we did on the past. The more we forget, the fewer states we have, and the more efficient our algorithm. So the name of the game is to find the minimal set of states that suffice. It’s a fun game.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dynamicProgramming</span><span class="params">(problem)</span>:</span></span><br><span class="line">    cache = &#123;&#125; <span class="comment"># state -&gt; futureCost(state)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">futureCost</span><span class="params">(state)</span>:</span></span><br><span class="line">        <span class="comment"># Base case</span></span><br><span class="line">        <span class="keyword">if</span> problem.isEnd(state):</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">in</span> cache: <span class="comment"># Exponential savings</span></span><br><span class="line">            <span class="keyword">return</span> cache[state]</span><br><span class="line">        <span class="comment"># Actually doing work</span></span><br><span class="line">        result = min(cost+futureCost(newState) \</span><br><span class="line">                <span class="keyword">for</span> action, newState, cost <span class="keyword">in</span> problem.succAndCost(state))</span><br><span class="line">        cache[state] = result</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> (futureCost(problem.startState()), [])</span><br></pre></td></tr></table></figure><h3 id="Uniform-cost-search"><a href="#Uniform-cost-search" class="headerlink" title="Uniform cost search"></a>Uniform cost search</h3><ul><li>UCS enumerates states in order of increasing past cost.</li><li>All action costs are non-negative: Cost(s, a) ≥ 0.</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="ufs.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Uniform cost search</div></center><ul><li>The key idea that uniform cost search (UCS) uses is to compute the past costs in order of increasing past cost. To make this efficient, we need to make an important assumption that all action costs are non-negative.</li><li>This assumption is reasonable in many cases, but doesn’t allow us to handle cases where actions have payoff. To handle negative costs (positive payoffs), we need the Bellman-Ford algorithm. When we talk about value iteration for MDPs, we will see a form of this algorithm.</li><li>Note: those of you who have studied algorithms should immediately recognize UCS as Dijkstra’s algorithm. Logically, the two are indeed equivalent. There is an important implementation difference: UCS takes as input a search problem, which implicitly defines a large and even infinite graph, whereas Dijkstra’s algorithm (in the typical exposition) takes as input a fully concrete graph. The implicitness is important in practice because we might be working with an enormous graph (a detailed map of world) but only need to find the path between two close by points (Stanford to Palo Alto).</li></ul><h4 id="High-level-strategy"><a href="#High-level-strategy" class="headerlink" title="High-level strategy"></a>High-level strategy</h4><ul><li><strong>Explored</strong>: states we’ve found the optimal path to</li><li><strong>Frontier</strong>: states we’ve seen, still figuring out how to get there<br>cheaply</li><li><strong>Unexplored</strong>: states we haven’t seen</li></ul><p>The general strategy of UCS is to maintain three sets of nodes: explored, frontier, and unexplored. Throughout the course of the algorithm, we will move states from unexplored to frontier, and from frontier to explored. The key invariant is that we have computed the minimum cost paths to all the nodes in the explored set. So when the end state moves into the explored set, then we are done.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">uniformCostSearch</span><span class="params">(problem)</span>:</span></span><br><span class="line">    frontier = util.PriorityQueue()</span><br><span class="line">    frontier.update(problem.startState(), <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        <span class="comment"># Move from frontier to explored</span></span><br><span class="line">        state, pastCost = frontier.removeMin()</span><br><span class="line">        <span class="keyword">if</span> problem.isEnd(state):</span><br><span class="line">            <span class="keyword">return</span> (pastCost, [])</span><br><span class="line">        <span class="comment"># Push out on the frontier</span></span><br><span class="line">        <span class="keyword">for</span> action, newState, cost <span class="keyword">in</span> problem.succAndCost(state):</span><br><span class="line">            frontier.update(newState, pastCost+cost)</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>CS221 course note and ppt <a href="https://stanford-cs221.github.io/autumn2019/" target="_blank" rel="noopener">https://stanford-cs221.github.io/autumn2019/</a></li></ul>]]></content>
    
    <summary type="html">
    
      we will proceed to the first type of state-based models, search problems.
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
    
  </entry>
  
  <entry>
    <title>AI - Overview</title>
    <link href="https://zhangruochi.com/AI-Overview/2020/02/05/"/>
    <id>https://zhangruochi.com/AI-Overview/2020/02/05/</id>
    <published>2020-02-06T02:58:42.000Z</published>
    <updated>2020-02-07T03:41:20.654Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Two-views-of-AI"><a href="#Two-views-of-AI" class="headerlink" title="Two views of AI"></a>Two views of AI</h2><ol><li><strong>AI agents</strong>: how can we create intelligence? </li></ol><ul><li>achieving human-level intelligence, still very far (e.g., generalize from few examples)</li><li>The first is what one would normally associate with AI: the science and engineering of building ”intelligent” agents. The inspiration of what constitutes intelligence comes from the types of capabilities that humans possess: the ability to perceive a very complex world and make enough sense of it to be able to manipulate it.</li></ul><ol><li><strong>AI tools</strong>: how can we benefit society? </li></ol><ul><li>need to think carefully about real-world consequences (e.g., security, biases)</li><li>The second views AI as a set of tools. We are simply trying to solve problems in the world, and techniques developed by the AI community happen to be useful for that, but these problems are not ones that humans necessarily do well on natively.</li></ul><h2 id="How-should-we-actually-solve-AI-tasks"><a href="#How-should-we-actually-solve-AI-tasks" class="headerlink" title="How should we actually solve AI tasks?"></a>How should we actually solve AI tasks?</h2><p>In this class, we will adopt the <strong>modeling-inference-learning</strong> paradigm to help us navigate the solution space. In reality the lines are blurry, but this paradigm serves as an ideal and a useful guiding principle.</p><h3 id="Modeling"><a href="#Modeling" class="headerlink" title="Modeling"></a>Modeling</h3><ul><li>The first pillar is modeling. Modeling takes messy real world problems and packages them into <strong>neat formal mathematical objects</strong> called models, which can be subject to rigorous analysis and can be operated on by computers. However, modeling is lossy: not all of the richness of the real world can be captured, and therefore there is an art of modeling: what does one keep versus ignore? (An exception to this are games such as Chess, Go or Sodoku, where the real world is identical to the model.)</li><li>As an example, suppose we’re trying to have an AI that can navigate through a busy city. We might formulate this as a graph where nodes represent points in the city, edges represent the roads, and the cost of an edge represents the traffic on that road.</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="modeling.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Modeling</div></center><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul><li>The second pillar is inference. Given a model, the task of inference is to answer questions with respect to the model. For example, given the model of the city, one could ask questions such as: what is the shortest path? what is the cheapest path?</li><li>The focus of inference is usually on <strong>efficient algorithms</strong> that can answer these questions. For some models, computational complexity can be a concern (games such as Go), and usually approximations are needed.</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="inference.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Inference</div></center><h3 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h3><ul><li>But where does the model come from? Remember that the real world is rich, so if the model is to be faithful, the model has to be rich as well. But we can’t possibly write down such a rich model manually.</li><li>The idea behind (machine) learning is to instead get it from data. Instead of constructing a model, one constructs a skeleton of a model (more precisely, a model family), which is a model without parameters. And then if we have the right type of data, we can run a machine learning algorithm to tune the parameters of the model.</li><li>Note that learning here is not tied to a particular approach (e.g., neural networks), but more of a philosophy. This general paradigm will allow us to bridge the gap between logic-based AI and statistical AI.</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="learning.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Learning</div></center><h2 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h2><p>We concentrate on the models that we can use to represent real-world tasks. The topics will in a sense advance from low-level intelligence to high-level intelligence, evolving from models that simply make a reflex decision to models that are based on logical reasoning.</p><p>Supporting all of these models is machine learning, which has been arguably the most crucial ingredient powering recent successes in AI. From a systems engineering perspective, machine learning allows us to shift the information complexity of the model from code to data, which is much easier to obtain (either naturally occurring or via crowdsourcing).</p><p>The main conceptually magical part of learning is that if done properly, the trained model will be able to produce good predictions beyond the set of training examples. This leap of faith is called generalization, and is, explicitly or implicitly, at the heart of any machine learning algorithm. This can even be formalized using tools from probability and statistical learning theory.</p><ul><li>The main driver of recent successes in AI</li><li>Move from ”code” to ”data” to manage the information complex-<br>ity</li><li>Requires a leap of faith: <strong>generalization</strong></li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="machine-learning.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">machine-learning.png</div></center><h3 id="Reflex-based-models"><a href="#Reflex-based-models" class="headerlink" title="Reflex-based models"></a>Reflex-based models</h3><p>A reflex-based model simply performs a fixed sequence of computations on a given input. Examples include most models found in machine learning, from simple linear classifiers to deep neural networks. The main characteristic of reflex-based models is that their computations are feed-forward; one doesn’t backtrack and consider alternative computations. Inference is trivial in these models because it is just running the fixed computations, which makes these models appealing.</p><ul><li>linear classifiers, deep neural networks</li><li>Most common models in machine learning</li><li>Fully feed-forward (no backtracking)</li></ul><h3 id="State-based-models"><a href="#State-based-models" class="headerlink" title="State-based models"></a>State-based models</h3><p>Reflex-based models are too simple for tasks that require more forethought (e.g., in playing chess or planning a big trip). State-based models overcome this limitation. The key idea is, at a high-level, to model the state of a world and transitions between states which are triggered by actions. Concretely, one can think of states as nodes in a graph and transitions as edges. This reduction is useful because we understand graphs well and have a lot of efficient algorithms for operating on graphs.</p><ul><li>Search problems: you control everything</li><li>Markov decision processes: against nature (e.g., Blackjack)</li><li>Adversarial games: against opponent (e.g., chess)</li></ul><p>application  </p><ul><li>Games: Chess, Go, Pac-Man, Starcraft, etc.</li><li>Robotics: motion planning</li><li>Natural language generation: machine translation, image caption-<br>ing</li></ul><h3 id="Variable-based-models"><a href="#Variable-based-models" class="headerlink" title="Variable-based models"></a>Variable-based models</h3><ul><li>Constraint satisfaction problems: hard constraints (e.g., Sudoku, scheduling)</li></ul><p>Constraint satisfaction problems are variable-based models where we only have hard constraints. For example, in scheduling, we can’t have two people in the same place at the same time.</p><ul><li>Bayesian networks: soft dependencies (e.g., tracking cars from sensors)</li></ul><p>Bayesian networks are variable-based models where variables are random variables which are dependent on each other. For example, the true location of an airplane Ht and its radar reading Et are related, as are the location Ht and the location at the last time step $H_{t−1}$. The exact dependency structure is given by the graph structure and it formally defines a joint probability distribution over all of the variables. This topic is studied thoroughly in probabilistic graphical models (CS228).</p><h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><p>We are now done with the high-level motivation for the class. Let us now dive into some technical details. Let us focus on the inference and the learning aspect of the modeling-inference-learning paradigm. We will approach <strong>inference</strong> and <strong>learning</strong> from an optimization perspective, which allows us to decouple the mathematical specification of what we want to compute from the algorithms for <strong>how to compute it</strong>.</p><p>In total generality, optimization problems ask that you find the x that lives in a constraint set C that makes the function F(x) as small as possible. There are two types of optimization problems we’ll consider:  discrete optimization problems (mostly for inference) and continuous optimization problems (mostly for learning). Both are backed by a rich research field and are interesting topics in their own right. For this course, we will use the most basic tools from these topics: dynamic programming and gradient descent.</p><ol><li>Discrete optimization: find the best discrete object</li></ol><script type="math/tex; mode=display">min_{p \in Paths} Cost(p)</script><p>Algorithmic tool: dynamic programming</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeEditDistance</span><span class="params">(s, t)</span>:</span></span><br><span class="line">    cache = &#123;&#125;  <span class="comment"># (m, n) =&gt; result</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recurse</span><span class="params">(m, n)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Return the minimum edit distance between:</span></span><br><span class="line"><span class="string">        - first m letters of s</span></span><br><span class="line"><span class="string">        - first n letters of t</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> (m, n) <span class="keyword">in</span> cache:</span><br><span class="line">            <span class="keyword">return</span> cache[(m, n)]</span><br><span class="line">        <span class="keyword">if</span> m == <span class="number">0</span>:  <span class="comment"># Base case</span></span><br><span class="line">            result = n</span><br><span class="line">        <span class="keyword">elif</span> n == <span class="number">0</span>:  <span class="comment"># Base case</span></span><br><span class="line">            result = m</span><br><span class="line">        <span class="keyword">elif</span> s[m - <span class="number">1</span>] == t[n - <span class="number">1</span>]:  <span class="comment"># Last letter matches</span></span><br><span class="line">            result = recurse(m - <span class="number">1</span>, n - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            subCost = <span class="number">1</span> + recurse(m - <span class="number">1</span>, n - <span class="number">1</span>)</span><br><span class="line">            delCost = <span class="number">1</span> + recurse(m - <span class="number">1</span>, n)</span><br><span class="line">            insCost = <span class="number">1</span> + recurse(m, n - <span class="number">1</span>)</span><br><span class="line">            result = min(subCost, delCost, insCost)</span><br><span class="line">        cache[(m, n)] = result</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> recurse(len(s), len(t))</span><br><span class="line"></span><br><span class="line"><span class="comment">#print(computeEditDistance('a cat!', 'the cats!'))</span></span><br><span class="line">print(computeEditDistance(<span class="string">'a cat!'</span> * <span class="number">10</span>, <span class="string">'the cats!'</span> * <span class="number">10</span>))</span><br></pre></td></tr></table></figure><ol><li>Continuous optimization: find the best vector of real numbers</li></ol><script type="math/tex; mode=display">min_{w \in \mathbb{R}^d} TrainingError(w)</script><p>Algorithmic tool: gradient descent</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">points = [(<span class="number">2</span>, <span class="number">4</span>), (<span class="number">4</span>, <span class="number">2</span>)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">F</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum((w * x - y)**<span class="number">2</span> <span class="keyword">for</span> x, y <span class="keyword">in</span> points)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dF</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum(<span class="number">2</span>*(w * x - y) * x <span class="keyword">for</span> x, y <span class="keyword">in</span> points)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Gradient descent</span></span><br><span class="line">w = <span class="number">0</span></span><br><span class="line">eta = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    value = F(w)</span><br><span class="line">    gradient = dF(w)</span><br><span class="line">    w = w - eta * gradient</span><br><span class="line">    print(<span class="string">'iteration &#123;&#125;: w = &#123;&#125;, F(w) = &#123;&#125;'</span>.format(t, w, value))</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>course note from <a href="https://stanford-cs221.github.io/autumn2019/" target="_blank" rel="noopener">cs221</a></li></ul>]]></content>
    
    <summary type="html">
    
      Two views of AI, modeling-inference-learning paradigm, Machine Learning
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
    
  </entry>
  
</feed>
