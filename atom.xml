<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RUOCHI.AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhangruochi.com/"/>
  <updated>2019-12-21T18:20:04.625Z</updated>
  <id>https://zhangruochi.com/</id>
  
  <author>
    <name>Ruochi Zhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Transformer</title>
    <link href="https://zhangruochi.com/Transformer/2019/12/20/"/>
    <id>https://zhangruochi.com/Transformer/2019/12/20/</id>
    <published>2019-12-20T17:24:40.000Z</published>
    <updated>2019-12-21T18:20:04.625Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention.</p><p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations. End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and<br>language modeling tasks.</p><p>To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution. </p><h2 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h2><p>Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\text{model}}$.  We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.  In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to <a href="https://arxiv.org/abs/1608.05859" target="_blank" rel="noopener">(cite)</a>. In the embedding layers, we multiply those weights by $\sqrt{d_{\text{model}}}$.           </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure><h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>Since our model contains no recurrence and no convolution, <strong>in order for the model to make use of the order of the sequence</strong>, we must inject some information about the relative or absolute position of the tokens in the sequence.  To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks.  The positional encodings have the same dimension $d_{\text{model}}$ as the embeddings, so that the two can be summed.   There are many choices of positional encodings, learned and fixed <a href="https://arxiv.org/pdf/1705.03122.pdf" target="_blank" rel="noopener">(cite)</a>. </p><p>In this work, we use sine and cosine functions of different frequencies:    </p><script type="math/tex; mode=display">PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\text{model}}})</script><script type="math/tex; mode=display">PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\text{model}}})</script><p>where $pos$ is the position and $i$ is the dimension.  That is, each dimension of the positional encoding corresponds to a sinusoid.  The wavelengths form a geometric progression from $2\pi$ to $10000 \cdot 2\pi$.  We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. </p><p>In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.  For the base model, we use a rate of $P_{drop}=0.1$. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><h2 id="Layer-Norm-and-residual-connection"><a href="#Layer-Norm-and-residual-connection" class="headerlink" title="Layer Norm and residual connection"></a>Layer Norm and residual connection</h2><p>We employ a residual connection <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">(cite)</a> around each of the two sub-layers, followed by layer normalization <a href="https://arxiv.org/abs/1607.06450" target="_blank" rel="noopener">(cite)</a>.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure><p>That is, the output of each sub-layer is $\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))$, where $\mathrm{Sublayer}(x)$ is the function implemented by the sub-layer itself.  We apply dropout <a href="http://jmlr.org/papers/v15/srivastava14a.html" target="_blank" rel="noopener">(cite)</a> to the output of each sub-layer, before it is added to the sub-layer input and normalized.  </p><p>To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{\text{model}}=512$.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="string">"Apply residual connection to any sublayer with the same size."</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure><p>Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.</p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p>An attention function can be described as mapping a <code>query</code> and a set of <code>key</code>-<code>value</code> pairs to an output, where the query, keys, values, and output are all vectors.  The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p><p>We call our particular attention <code>Scaled Dot-Product Attention</code>.   The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.  We compute the dot products of the query with all keys, divide each by $\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="ModalNet-19.png" width="20%" height="20%"></center><p>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.   The keys and values are also packed together into matrices $K$ and $V$.  We compute the matrix of outputs as:                      </p><script type="math/tex; mode=display">\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V</script><p>The two most commonly used attention functions are additive attention <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">(cite)</a>, and dot-product (multiplicative) attention.  Dot-product attention is identical to our algorithm, except for the scaling factor of $\frac{1}{\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.  While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.</p><p>While for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ <a href="https://arxiv.org/abs/1703.03906" target="_blank" rel="noopener">(cite)</a>. We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients  (To illustrate why the dot products get large, assume that the components of $q$ and $k$ are independent random variables with mean $0$ and variance $1$.  Then their dot product, $q \cdot k = \sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance $d_k$.). To counteract this effect, we scale the dot products by $\frac{1}{\sqrt{d_k}}$.          </p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="attention.png" width="50%" height="50%"></center><h3 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h3><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="ModalNet-20.png" width="50%" height="50%"></center><script type="math/tex; mode=display">\mathrm{MultiHead}(Q, K, V) = \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O    \\                                               \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)</script><p>Where the projections are parameter matrices $W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}$ and $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$.                                                                                                                                                                                            In this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_k=d_v=d_{\text{model}}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="keyword">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">"Implements Figure 2"</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) "Concat" using a view and apply a final linear. </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br></pre></td></tr></table></figure><h3 id="Applications-of-Attention-in-our-Model"><a href="#Applications-of-Attention-in-our-Model" class="headerlink" title="Applications of Attention in our Model"></a>Applications of Attention in our Model</h3><p>The Transformer uses multi-head attention in three different ways:                                                        </p><ol><li><p>In <strong>encoder-decoder attention</strong> layers, the queries come from the previous decoder layer, and the memory keys and values come from the <code>output of the encoder</code>.   This allows every position in the decoder to attend over all positions in the input sequence.  This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as <a href="https://arxiv.org/abs/1609.08144" target="_blank" rel="noopener">(cite)</a>.    </p></li><li><p>The encoder contains self-attention layers.  In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.   Each position in the encoder can attend to all positions in the previous layer of the encoder.                                                   </p></li><li>Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.  We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.  We implement this inside of scaled dot-product attention by masking out (setting to $-\infty$) all values in the input of the softmax which correspond to illegal connections. </li></ol><h2 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h2><p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.  This consists of two linear transformations with a ReLU activation in between.</p><script type="math/tex; mode=display">\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2</script><p>While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.  The dimensionality of input and output is $d_{\text{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implements FFN equation."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>Most competitive neural sequence transduction models have an encoder-decoder structure <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">(cite)</a>. Here, the encoder maps an input sequence of symbol representations $(x_1, …, x_n)$ to a sequence of continuous representations $\mathbf{z} = (z_1, …, z_n)$. Given $\mathbf{z}$, the decoder then generates an output sequence $(y_1,…,y_m)$ of symbols one element at a time. At each step the model is auto-regressive <a href="https://arxiv.org/abs/1308.0850" target="_blank" rel="noopener">(cite)</a>, consuming the previously generated symbols as additional input when generating the next. </p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="ModalNet-21.png" width="50%" height="50%"></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many </span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Take in and process masked src and target sequences."</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Define standard linear + softmax generation step."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="Encoder-and-Decoder-Stacks"><a href="#Encoder-and-Decoder-Stacks" class="headerlink" title="Encoder and Decoder Stacks"></a>Encoder and Decoder Stacks</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>The encoder is composed of a stack of $N=6$ identical layers. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span><span class="params">(module, N)</span>:</span></span><br><span class="line">    <span class="string">"Produce N identical layers."</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Pass the input (and mask) through each layer in turn."</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Encoder is made up of self-attn and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (left) for connections."</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>The decoder is also composed of a stack of $N=6$ identical layers.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Generic N layer decoder with masking."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><p>In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.  Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (right) for connections."</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure></p><p>We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.  This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="string">"Mask out subsequent positions."</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br></pre></td></tr></table></figure><h2 id="Full-Model"><a href="#Full-Model" class="headerlink" title="Full Model"></a>Full Model</h2><p>Here we define a function from hyperparameters to a full model. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></li><li><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-transformer/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;p&gt;The goal of reducing sequential comput
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Subword Models</title>
    <link href="https://zhangruochi.com/Subword-Models/2019/12/19/"/>
    <id>https://zhangruochi.com/Subword-Models/2019/12/19/</id>
    <published>2019-12-19T18:07:57.000Z</published>
    <updated>2019-12-19T21:04:03.603Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Character-Level-Models"><a href="#Character-Level-Models" class="headerlink" title="Character-Level Models"></a>Character-Level Models</h2><ol><li>Word embeddings can be composed from character embeddings<ul><li>Generates embeddings for unknown words</li><li>Similar spellings share similar embeddings</li><li>Solves OOV problem</li></ul></li><li>Motivation<ul><li>Derive a powerful,robust language model effective across a variety of languages.</li><li>Encode subword relatedness:eventful,eventfully, uneventful…</li><li>Address rare-word problem of prior models. </li><li>Obtain comparable expressivity with fewer parameters.</li></ul></li></ol><h2 id="Two-trends"><a href="#Two-trends" class="headerlink" title="Two trends"></a>Two trends</h2><ol><li>Same architecture as forword-level model but use smaller units: “word pieces”</li><li>Hybrid architectures: Main model has words; something else for characters</li></ol><h2 id="Byte-Pair-Encoding"><a href="#Byte-Pair-Encoding" class="headerlink" title="Byte Pair Encoding"></a>Byte Pair Encoding</h2><p>A word segmentation algorithm:</p><ul><li>Start with a vocabulary of characters</li><li>Most frequent ngram pairs -&gt; a new ngram</li><li>Have a target vocabulary size and stop when you reach it</li><li>Do deterministic longest piece segmentation of words</li><li>Segmentation is only within words identified by some prior tokenizer</li></ul><p>For example, all the words in our documents database and their frequency are</p><blockquote><p>{‘l o w’: 5, ‘l o w e r’: 2, ‘n e w e s t’: 6, ‘w i d e s t’: 3}</p></blockquote><p>We can initialize our vocabulary library as:  </p><blockquote><p>{ ‘l’, ‘o’, ‘w’, ‘e’, ‘r’, ‘n’, ‘w’, ‘s’, ‘t’, ‘i’, ‘d’}</p></blockquote><p>The most frequent ngram pair is (‘e’,’s’) and its count is 9. So we add the ‘es’ to our vocabulary library. </p><p>Our documents database now is:</p><blockquote><p>{‘l o w’: 5, ‘l o w e r’: 2, ‘n e w es t’: 6, ‘w i d es t’: 3}.</p></blockquote><p>Our vocabulary library now is:</p><blockquote><p>{ ‘l’, ‘o’, ‘w’, ‘e’, ‘r’, ‘n’, ‘w’, ‘s’, ‘t’, ‘i’, ‘d’, ‘es’}</p></blockquote><p><strong>Again</strong>, the most frequent ngram pair is (‘es’,’t’) and its count is 9，So we add the ‘est’ to our vocabulary library.</p><p>Our documents database now is: </p><blockquote><p>{‘l o w’: 5, ‘l o w e r’: 2, ‘n e w est’: 6, ‘w i d est’: 3}</p></blockquote><p>Our vocabulary library now is:</p><blockquote><p>{ ‘l’, ‘o’, ‘w’, ‘e’, ‘r’, ‘n’, ‘w’, ‘s’, ‘t’, ‘i’, ‘d’, ‘es’,’est’}</p></blockquote><p>the rest can be done in the same manner. We can set a threshold of total count of our vocabulary library. By doing so, we can use BPE to construct a vocabulary library to represent all the words based on subword unit.</p><p>Google NMT(GNMT) uses a variant of this:</p><ul><li>V1: wordpiece model (Word piece model tokenizes inside words)</li><li>V2: sentencepiece model (Sentence piece model works from raw text)</li></ul><h2 id="Character-level-to-build-word-level"><a href="#Character-level-to-build-word-level" class="headerlink" title="Character-level to build word-level"></a>Character-level to build word-level</h2><ol><li>Convolution over characters to generate word embeddings<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%"></center></li><li>Character-based LSTM to build word representation<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center></li></ol><h2 id="CS224n-Assignment5"><a href="#CS224n-Assignment5" class="headerlink" title="CS224n Assignment5"></a>CS224n Assignment5</h2><h3 id="Character-based-convolutional-encoder-for-NMT"><a href="#Character-based-convolutional-encoder-for-NMT" class="headerlink" title="Character-based convolutional encoder for NMT."></a>Character-based convolutional encoder for NMT.</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Figure from cs224n. Character-based convolutional encoder, which ultimately produces a word embedding of length $e_{word}$</div></center><ol><li>Convert word to character indices. We have a word $x$ (e.g. Anarchy in above figure) that we wish to represent. Assume we have a predefined ‘vocabulary’ of characters (for example, all lowercase letters, uppercase letters, numbers, and some punctuation). By looking up the index of each character, we can thus represent the length-l word x as a vector of integers:<script type="math/tex; mode=display">x = \left[ c_1,c_2,\cdots,c_l  \right ] \in \mathbb{Z}^{l}</script>where each $c_i$ is an integer index into the character vocabulary.</li><li><p>Padding and embedding lookup. Using a special <pad> ‘character’, we pad (or truncate) every word so that it has length $m_word$ (this is some predefined hyperparameter representing maximum word length):</pad></p><script type="math/tex; mode=display">x_{padded} = \left [ c_1,c_2,\cdots,c_{m_{word}}  \right ] \in \mathbb{Z}^{m_{word}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_sents_char</span><span class="params">(sents, char_pad_token)</span>:</span></span><br><span class="line">    <span class="string">""" Pad list of sentences according to the longest sentence in the batch and max_word_length.</span></span><br><span class="line"><span class="string">    @param sents (list[list[list[int]]]): list of sentences, result of `words2charindices()`</span></span><br><span class="line"><span class="string">        from `vocab.py`</span></span><br><span class="line"><span class="string">    @param char_pad_token (int): index of the character-padding token</span></span><br><span class="line"><span class="string">    @returns sents_padded (list[list[list[int]]]): list of sentences where sentences/words shorter</span></span><br><span class="line"><span class="string">        than the max length sentence/word are padded out with the appropriate pad token, such that</span></span><br><span class="line"><span class="string">        each sentence in the batch now has same number of words and each word has an equal</span></span><br><span class="line"><span class="string">        number of characters</span></span><br><span class="line"><span class="string">        Output shape: (batch_size, max_sentence_length, max_word_length)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Words longer than 21 characters should be truncated</span></span><br><span class="line">    max_word_length = <span class="number">21</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE for part 1f</span></span><br><span class="line">    <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">    <span class="comment">###     Perform necessary padding to the sentences in the batch similar to the pad_sents()</span></span><br><span class="line">    <span class="comment">###     method below using the padding character from the arguments. You should ensure all</span></span><br><span class="line">    <span class="comment">###     sentences have the same number of words and each word has the same number of</span></span><br><span class="line">    <span class="comment">###     characters.</span></span><br><span class="line">    <span class="comment">###     Set padding words to a `max_word_length` sized vector of padding characters.</span></span><br><span class="line">    <span class="comment">###</span></span><br><span class="line">    <span class="comment">###     You should NOT use the method `pad_sents()` below because of the way it handles</span></span><br><span class="line">    <span class="comment">###     padding and unknown words.</span></span><br><span class="line">    sents_padded = []</span><br><span class="line">    max_sent_len = max([len(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> sents])</span><br><span class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> sents:</span><br><span class="line">        sent = sent + [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(max_sent_len - len(sent))]</span><br><span class="line">        <span class="keyword">assert</span> len(sent) == max_sent_len</span><br><span class="line">        tmp_sent = []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sent:</span><br><span class="line">            word = word[:max_word_length]</span><br><span class="line">            diff = max_word_length - len(word)</span><br><span class="line">            word = word + [char_pad_token] * diff</span><br><span class="line">            <span class="keyword">assert</span> len(word) == max_word_length</span><br><span class="line">            tmp_sent.append(word)</span><br><span class="line">        sents_padded.append(tmp_sent)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sents_padded</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VocabEntry</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">words2charindices</span><span class="params">(self, sents)</span>:</span></span><br><span class="line">            <span class="string">""" Convert list of sentences of words into list of list of list of character indices.</span></span><br><span class="line"><span class="string">            @param sents (list[list[str]]): sentence(s) in words</span></span><br><span class="line"><span class="string">            @return word_ids (list[list[list[int]]]): sentence(s) in indices</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            <span class="comment">### YOUR CODE HERE for part 1e</span></span><br><span class="line">            <span class="comment">### <span class="doctag">TODO:</span> </span></span><br><span class="line">            <span class="comment">###     This method should convert characters in the input sentences into their </span></span><br><span class="line">            <span class="comment">###     corresponding character indices using the character vocabulary char2id </span></span><br><span class="line">            <span class="comment">###     defined above.</span></span><br><span class="line">            <span class="comment">###</span></span><br><span class="line">            <span class="comment">###     You must prepend each word with the `start_of_word` character and append </span></span><br><span class="line">            <span class="comment">###     with the `end_of_word` character. </span></span><br><span class="line"></span><br><span class="line">            word_ids = []</span><br><span class="line">            <span class="keyword">for</span> sent <span class="keyword">in</span> sents:</span><br><span class="line">                sent_chars_id = []</span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> sent:</span><br><span class="line">                    sent_chars_id.append([<span class="number">1</span>] + [ self.char2id.get(char,<span class="number">3</span>) <span class="keyword">for</span> char <span class="keyword">in</span> word ] + [<span class="number">2</span>])</span><br><span class="line">                word_ids.append(sent_chars_id)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> word_ids</span><br><span class="line"></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">to_input_tensor_char</span><span class="params">(self, sents: List[List[str]], device: torch.device)</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">""" Convert list of sentences (words) into tensor with necessary padding for </span></span><br><span class="line"><span class="string">        shorter sentences.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param sents (List[List[str]]): list of sentences (words)</span></span><br><span class="line"><span class="string">        @param device: device on which to load the tensor, i.e. CPU or GPU</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns sents_var: tensor of (max_sentence_length, batch_size, max_word_length)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 1g</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span> </span></span><br><span class="line">        <span class="comment">###     Connect `words2charindices()` and `pad_sents_char()` which you've defined in </span></span><br><span class="line">        <span class="comment">###     previous parts</span></span><br><span class="line">        char_sents =  self.words2charindices(sents)</span><br><span class="line">        padded_char_sents = pad_sents_char(char_sents, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        sents_var = torch.tensor(padded_char_sents, dtype=torch.int8, device= device)</span><br><span class="line">        sents_var = sents_var.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sents_var</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ......</span></span><br></pre></td></tr></table></figure></li><li><p>For each of these characters $c_i$, we lookup a dense character embedding (which has shape $e_{char}$). This yields a tensor $x_{emb}$:</p><script type="math/tex; mode=display">x_{emb} = CharEmbedding(X_{padded}) \in \mathbb{R}^{m_{word} \times e_{char}}</script><p>We’ll reshape $x_{emb}$ to obtain $x_{reshaped} in \mathbb{R}^{e_{char} \times m_{word}}$ before feeding into the convolutional network.</p></li><li><p><strong>Convolutional network</strong>. To combine these character embeddings, we’ll use 1-dimensional convolutions. The convolutional layer has two hyperparameters: the kernel size $k$ (also called window size), which dictates the size of the window used to compute features, and the number of filters $f$, (also called number of output features or number of output channels). The convolutional layer has a weight matrix $W \in \mathbb{R}^{f \times e_{char} \times k}$ and a bias vector $b \in \mathbb{R}^{f}$. Overall this produces output $x_{conv}$.</p><script type="math/tex; mode=display">x_{conv} = Conv1D(x_{reshaped}) \in \mathbb{R}^{f \times {m_{word} - k + 1}}</script><p>For our application, we’ll set $f$ to be equal to $e_{word}$, the size of the final word embedding for word x. Therefore,</p><script type="math/tex; mode=display">x_{conv} \in \mathbb{R}^{e_{word} \times (m_{word} - k + 1)}</script><p>Finally, we apply the <code>ReLU</code> function to $x_{conv}$, then use max-pooling to reduce this to a single vector $x_{conv_out} \in \mathbb{R}^{e_{word}}$, which is the final output of the Convolutional Network:</p><script type="math/tex; mode=display">x_{conv\_out} = MaxPool(ReLU(x_{conv})) \in \mathbb{R}^{e_{word}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### YOUR CODE HERE for part 1i</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" </span></span><br><span class="line"><span class="string">        Init CNN which is a 1-D cnn.</span></span><br><span class="line"><span class="string">        @param embed_size (int): embedding size of char (dimensionality)</span></span><br><span class="line"><span class="string">        @param k: kernel size, also called window size</span></span><br><span class="line"><span class="string">        @param f: number of filters, should be embed_size of word</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">            embed_size: int = <span class="number">50</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 m_word: int = <span class="number">21</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 k: int = <span class="number">5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 f: int = <span class="number">50</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        Init CNN which is a 1-D cnn.</span></span><br><span class="line"><span class="string">        @param embed_size (int): embedding size of char (dimensionality)</span></span><br><span class="line"><span class="string">        @param k: kernel size, also called window size</span></span><br><span class="line"><span class="string">        @param f: number of filters, should be embed_size of word</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        super(CNN, self).__init__()</span><br><span class="line">        self.conv1d = nn.Conv1d(in_channels = embed_size, </span><br><span class="line">                    out_channels = f,</span><br><span class="line">                    kernel_size = k)</span><br><span class="line"></span><br><span class="line">        self.maxpool = nn.MaxPool1d(kernel_size = m_word - k + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X_reshaped: torch.Tensor)</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        map from X_reshaped to X_conv_out</span></span><br><span class="line"><span class="string">        @param X_reshaped (Tensor): Tensor of char-level embedding with shape ( </span></span><br><span class="line"><span class="string">                                    batch_size, e_char, m_word), where e_char = embed_size of char, </span></span><br><span class="line"><span class="string">                                    m_word = max_word_length.</span></span><br><span class="line"><span class="string">        @return X_conv_out (Tensor): Tensor of word-level embedding with shape (max_sentence_length,</span></span><br><span class="line"><span class="string">                                    batch_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        X_conv = self.conv1d(X_reshaped)</span><br><span class="line">        <span class="comment"># print(X_conv.size())</span></span><br><span class="line">        X_conv_out = self.maxpool(F.relu(X_conv))</span><br><span class="line">        <span class="comment"># print(X_conv_out.size())</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X_conv_out.squeeze(<span class="number">-1</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>Highway layer and dropout</strong>. Highway Networks6 have a skip-connection controlled by a dynamic gate. Given the input $x_{conv\_out} \in \mathbb{R}^{e_{word}}$, we compute:<br><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%"> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Figure from cs224n. Highway Network (Srivastava et al. 2015)</div></center></p><script type="math/tex; mode=display">\begin{align}& x_{proj} = RELU(W_{proj}x_{conv\_cout} + b_{proj}) \quad \in \mathbb{R}^{e_{word}} \\& x_{gate} = \sigma(W_{gate}x_{conv\_out} + b_{gate}) \quad \in \mathbb{R}^{e_{word}} \\& x_{highway} = x_{gate} \circ x_{proj} + ( 1 - x_{gate}) \circ x_{conv\_out}  \quad \in \mathbb{R}^{e_{word}}\\& x_{word_emb} = Dropout(x_{highway}) \quad \in \mathbb{R}^{e_{word}} \end{align}</script><p>Where $W_{proj},W_{gate} \in \mathbb{R}^{e_{word} \times e_{word}}$, and $\circ$ denotes element-wise multiplication.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment">### YOUR CODE HERE for part 1h</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Highway</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" Highway network for ConvNN</span></span><br><span class="line"><span class="string">        - Relu</span></span><br><span class="line"><span class="string">        - Sigmoid</span></span><br><span class="line"><span class="string">        - gating mechanism from LSTM</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,embed_size)</span>:</span></span><br><span class="line">        <span class="string">""" Init Higway network</span></span><br><span class="line"><span class="string">            @param embed_size (int): Embedding size of word, in handout, </span></span><br><span class="line"><span class="string">                                     it's e_&#123;word&#125; (dimensionality)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        super(Highway, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.projection = nn.Linear(embed_size,embed_size,bias = <span class="keyword">True</span>)</span><br><span class="line">        self.gate = nn.Linear(embed_size,embed_size, bias = <span class="keyword">True</span>)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X_conv_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            Take mini-batch of sentence of ConvNN</span></span><br><span class="line"><span class="string">            @param X_conv_out (Tensor): Tensor with shape (max_sentence_length, batch_size, embed_size)</span></span><br><span class="line"><span class="string">            @return X_highway (Tensor): combinded output with shape (max_sentence_length, batch_size, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        X_proj = F.relu(self.projection(X_conv_out))</span><br><span class="line">        X_gate = torch.sigmoid(self.gate(X_conv_out))</span><br><span class="line">        X_highway =  torch.mul(X_gate, X_proj) + torch.mul((<span class="number">1</span> - X_gate),X_conv_out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X_highway</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### END YOUR CODE</span></span><br></pre></td></tr></table></figure></li><li><p>Combine above steps together to get our <strong>Character-based word embedding model</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 5</span></span><br><span class="line"><span class="string">model_embeddings.py: Embeddings for the NMT model</span></span><br><span class="line"><span class="string">Pencheng Yin &lt;pcyin@cs.cmu.edu&gt;</span></span><br><span class="line"><span class="string">Sahil Chopra &lt;schopra8@stanford.edu&gt;</span></span><br><span class="line"><span class="string">Anand Dhoot &lt;anandd@stanford.edu&gt;</span></span><br><span class="line"><span class="string">Michael Hahn &lt;mhahn2@stanford.edu&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not change these imports; your module names should be</span></span><br><span class="line"><span class="comment">#   `CNN` in the file `cnn.py`</span></span><br><span class="line"><span class="comment">#   `Highway` in the file `highway.py`</span></span><br><span class="line"><span class="comment"># Uncomment the following two imports once you're ready to run part 1(j)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> cnn <span class="keyword">import</span> CNN</span><br><span class="line"><span class="keyword">from</span> highway <span class="keyword">import</span> Highway</span><br><span class="line"></span><br><span class="line"><span class="comment"># End "do not change" </span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelEmbeddings</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Class that converts input words to their CNN-based embeddings.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embed_size, vocab)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Init the Embedding layer for one language</span></span><br><span class="line"><span class="string">        @param embed_size (int): Embedding size (dimensionality) for the output </span></span><br><span class="line"><span class="string">        @param vocab (VocabEntry): VocabEntry object. See vocab.py for documentation.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ModelEmbeddings, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment">## A4 code</span></span><br><span class="line">        <span class="comment"># pad_token_idx = vocab.src['&lt;pad&gt;']</span></span><br><span class="line">        <span class="comment"># self.embeddings = nn.Embedding(len(vocab.src), embed_size, padding_idx=pad_token_idx)</span></span><br><span class="line">        <span class="comment">## End A4 code</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 1j</span></span><br><span class="line">        pad_token_idx = vocab.char2id[<span class="string">'&lt;pad&gt;'</span>]</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        char_embed_size = <span class="number">50</span></span><br><span class="line">        self.char_embedding = nn.Embedding(len(vocab.char2id),</span><br><span class="line">                                           char_embed_size,</span><br><span class="line">                                           pad_token_idx)</span><br><span class="line">        self.convNN = CNN(f=self.embed_size)</span><br><span class="line">        self.highway = Highway(embed_size=self.embed_size)</span><br><span class="line">        self.dropout = nn.Dropout(p=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Looks up character-based CNN embeddings for the words in a batch of sentences.</span></span><br><span class="line"><span class="string">        @param input: Tensor of integers of shape (sentence_length, batch_size, max_word_length) where</span></span><br><span class="line"><span class="string">            each integer is an index into the character vocabulary</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param output: Tensor of shape (sentence_length, batch_size, embed_size), containing the </span></span><br><span class="line"><span class="string">            CNN-based embeddings for each word of the sentences in the batch</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">## A4 code</span></span><br><span class="line">        <span class="comment"># output = self.embeddings(input)</span></span><br><span class="line">        <span class="comment"># return output</span></span><br><span class="line">        <span class="comment">## End A4 code</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 1j</span></span><br><span class="line">        X_word_emb_list = []</span><br><span class="line">        <span class="keyword">for</span> X_padded <span class="keyword">in</span> input:</span><br><span class="line">            <span class="comment"># (batch_size,max_word_length) -&gt; (batch_size,max_word_length,embed_size)</span></span><br><span class="line">            X_emb = self.char_embedding(X_padded)</span><br><span class="line">            <span class="comment"># print(X_emb.size())</span></span><br><span class="line">            X_reshaped = X_emb.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">            X_conv_out = self.convNN(X_reshaped)</span><br><span class="line">            X_highway = self.highway(X_conv_out)</span><br><span class="line">            X_word_emb = self.dropout(X_highway)</span><br><span class="line">            X_word_emb_list.append(X_word_emb)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (sentence_length, batch_size, embed_size)</span></span><br><span class="line">        X_word_emb = torch.stack(X_word_emb_list, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X_word_emb</span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="Character-based-LSTM-decoder-for-NMT"><a href="#Character-based-LSTM-decoder-for-NMT" class="headerlink" title="Character-based LSTM decoder for NMT"></a>Character-based LSTM decoder for NMT</h3><p>We will now add a LSTM-based character-level decoder to our NMT system. The main idea is that when our word-level decoder produces and <code>&lt;UNK&gt;</code> token, we run our character-level decoder (which you can think of as a character-level conditional language model) to instead generate the target word one character at a time, as shown in below figure. This will help us to produce rare and out-of-vocabulary target words.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Figure from cs224n. A character-based decoder which is triggered if the word-based decoder produces an UNK. Figure courtesy of Luong & Manning.</div></center><p>We now describe the model in three sections:</p><ol><li><p><strong>Forward computation of Character Decoder</strong>: Given a sequence of integers $x_i,\cdots,x_n \in \mathbb{Z}$ representing a sequence of characters, we lookup their character embeddings $x_i,\cdots,x_n \in \mathbb{Z}^{e_{char}}$ and pass these as input in to the(unidirectional)LSTM,obtaining hidden states $h1, \cdots, h_n$ and cell states $c_1, \cdots, c_n$</p><script type="math/tex; mode=display">h_t,c_t = CharDecoderLSTM(x_t,h_{t-1},c_{t-1}) \quad \text{where} \quad h_t,c_t \in \mathbb{R}^{h}</script><p>where h is the hidden size of the CharDecoderLSTM. The initial hidden and cell states $h_0$ and $c_0$ are both set to the <strong>combined output</strong> vector (attentioned) for the current timestep of the main word-level NMT decoder.<br>For every timestep $t \in { 1, \cdots, n }$ we compute scores (also called logits) $s_t \in \mathbb{R}^{V_{char}}$</p><script type="math/tex; mode=display">s_t = W_{dec}h_t + b_{dec} \in \mathbb{R}^{V_{char}}</script><p>Where the weight matrix $W_{dec} \in \mathbb{R}^{V_{char} \times h}$ and the bias vector $b_{dec} \in \mathbb{R}^{V_{char}}$. If we passed $s_t$ through a softmax function, we would have the probability distribution for the next character in the sequence.</p></li><li><p><strong>Training of Character Decoder</strong> When we train the NMT system, we train the character decoder on <strong>every word</strong> in the target sentence (not just the words represented by <unk>). For example, on a particular step of the main NMT decoder, if the target word is music then the input sequence for the CharDecoderLSTM is $[x_1,…,x_n]$ = [<start>,m,u,s,i,c] and the target sequence for the CharDecoderLSTM is $[x_{2}, . . . , x_{n+1}]$ = [m,u,s,i,c,<end>].<br>We pass the input sequence $x_1, \cdots, x_n$, along with the initial states $h_0$ and $c_0$ obtained from the combined output vector) into the CharDecoderLSTM, thus obtaining scores $s_1,\cdots, s_n$ which we will compare to the target sequence $x_2,\cdots, x_{n+1}$. We optimize with respect to sum of the cross-entropy loss:</end></start></unk></p><script type="math/tex; mode=display">p_t = softmax(s_t) \in \mathbb{R}^{V_{char}}</script><script type="math/tex; mode=display">loss_{char\_dec} = -\sum_{t=1}^{n}log p_t(x_{t+1})</script></li><li><p><strong>Decoding from the Character Decoder</strong> t test time, first we produce a translation from our word- based NMT system in the usual way (e.g. a decoding algorithm like beam search). If the translation contains any <unk> tokens, then for each of those positions, we use the word-based decoder’s combined output vector to initialize the CharDecoderLSTM initial $h_0$ and $c_0$, then use CharDecoderLSTM to generate a sequence of characters. To generate the sequence of characters, we use the greedy decoding algorithm, which repeatedly chooses the most probable next character, until either the <end> token is produced or we reach a predetermined max length. The algorithm is given below, for a single example (not batched).</end></unk></p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="100%" height="100%"> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Figure from cs224n. Greedy Decoding</div></center></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CharDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, char_embedding_size=<span class="number">50</span>, target_vocab=None)</span>:</span></span><br><span class="line">        <span class="string">""" Init Character Decoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param hidden_size (int): Hidden size of the decoder LSTM</span></span><br><span class="line"><span class="string">        @param char_embedding_size (int): dimensionality of character embeddings</span></span><br><span class="line"><span class="string">        @param target_vocab (VocabEntry): vocabulary for the target language. See vocab.py for documentation.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 2a</span></span><br><span class="line">        <span class="comment">### TODO - Initialize as an nn.Module.</span></span><br><span class="line">        <span class="comment">###      - Initialize the following variables:</span></span><br><span class="line">        <span class="comment">###        self.charDecoder: LSTM. Please use nn.LSTM() to construct this.</span></span><br><span class="line">        <span class="comment">###        self.char_output_projection: Linear layer, called W_&#123;dec&#125; and b_&#123;dec&#125; in the PDF</span></span><br><span class="line">        <span class="comment">###        self.decoderCharEmb: Embedding matrix of character embeddings</span></span><br><span class="line">        <span class="comment">###        self.target_vocab: vocabulary for the target language</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Hint: - Use target_vocab.char2id to access the character vocabulary for the target language.</span></span><br><span class="line">        <span class="comment">###       - Set the padding_idx argument of the embedding matrix.</span></span><br><span class="line">        <span class="comment">###       - Create a new Embedding layer. Do not reuse embeddings created in Part 1 of this assignment.</span></span><br><span class="line">        </span><br><span class="line">        super(CharDecoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.char_embedding_size = char_embedding_size</span><br><span class="line">        self.target_vocab = target_vocab</span><br><span class="line">        self.padding_idx = self.target_vocab.char2id[<span class="string">'&lt;pad&gt;'</span>]</span><br><span class="line">        self.decoderCharEmb = nn.Embedding(len(self.target_vocab.char2id),</span><br><span class="line">                                           char_embedding_size,</span><br><span class="line">                                           self.padding_idx)</span><br><span class="line">        self.charDecoder = nn.LSTM(input_size=char_embedding_size,</span><br><span class="line">                                   hidden_size=hidden_size)</span><br><span class="line">        self.char_output_projection = nn.Linear(hidden_size,</span><br><span class="line">                                                len(self.target_vocab.char2id))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, dec_hidden=None)</span>:</span></span><br><span class="line">        <span class="string">""" Forward pass of character decoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param input: tensor of integers, shape (length, batch)</span></span><br><span class="line"><span class="string">        @param dec_hidden: internal state of the LSTM before reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns scores: called s_t in the PDF, shape (length, batch, self.vocab_size)</span></span><br><span class="line"><span class="string">        @returns dec_hidden: internal state of the LSTM after reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 2b</span></span><br><span class="line">        <span class="comment">### TODO - Implement the forward pass of the character decoder.</span></span><br><span class="line">        char_embeddings = self.decoderCharEmb(input)        <span class="comment"># (length, batch, char_embed_size)</span></span><br><span class="line">        hidden_states, dec_hidden = self.charDecoder(</span><br><span class="line">            char_embeddings, dec_hidden)    <span class="comment"># (length, batch, hidden_size)</span></span><br><span class="line">        scores = self.char_output_projection(hidden_states)     <span class="comment"># (len, batch, vocab)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> scores, dec_hidden</span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_forward</span><span class="params">(self, char_sequence, dec_hidden=None)</span>:</span></span><br><span class="line">        <span class="string">""" Forward computation during training.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param char_sequence: tensor of integers, shape (length, batch). Note that "length" here and in forward() need not be the same.</span></span><br><span class="line"><span class="string">        @param dec_hidden: initial internal state of the LSTM, obtained from the output of the word-level decoder. A tuple of two tensors of shape (1, batch, hidden_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns The cross-entropy loss, computed as the *sum* of cross-entropy losses of all the words in the batch.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 2c</span></span><br><span class="line">        <span class="comment">### TODO - Implement training forward pass.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Hint: - Make sure padding characters do not contribute to the cross-entropy loss.</span></span><br><span class="line">        <span class="comment">###       - char_sequence corresponds to the sequence x_1 ... x_&#123;n+1&#125; from the handout (e.g., &lt;START&gt;,m,u,s,i,c,&lt;END&gt;).</span></span><br><span class="line">        scores, dec_hidden = self.forward(char_sequence[:<span class="number">-1</span>], dec_hidden)</span><br><span class="line">        <span class="comment"># char_embeddings = self.decoderCharEmb(char_sequence)</span></span><br><span class="line">        <span class="comment"># hidden_states, dec_hidden = self.charDecoder(char_embeddings[:-1], dec_hidden)</span></span><br><span class="line">        <span class="comment"># scores = self.char_output_projection(hidden_states)  # (len, batch, vocab)</span></span><br><span class="line">        loss = nn.CrossEntropyLoss(ignore_index=self.padding_idx,</span><br><span class="line">                                   reduction=<span class="string">'sum'</span>)</span><br><span class="line">        ce_loss = loss(scores.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">                       char_sequence[<span class="number">1</span>:].transpose(<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ce_loss</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode_greedy</span><span class="params">(self, initialStates, device, max_length=<span class="number">21</span>)</span>:</span></span><br><span class="line">        <span class="string">""" Greedy decoding</span></span><br><span class="line"><span class="string">        @param initialStates: initial internal state of the LSTM, a tuple of two tensors of size (1, batch, hidden_size)</span></span><br><span class="line"><span class="string">        @param device: torch.device (indicates whether the model is on CPU or GPU)</span></span><br><span class="line"><span class="string">        @param max_length: maximum length of words to decode</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns decodedWords: a list (of length batch) of strings, each of which has length &lt;= max_length.</span></span><br><span class="line"><span class="string">                              The decoded strings should NOT contain the start-of-word and end-of-word characters.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 2d</span></span><br><span class="line">        <span class="comment">### TODO - Implement greedy decoding.</span></span><br><span class="line">        <span class="comment">### Hints:</span></span><br><span class="line">        <span class="comment">###      - Use target_vocab.char2id and target_vocab.id2char to convert between integers and characters</span></span><br><span class="line">        <span class="comment">###      - Use torch.tensor(..., device=device) to turn a list of character indices into a tensor.</span></span><br><span class="line">        <span class="comment">###      - We use curly brackets as start-of-word and end-of-word characters. That is, use the character '&#123;' for &lt;START&gt; and '&#125;' for &lt;END&gt;.</span></span><br><span class="line">        <span class="comment">###        Their indices are self.target_vocab.start_of_word and self.target_vocab.end_of_word, respectively.</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">        output_words = []</span><br><span class="line">        decodedWords = []</span><br><span class="line">        start_idx = self.target_vocab.start_of_word</span><br><span class="line">        end_idx = self.target_vocab.end_of_word</span><br><span class="line">        dec_hidden = initialStates</span><br><span class="line">        batch_size = dec_hidden[<span class="number">0</span>].shape[<span class="number">1</span>]</span><br><span class="line">        current_char = torch.tensor([[start_idx] * batch_size],</span><br><span class="line">                                    device=device)  <span class="comment"># idx of '&lt;start&gt;' token</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(max_length):</span><br><span class="line">            scores, dec_hidden = self.forward(current_char, dec_hidden)</span><br><span class="line">            current_char = scores.argmax(<span class="number">-1</span>)</span><br><span class="line">            output_words += [current_char]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        output_words = torch.cat(output_words).t().tolist()</span><br><span class="line">        <span class="keyword">for</span> foo <span class="keyword">in</span> output_words:</span><br><span class="line">            word = <span class="string">""</span></span><br><span class="line">            <span class="keyword">for</span> bar <span class="keyword">in</span> foo:</span><br><span class="line">                <span class="keyword">if</span> bar == end_idx:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                word += self.target_vocab.id2char[bar]</span><br><span class="line">            decodedWords += [word]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> decodedWords</span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Course note and slides of <a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">cs224n</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Character-Level-Models&quot;&gt;&lt;a href=&quot;#Character-Level-Models&quot; class=&quot;headerlink&quot; title=&quot;Character-Level Models&quot;&gt;&lt;/a&gt;Character-Level Mode
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Attention</title>
    <link href="https://zhangruochi.com/Attention/2019/12/16/"/>
    <id>https://zhangruochi.com/Attention/2019/12/16/</id>
    <published>2019-12-17T01:55:07.000Z</published>
    <updated>2019-12-19T18:10:20.263Z</updated>
    
    <content type="html"><![CDATA[<h2 id="General-definition-of-attention"><a href="#General-definition-of-attention" class="headerlink" title="General definition of attention"></a>General definition of attention</h2><p>Given a set of vector <strong>values</strong>, and a vector <strong>query</strong>, attention is a technique to compute a <strong>weighted sum</strong> of the values, dependent on the query.</p><ul><li>We sometimes say that the query attends to the values.</li><li>For example, in the seq2seq + attention model, each decoder hidden state (query) attends to all the encoder hidden states<br>75 (values).<ul><li>The weighted sum is a <strong>selective</strong> summary of the information contained in the values, where the query determines which values to focus on.</li><li>Attention is a way to obtain a <strong>fixed-size representation</strong> of an arbitrary set of representations (the values), dependent on some other representation (the query).</li></ul></li></ul><h2 id="How-to-do-attention"><a href="#How-to-do-attention" class="headerlink" title="How to do attention"></a>How to do attention</h2><ol><li>We have some <strong>values</strong> $h1$,$\cdots$,$h_N$ $\in \mathbb{R}^{d_1}$ and a <strong>query</strong> $s \in \mathbb{R}^{d_2}$</li><li>Computing the attention scores (multiple ways to do this)<script type="math/tex; mode=display">e \in \mathbb{R}^{N}</script></li><li>Taking softmax to get attention distribution $\alpha$<script type="math/tex; mode=display">\alpha = softmax(e) \in \mathbb{R}^{N}</script></li><li>Using attention distribution to take weighted sum of values:<script type="math/tex; mode=display">a = \sum_{i=1}^{N}\alpha_i h_i \in \mathbb{R}^{d_1}</script>thus obtaining the attention output a (sometimes called the <strong>context vector</strong>)</li></ol><h2 id="Bidirectional-RNNs"><a href="#Bidirectional-RNNs" class="headerlink" title="Bidirectional RNNs"></a>Bidirectional RNNs</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture notes of cs224n</div>    <br>    <br></center><p>Bidirectional RNNs fix this problem by traversing a sequence in both directions and concatenating the resulting outputs (both cell outputs and final hidden states). For every RNN cell, we simply add another cell but feed inputs to it in the opposite direction; the output $O_t$ corresponding to the $t\prime$ word is the concatenated vector $\left [ o_t^{(f)}, o_t^{(b)}  \right ]$ where $o_t^{(f)}$ is the output of the forward-direction RNN on word t and $o_t^{(b)}$ is the corresponding output from the reverse-direction RNN. Similarly, the final hidden state is $h = \left [   h^{(f)}, h^{(b)}  \right ]$.</p><h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><p>Sequence-to-sequence, or “Seq2Seq”, is a relatively new paradigm,with its first published usage in 2014 for English-French translation. At a high level, a sequence-to-sequence model is an end-to-end model made up of two recurrent neural networks:<br>Sutskever et al. 2014, “Sequence to Sequence Learning with Neural Networks”</p><ul><li>an encoder, which takes the model’s input sequence as input and encodes it into a fixed-size “context vector”</li><li>a decoder, which uses the context vector from above as a “seed” from which to generate an output sequence.<br>For this reason, Seq2Seq models are often referred to as “encoder- decoder models.” We’ll look at the details of these two networks separately.</li></ul><h3 id="Seq2Seq-architecture-encoder"><a href="#Seq2Seq-architecture-encoder" class="headerlink" title="Seq2Seq architecture - encoder"></a>Seq2Seq architecture - encoder</h3><blockquote><p>Encoder RNN produces an encoding of the source sentence.</p></blockquote><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture notes of cs224n</div>    <br>    <br></center><p>The encoder network’s job is to read the input sequence to our Seq2Seq model and generate a fixed-dimensional context vector <strong>C</strong> for the sequence. To do so, the encoder will use a recurrent neural network cell – usually an LSTM – to read the input tokens one at a time. The final hidden state of the cell will then become C. However, because it’s so difficult to compress an arbitrary-length sequence into a single fixed-size vector (especially for difficult tasks like transla- tion), the encoder will usually consist of stacked LSTMs: a series of LSTM “layers” where each layer’s outputs are the input sequence to the next layer. The final layer’s LSTM hidden state will be used as <strong>C</strong>.</p><p>Seq2Seq encoders will often do something strange: they will pro- cess the input sequence in reverse. This is actually done on purpose. The idea is that, by doing this, the last thing that the encoder sees will (roughly) corresponds to the first thing that the model outputs; this makes it easier for the decoder to “get started” on the output, which makes then gives the decoder an easier time generating a proper output sentence. In the context of translation, we’re allowing the network to translate the first few words of the input as soon as it sees them; once it has the first few words translated correctly, it’s much easier to go on to construct a correct sentence than it is to do so from scratch.</p><h3 id="Seq2Seq-architecture-decoder"><a href="#Seq2Seq-architecture-decoder" class="headerlink" title="Seq2Seq architecture - decoder"></a>Seq2Seq architecture - decoder</h3><blockquote><p>Decoder RNN is a Language Model that generates target sentence, conditioned on encoding.</p></blockquote><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture notes of cs224n</div>    <br>    <br></center><p>The decoder is also an LSTM network, but its usage is a little more complex than the encoder network. Essentially, we’d like to use it as a <strong>language model</strong> that’s “aware” of the words that it’s generated so far and of the input. To that end, we’ll keep the “stacked” LSTM architecture from the encoder, but we’ll initialize the hidden state of our first layer with the context vector from above; the decoder will literally use the context of the input to generate an output.</p><p>Once the decoder is set up with its context, we’ll pass in a special token to signify the start of output generation; in literature, this is usually an <eos> token appended to the end of the input (there’s also one at the end of the output). Then, we’ll run all three layers of LSTM, one after the other, following up with a softmax on the final layer’s output to generate the first output word. Then, we pass that word into the first layer, and repeat the generation. This is how we get the LSTMs to act like a language model. See Fig. 2 for an example of a decoder network.</eos></p><p>Once we have the output sequence, we use the same learning strat- egy as usual. We define a loss, the cross entropy on the prediction sequence, and we minimize it with a gradient descent algorithm and back-propagation. Both the encoder and decoder are trained at the same time, so that they both learn the same context vector represen- tation.</p><h2 id="Training-a-Neural-Machine-Translation-system"><a href="#Training-a-Neural-Machine-Translation-system" class="headerlink" title="Training a Neural Machine Translation system"></a>Training a Neural Machine Translation system</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture notes of cs224n</div>    <br>    <br></center><h3 id="Greedy-Search"><a href="#Greedy-Search" class="headerlink" title="Greedy Search"></a>Greedy Search</h3><p>At each time step, we pick the most probable token. In other words</p><script type="math/tex; mode=display">x_t = argmax_{\tilde{x_t} \mathbb{P}(\tilde(x_t)| x_1, \cdots, x_t)}</script><p>This technique is efficient and natural, however it explores a small part of the search space and if we make a mistake at one time step, the rest of the sentence could be heavily impacted.</p><h3 id="Beam-search-decoding"><a href="#Beam-search-decoding" class="headerlink" title="Beam search decoding"></a>Beam search decoding</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture notes of cs224n</div>    <br>    <br></center><p>the idea is to maintain K candidates at each time step.</p><script type="math/tex; mode=display">H_t = \left\{ (x_1^{1}, \cdots, x_t^1), \cdots, (x_1^k, \cdots, x_t^k) \right\}</script><p>and compute $H_{t+1}$ by expanding $H_t$ and keeping the best K candi- dates. In other words, we pick the best K sequence in the following set</p><script type="math/tex; mode=display">\tilde{H_{t+1}} = \cup_{k=1}^{k}H_{t+1}^{\tilde{k}}</script><p>where</p><script type="math/tex; mode=display">\tilde{H_t} = \left\{ (x_1^{k}, \cdots, x_t^{k}, v_1), \cdots, (x_1^{k}, \cdots, x_t^{k}, V_{|v|}) \right\}</script><p>As we increase K, we gain precision and we are asymptotically exact. However, the improvement is not monotonic and we can set a K that combines reasonable performance and computational efficiency. </p><h2 id="CS224n-Assignment4"><a href="#CS224n-Assignment4" class="headerlink" title="CS224n Assignment4"></a>CS224n Assignment4</h2><p>In Machine Translation, our goal is to convert a sentence from the source language (e.g. Spanish) to the target language (e.g. English). In this assignment, we will implement a sequence-to-sequence (Seq2Seq) network with attention, to build a Neural Machine Translation (NMT) system. In this section, we describe the training procedure for the proposed NMT system, which uses a Bidirectional LSTM Encoder and a Unidirectional LSTM Decoder.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture notes of cs224n</div>    <br>    <br></center><h3 id="Initialize"><a href="#Initialize" class="headerlink" title="Initialize"></a>Initialize</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embed_size, hidden_size, vocab, dropout_rate=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        <span class="string">""" Init NMT Model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param embed_size (int): Embedding size (dimensionality)</span></span><br><span class="line"><span class="string">        @param hidden_size (int): Hidden Size (dimensionality)</span></span><br><span class="line"><span class="string">        @param vocab (Vocab): Vocabulary object containing src and tgt languages</span></span><br><span class="line"><span class="string">                              See vocab.py for documentation.</span></span><br><span class="line"><span class="string">        @param dropout_rate (float): Dropout probability, for attention</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(NMT, self).__init__()</span><br><span class="line">        self.model_embeddings = ModelEmbeddings(embed_size, vocab)</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.dropout_rate = dropout_rate</span><br><span class="line">        self.vocab = vocab</span><br><span class="line"></span><br><span class="line">        <span class="comment"># default values</span></span><br><span class="line">        self.encoder = <span class="keyword">None</span> </span><br><span class="line">        self.decoder = <span class="keyword">None</span></span><br><span class="line">        self.h_projection = <span class="keyword">None</span></span><br><span class="line">        self.c_projection = <span class="keyword">None</span></span><br><span class="line">        self.att_projection = <span class="keyword">None</span></span><br><span class="line">        self.combined_output_projection = <span class="keyword">None</span></span><br><span class="line">        self.target_vocab_projection = <span class="keyword">None</span></span><br><span class="line">        self.dropout = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~8 Lines)</span></span><br><span class="line">        <span class="comment">### TODO - Initialize the following variables:</span></span><br><span class="line">        <span class="comment">###     self.encoder (Bidirectional LSTM with bias)</span></span><br><span class="line">        <span class="comment">###     self.decoder (LSTM Cell with bias)</span></span><br><span class="line">        <span class="comment">###     self.h_projection (Linear Layer with no bias), called W_&#123;h&#125; in the PDF.</span></span><br><span class="line">        <span class="comment">###     self.c_projection (Linear Layer with no bias), called W_&#123;c&#125; in the PDF.</span></span><br><span class="line">        <span class="comment">###     self.att_projection (Linear Layer with no bias), called W_&#123;attProj&#125; in the PDF.</span></span><br><span class="line">        <span class="comment">###     self.combined_output_projection (Linear Layer with no bias), called W_&#123;u&#125; in the PDF.</span></span><br><span class="line">        <span class="comment">###     self.target_vocab_projection (Linear Layer with no bias), called W_&#123;vocab&#125; in the PDF.</span></span><br><span class="line">        <span class="comment">###     self.dropout (Dropout Layer)</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Use the following docs to properly initialize these variables:</span></span><br><span class="line">        <span class="comment">###     LSTM:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM</span></span><br><span class="line">        <span class="comment">###     LSTM Cell:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell</span></span><br><span class="line">        <span class="comment">###     Linear Layer:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear</span></span><br><span class="line">        <span class="comment">###     Dropout Layer:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout</span></span><br><span class="line"></span><br><span class="line">        self.encoder = nn.LSTM(embed_size, self.hidden_size, dropout=self.dropout_rate,bias = <span class="keyword">True</span>, bidirectional = <span class="keyword">True</span>)</span><br><span class="line">        self.decoder = nn.LSTMCell(embed_size + self.hidden_size, self.hidden_size, bias = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        self.h_projection = nn.Linear(<span class="number">2</span> * self.hidden_size, self.hidden_size, bias = <span class="keyword">False</span>)</span><br><span class="line">        self.c_projection = nn.Linear(<span class="number">2</span> * self.hidden_size, self.hidden_size, bias = <span class="keyword">False</span>)</span><br><span class="line">        self.att_projection = nn.Linear(<span class="number">2</span> * self.hidden_size, self.hidden_size, bias = <span class="keyword">False</span>)</span><br><span class="line">        self.combined_output_projection = nn.Linear(<span class="number">3</span> * self.hidden_size, self.hidden_size, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.target_vocab_projection = nn.Linear(self.hidden_size, self.model_embeddings.target.weight.shape[<span class="number">0</span>])</span><br><span class="line">        self.dropout = nn.Dropout(p = self.dropout_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br></pre></td></tr></table></figure><h3 id="Encode"><a href="#Encode" class="headerlink" title="Encode"></a>Encode</h3><p>Given a sentence in the source language, we look up the word embeddings from an embeddings matrix, yielding $x_1,\cdots,x_m | x_i \in \mathbb{R}^{e x 1}$,  where m is the length of the source sentence and e is the embedding size. We feed these embeddings to the bidirectional Encoder, yielding hidden states and cell states for both the forwards (-&gt;) and backwards (&lt;-) LSTMs. The forwards and backwards versions are concatenated<br>to give hidden states $h_i^{enc}$ and cell states $c_i^{enc}$</p><script type="math/tex; mode=display">\begin{align}& h_i^{enc} = \left [  \overleftarrow{h_i^{enc}}; \overrightarrow{h_i^{enc}} \right ] \qquad \text{where} \qquad h_i^{enc} \in \mathbb{R}^{2h x 1},  \overleftarrow{h_i^{enc}}, \overrightarrow{h_i^{enc}} \in \mathbb{R}^{h x 1}  \qquad 1 \leq i \leq m  \\& c_i^{enc} = \left [  \overleftarrow{c_i^{enc}}; \overrightarrow{c_i^{enc}} \right ] \qquad \text{where}  \qquad c_i^{enc} \in \mathbb{R}^{2h x 1},  \overleftarrow{c_i^{enc}}, \overrightarrow{c_i^{enc}} \in \mathbb{R}^{h x 1}  \qquad 1 \leq i \leq m \\ \end{align}</script><p>We then initialize the Decoder’s first hidden state $h_0^{dec}$ and cell state $c_0^{dec}$ with a linear projection of the Encoder’s final hidden state and final cell state</p><script type="math/tex; mode=display">\begin{align}& h_0^{dec} = W_h \left [  \overleftarrow{h_1^{enc}}; \overrightarrow{h_m^{enc}} \right ] \qquad \text{where} \qquad h_0^{dec} \in \mathbb{R}^{h x 1},  W_h \in \mathbb{R}^{h x 2h} \\& c_0^{dec} = W_h \left [  \overleftarrow{c_1^{enc}}; \overrightarrow{c_m^{enc}} \right ] \qquad \text{where} \qquad c_0^{dec} \in \mathbb{R}^{h x 1},  W_c \in \mathbb{R}^{h x 2h} \\\end{align}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, source_padded: torch.Tensor, source_lengths: List[int])</span> -&gt; Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:</span></span><br><span class="line">        <span class="string">""" Apply the encoder to source sentences to obtain encoder hidden states.</span></span><br><span class="line"><span class="string">            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where</span></span><br><span class="line"><span class="string">                                        b = batch_size, src_len = maximum source sentence length. Note that </span></span><br><span class="line"><span class="string">                                       these have already been sorted in order of longest to shortest sentence.</span></span><br><span class="line"><span class="string">        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch</span></span><br><span class="line"><span class="string">        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where</span></span><br><span class="line"><span class="string">                                        b = batch size, src_len = maximum source sentence length, h = hidden size.</span></span><br><span class="line"><span class="string">        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial</span></span><br><span class="line"><span class="string">                                                hidden state and cell.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        enc_hiddens, dec_init_state = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~ 8 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1. Construct Tensor `X` of source sentences with shape (src_len, b, e) using the source model embeddings.</span></span><br><span class="line">        <span class="comment">###         src_len = maximum source sentence length, b = batch size, e = embedding size. Note</span></span><br><span class="line">        <span class="comment">###         that there is no initial hidden state or cell for the decoder.</span></span><br><span class="line">        <span class="comment">###     2. Compute `enc_hiddens`, `last_hidden`, `last_cell` by applying the encoder to `X`.</span></span><br><span class="line">        <span class="comment">###         - Before you can apply the encoder, you need to apply the `pack_padded_sequence` function to X.</span></span><br><span class="line">        <span class="comment">###         - After you apply the encoder, you need to apply the `pad_packed_sequence` function to enc_hiddens.</span></span><br><span class="line">        <span class="comment">###         - Note that the shape of the tensor returned by the encoder is (src_len b, h*2) and we want to</span></span><br><span class="line">        <span class="comment">###           return a tensor of shape (b, src_len, h*2) as `enc_hiddens`.</span></span><br><span class="line">        <span class="comment">###     3. Compute `dec_init_state` = (init_decoder_hidden, init_decoder_cell):</span></span><br><span class="line">        <span class="comment">###         - `init_decoder_hidden`:</span></span><br><span class="line">        <span class="comment">###             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.</span></span><br><span class="line">        <span class="comment">###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).</span></span><br><span class="line">        <span class="comment">###             Apply the h_projection layer to this in order to compute init_decoder_hidden.</span></span><br><span class="line">        <span class="comment">###             This is h_0^&#123;dec&#125; in the PDF. Here b = batch size, h = hidden size</span></span><br><span class="line">        <span class="comment">###         - `init_decoder_cell`:</span></span><br><span class="line">        <span class="comment">###             `last_cell` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.</span></span><br><span class="line">        <span class="comment">###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).</span></span><br><span class="line">        <span class="comment">###             Apply the c_projection layer to this in order to compute init_decoder_cell.</span></span><br><span class="line">        <span class="comment">###             This is c_0^&#123;dec&#125; in the PDF. Here b = batch size, h = hidden size</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### See the following docs, as you may need to use some of the following functions in your implementation:</span></span><br><span class="line">        <span class="comment">###     Pack the padded sequence X before passing to the encoder:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence</span></span><br><span class="line">        <span class="comment">###     Pad the packed sequence, enc_hiddens, returned by the encoder:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence</span></span><br><span class="line">        <span class="comment">###     Tensor Concatenation:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.cat</span></span><br><span class="line">        <span class="comment">###     Tensor Permute:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute</span></span><br><span class="line"></span><br><span class="line">        X = self.model_embeddings.source(source_padded)</span><br><span class="line">        output, (h_enc, c_enc) = self.encoder(</span><br><span class="line">            pack_padded_sequence(X, source_lengths))</span><br><span class="line">        enc_hiddens,sequence_length = pad_packed_sequence(output, batch_first = <span class="keyword">True</span>) <span class="comment"># output of shape (batch, seq_len, num_directions * hidden_size)</span></span><br><span class="line">        h_0_dec = self.h_projection(torch.cat((h_enc[<span class="number">0</span>,:],h_enc[<span class="number">1</span>,:]), <span class="number">1</span>))</span><br><span class="line">        c_0_dec = self.c_projection(torch.cat((c_enc[<span class="number">0</span>,:],c_enc[<span class="number">1</span>,:]), <span class="number">1</span>))</span><br><span class="line">        dec_init_state = (h_0_dec,c_0_dec)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> enc_hiddens, dec_init_state</span><br></pre></td></tr></table></figure><h3 id="Decode"><a href="#Decode" class="headerlink" title="Decode"></a>Decode</h3><p>With the Decoder initialized, we must now feed it a matching sentence in the target language. On the $t^{th}$ step, we look up the embedding for the $t^{th}$ word, $y_t \in \mathbb{R}^{e x 1}$, we then concatenate $y_t$ with the combined-output vector $O_{t-1} \in \mathbb{R}^{h x 1}$ from the previous step to produce $\bar{y_t} \in \mathbb{R}^{(e+h) x 1}$. Note that for the first target word $O_0$ is zero-vector. We then fedd $\bar{y_t}$ as input to the Decoder LSTM.</p><script type="math/tex; mode=display">h_t^{dec}, c_t^{dec} = Decoder(\bar{y_t},h_{t-1}^{dec}, c_{t-1}^{dec} ) \quad \text{where} \quad h_t^{dec} \in \mathbb{R}^{h x 1}</script><p><strong>We then use $h_t^{dec}$ to compute multiplicative attention ovev $h_t^{enc}, \cdots, h_m^{enc}$</strong></p><script type="math/tex; mode=display">\begin{align}& e_{t_i} = (h_t^{dec})^{T}W_{attProj}h_i^{enc} \quad \text{where} \quad e_t \in \mathbb{R}^{m x 1}, W_{attProj} \in \mathbb{R}^{h x 2h} \\ & \alpha_{t} = Softmax(e_t) \quad \text{where} \quad \alpha_t \in \mathbb{R}^{m x 1} \\ & a_t = \sum_i^{m} \alpha_{t,i}h_i^{enc}  \quad \text{where} \quad a_t \in \mathbb{R}^{2h x 1}\\\end{align}</script><p>We now <strong>concatenate</strong> the attention output $a_t$ with the decoder hidden state $h_t^{dec}$ and pass this through a linear layer, Tanh, and Dropout to attain the <strong>combined-output vector</strong> $o_t$</p><script type="math/tex; mode=display">\begin{align}& u_t = \left[ a_t; h_t^{dec} \right ]  \quad \text{where} \quad u_t \in \mathbb{R}^{3h x 1} \\& v_t = W_u u_t \quad \text{where} \quad v_t \in \mathbb{R}^{h x 1}, W_u \in \mathbb{R}^{h x 1} \\& O_t = Dropout(Tanh(v_t)) \quad \text{where} \quad o_t \in \mathbb{R}^{h x 1} \\\end{align}</script><p>Then, we produce a probability distribution $P_t$ over target words at the $t^{th}$ timestep:</p><script type="math/tex; mode=display">P_t = Softmax(W_{vocab}O_t)  \quad \text{where} \quad P_t \in \mathbb{R}^{v_t x h}</script><p>Here, $V_t$ is the size of  the target vocabulary. Finally, to train the network we then compute the softmax cross entropy loss between $P_t$ and $g_t$, where $g_t$ is the 1-hot vector of the target word at timestep t:</p><script type="math/tex; mode=display">J(\theta) = CE(P_t, g_t)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor)</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">"""Compute combined output vectors for a batch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where</span></span><br><span class="line"><span class="string">                                     b = batch size, src_len = maximum source sentence length, h = hidden size.</span></span><br><span class="line"><span class="string">        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where</span></span><br><span class="line"><span class="string">                                     b = batch size, src_len = maximum source sentence length.</span></span><br><span class="line"><span class="string">        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder</span></span><br><span class="line"><span class="string">        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where</span></span><br><span class="line"><span class="string">                                       tgt_len = maximum target sentence length, b = batch size. </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where</span></span><br><span class="line"><span class="string">                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Chop of the &lt;END&gt; token for max length sentences.</span></span><br><span class="line">        target_padded = target_padded[:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize the decoder state (hidden and cell)</span></span><br><span class="line">        dec_state = dec_init_state</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize previous combined output vector o_&#123;t-1&#125; as zero</span></span><br><span class="line">        batch_size = enc_hiddens.size(<span class="number">0</span>)</span><br><span class="line">        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize a list we will use to collect the combined output o_t on each step</span></span><br><span class="line">        combined_outputs = []</span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~9 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1. Apply the attention projection layer to `enc_hiddens` to obtain `enc_hiddens_proj`,</span></span><br><span class="line">        <span class="comment">###         which should be shape (b, src_len, h),</span></span><br><span class="line">        <span class="comment">###         where b = batch size, src_len = maximum source length, h = hidden size.</span></span><br><span class="line">        <span class="comment">###         This is applying W_&#123;attProj&#125; to h^enc, as described in the PDF.</span></span><br><span class="line">        <span class="comment">###     2. Construct tensor `Y` of target sentences with shape (tgt_len, b, e) using the target model embeddings.</span></span><br><span class="line">        <span class="comment">###         where tgt_len = maximum target sentence length, b = batch size, e = embedding size.</span></span><br><span class="line">        <span class="comment">###     3. Use the torch.split function to iterate over the time dimension of Y.</span></span><br><span class="line">        <span class="comment">###         Within the loop, this will give you Y_t of shape (1, b, e) where b = batch size, e = embedding size.</span></span><br><span class="line">        <span class="comment">###             - Squeeze Y_t into a tensor of dimension (b, e). </span></span><br><span class="line">        <span class="comment">###             - Construct Ybar_t by concatenating Y_t with o_prev.</span></span><br><span class="line">        <span class="comment">###             - Use the step function to compute the the Decoder's next (cell, state) values</span></span><br><span class="line">        <span class="comment">###               as well as the new combined output o_t.</span></span><br><span class="line">        <span class="comment">###             - Append o_t to combined_outputs</span></span><br><span class="line">        <span class="comment">###             - Update o_prev to the new o_t.</span></span><br><span class="line">        <span class="comment">###     4. Use torch.stack to convert combined_outputs from a list length tgt_len of</span></span><br><span class="line">        <span class="comment">###         tensors shape (b, h), to a single tensor shape (tgt_len, b, h)</span></span><br><span class="line">        <span class="comment">###         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Note:</span></span><br><span class="line">        <span class="comment">###    - When using the squeeze() function make sure to specify the dimension you want to squeeze</span></span><br><span class="line">        <span class="comment">###      over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.</span></span><br><span class="line">        <span class="comment">###   </span></span><br><span class="line">        <span class="comment">### Use the following docs to implement this functionality:</span></span><br><span class="line">        <span class="comment">###     Zeros Tensor:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.zeros</span></span><br><span class="line">        <span class="comment">###     Tensor Splitting (iteration):</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.split</span></span><br><span class="line">        <span class="comment">###     Tensor Dimension Squeezing:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.squeeze</span></span><br><span class="line">        <span class="comment">###     Tensor Concatenation:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.cat</span></span><br><span class="line">        <span class="comment">###     Tensor Stacking:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.stack</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#   (b, src_len, h*2) * [2h , h]  = (b, src_len, h)</span></span><br><span class="line">        enc_hiddens_proj = self.att_projection(enc_hiddens)</span><br><span class="line">        <span class="comment">#   (tgt_len, b, e)</span></span><br><span class="line">        Y = self.model_embeddings.target(target_padded)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> Y_t <span class="keyword">in</span> torch.split(Y, split_size_or_sections = <span class="number">1</span>, dim = <span class="number">0</span>):</span><br><span class="line">            squeezed_Y_t = torch.squeeze(Y_t) <span class="comment"># (b, e) + (b,h) = (b,e+h)</span></span><br><span class="line">            Ybar_t = torch.cat((o_prev,squeezed_Y_t), dim = <span class="number">1</span>)</span><br><span class="line">            dec_state, o_t, _ = self.step(Ybar_t,dec_state,enc_hiddens,enc_hiddens_proj,enc_masks)</span><br><span class="line">            combined_outputs.append(o_t)</span><br><span class="line">            o_prev = o_t</span><br><span class="line"></span><br><span class="line">        <span class="comment">#  (b, h) -&gt; (tgt_len, b, h)</span></span><br><span class="line">        combined_outputs = torch.stack(combined_outputs,dim = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined_outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, Ybar_t: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            dec_state: Tuple[torch.Tensor, torch.Tensor],</span></span></span><br><span class="line"><span class="function"><span class="params">            enc_hiddens: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            enc_hiddens_proj: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            enc_masks: torch.Tensor)</span> -&gt; Tuple[Tuple, torch.Tensor, torch.Tensor]:</span></span><br><span class="line">        <span class="string">""" Compute one forward step of the LSTM decoder, including the attention computation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,</span></span><br><span class="line"><span class="string">                                where b = batch size, e = embedding size, h = hidden size.</span></span><br><span class="line"><span class="string">        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.</span></span><br><span class="line"><span class="string">                First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.</span></span><br><span class="line"><span class="string">        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,</span></span><br><span class="line"><span class="string">                                    src_len = maximum source length, h = hidden size.</span></span><br><span class="line"><span class="string">        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),</span></span><br><span class="line"><span class="string">                                    where b = batch size, src_len = maximum source length, h = hidden size.</span></span><br><span class="line"><span class="string">        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),</span></span><br><span class="line"><span class="string">                                    where b = batch size, src_len is maximum source length. </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.</span></span><br><span class="line"><span class="string">                First tensor is decoder's new hidden state, second tensor is decoder's new cell.</span></span><br><span class="line"><span class="string">        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.</span></span><br><span class="line"><span class="string">        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.</span></span><br><span class="line"><span class="string">                                Note: You will not use this outside of this function.</span></span><br><span class="line"><span class="string">                                      We are simply returning this value so that we can sanity check</span></span><br><span class="line"><span class="string">                                      your implementation.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        combined_output = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~3 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1. Apply the decoder to `Ybar_t` and `dec_state`to obtain the new dec_state.</span></span><br><span class="line">        <span class="comment">###     2. Split dec_state into its two parts (dec_hidden, dec_cell)</span></span><br><span class="line">        <span class="comment">###     3. Compute the attention scores e_t, a Tensor shape (b, src_len). </span></span><br><span class="line">        <span class="comment">###        Note: b = batch_size, src_len = maximum source length, h = hidden size.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">###       Hints:</span></span><br><span class="line">        <span class="comment">###         - dec_hidden is shape (b, h) and corresponds to h^dec_t in the PDF (batched)</span></span><br><span class="line">        <span class="comment">###         - enc_hiddens_proj is shape (b, src_len, h) and corresponds to W_&#123;attProj&#125; h^enc (batched).</span></span><br><span class="line">        <span class="comment">###         - Use batched matrix multiplication (torch.bmm) to compute e_t.</span></span><br><span class="line">        <span class="comment">###         - To get the tensors into the right shapes for bmm, you will need to do some squeezing and unsqueezing.</span></span><br><span class="line">        <span class="comment">###         - When using the squeeze() function make sure to specify the dimension you want to squeeze</span></span><br><span class="line">        <span class="comment">###             over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Use the following docs to implement this functionality:</span></span><br><span class="line">        <span class="comment">###     Batch Multiplication:</span></span><br><span class="line">        <span class="comment">###        https://pytorch.org/docs/stable/torch.html#torch.bmm</span></span><br><span class="line">        <span class="comment">###     Tensor Unsqueeze:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.unsqueeze</span></span><br><span class="line">        <span class="comment">###     Tensor Squeeze:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.squeeze</span></span><br><span class="line"></span><br><span class="line">        dec_state = self.decoder(Ybar_t, dec_state)</span><br><span class="line">        h_t_dec, c_t_dec = dec_state</span><br><span class="line">        <span class="comment">#  enc_hiddens_proj(b, src_len, h) * h_t_dec (b,h,1) = (b,src_len)</span></span><br><span class="line">        e_t = torch.squeeze(torch.bmm(enc_hiddens_proj, torch.unsqueeze(h_t_dec,<span class="number">2</span>)),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set e_t to -inf where enc_masks has 1</span></span><br><span class="line">        <span class="keyword">if</span> enc_masks <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            e_t.data.masked_fill_(enc_masks.byte(), -float(<span class="string">'inf'</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~6 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1. Apply softmax to e_t to yield alpha_t</span></span><br><span class="line">        <span class="comment">###     2. Use batched matrix multiplication between alpha_t and enc_hiddens to obtain the</span></span><br><span class="line">        <span class="comment">###         attention output vector, a_t.</span></span><br><span class="line">        <span class="comment">#$$     Hints:</span></span><br><span class="line">        <span class="comment">###           - alpha_t is shape (b, src_len)</span></span><br><span class="line">        <span class="comment">###           - enc_hiddens is shape (b, src_len, 2h)</span></span><br><span class="line">        <span class="comment">###           - a_t should be shape (b, 2h)</span></span><br><span class="line">        <span class="comment">###           - You will need to do some squeezing and unsqueezing.</span></span><br><span class="line">        <span class="comment">###     Note: b = batch size, src_len = maximum source length, h = hidden size.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">###     3. Concatenate dec_hidden with a_t to compute tensor U_t</span></span><br><span class="line">        <span class="comment">###     4. Apply the combined output projection layer to U_t to compute tensor V_t</span></span><br><span class="line">        <span class="comment">###     5. Compute tensor O_t by first applying the Tanh function and then the dropout layer.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Use the following docs to implement this functionality:</span></span><br><span class="line">        <span class="comment">###     Softmax:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.functional.softmax</span></span><br><span class="line">        <span class="comment">###     Batch Multiplication:</span></span><br><span class="line">        <span class="comment">###        https://pytorch.org/docs/stable/torch.html#torch.bmm</span></span><br><span class="line">        <span class="comment">###     Tensor View:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view</span></span><br><span class="line">        <span class="comment">###     Tensor Concatenation:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.cat</span></span><br><span class="line">        <span class="comment">###     Tanh:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.tanh</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># (b,src_len)</span></span><br><span class="line">        alpha_t = nn.functional.softmax(e_t, dim = <span class="number">1</span>) </span><br><span class="line">        <span class="comment"># alpha_t(b,src_len) - (b,1,src_len) * enc_hiddens(b, src_len, h * 2) = (b, 1, h * 2) -&gt; (b,2h)</span></span><br><span class="line">        a_t = torch.squeeze(torch.bmm(torch.unsqueeze(alpha_t,<span class="number">1</span>),enc_hiddens),<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#(b,2h) + (b,h)</span></span><br><span class="line">        U_t = torch.cat((a_t,h_t_dec), dim = <span class="number">1</span>)</span><br><span class="line">        V_t = self.combined_output_projection(U_t)</span><br><span class="line">        O_t = self.dropout(nn.functional.tanh(V_t))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line">        combined_output = O_t</span><br><span class="line">        <span class="keyword">return</span> dec_state, combined_output, e_t</span><br></pre></td></tr></table></figure><h3 id="Helpers"><a href="#Helpers" class="headerlink" title="Helpers"></a>Helpers</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, source: List[List[str]], target: List[List[str]])</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">""" Take a mini-batch of source and target sentences, compute the log-likelihood of</span></span><br><span class="line"><span class="string">        target sentences under the language models learned by the NMT system.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param source (List[List[str]]): list of source sentence tokens</span></span><br><span class="line"><span class="string">        @param target (List[List[str]]): list of target sentence tokens, wrapped by `&lt;s&gt;` and `&lt;/s&gt;`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the</span></span><br><span class="line"><span class="string">                                    log-likelihood of generating the gold-standard target sentence for</span></span><br><span class="line"><span class="string">                                    each example in the input batch. Here b = batch size.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Compute sentence lengths</span></span><br><span class="line">        source_lengths = [len(s) <span class="keyword">for</span> s <span class="keyword">in</span> source]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert list of lists into tensors</span></span><br><span class="line">        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   <span class="comment"># Tensor: (src_len, b)</span></span><br><span class="line">        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   <span class="comment"># Tensor: (tgt_len, b)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">###     Run the network forward:</span></span><br><span class="line">        <span class="comment">###     1. Apply the encoder to `source_padded` by calling `self.encode()`</span></span><br><span class="line">        <span class="comment">###     2. Generate sentence masks for `source_padded` by calling `self.generate_sent_masks()`</span></span><br><span class="line">        <span class="comment">###     3. Apply the decoder to compute combined-output by calling `self.decode()`</span></span><br><span class="line">        <span class="comment">###     4. Compute log probability distribution over the target vocabulary using the</span></span><br><span class="line">        <span class="comment">###        combined_outputs returned by the `self.decode()` function.</span></span><br><span class="line"></span><br><span class="line">        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)</span><br><span class="line">        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)</span><br><span class="line">        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)</span><br><span class="line">        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Zero out, probabilities for which we have nothing in the target text</span></span><br><span class="line">        target_masks = (target_padded != self.vocab.tgt[<span class="string">'&lt;pad&gt;'</span>]).float()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute log probability of generating true target words</span></span><br><span class="line">        target_gold_words_log_prob = torch.gather(P, index=target_padded[<span class="number">1</span>:].unsqueeze(<span class="number">-1</span>), dim=<span class="number">-1</span>).squeeze(<span class="number">-1</span>) * target_masks[<span class="number">1</span>:]</span><br><span class="line">        scores = target_gold_words_log_prob.sum(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 4</span></span><br><span class="line"><span class="string">model_embeddings.py: Embeddings for the NMT model</span></span><br><span class="line"><span class="string">Pencheng Yin &lt;pcyin@cs.cmu.edu&gt;</span></span><br><span class="line"><span class="string">Sahil Chopra &lt;schopra8@stanford.edu&gt;</span></span><br><span class="line"><span class="string">Anand Dhoot &lt;anandd@stanford.edu&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelEmbeddings</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Class that converts input words to their embeddings.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embed_size, vocab)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Init the Embedding layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param embed_size (int): Embedding size (dimensionality)</span></span><br><span class="line"><span class="string">        @param vocab (Vocab): Vocabulary object containing src and tgt languages</span></span><br><span class="line"><span class="string">                              See vocab.py for documentation.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ModelEmbeddings, self).__init__()</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># default values</span></span><br><span class="line">        self.source = <span class="keyword">None</span></span><br><span class="line">        self.target = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        src_pad_token_idx = vocab.src[<span class="string">'&lt;pad&gt;'</span>]</span><br><span class="line">        tgt_pad_token_idx = vocab.tgt[<span class="string">'&lt;pad&gt;'</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~2 Lines)</span></span><br><span class="line">        <span class="comment">### TODO - Initialize the following variables:</span></span><br><span class="line">        <span class="comment">###     self.source (Embedding Layer for source language)</span></span><br><span class="line">        <span class="comment">###     self.target (Embedding Layer for target langauge)</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Note:</span></span><br><span class="line">        <span class="comment">###     1. `vocab` object contains two vocabularies:</span></span><br><span class="line">        <span class="comment">###            `vocab.src` for source</span></span><br><span class="line">        <span class="comment">###            `vocab.tgt` for target</span></span><br><span class="line">        <span class="comment">###     2. You can get the length of a specific vocabulary by running:</span></span><br><span class="line">        <span class="comment">###             `len(vocab.&lt;specific_vocabulary&gt;)`</span></span><br><span class="line">        <span class="comment">###     3. Remember to include the padding token for the specific vocabulary</span></span><br><span class="line">        <span class="comment">###        when creating your Embedding.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Use the following docs to properly initialize these variables:</span></span><br><span class="line">        <span class="comment">###     Embedding Layer:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding</span></span><br><span class="line">        self.source = nn.Embedding(len(vocab.src),self.embed_size, padding_idx = src_pad_token_idx)</span><br><span class="line">        self.target = nn.Embedding(len(vocab.tgt), self.embed_size, padding_idx = tgt_pad_token_idx) </span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_sents</span><span class="params">(sents, pad_token)</span>:</span></span><br><span class="line">    <span class="string">""" Pad list of sentences according to the longest sentence in the batch.</span></span><br><span class="line"><span class="string">    @param sents (list[list[str]]): list of sentences, where each sentence</span></span><br><span class="line"><span class="string">                                    is represented as a list of words</span></span><br><span class="line"><span class="string">    @param pad_token (str): padding token</span></span><br><span class="line"><span class="string">    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter</span></span><br><span class="line"><span class="string">        than the max length sentence are padded out with the pad_token, such that</span></span><br><span class="line"><span class="string">        each sentences in the batch now has equal length.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    sents_padded = []</span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE (~6 Lines)</span></span><br><span class="line">    max_sentence_len = max([len(s) <span class="keyword">for</span> s <span class="keyword">in</span> sents])</span><br><span class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> sents:</span><br><span class="line">        sents_padded.append(sent + [pad_token] * (max_sentence_len - len(sent)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sents_padded</span><br></pre></td></tr></table></figure><h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ol><li>course slides and notes from cs224n (<a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/</a>)</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;General-definition-of-attention&quot;&gt;&lt;a href=&quot;#General-definition-of-attention&quot; class=&quot;headerlink&quot; title=&quot;General definition of attentio
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Gated RNN Units</title>
    <link href="https://zhangruochi.com/Gated-RNN-Units/2019/12/15/"/>
    <id>https://zhangruochi.com/Gated-RNN-Units/2019/12/15/</id>
    <published>2019-12-15T06:40:39.000Z</published>
    <updated>2019-12-19T18:09:48.113Z</updated>
    
    <content type="html"><![CDATA[<h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><p>RNNs have been found to perform better with the use of more complex units for activation. Here, we discuss the use of a gated activation function thereby modifying the RNN architecture. What motivates this? Well, although RNNs can theoretically capture long-term dependencies, they are very hard to actually train to do this. Gated recurrent units are designed in a manner to have more persistent memory thereby making it easier for RNNs to capture long-term dependencies. Let us see mathematically how a GRU uses $h_{t−1}$ and $x_t$ to generate the next hidden state ht. We will then dive into the intuition of this architecture.</p><script type="math/tex; mode=display">\begin{aligned}& z_t = \sigma(W^{(z)}x_t + U^{(z)}h_{t-1}) & \text{(Update gate)} \\ & r_t = \sigma(W^{(r)}x_t + U^{(r)}h_{t-1})  & \text{(Reset gate)}\\ & \tilde{h_t} = tanh{r_t \circ Uh_{t-1} + Wx_t} & \text{(New memory)} \\ & h_t = (1 - z_t) \circ \tilde{h_t} + z_t \circ h_{t-1} & \text{(Hidden state)}\end{aligned}</script><p>The above equations can be thought of a GRU’s four fundamental operational stages and they have intuitive interpretations that make this model much more intellectually.</p><ol><li><strong>Reset gate</strong>: controls what parts of previous hidden state are used to compute new content</li><li><strong>Update gate</strong>: controls what parts of hidden state are updated vs preserved</li><li><strong>New hidden state content</strong>: reset gate selects useful parts of prev hidden state. Use this and current input to compute new hidden content.</li><li><strong>Hidden state</strong>: update gate simultaneously controls what is kept from previous hidden state, and what is updated to new hidden state content</li></ol><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>Long-Short-Term-Memories are another type of complex activation unit that differ a little from GRUs. The motivation for using these is similar to those for GRUs however the architecture of such units does differ. Let us first take a look at the mathematical formulation of LSTM units before diving into the intuition behind this design:</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="LSTM3.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">slides of cs224n</div></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="LSTM1.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/</div></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="LSTM2.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/</div></center><ol><li>The LSTM architecture makes it easier for the RNN to preserve information over many timesteps<ul><li>e.g. if the forget gate is set to remember everything on every timestep, then the info in the cell is preserved indefinitely</li><li>By contrast, it’s harder for vanilla RNN to learn a recurrent weight matrix Wh that preserves info in hidden state</li></ul></li><li>LSTM doesn’t guarantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies.</li></ol><h2 id="LSTM-vs-GRU"><a href="#LSTM-vs-GRU" class="headerlink" title="LSTM vs GRU"></a>LSTM vs GRU</h2><ul><li>Researchers have proposed many gated RNN variants, but LSTM and GRU are the most widely-used</li><li>The biggest difference is that GRU is quicker to compute and has fewer parameters</li><li>There is no conclusive evidence that one consistently performs better than the other</li><li>LSTM is a good default choice (especially if your data has particularly long dependencies, or you have lots of training data)</li><li><strong>Rule of thumb</strong>: start with LSTM, but switch to GRU if you want something more efficient</li></ul><h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ol><li>course slides and notes from cs224n (<a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/</a>)</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;GRU&quot;&gt;&lt;a href=&quot;#GRU&quot; class=&quot;headerlink&quot; title=&quot;GRU&quot;&gt;&lt;/a&gt;GRU&lt;/h2&gt;&lt;p&gt;RNNs have been found to perform better with the use of more comple
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Human Factor</title>
    <link href="https://zhangruochi.com/Human-Factor/2019/12/12/"/>
    <id>https://zhangruochi.com/Human-Factor/2019/12/12/</id>
    <published>2019-12-12T09:00:04.000Z</published>
    <updated>2019-12-12T09:02:16.947Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Course-Description"><a href="#Course-Description" class="headerlink" title="Course Description"></a>Course Description</h2><p> This course provides an introduction to human factors research and applications with emphasis on mature areas such as sensation and perception and manual control. <strong>Each class will introduce some concrete human factors problem and explore theory and application relevant to solving it</strong>. The term long conceptual design assignment is intended to help maintain focus on applications to design.</p><h2 id="Week1"><a href="#Week1" class="headerlink" title="Week1"></a>Week1</h2><p>Learning Objectives:</p><ol><li><strong>Theoretical understanding</strong>: Develop an understanding of the historical context, disciplines, and schools of thought that led to the development of the current field of human factors engineering/psychology; Develop an acquaintance with basic elements of <strong>scientific method</strong> and <strong>decision making</strong> including tests of hypotheses, dependent and independent variables, and inferential and descriptive use of statistics. </li><li><strong>Apply theory and skills</strong>: Use scientific methods including selection of measures and experimental tasks to evaluate the utility of a variety of pointing devices</li><li><strong>Proficiency in information related skills</strong>: Learn how to conduct a critical incident-based evaluation of an interactive system and to perform a standard task analysis for such a system</li></ol><h3 id="Research-Methods"><a href="#Research-Methods" class="headerlink" title="Research Methods"></a>Research Methods</h3><ol><li>IV &amp; DV</li><li>Cause vs. Chance</li><li>Descriptive vs. Inferential Statistics</li><li>Null hypothesis(H0) &amp; Experimentalhypothesis(H1)</li><li>Hypothesis Testing</li><li>Statistical Significance &amp; Practical Significance</li><li>Graph &amp; Interpetation</li></ol><h3 id="Goals-of-System-Evaluation"><a href="#Goals-of-System-Evaluation" class="headerlink" title="Goals of System Evaluation"></a>Goals of System Evaluation</h3><ul><li>Functionality<ul><li>Can it do what it is supposed to do?</li></ul></li><li>Usability<ul><li>Does it make the task easier?</li></ul></li><li>Diagnosticity<ul><li>Does it pinpoint what is wrong?</li></ul></li></ul><ol><li>User Population<ul><li>Who are they?</li><li>What are their goals?</li><li>What do they already know how to do?</li><li>Ask, don’t assume!</li></ul></li><li>Research Objectives<ul><li>Generalizability</li><li>Precision</li><li>Realism</li></ul></li><li>Usability Testing</li><li>Design an Experiment</li></ol><h3 id="Two-Models-for-Human-Factors"><a href="#Two-Models-for-Human-Factors" class="headerlink" title="Two Models for Human Factors"></a>Two Models for Human Factors</h3><ul><li>System component (computer)</li><li>Embedded organism (cybernetic)</li></ul><h3 id="Task-Analyses"><a href="#Task-Analyses" class="headerlink" title="Task Analyses"></a>Task Analyses</h3><p>为了研究如何使系统更好地和人的能力相匹配所进行的一种描述人机交互的方法</p><ul><li><strong>Sequential</strong><br>  任务的顺序和不同任务在时间序列上的关系<ul><li>Procedural</li><li>Therp</li></ul></li><li><strong>Hierarchical</strong><br>  描述一个大的任务如何由子任务组成以及这些任务又是如何联系起来体现其功能的<ul><li>HTA</li><li>GOMS</li><li>Cognitive</li></ul></li></ul><h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul><li>Types of Evaluation<ul><li>Laboratory Studies</li><li>Field Studies</li><li>Participatory Design/Rapid Prototyping</li><li>Brainstorming</li><li>StoryBoarding/Wizard of Oz</li><li>Workshops/Role Playing</li><li>Walkthrough/Talkthrough</li></ul></li><li>Observation Evaluation</li></ul><h4 id="Function-Allocation"><a href="#Function-Allocation" class="headerlink" title="Function Allocation"></a>Function Allocation</h4><p>每个功能由人来实现还是系统来实现</p><ul><li>Mandatory</li><li>Relative value</li><li>Cost based</li><li>Cognitive/affective</li></ul><h2 id="Week2-Sensation"><a href="#Week2-Sensation" class="headerlink" title="Week2 Sensation"></a>Week2 Sensation</h2><p>Learning Objectives:</p><ul><li>Psychophysical methods &amp; the problem of separating bias from measurement </li><li>Signal Detection: Perform specific calculations      <ul><li><strong>Conceptual model</strong> (associating hits, FA’s, misses, &amp; CR with areas under the curves) </li><li><strong>Computational understanding</strong> (interpreting problems, using table, &amp; finding d’) </li><li><strong>Vigilance</strong> - what the vigilance decrement is and how signal detection has been used to better understand it </li></ul></li><li>DESIGN: understanding of the cognitive information capabilities of humans General knowledge- wide range of sensitivity, approximately logarithmic, etc. </li><li>Place encoding of frequency and its consequences for: <ul><li>Masking </li><li>Threshold shift &amp; frequency related loss, etc. </li></ul></li><li>Measurement issues<ul><li>Loudness &amp; equal loudness contours </li><li>SPL meters </li><li>Articulation index, SIL, and other speech &amp; noise issues </li><li>Noise &amp; annoyance </li></ul></li></ul><h3 id="Classical-Errors"><a href="#Classical-Errors" class="headerlink" title="Classical Errors"></a>Classical Errors</h3><p>Error of Habituation</p><ul><li>Keep on saying the same thing<br>Error of Anticipation </li><li>Shift to new response</li></ul><h3 id="Signal-Detection-Model"><a href="#Signal-Detection-Model" class="headerlink" title="Signal Detection Model"></a>Signal Detection Model</h3><p><img src="resources/E893E7BC11981E867B4A94DA85979D8E.png" alt="Screen Shot 2019-10-24 at 00.00.37.png"><br><img src="resources/C2A377503B642EB567613413A81A15E0.png" alt="Screen Shot 2019-10-24 at 00.00.54.png"></p><h3 id="Vigilance"><a href="#Vigilance" class="headerlink" title="Vigilance"></a>Vigilance</h3><p>警报的设计必须建立在对人类的听觉加工充分了解的基础上</p><ul><li>Near threshold signals</li><li>Low rate of occurrence</li><li>Extended Watch</li><li>Inspection(definitetrial)</li><li>Free response time arbitrarily brokeninto intervals</li><li>Successive(noavailablestandard)</li><li>Simultaneous(signal to standard comparison)</li></ul><h3 id="Fatigue"><a href="#Fatigue" class="headerlink" title="Fatigue"></a>Fatigue</h3><ol><li>Sustained attention leads to fatigue</li><li>Load on working memory to keep target in mind depletes resources</li></ol><h3 id="Expectancy"><a href="#Expectancy" class="headerlink" title="Expectancy"></a>Expectancy</h3><h3 id="Remedies"><a href="#Remedies" class="headerlink" title="Remedies"></a>Remedies</h3><h3 id="Sound"><a href="#Sound" class="headerlink" title="Sound"></a>Sound</h3><p>Patterns of rare faction/compression of air</p><ol><li>Physics<ul><li>Intensity = amplitude</li><li>Frequency = cycles(hz)</li></ul></li><li>Perception<ul><li>Loudness</li><li>Pitch</li></ul></li></ol><ul><li>强度决定响度</li><li>频率决定音调</li><li>位置决定听觉定位</li><li>品质由频率及其掩蔽来决定</li></ul><h3 id="DECIBEL"><a href="#DECIBEL" class="headerlink" title="DECIBEL"></a>DECIBEL</h3><ul><li>dB(a): dB weighted by threshold equal loudness curve</li><li>dB(c)/dB (spl): dB with no weighting</li><li>dB(d) weighted by equal annoyance curves</li></ul><h3 id="Encoding-Pitch-by-Place"><a href="#Encoding-Pitch-by-Place" class="headerlink" title="Encoding Pitch by Place"></a>Encoding Pitch by Place</h3><h3 id="Auditory-Masking"><a href="#Auditory-Masking" class="headerlink" title="Auditory Masking"></a>Auditory Masking</h3><p>the presence of tone that inhibits the perception of another tone that occurs before, at the same time, or after it.</p><h2 id="Week3-Vision-and-Color"><a href="#Week3-Vision-and-Color" class="headerlink" title="Week3 Vision and Color"></a>Week3 Vision and Color</h2><p>Learning Objectives:</p><ul><li>Even more than place perception of pitch vision is all about relative differences (contrasts) and adaptation</li><li>The relation between what we experience and what is physically out there isn’t direct</li><li>Good Human Factors engineering requires designing so our users don’t notice</li></ul><p>Overview of vision</p><ul><li>Peripheral/central processing</li><li>Dark Adaptation &amp; Illusions &amp; their relation to the structure of the eye &amp; vision</li><li>Visual angle &amp; spatial frequency</li><li>VDTs, visual fatigue, &amp; ergonomic effects</li><li>Color measurement Munsell color wheel vs. CIE</li></ul><h3 id="Rods-and-Cones"><a href="#Rods-and-Cones" class="headerlink" title="Rods and Cones"></a>Rods and Cones</h3><ol><li>位置 location</li><li>视敏度 acuity（解析细节的能力）</li><li>敏感性 sensitivity (即使光很少，Rods也能工作)</li><li>color sensitivity （rods 是色盲）</li><li>adaption （rods 受光刺激的影响大）</li><li>diffrrential wavelength sensitivity (cones对所有光敏感，rods对红光不敏感）</li></ol><h3 id="对比敏感度"><a href="#对比敏感度" class="headerlink" title="对比敏感度"></a>对比敏感度</h3><p>c = (L-D) / (L+D)</p><h3 id="color-sensation"><a href="#color-sensation" class="headerlink" title="color sensation"></a>color sensation</h3><p>对单色进行设计，然后将颜色作为冗余编码信息提供</p><h3 id="dark-sensation"><a href="#dark-sensation" class="headerlink" title="dark sensation"></a>dark sensation</h3><p>当照明条件比较差时，所有空间评率的对比度都会降低</p><h3 id="检测"><a href="#检测" class="headerlink" title="检测"></a>检测</h3><ul><li>在很多系统中，影响作业绩效的重要因素是密切相关的两个过程：视觉搜索和物体或时间的检测</li><li>在视觉搜索领域，一旦某个项目被确定可能是目标，就必须对它是否是真的目标进行确认。</li><li>d’ 反映了一个操作者从噪音中分辨出信号的能力，它等于好结果的数目除以所有结果的总和</li></ul><h3 id="Peripheral-Processing"><a href="#Peripheral-Processing" class="headerlink" title="Peripheral Processing"></a>Peripheral Processing</h3><ul><li>Photo receptors are interconnected and can reciprocally inhibit one another</li><li>Can be tuned for featurere cognition</li></ul><h3 id="Adaptation"><a href="#Adaptation" class="headerlink" title="Adaptation"></a>Adaptation</h3><p>Adaptation is a major characteristic of sensation</p><h2 id="Week4-Reaction-time"><a href="#Week4-Reaction-time" class="headerlink" title="Week4 Reaction time"></a>Week4 Reaction time</h2><p>Learning Objectives:</p><ul><li>Subtractive&amp;additive factors analyses of choice reaction time</li><li>Limits&amp;values of psychological experimentation</li><li>Human bottleneck in choice responses</li><li>Power law of learning</li><li>Automaticvs.controlledresponding</li><li>Information theoretic interpretations of reaction time</li></ul><h3 id="Hick’s-Law"><a href="#Hick’s-Law" class="headerlink" title="Hick’s Law"></a>Hick’s Law</h3><p>Hick’s Law holds that choice reaction time is proportional to log2 of the number of alternatives.（反应时间是log2N的函数）<br>RT = a + blog2N</p><h3 id="Conventional-Controls-amp-Displays"><a href="#Conventional-Controls-amp-Displays" class="headerlink" title="Conventional Controls &amp; Displays"></a>Conventional Controls &amp; Displays</h3><ul><li>conventional practice in design of controls</li><li>displays Acquire familiarity with human factors design principles and heuristics </li></ul><h3 id="Control"><a href="#Control" class="headerlink" title="Control"></a>Control</h3><p>Controls are used by the human operator to communicate with the machine/device in the system. It’simportantthatcontrolsservetheirfunction. Based on:</p><ul><li>Ease of operation (considering population, biomechanics, etc.).</li><li>Nature of the task (force, precision, etc.). </li><li>Arrangement.</li></ul><h3 id="Basic-dimensions"><a href="#Basic-dimensions" class="headerlink" title="Basic dimensions:"></a>Basic dimensions:</h3><ul><li>Discrete (e.g., light switch) vs. continuous (e.g., dimmer)</li><li>Linear vs. rotary</li><li>Unidimensional vs. multidimensional</li><li>Isometric vs. isotonic</li><li>Plus mass, shape, range of motion, resistance to movement.</li></ul><h3 id="Control-Features"><a href="#Control-Features" class="headerlink" title="Control Features"></a>Control Features</h3><ol><li>Control resistance<ul><li>Elastic resistance: Spring loaded.<ul><li>Resistance increases as control gets farther from neutral.</li><li>Gives proprioceptive feedback about <strong>control position</strong>.</li><li>Returns to neutral when released (deadman switch).</li></ul></li><li>Frictional resistance<ul><li>Static friction for resting state, decreases when pushed. </li><li>Sliding friction not influenced by velocity or position.</li></ul></li><li>Viscous resistance<ul><li>Increases as a function of velocity.</li><li>Gives proprioceptive feedback about speed.</li><li>Promotes smooth movement.</li></ul></li><li>Inertial resistance<ul><li>Hard to start and stop.</li><li>Users tend to overshoot (revolving doors).</li></ul></li><li>Performance and resistance<ul><li>For frictional and inertial, the JND is 10%-20% of resting state.</li><li>Lighter controls preferred to heavy.</li><li>Viscous preferred to frictional.</li><li>For continuous, inertial hurts performance; </li><li>Elastic is the best.</li><li>For all of these, hard-and-fast rules are not available. It’s a function of the system.</li></ul></li></ul></li><li>Control-display ratio: <ul><li>Ratio of magnitude of control adjustment to magnitude of change in display.</li></ul></li><li>Gain: <ul><li>Responsiveness of control.</li></ul></li></ol><h3 id="Control-Panels"><a href="#Control-Panels" class="headerlink" title="Control Panels"></a>Control Panels</h3><ol><li>Location coding<ul><li>Need to be able to reliably distinguish locations.</li><li>Vertical localization is easier than horizontal.</li><li>Overuse of location coding is still a factor in some aircraft designs.</li></ul></li><li>Labels<ul><li>Not recommended as the sole code</li></ul></li><li>General rules for labels<ul><li>Locate labels systematically with respect to controls (all above, etc.).</li><li>Make labels brief.</li><li>Avoid abstract symbols; use standards.</li><li>Attend to fonts.</li><li>Position labels so they can be seen while the control is in use.</li></ul></li><li>Coding of controls<ul><li>Color coding</li><li>Shape coding</li><li>Size coding</li><li>Texture coding.<br>– Coding by type of operation.<br>– Redundant coding: Multiple dimensions</li></ul></li><li>Control arrangements<ul><li>Grouping is important.</li><li>Population stereotypes for control arrangements can be device specific</li><li>Attend to the reach envelope</li></ul></li><li>Preventing accidental operation</li><li>Specific Controls<ul><li>Hand operated controls</li><li>Foot operated controls</li><li>Specialized controls</li></ul></li></ol><h3 id="Visual-Displays"><a href="#Visual-Displays" class="headerlink" title="Visual Displays"></a>Visual Displays</h3><p>Display - anything that conveys information</p><ol><li>Requirements:<ul><li>Compatibility to senses</li><li>Language compatibility</li><li>Right info at the right time</li></ul></li><li>Types of information to display: <ul><li>Instructional</li><li>Command - direct orders </li><li>Advisory</li><li>Historical/predictive</li><li>Answers</li></ul></li><li>Functions of Dynamic Visual<br>Displays<ul><li>Continuous System Control</li><li>System Status Monitoring</li><li>Briefing</li><li>Search and Identification</li><li>Decision Making</li></ul></li><li>Visual Display Technology<ul><li>Mechanical </li><li>Electronic</li><li>Optical Projection</li></ul></li><li>General Display Principles<ul><li>Color</li><li>Shape</li><li>Coding</li><li>Approximation<ul><li>Get attention with one display<br>– Present detailed info with another</li></ul></li><li>Integration</li></ul></li></ol><h3 id="Principles-of-Display-Design"><a href="#Principles-of-Display-Design" class="headerlink" title="Principles of Display Design"></a>Principles of Display Design</h3><ul><li>Perceptual Principles<ul><li>Avoid absolute judgments</li><li>Top-down processing （信号的显示方式尽量与人的经验相符合）</li><li>Redundancy gain</li><li>Discriminability</li><li><img src="resources/088856BA3C1FA5A601838DAD16F27045.png" alt="Screen Shot 2019-12-11 at 14.23.03.png"></li></ul></li><li>Mental Model Principles （显示方式与操作员的心理模型一致，有助于提高正确操作）<ul><li>Pictorial realism （形如其表）</li><li>Principle of the moving part （运动一致）</li></ul></li><li>Principles Based on Attention<ul><li>Minimizing information access costs （将访问信息的消耗降到最低）</li><li>Proximity compatibility principle （接近相容原则）</li><li>Principle of multiple resources （要同时对多种信息进行加工时，可以将信息的呈现方式区分开）</li></ul></li><li>Memory Principles<ul><li>Principle of predictive aiding （预测辅助原则）</li><li>Principle of knowledge in the world （利用知识降低记忆负荷）</li><li>Principle of consistency （一致性原则）</li></ul></li></ul><ul><li>Two-Valued Info</li><li>Quantitative Information</li><li>Qualitative readings</li><li>Check Reading</li><li>Situation awareness</li></ul><h3 id="Three-Heuristics"><a href="#Three-Heuristics" class="headerlink" title="Three Heuristics"></a>Three Heuristics</h3><ol><li>Minimize Information</li><li>Promote Good C-D mappings</li><li>Provide Feedback</li></ol><h2 id="Spatial-amp-Integrative-Displays"><a href="#Spatial-amp-Integrative-Displays" class="headerlink" title="Spatial &amp; Integrative Displays"></a>Spatial &amp; Integrative Displays</h2><p>Learning Objectives:</p><ul><li>6 DOF &amp; moving &amp; orienting in 3 space</li><li>Problems with viewpoint &amp; situation awareness</li><li>Applications VR, games, &amp; robotics – Attitude &amp; pose</li><li>Navigation &amp; search</li><li><p>Cues to depth &amp; distance</p></li><li><p>Use of emergent features and perceptual salience to integrate displays</p></li><li>TMI and need to provide context to events</li></ul><h3 id="6-Degrees-of-Freedom-6DOF"><a href="#6-Degrees-of-Freedom-6DOF" class="headerlink" title="6 Degrees of Freedom (6DOF)"></a>6 Degrees of Freedom (6DOF)</h3><p>– Position (X,Y, Z)<br>– Orientation (Yaw, Pitch, Roll)</p><h3 id="3d-display-on-2d-panels"><a href="#3d-display-on-2d-panels" class="headerlink" title="3d display on 2d panels"></a>3d display on 2d panels</h3><ul><li>Depth is often poorly represented &amp; less discernable than other dimensions</li></ul><h3 id="Configural-Displays"><a href="#Configural-Displays" class="headerlink" title="Configural Displays"></a>Configural Displays</h3><ul><li>Low level data: usually individual sensor data</li><li>High level relation: a more global and general display of what the data means</li><li>Emergent property or emergent feature: a pattern or shape that is created from the low level data, is recognisable and has meaning</li></ul><h3 id="Separable"><a href="#Separable" class="headerlink" title="Separable"></a>Separable</h3><p>Show each variable as a single output</p><h3 id="Separable-vs-Configural-vs-Integral"><a href="#Separable-vs-Configural-vs-Integral" class="headerlink" title="Separable vs Configural vs Integral"></a>Separable vs Configural vs Integral</h3><ul><li>Separable generally makes it easier to extract low level information</li><li>Integral Show high level information but not low level information</li><li>Configural Arrange low level data into a meaningful form,whole is greater than the sum of the parts</li><li>Configural makes it harder to extract low level information</li></ul><h2 id="Tracking-manual-control"><a href="#Tracking-manual-control" class="headerlink" title="Tracking (manual control)"></a>Tracking (manual control)</h2><p>Learning Objectives:</p><ul><li>Fitts’ Law Ability to apply theory and skills: Design Use Fitts’ Law to evaluate/predict pointing performance </li><li>Theoretical understanding: Order of control </li></ul><h3 id="Open-versus-Closed-Loop-Systems"><a href="#Open-versus-Closed-Loop-Systems" class="headerlink" title="Open versus Closed Loop Systems"></a>Open versus Closed Loop Systems</h3><h3 id="Tracking-Terms"><a href="#Tracking-Terms" class="headerlink" title="Tracking Terms"></a>Tracking Terms</h3><ul><li>Control movement</li><li>Controlled element</li><li>Target</li><li>Forcing function- disturbances to target</li></ul><h3 id="Pursuit-and-Compensatory"><a href="#Pursuit-and-Compensatory" class="headerlink" title="Pursuit and Compensatory"></a>Pursuit and Compensatory</h3><p>补偿追踪与尾随追踪</p><ul><li>Pursuit   <ul><li>Target moved</li><li>Usually more accurate</li></ul></li><li>Compensatory<ul><li>Target fixed</li><li>Target &amp; control movements confounded</li></ul></li></ul><h3 id="Fitts-Law"><a href="#Fitts-Law" class="headerlink" title="Fitts Law"></a>Fitts Law</h3><p>MT(movement time) = a + blog2(2A/W)</p><h3 id="Tracking-vs-pointing"><a href="#Tracking-vs-pointing" class="headerlink" title="Tracking vs. pointing"></a>Tracking vs. pointing</h3><p>Pointing as expressed by Fitts law is a very special case of tracking.<br>In pointing:</p><ul><li>Stationary target</li><li>No lag</li><li>Gain is only control system parameter</li></ul><h3 id="Gain-amp-C-D-ratio"><a href="#Gain-amp-C-D-ratio" class="headerlink" title="Gain &amp; C/D ratio"></a>Gain &amp; C/D ratio</h3><ul><li>Gain describes the change in the controlled element (display) corresponding to a movement of the control: gain = y/x..</li><li>C/D ratio describes the movement of a control needed for a given change in the display: C/D = x/y</li></ul><h3 id="Order-of-Control"><a href="#Order-of-Control" class="headerlink" title="Order of Control"></a>Order of Control</h3><ul><li>0 order: Position<ul><li>A 0 order system has <strong>no</strong> integrations between input and output</li></ul></li><li>1 order: Velocity<ul><li>A 1 order system has <strong>one</strong> integrations between input and output</li></ul></li><li>2 order Acceleration<ul><li>A 2 order system has <strong>two</strong> integrations between input and output</li></ul></li></ul><h2 id="Week-8-HIP-amp-Workload"><a href="#Week-8-HIP-amp-Workload" class="headerlink" title="Week 8 HIP &amp; Workload"></a>Week 8 HIP &amp; Workload</h2><p>Learning Objectives:</p><ol><li>What is mental workload?<br> – Subjective, performance, &amp; physiological measures</li><li>HIP &amp; human factors<br> – Working memory, absolute judgment, &amp; other aspects of the bottleneck</li><li>Mental representation &amp; difficulty</li></ol><h3 id="Basic-approaches-to-measuring-mental-workload"><a href="#Basic-approaches-to-measuring-mental-workload" class="headerlink" title="Basic approaches to measuring mental workload"></a>Basic approaches to measuring mental workload</h3><ul><li>Analytic<br>  – Task difficulty<ul><li>Number of simultaneous tasks</li></ul></li><li>Task performance <ul><li>Primary task</li><li>Secondary task</li></ul></li><li>Physiological (arousal/effort)<ul><li>heart rate</li><li>evoked response amplitude</li><li>……</li></ul></li><li>Subjective assessment<ul><li>Cooper-Harris</li><li>SWAT</li><li>NASA</li></ul></li></ul><p><img src="resources/B1FE7729E735139A9174D4D525B76743.png" alt="Screen Shot 2019-12-11 at 17.05.25.png"></p><h3 id="Selective-Attention"><a href="#Selective-Attention" class="headerlink" title="Selective Attention"></a>Selective Attention</h3><p>通道的选择性注意主要受下面因素的影响<br><img src="resources/505A2612DA444E6ABADCD8DA347E4794.png" alt="Screen Shot 2019-12-11 at 17.04.47.png"></p><h3 id="Three-aspects-of-perception"><a href="#Three-aspects-of-perception" class="headerlink" title="Three aspects of perception"></a>Three aspects of perception</h3><ol><li>Sensory based<ul><li><strong>Bottom-up</strong> feature analysis<ul><li>clear stimuli/minimize sensory similarities</li></ul></li></ul></li><li>Memory based<ul><li><strong>Unitization</strong><br>  – perceive grouped features as a whole (Gestalt)</li><li><strong>Top-down</strong> [correct guesses &amp; fill-ins]</li></ul></li></ol><h3 id="working-memory"><a href="#working-memory" class="headerlink" title="working memory"></a>working memory</h3><ul><li>working memory</li><li>long term memory</li></ul><p><img src="resources/8424FD27B3344B794E85F01390B2C560.png" alt="Screen Shot 2019-12-12 at 03.50.04.png"></p><h3 id="90’s-model-nods-to-Baddeley-amp-Schneider"><a href="#90’s-model-nods-to-Baddeley-amp-Schneider" class="headerlink" title="90’s model (nods to Baddeley &amp; Schneider)"></a>90’s model (nods to Baddeley &amp; Schneider)</h3><p><img src="resources/2D6E557C69F004CC508948E5AF2DA91B.png" alt="Screen Shot 2019-12-11 at 17.13.34.png"></p><ol><li>Central executive<ul><li>Coordinate multiple tasks (OS)</li><li>Hold &amp; manipulate info from LTM (RAM)</li><li>Control retrieval strategies from LTM (data<br>access)</li><li>Attend selectively to stimuli (time share)</li><li>协调两个存储子系统</li></ul></li><li>Visual sketchpad<ul><li>以模拟的，空间的形式保持正在使用的信息</li></ul></li><li>Phonological store<ul><li>存储以声音的形式存在的信息</li></ul></li></ol><h2 id="Human-Error-and-Reliability"><a href="#Human-Error-and-Reliability" class="headerlink" title="Human Error and Reliability"></a>Human Error and Reliability</h2><ol><li>Understanding Mechanisms underlying human error </li><li>What types of errors can be predicted? </li><li>Proficiency in information-related skills: Analysis Perform THERP analyses to predict errors for a design/task </li></ol><h3 id="Error"><a href="#Error" class="headerlink" title="Error"></a>Error</h3><ul><li>Any act with adverse consequences</li><li>An act resulting from an inappropriate intention</li><li>Keeping the pressurizer level under control</li></ul><h3 id="Action-Schema"><a href="#Action-Schema" class="headerlink" title="Action Schema"></a>Action Schema</h3><p><img src="resources/5CB1A89659EE40E74A848B269D451306.png" alt="Screen Shot 2019-12-11 at 19.22.21.png"></p><ol><li>Intention<ul><li>mode errors</li><li>description errors</li></ul></li><li>Activation<ul><li>capture errors</li><li>data-driven</li><li>associative activation</li><li>loss of activation</li><li>sequence error</li></ul></li><li>Faulty Triggering<ul><li>Out of sequence and “mangled” execution..</li></ul></li></ol><h3 id="Reliability-Engineering"><a href="#Reliability-Engineering" class="headerlink" title="Reliability Engineering"></a>Reliability Engineering</h3><ul><li>The <strong>Key</strong> to reliability is Redundancy</li></ul><h3 id="Components-in-Series"><a href="#Components-in-Series" class="headerlink" title="Components in Series"></a>Components in Series</h3><p><img src="resources/E81CF8AF176C7942E34394DC8A2FF674.png" alt="Screen Shot 2019-12-12 at 03.52.33.png"></p><h3 id="Components-in-Parallel"><a href="#Components-in-Parallel" class="headerlink" title="Components in Parallel"></a>Components in Parallel</h3><p><img src="resources/9C5CE086D1C126A6965F4602030EB667.png" alt="Screen Shot 2019-12-12 at 03.52.39.png"></p><h3 id="Tradeoffs-Redundant-or-not"><a href="#Tradeoffs-Redundant-or-not" class="headerlink" title="Tradeoffs (Redundant or not)"></a>Tradeoffs (Redundant or not)</h3><h3 id="How-redundancy-works"><a href="#How-redundancy-works" class="headerlink" title="How redundancy works"></a>How redundancy works</h3><p>For the mathematics to work out the probabilities of failure for redundant components or subsystems must be <strong>completely independent</strong></p><h3 id="What-Reliability-Engineers-do"><a href="#What-Reliability-Engineers-do" class="headerlink" title="What Reliability Engineers do"></a>What Reliability Engineers do</h3><ul><li>The primary task of a reliability engineer is to defend redundancy against <strong>unexpected violations of independence</strong>.</li><li>In a well designed system only the <strong>human operator</strong> bridges these islands of independence</li></ul><h3 id="THERP"><a href="#THERP" class="headerlink" title="THERP"></a>THERP</h3><ul><li>Quality control method for estimating errors</li><li>Model<ul><li><strong>Errors</strong>: such as reading or omitting an instructional step, or choosing the wrong switch, are presumed to occur at constant rates</li><li>If tasks can be broken down into subtasks for which errors can be predicted, then the probability of the successful completion of the overall task can be predicted</li><li>The probability of successfully completing the task (if its something like warhead assembly) is then simply the <strong>joint probability</strong> that everything is done correctly</li></ul></li></ul><h3 id="Points-on-THERP-analyses"><a href="#Points-on-THERP-analyses" class="headerlink" title="Points on THERP analyses"></a>Points on THERP analyses</h3><ul><li>Tree is not sacrosanct but a convenient way to organize independent tasks</li><li>Probabilities for errors and recoveries should be entered into trees at level of aggregation at which independence holds</li><li>Method is ultimately simply a way to make our commonsense about the likelihood of failing more explicit</li></ul><p><img src="resources/81B1E00736CFE5835EC5B0CB6ED3E51C.png" alt="Screen Shot 2019-12-11 at 20.05.14.png"></p><h2 id="Human-Computer-interaction"><a href="#Human-Computer-interaction" class="headerlink" title="Human-Computer interaction"></a>Human-Computer interaction</h2><ol><li>Understand basic assumptions and mechanics of constructing GOMS keystroke level model</li><li>Contrast the HIP vs. Ecological vision of problems in HCI</li><li>Standard visualizations and the problem(s) they solve- finding context for local views</li></ol><h3 id="GOMS-Models"><a href="#GOMS-Models" class="headerlink" title="GOMS Models"></a>GOMS Models</h3><p>用于设计的用户绩效模型</p><ul><li>Goals</li><li>Operators </li><li>Methods</li><li>Selection rules</li></ul><p>用户可以通过方法和选择形成他们要达到的目标和子目标。方法是一系列知觉的、认知的或行为操作的步骤。</p><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ul><li>列出目标和子目标</li><li>明确达到问题的方法</li><li>写出选择关系</li><li>揭示问题</li></ul><h3 id="Norman’s-7-Stages-amp-design"><a href="#Norman’s-7-Stages-amp-design" class="headerlink" title="Norman’s 7 Stages &amp; design"></a>Norman’s 7 Stages &amp; design</h3><p>用户导向界面设计的七阶段理论<br><img src="resources/64FB6BD735C5ACC5560D714AD40CEBF1.png" alt="Screen Shot 2019-12-11 at 20.22.00.png"></p><ol><li>实施的鸿沟：用户的目的和软件所支持的行为之间的错误匹配（通过好的人因学方案解决，input tracking position）</li><li>评价的鸿沟：用户期望与系统状态的不匹配 (好的说明性显示)</li></ol><h3 id="Mplications-of-working-memory-amp-absolute-judgment-limitations"><a href="#Mplications-of-working-memory-amp-absolute-judgment-limitations" class="headerlink" title="Mplications of working memory &amp; absolute judgment limitations"></a>Mplications of working memory &amp; absolute judgment limitations</h3><ul><li>Recognition is MUCH easier than Recall :Make the objects of working memory available to perception..</li></ul><h3 id="The-Power-Law-of-Practice"><a href="#The-Power-Law-of-Practice" class="headerlink" title="The Power Law of Practice"></a>The Power Law of Practice</h3><p>Improvement in performance is logarithmic in the N of trials</p><h3 id="Mental-Models"><a href="#Mental-Models" class="headerlink" title="Mental Models"></a>Mental Models</h3><p>跨越实施和评价的鸿沟依赖于心理模型，好的心理模型可以帮助房主错误和改进绩效</p><ul><li>Allows people to make predictions about how things will work</li><li>Mental models are often wrong</li></ul><h3 id="Conceptual-Models"><a href="#Conceptual-Models" class="headerlink" title="Conceptual Models"></a>Conceptual Models</h3><p>使用户看不见的部分变为可见 比如”房间”</p><h3 id="State-Transition-Models-of-Devices"><a href="#State-Transition-Models-of-Devices" class="headerlink" title="State Transition Models of Devices"></a>State Transition Models of Devices</h3><p><img src="resources/B8C3FE244974F3F50AE2A26BFAE36589.png" alt="Screen Shot 2019-12-11 at 20.45.26.png"></p><h2 id="Speech"><a href="#Speech" class="headerlink" title="Speech"></a>Speech</h2><ul><li>Theoretical Understanding: Problems in building applications using speech recognition Design</li><li>Strategies for getting good performance despite poor recognition </li><li>Theoretical Understanding: Computer supported cooperative work </li><li>Theoretical Understanding: Design Strategies for using sensors to augment human inputs and improve interaction </li></ul><h3 id="Basic-Speech-Parameters"><a href="#Basic-Speech-Parameters" class="headerlink" title="Basic Speech Parameters"></a>Basic Speech Parameters</h3><ul><li>Speaker Dependent/Independent</li><li>Size/Type of Vocabulary</li><li>Isolated word vs. Continuous Speech</li><li>Grammar/constraint</li><li>Environment/noise tolerance</li><li>Noise canceling unidirectional microphones<br>– Quiet environments</li></ul><h3 id="Rec-System-maximizes-chance-of-getting-things-right-by"><a href="#Rec-System-maximizes-chance-of-getting-things-right-by" class="headerlink" title="Rec System maximizes chance of getting things right by"></a>Rec System maximizes chance of getting things right by</h3><ul><li>Restricting vocabulary</li><li>Specifying order &amp; transitions for recognition</li><li>Associating actions/meaning with partial recognition of phrase</li><li>Use of context particularly within dialog to adjust constraints</li></ul><h3 id="Lombard-Effect"><a href="#Lombard-Effect" class="headerlink" title="Lombard Effect"></a>Lombard Effect</h3><p>During noise, speakers have an automatic normalization response that causes systematic speech modifications, including increased volume, reduced speaking rate, and changes in articulation and pitch.</p><h3 id="Issues-in-Ubicomp"><a href="#Issues-in-Ubicomp" class="headerlink" title="Issues in Ubicomp"></a>Issues in Ubicomp</h3><p>Issues in Ubicomp</p><ul><li>Context</li><li>Uneven conditioning</li><li>Inferring user intent</li><li>System interoperation</li></ul><h2 id="Decision-Making-amp-Diagnosis"><a href="#Decision-Making-amp-Diagnosis" class="headerlink" title="Decision Making &amp; Diagnosis"></a>Decision Making &amp; Diagnosis</h2><p>Learning Objectives</p><ul><li>Theoretical Understanding: Normative vs. Behavioral theories of decision making</li><li>Models of decision making in diagnosis Name &amp; illustrate standard fallacies in decision making </li></ul><h3 id="Rational-Decision-Making"><a href="#Rational-Decision-Making" class="headerlink" title="Rational Decision Making"></a>Rational Decision Making</h3><ul><li>A rational decision maker is one who chooses the alternative which maximizes his expected utility.</li><li>A rational decision maker is presumed to maximize her <strong>Subjective Utility</strong> which is likely to be some function of objective</li></ul><h3 id="Prospect-theory"><a href="#Prospect-theory" class="headerlink" title="Prospect theory"></a>Prospect theory</h3><p>Loss hurts more than Gain helps （抛硬币，正面赢20，反面输 10，大多数人选择不玩）</p><h3 id="Base-Rate-Fallacy"><a href="#Base-Rate-Fallacy" class="headerlink" title="Base Rate Fallacy"></a>Base Rate Fallacy</h3><p>Undervalue base rates!! </p><h3 id="Behavioral-Decision-Making"><a href="#Behavioral-Decision-Making" class="headerlink" title="Behavioral Decision Making"></a>Behavioral Decision Making</h3><ol><li>Treate extreme values as more moderate  （感知不到极端数值，就比如考试明明只剩三天，但还觉得时间很充裕不好好看hf）</li><li>Imperfections in memory （记忆缺陷， 不能收集到所有过去的信息帮助做决策，比如期中复习hf就十分紧张期末还是这样） </li><li>Inability to do complex math in our heads （做不了复杂算数） </li></ol><h3 id="Gambler’s-Fallacy"><a href="#Gambler’s-Fallacy" class="headerlink" title="Gambler’s Fallacy"></a>Gambler’s Fallacy</h3><p>Error: treating independent events as though they were dependent</p><h3 id="Availability-heuristic可得性偏差"><a href="#Availability-heuristic可得性偏差" class="headerlink" title="Availability heuristic可得性偏差"></a>Availability heuristic可得性偏差</h3><p>人们做决策总会基于 avaliability &amp; imaginability </p><h3 id="Imaginability"><a href="#Imaginability" class="headerlink" title="Imaginability"></a>Imaginability</h3><h3 id="Confirmation-bias"><a href="#Confirmation-bias" class="headerlink" title="Confirmation bias"></a>Confirmation bias</h3><p>确定偏差:简而言之就是听不进新的观点，无论怎样论证都是认为自己原本认为的是对的， 本质还是overconfidence<br>例如：给一些本身对于某件事有观点的人接受正反两面信息，人们通常都只注意到支持自己观点的理论，暗中反驳不符合自己观点的理论 </p><h3 id="Representativeness-bias-代表性偏差"><a href="#Representativeness-bias-代表性偏差" class="headerlink" title="Representativeness bias 代表性偏差"></a>Representativeness bias 代表性偏差</h3><p>人类在对事件做出判断的时候，过度关注于这个事件的某个特征，而忽略了这个事件发生的大环境概率和样本大小。<br>例如，你看到一家公司连续3年利润都翻番，然后立即对它的股票做出判断——买！错在代表性偏差。连续3年利润翻番，是一个好公司的代表性特征。但这并不意味着这家公司真的就是一家好公司，这家公司还有好多信息都被你忽略掉了。比如说，业绩可能是有意调整出来的；再比如说，这家公司未来的盈利机会消失，业绩不能持续。 </p><h3 id="Anchoring锚定效应"><a href="#Anchoring锚定效应" class="headerlink" title="Anchoring锚定效应"></a>Anchoring锚定效应</h3><p>人们在对某人某事做出判断时，易受第一印象影响从而先入为主 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Course-Description&quot;&gt;&lt;a href=&quot;#Course-Description&quot; class=&quot;headerlink&quot; title=&quot;Course Description&quot;&gt;&lt;/a&gt;Course Description&lt;/h2&gt;&lt;p&gt; This 
      
    
    </summary>
    
    
      <category term="Product Design" scheme="https://zhangruochi.com/categories/Product-Design/"/>
    
    
  </entry>
  
  <entry>
    <title>Dependency Parsing and Assignment3 of CS224n</title>
    <link href="https://zhangruochi.com/Dependency-Parsing-and-Assignment3-of-CS224n/2019/12/10/"/>
    <id>https://zhangruochi.com/Dependency-Parsing-and-Assignment3-of-CS224n/2019/12/10/</id>
    <published>2019-12-10T09:13:41.000Z</published>
    <updated>2019-12-19T18:09:55.553Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Dependency-Grammar-and-Dependency-Structure"><a href="#Dependency-Grammar-and-Dependency-Structure" class="headerlink" title="Dependency Grammar and Dependency Structure"></a>Dependency Grammar and Dependency Structure</h2><p>Parse trees in NLP, analogous to those in compilers, are used to analyze the syntactic structure of sentences. There are two main types of structures used:</p><ol><li>constituency structures</li><li>dependency structures</li></ol><p>Dependency structure of sentences shows which words depend on (modify or are arguments of) which other words. These binary asymmetric relations between the words are called dependencies and are depicted as arrows going from the <strong>head</strong> (or governor, superior, regent) to the <strong>dependent</strong> (or modifier, inferior, subordinate).  Usually these dependencies form a tree structure. They are often typed with the name of grammatical relations (subject, prepositional object, apposition, etc.). An example of such a dependency tree is shown in below</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Figure from cs224n</div></center><p>Usually some constraints:</p><ol><li>Only one word is adependent of ROOT</li><li>Don’twantcyclesA-&gt;B,B-&gt;A (tree structure)</li><li>Final issue is whether arrows can cross (non-projective) or not<ul><li>Defn: There are no crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words</li><li>Dependencies parallel to a CFG tree must be <strong>projective</strong>: Forming dependencies by taking 1 child of each category as head</li><li>But dependency theory normally does allow non-projective structures to account for displaced constituents: You can’t easily get the semantics of certain constructions right without these <strong>non-projective</strong> dependencies</li></ul></li></ol><h2 id="Parsing"><a href="#Parsing" class="headerlink" title="Parsing"></a>Parsing</h2><p>Given a parsing model M and a sentence S, derive the optimal dependency graph D for S according to M.</p><ol><li><strong>Dynamic programming</strong><br>Eisner (1996) gives a clever algorithm with complexity O(n3), by producing parse items with heads at the ends rather than in the middle</li><li><strong>Graph algorithms</strong><br>You create a Minimum Spanning Tree for a sentence McDonald et al.’s (2005) MSTParser scores dependencies independently using an ML classifier (he uses MIRA, for online learning, but it can be something else)</li><li><strong>Constraint Satisfaction</strong><br>Edges are eliminated that don’t satisfy hard constraints. Karlsson (1990), etc.</li><li><strong>Transition-based parsing</strong> or <strong>deterministic dependency parsing</strong><br>Greedy choice of attachments guided by good machine learning classifiers MaltParser (Nivre et al. 2008). Has proven highly effective.</li></ol><h2 id="Neural-Transition-Based-Dependency-Parsing"><a href="#Neural-Transition-Based-Dependency-Parsing" class="headerlink" title="Neural Transition-Based Dependency Parsing"></a>Neural Transition-Based Dependency Parsing</h2><p>A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between head words, and words which modify those heads. Your implementation will be a transition-based parser, which incrementally builds up a parse one step at a time. At every step it maintains a partial parse, which is represented as follows:</p><ul><li>A <strong>stack</strong> of words that are currently being processed. </li><li>A <strong>buffer</strong> of words yet to be processed.</li><li>A <strong>list</strong> of dependencies predicted by the parser.</li></ul><p>Initially, the stack only contains ROOT, the dependencies list is empty, and the buffer contains all words of the sentence in order. At each step, the parser applies a transition to the partial parse until its buffer is empty and the stack size is 1. The following transitions can be applied:</p><ul><li><strong>SHIFT</strong>: removes the first word from the buffer and pushes it onto the stack.</li><li><strong>LEFT-ARC</strong>: marks the second (second most recently added) item on the stack as a dependent of<br>the first item and removes the second item from the stack.</li><li><strong>RIGHT-ARC</strong>: marks the first (most recently added) item on the stack as a dependent of the second item and removes the first item from the stack.</li></ul><p>On each step, your parser will decide among the three transitions using a neural network classifier.Go through the sequence of transitions needed for parsing the sentence “I parsed this sentence correctly”. The dependency tree for the sentence is shown below. At each step, give the configuration of the stack and buffer, as well as what transition was applied this step and what new dependency was added (if any). The first three steps are provided below as an example.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="100%" height="100%">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Figure from cs224n</div></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 3</span></span><br><span class="line"><span class="string">parser_transitions.py: Algorithms for completing partial parsess.</span></span><br><span class="line"><span class="string">Sahil Chopra &lt;schopra8@stanford.edu&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PartialParse</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">        <span class="string">"""Initializes this partial parse.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param sentence (list of str): The sentence to be parsed as a list of words.</span></span><br><span class="line"><span class="string">                                        Your code should not modify the sentence.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># The sentence being parsed is kept for bookkeeping purposes. Do not alter it in your code.</span></span><br><span class="line">        self.sentence = sentence</span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (3 Lines)</span></span><br><span class="line">        <span class="comment">### Your code should initialize the following fields:</span></span><br><span class="line">        <span class="comment">###     self.stack: The current stack represented as a list with the top of the stack as the</span></span><br><span class="line">        <span class="comment">###                 last element of the list.</span></span><br><span class="line">        <span class="comment">###     self.buffer: The current buffer represented as a list with the first item on the</span></span><br><span class="line">        <span class="comment">###                  buffer as the first item of the list</span></span><br><span class="line">        <span class="comment">###     self.dependencies: The list of dependencies produced so far. Represented as a list of</span></span><br><span class="line">        <span class="comment">###             tuples where each tuple is of the form (head, dependent).</span></span><br><span class="line">        <span class="comment">###             Order for this list doesn't matter.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Note: The root token should be represented with the string "ROOT"</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line"></span><br><span class="line">        self.stack = [<span class="string">"ROOT"</span>]</span><br><span class="line">        self.buffer = sentence[:]</span><br><span class="line">        self.dependencies = []</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_step</span><span class="params">(self, transition)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single parse step by applying the given transition to this partial parse</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param transition (str): A string that equals "S", "LA", or "RA" representing the shift,</span></span><br><span class="line"><span class="string">                                left-arc, and right-arc transitions. You can assume the provided</span></span><br><span class="line"><span class="string">                                transition is a legal transition.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~7-10 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     Implement a single parsing step, i.e. the logic for the following as</span></span><br><span class="line">        <span class="comment">###     described in the pdf handout:</span></span><br><span class="line">        <span class="comment">###         1. Shift</span></span><br><span class="line">        <span class="comment">###         2. Left Arc</span></span><br><span class="line">        <span class="comment">###         3. Right Arc</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># if self.buffer and transition == "S":</span></span><br><span class="line">        <span class="comment">#     self.stack.append(self.buffer.pop(0))</span></span><br><span class="line">        <span class="comment"># elif len(self.stack) &gt;=2 and self.stack[-2] != "ROOT" and transition == "LA":</span></span><br><span class="line">        <span class="comment">#     self.dependencies.append(( self.stack[-1],self.stack[-2]))</span></span><br><span class="line">        <span class="comment">#     self.stack.pop(-2)</span></span><br><span class="line">        <span class="comment"># elif len(self.stack) &gt;= 2 and transition == "RA":</span></span><br><span class="line">        <span class="comment">#     self.dependencies.append((self.stack[-2], self.stack[-1]))</span></span><br><span class="line">        <span class="comment">#     self.stack.pop()</span></span><br><span class="line">        <span class="keyword">if</span> self.buffer <span class="keyword">and</span> transition == <span class="string">"S"</span>:</span><br><span class="line">            self.stack.append(self.buffer.pop(<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">elif</span> len(self.stack) &gt;= <span class="number">2</span> <span class="keyword">and</span> transition == <span class="string">"LA"</span>:</span><br><span class="line">            self.dependencies.append((self.stack[<span class="number">-1</span>], self.stack[<span class="number">-2</span>]))</span><br><span class="line">            self.stack.pop(<span class="number">-2</span>)</span><br><span class="line">        <span class="keyword">elif</span> len(self.stack) &gt;= <span class="number">2</span> <span class="keyword">and</span> transition == <span class="string">"RA"</span>:</span><br><span class="line">            self.dependencies.append((self.stack[<span class="number">-2</span>], self.stack[<span class="number">-1</span>]))</span><br><span class="line">            self.stack.pop(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, transitions)</span>:</span></span><br><span class="line">        <span class="string">"""Applies the provided transitions to this PartialParse</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param transitions (list of str): The list of transitions in the order they should be applied</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @return dsependencies (list of string tuples): The list of dependencies produced when</span></span><br><span class="line"><span class="string">                                                        parsing the sentence. Represented as a list of</span></span><br><span class="line"><span class="string">                                                        tuples where each tuple is of the form (head, dependent).</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> transitions:</span><br><span class="line">            self.parse_step(transition)</span><br><span class="line">        <span class="keyword">return</span> self.dependencies</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minibatch_parse</span><span class="params">(sentences, model, batch_size)</span>:</span></span><br><span class="line">    <span class="string">"""Parses a list of sentences in minibatches using a model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    @param sentences (list of list of str): A list of sentences to be parsed</span></span><br><span class="line"><span class="string">                                            (each sentence is a list of words and each word is of type string)</span></span><br><span class="line"><span class="string">    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function</span></span><br><span class="line"><span class="string">                                model.predict(partial_parses) that takes in a list of PartialParses as input and</span></span><br><span class="line"><span class="string">                                returns a list of transitions predicted for each parse. That is, after calling</span></span><br><span class="line"><span class="string">                                    transitions = model.predict(partial_parses)</span></span><br><span class="line"><span class="string">                                transitions[i] will be the next transition to apply to partial_parses[i].</span></span><br><span class="line"><span class="string">    @param batch_size (int): The number of PartialParses to include in each minibatch</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    @return dependencies (list of dependency lists): A list where each element is the dependencies</span></span><br><span class="line"><span class="string">                                                    list for a parsed sentence. Ordering should be the</span></span><br><span class="line"><span class="string">                                                    same as in sentences (i.e., dependencies[i] should</span></span><br><span class="line"><span class="string">                                                    contain the parse for sentences[i]).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dependencies = []</span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE (~8-10 Lines)</span></span><br><span class="line">    <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">    <span class="comment">###     Implement the minibatch parse algorithm as described in the pdf handout</span></span><br><span class="line">    <span class="comment">###</span></span><br><span class="line">    <span class="comment">###     Note: A shallow copy (as denoted in the PDF) can be made with the "=" sign in python, e.g.</span></span><br><span class="line">    <span class="comment">###                 unfinished_parses = partial_parses[:].</span></span><br><span class="line">    <span class="comment">###             Here `unfinished_parses` is a shallow copy of `partial_parses`.</span></span><br><span class="line">    <span class="comment">###             In Python, a shallow copied list like `unfinished_parses` does not contain new instances</span></span><br><span class="line">    <span class="comment">###             of the object stored in `partial_parses`. Rather both lists refer to the same objects.</span></span><br><span class="line">    <span class="comment">###             In our case, `partial_parses` contains a list of partial parses. `unfinished_parses`</span></span><br><span class="line">    <span class="comment">###             contains references to the same objects. Thus, you should NOT use the `del` operator</span></span><br><span class="line">    <span class="comment">###             to remove objects from the `unfinished_parses` list. This will free the underlying memory that</span></span><br><span class="line">    <span class="comment">###             is being accessed by `partial_parses` and may cause your code to crash.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> batch_size != <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    partial_parses = [PartialParse(s) <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">    unfinished_parses = partial_parses</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> unfinished_parses:</span><br><span class="line">        batch_parser = unfinished_parses[:batch_size]</span><br><span class="line">        <span class="keyword">while</span> batch_parser:</span><br><span class="line">            transitions = model.predict(batch_parser)</span><br><span class="line">            <span class="comment"># print(transitions)</span></span><br><span class="line">            <span class="keyword">for</span> parser,transition <span class="keyword">in</span> zip(batch_parser,transitions):</span><br><span class="line">                parser.parse_step(transition)</span><br><span class="line">            batch_parser = [parser <span class="keyword">for</span> parser <span class="keyword">in</span> batch_parser <span class="keyword">if</span> len(parser.stack) &gt; <span class="number">1</span> <span class="keyword">or</span> parser.buffer]</span><br><span class="line">            <span class="comment"># print(len(batch_parser))</span></span><br><span class="line">        unfinished_parses = unfinished_parses[batch_size:]</span><br><span class="line">    </span><br><span class="line">    dependencies = [parser.dependencies <span class="keyword">for</span> parser <span class="keyword">in</span> partial_parses]</span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dependencies</span><br></pre></td></tr></table></figure><p>We are now going to train a neural network to predict, given the state of the stack, buffer, and<br>dependencies, which transition should be applied next. First, the model extracts a feature vector<br>representing the current state. They can be represented as a list of integers $[w_1,w_2,\cdots,w_m]$ where m is the number of features and each $0 \leq w_i &lt; |V|$ is the index of a token in the vocabulary (|V| is the vocabulary size). First our network looks up an embedding for each word and concatenates them into a single input vector:</p><script type="math/tex; mode=display">x = [E_{w_1},\cdots,E_{w_m} ] \in \mathbb{R}^{dm}</script><p>We then compute our prediction as:</p><script type="math/tex; mode=display">\begin{aligned}& h = ReLU(xW + b_1) \\& l = hU + b_2 \\ & \hat{y} = softmax(l)\end{aligned}</script><p>where $h$ is referred to as the hidden layer,$l$ is referred to as the logits, $\hat{y}$ is referred to as the predictions. We will train the model to minimize cross-entropy loss:</p><script type="math/tex; mode=display">J(\theta) = CE(y,\hat{y}) = -\sum_{i=1}^{3}y_i log{\hat{y_i}}</script><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="100%" height="100%">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Figure from cs224n</div></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 3</span></span><br><span class="line"><span class="string">parser_model.py: Feed-Forward Neural Network for Dependency Parsing</span></span><br><span class="line"><span class="string">Sahil Chopra &lt;schopra8@stanford.edu&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ParserModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" Feedforward neural network with an embedding layer and single hidden layer.</span></span><br><span class="line"><span class="string">    The ParserModel will predict which transition should be applied to a</span></span><br><span class="line"><span class="string">    given partial parse configuration.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    PyTorch Notes:</span></span><br><span class="line"><span class="string">        - Note that "ParserModel" is a subclass of the "nn.Module" class. In PyTorch all neural networks</span></span><br><span class="line"><span class="string">            are a subclass of this "nn.Module".</span></span><br><span class="line"><span class="string">        - The "__init__" method is where you define all the layers and their respective parameters</span></span><br><span class="line"><span class="string">            (embedding layers, linear layers, dropout layers, etc.).</span></span><br><span class="line"><span class="string">        - "__init__" gets automatically called when you create a new instance of your class, e.g.</span></span><br><span class="line"><span class="string">            when you write "m = ParserModel()".</span></span><br><span class="line"><span class="string">        - Other methods of ParserModel can access variables that have "self." prefix. Thus,</span></span><br><span class="line"><span class="string">            you should add the "self." prefix layers, values, etc. that you want to utilize</span></span><br><span class="line"><span class="string">            in other ParserModel methods.</span></span><br><span class="line"><span class="string">        - For further documentation on "nn.Module" please see https://pytorch.org/docs/stable/nn.html.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embeddings, n_features=<span class="number">36</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        hidden_size=<span class="number">200</span>, n_classes=<span class="number">3</span>, dropout_prob=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        <span class="string">""" Initialize the parser model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param embeddings (Tensor): word embeddings (num_words, embedding_size)</span></span><br><span class="line"><span class="string">        @param n_features (int): number of input features</span></span><br><span class="line"><span class="string">        @param hidden_size (int): number of hidden units</span></span><br><span class="line"><span class="string">        @param n_classes (int): number of output classes</span></span><br><span class="line"><span class="string">        @param dropout_prob (float): dropout probability</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ParserModel, self).__init__()</span><br><span class="line">        self.n_features = n_features</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.dropout_prob = dropout_prob</span><br><span class="line">        self.embed_size = embeddings.shape[<span class="number">1</span>]</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.pretrained_embeddings = nn.Embedding(embeddings.shape[<span class="number">0</span>], self.embed_size)</span><br><span class="line">        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))</span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~5 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1) Construct `self.embed_to_hidden` linear layer, initializing the weight matrix</span></span><br><span class="line">        <span class="comment">###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)</span></span><br><span class="line">        <span class="comment">###     2) Construct `self.dropout` layer.</span></span><br><span class="line">        <span class="comment">###     3) Construct `self.hidden_to_logits` linear layer, initializing the weight matrix</span></span><br><span class="line">        <span class="comment">###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Note: Here, we use Xavier Uniform Initialization for our Weight initialization.</span></span><br><span class="line">        <span class="comment">###         It has been shown empirically, that this provides better initial weights</span></span><br><span class="line">        <span class="comment">###         for training networks than random uniform initialization.</span></span><br><span class="line">        <span class="comment">###         For more details checkout this great blogpost:</span></span><br><span class="line">        <span class="comment">###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization </span></span><br><span class="line">        <span class="comment">### Hints:</span></span><br><span class="line">        <span class="comment">###     - After you create a linear layer you can access the weight</span></span><br><span class="line">        <span class="comment">###       matrix via:</span></span><br><span class="line">        <span class="comment">###         linear_layer.weight</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Please see the following docs for support:</span></span><br><span class="line">        <span class="comment">###     Linear Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Linear</span></span><br><span class="line">        <span class="comment">###     Xavier Init: https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_</span></span><br><span class="line">        <span class="comment">###     Dropout: https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout</span></span><br><span class="line">        </span><br><span class="line">        self.embed_to_hidden = nn.Linear(self.embed_size * self.n_features, hidden_size)</span><br><span class="line">        self.dropout = nn.Dropout(p = self.dropout_prob)</span><br><span class="line">        self.hidden_to_logits = nn.Linear(hidden_size,self.n_classes)</span><br><span class="line">        nn.init.xavier_uniform_(self.embed_to_hidden.weight,gain=<span class="number">1</span>)</span><br><span class="line">        nn.init.xavier_uniform_(self.hidden_to_logits.weight,gain=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">embedding_lookup</span><span class="params">(self, t)</span>:</span></span><br><span class="line">        <span class="string">""" Utilize `self.pretrained_embeddings` to map input `t` from input tokens (integers)</span></span><br><span class="line"><span class="string">            to embedding vectors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            PyTorch Notes:</span></span><br><span class="line"><span class="string">                - `self.pretrained_embeddings` is a torch.nn.Embedding object that we defined in __init__</span></span><br><span class="line"><span class="string">                - Here `t` is a tensor where each row represents a list of features. Each feature is represented by an integer (input token).</span></span><br><span class="line"><span class="string">                - In PyTorch the Embedding object, e.g. `self.pretrained_embeddings`, allows you to</span></span><br><span class="line"><span class="string">                    go from an index to embedding. Please see the documentation (https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)</span></span><br><span class="line"><span class="string">                    to learn how to use `self.pretrained_embeddings` to extract the embeddings for your tensor `t`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            @param t (Tensor): input tensor of tokens (batch_size, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            @return x (Tensor): tensor of embeddings for words represented in t</span></span><br><span class="line"><span class="string">                                (batch_size, n_features * embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~1-3 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1) Use `self.pretrained_embeddings` to lookup the embeddings for the input tokens in `t`.</span></span><br><span class="line">        <span class="comment">###     2) After you apply the embedding lookup, you will have a tensor shape (batch_size, n_features, embedding_size).</span></span><br><span class="line">        <span class="comment">###         Use the tensor `view` method to reshape the embeddings tensor to (batch_size, n_features * embedding_size)</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Note: In order to get batch_size, you may need use the tensor .size() function:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">###  Please see the following docs for support:</span></span><br><span class="line">        <span class="comment">###     Embedding Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding</span></span><br><span class="line">        <span class="comment">###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line">        tmp_features = self.pretrained_embeddings(t)</span><br><span class="line">        shape = tmp_features.size()</span><br><span class="line">        x = tmp_features.view(shape[<span class="number">0</span>],shape[<span class="number">1</span>]*shape[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, t)</span>:</span></span><br><span class="line">        <span class="string">""" Run the model forward.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            PyTorch Notes:</span></span><br><span class="line"><span class="string">                - Every nn.Module object (PyTorch model) has a `forward` function.</span></span><br><span class="line"><span class="string">                - When you apply your nn.Module to an input tensor `t` this function is applied to the tensor.</span></span><br><span class="line"><span class="string">                    For example, if you created an instance of your ParserModel and applied it to some `t` as follows,</span></span><br><span class="line"><span class="string">                    the `forward` function would called on `t` and the result would be stored in the `output` variable:</span></span><br><span class="line"><span class="string">                        model = ParserModel()</span></span><br><span class="line"><span class="string">                        output = model(t) # this calls the forward function</span></span><br><span class="line"><span class="string">                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param t (Tensor): input tensor of tokens (batch_size, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)</span></span><br><span class="line"><span class="string">                                 without applying softmax (batch_size, n_classes)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">###  YOUR CODE HERE (~3-5 lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1) Apply `self.embedding_lookup` to `t` to get the embeddings</span></span><br><span class="line">        <span class="comment">###     2) Apply `embed_to_hidden` linear layer to the embeddings</span></span><br><span class="line">        <span class="comment">###     3) Apply relu non-linearity to the output of step 2 to get the hidden units.</span></span><br><span class="line">        <span class="comment">###     4) Apply dropout layer to the output of step 3.</span></span><br><span class="line">        <span class="comment">###     5) Apply `hidden_to_logits` layer to the output of step 4 to get the logits.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Note: We do not apply the softmax to the logits here, because</span></span><br><span class="line">        <span class="comment">### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Please see the following docs for support:</span></span><br><span class="line">        <span class="comment">###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu</span></span><br><span class="line">        x = self.embedding_lookup(t)</span><br><span class="line">        x = self.embed_to_hidden(x)</span><br><span class="line">        x = nn.functional.relu(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        logits = self.hidden_to_logits(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><h2 id="Runing-the-model"><a href="#Runing-the-model" class="headerlink" title="Runing the model"></a>Runing the model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 3</span></span><br><span class="line"><span class="string">run.py: Run the dependency parser.</span></span><br><span class="line"><span class="string">Sahil Chopra &lt;schopra8@stanford.edu&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> parser_model <span class="keyword">import</span> ParserModel</span><br><span class="line"><span class="keyword">from</span> utils.parser_utils <span class="keyword">import</span> minibatches, load_and_preprocess_data, AverageMeter</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------</span></span><br><span class="line"><span class="comment"># Primary Functions</span></span><br><span class="line"><span class="comment"># -----------------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(parser, train_data, dev_data, output_path, batch_size=<span class="number">1024</span>, n_epochs=<span class="number">10</span>, lr=<span class="number">0.0005</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Train the neural dependency parser.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    @param parser (Parser): Neural Dependency Parser</span></span><br><span class="line"><span class="string">    @param train_data ():</span></span><br><span class="line"><span class="string">    @param dev_data ():</span></span><br><span class="line"><span class="string">    @param output_path (str): Path to which model weights and results are written.</span></span><br><span class="line"><span class="string">    @param batch_size (int): Number of examples in a single batch</span></span><br><span class="line"><span class="string">    @param n_epochs (int): Number of training epochs</span></span><br><span class="line"><span class="string">    @param lr (float): Learning rate</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    best_dev_UAS = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE (~2-7 lines)</span></span><br><span class="line">    <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">    <span class="comment">###      1) Construct Adam Optimizer in variable `optimizer`</span></span><br><span class="line">    <span class="comment">###      2) Construct the Cross Entropy Loss Function in variable `loss_func`</span></span><br><span class="line">    <span class="comment">###</span></span><br><span class="line">    <span class="comment">### Hint: Use `parser.model.parameters()` to pass optimizer</span></span><br><span class="line">    <span class="comment">###       necessary parameters to tune.</span></span><br><span class="line">    <span class="comment">### Please see the following docs for support:</span></span><br><span class="line">    <span class="comment">###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html</span></span><br><span class="line">    <span class="comment">###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss</span></span><br><span class="line">    optimizer = optim.Adam(parser.model.parameters(),lr=lr)</span><br><span class="line">    loss_func = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        print(<span class="string">"Epoch &#123;:&#125; out of &#123;:&#125;"</span>.format(epoch + <span class="number">1</span>, n_epochs))</span><br><span class="line">        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)</span><br><span class="line">        <span class="keyword">if</span> dev_UAS &gt; best_dev_UAS:</span><br><span class="line">            best_dev_UAS = dev_UAS</span><br><span class="line">            print(<span class="string">"New best dev UAS! Saving model."</span>)</span><br><span class="line">            torch.save(parser.model.state_dict(), output_path)</span><br><span class="line">        print(<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_for_epoch</span><span class="params">(parser, train_data, dev_data, optimizer, loss_func, batch_size)</span>:</span></span><br><span class="line">    <span class="string">""" Train the neural dependency parser for single epoch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note: In PyTorch we can signify train versus test and automatically have</span></span><br><span class="line"><span class="string">    the Dropout Layer applied and removed, accordingly, by specifying</span></span><br><span class="line"><span class="string">    whether we are training, `model.train()`, or evaluating, `model.eval()`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    @param parser (Parser): Neural Dependency Parser</span></span><br><span class="line"><span class="string">    @param train_data ():</span></span><br><span class="line"><span class="string">    @param dev_data ():</span></span><br><span class="line"><span class="string">    @param optimizer (nn.Optimizer): Adam Optimizer</span></span><br><span class="line"><span class="string">    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function</span></span><br><span class="line"><span class="string">    @param batch_size (int): batch size</span></span><br><span class="line"><span class="string">    @param lr (float): learning rate</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    parser.model.train() <span class="comment"># Places model in "train" mode, i.e. apply dropout layer</span></span><br><span class="line">    n_minibatches = math.ceil(len(train_data) / batch_size)</span><br><span class="line">    loss_meter = AverageMeter()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tqdm(total=(n_minibatches)) <span class="keyword">as</span> prog:</span><br><span class="line">        <span class="keyword">for</span> i, (train_x, train_y) <span class="keyword">in</span> enumerate(minibatches(train_data, batch_size)):</span><br><span class="line">            optimizer.zero_grad()   <span class="comment"># remove any baggage in the optimizer</span></span><br><span class="line">            loss = <span class="number">0.</span> <span class="comment"># store loss for this batch here</span></span><br><span class="line">            train_x = torch.from_numpy(train_x).long()</span><br><span class="line">            train_y = torch.from_numpy(train_y.nonzero()[<span class="number">1</span>]).long()</span><br><span class="line"></span><br><span class="line">            <span class="comment">### YOUR CODE HERE (~5-10 lines)</span></span><br><span class="line">            <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">            <span class="comment">###      1) Run train_x forward through model to produce `logits`</span></span><br><span class="line">            <span class="comment">###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.</span></span><br><span class="line">            <span class="comment">###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss</span></span><br><span class="line">            <span class="comment">###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)</span></span><br><span class="line">            <span class="comment">###         are the predictions (y^ from the PDF).</span></span><br><span class="line">            <span class="comment">###      3) Backprop losses</span></span><br><span class="line">            <span class="comment">###      4) Take step with the optimizer</span></span><br><span class="line">            <span class="comment">### Please see the following docs for support:</span></span><br><span class="line">            <span class="comment">###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step</span></span><br><span class="line">            logits = parser.model.forward(train_x)</span><br><span class="line">            loss = loss_func(logits,train_y)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            <span class="comment">### END YOUR CODE</span></span><br><span class="line">            prog.update(<span class="number">1</span>)</span><br><span class="line">            loss_meter.update(loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Average Train Loss: &#123;&#125;"</span>.format(loss_meter.avg))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Evaluating on dev set"</span>,)</span><br><span class="line">    parser.model.eval() <span class="comment"># Places model in "eval" mode, i.e. don't apply dropout layer</span></span><br><span class="line">    dev_UAS, _ = parser.parse(dev_data)</span><br><span class="line">    print(<span class="string">"- dev UAS: &#123;:.2f&#125;"</span>.format(dev_UAS * <span class="number">100.0</span>))</span><br><span class="line">    <span class="keyword">return</span> dev_UAS</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># Note: Set debug to False, when training on entire corpus</span></span><br><span class="line">    debug = <span class="keyword">True</span></span><br><span class="line">    <span class="comment"># debug = False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(torch.__version__ == <span class="string">"1.0.0"</span>),  <span class="string">"Please install torch version 1.0.0"</span></span><br><span class="line"></span><br><span class="line">    print(<span class="number">80</span> * <span class="string">"="</span>)</span><br><span class="line">    print(<span class="string">"INITIALIZING"</span>)</span><br><span class="line">    print(<span class="number">80</span> * <span class="string">"="</span>)</span><br><span class="line">    parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)</span><br><span class="line"></span><br><span class="line">    start = time.time()</span><br><span class="line">    model = ParserModel(embeddings)</span><br><span class="line">    parser.model = model</span><br><span class="line">    print(<span class="string">"took &#123;:.2f&#125; seconds\n"</span>.format(time.time() - start))</span><br><span class="line"></span><br><span class="line">    print(<span class="number">80</span> * <span class="string">"="</span>)</span><br><span class="line">    print(<span class="string">"TRAINING"</span>)</span><br><span class="line">    print(<span class="number">80</span> * <span class="string">"="</span>)</span><br><span class="line">    output_dir = <span class="string">"results/&#123;:%Y%m%d_%H%M%S&#125;/"</span>.format(datetime.now())</span><br><span class="line">    output_path = output_dir + <span class="string">"model.weights"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(output_dir):</span><br><span class="line">        os.makedirs(output_dir)</span><br><span class="line"></span><br><span class="line">    train(parser, train_data, dev_data, output_path, batch_size=<span class="number">1024</span>, n_epochs=<span class="number">10</span>, lr=<span class="number">0.0005</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> debug:</span><br><span class="line">        print(<span class="number">80</span> * <span class="string">"="</span>)</span><br><span class="line">        print(<span class="string">"TESTING"</span>)</span><br><span class="line">        print(<span class="number">80</span> * <span class="string">"="</span>)</span><br><span class="line">        print(<span class="string">"Restoring the best model weights found on the dev set"</span>)</span><br><span class="line">        parser.model.load_state_dict(torch.load(output_path))</span><br><span class="line">        print(<span class="string">"Final evaluation on test set"</span>,)</span><br><span class="line">        parser.model.eval()</span><br><span class="line">        UAS, dependencies = parser.parse(test_data)</span><br><span class="line">        print(<span class="string">"- test UAS: &#123;:.2f&#125;"</span>.format(UAS * <span class="number">100.0</span>))</span><br><span class="line">        print(<span class="string">"Done!"</span>)</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>slides and course notes of <a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">cs224n</a></li></ol>]]></content>
    
    <summary type="html">
    
      the course note of deependency parsing and the details of assignment 3
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>CS224n Assignment 2</title>
    <link href="https://zhangruochi.com/CS224n-Assignment-2/2019/12/07/"/>
    <id>https://zhangruochi.com/CS224n-Assignment-2/2019/12/07/</id>
    <published>2019-12-08T00:57:10.000Z</published>
    <updated>2019-12-19T18:10:14.128Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.gradcheck <span class="keyword">import</span> gradcheck_naive</span><br><span class="line"><span class="keyword">from</span> utils.utils <span class="keyword">import</span> normalizeRows, softmax</span><br></pre></td></tr></table></figure><h2 id="Helper-functions"><a href="#Helper-functions" class="headerlink" title="Helper functions"></a>Helper functions</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid function for the input here.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array.</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE</span></span><br><span class="line">    s = <span class="number">1.</span>/(<span class="number">1.</span> + np.exp(-x))</span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getNegativeSamples</span><span class="params">(outsideWordIdx, dataset, K)</span>:</span></span><br><span class="line">    <span class="string">""" Samples K indexes which are not the outsideWordIdx """</span></span><br><span class="line"></span><br><span class="line">    negSampleWordIndices = [<span class="keyword">None</span>] * K</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(K):</span><br><span class="line">        newidx = dataset.sampleTokenIdx()</span><br><span class="line">        <span class="keyword">while</span> newidx == outsideWordIdx:</span><br><span class="line">            newidx = dataset.sampleTokenIdx()</span><br><span class="line">        negSampleWordIndices[k] = newidx</span><br><span class="line">    <span class="keyword">return</span> negSampleWordIndices</span><br></pre></td></tr></table></figure><h2 id="Naive-Softmax-Loss-And-Its-Gradient"><a href="#Naive-Softmax-Loss-And-Its-Gradient" class="headerlink" title="Naive Softmax Loss And Its Gradient"></a>Naive Softmax Loss And Its Gradient</h2><p>In word2vec, the conditional probability distribution is given by taking vector dot-products and applying the softmax function:</p><script type="math/tex; mode=display">P(o\|c) = \frac{exp^{u_o^{T}v_c}}{\sum_{w\in v}exp^{u_w^{T}v_c}}</script><ul><li>$u_o$ is the ‘outside’ vector representing outside word o</li><li>$v_c$ is the ‘center’ vector representing center word c</li></ul><p>The Cross Entropy Loss between the true (discrete) probability distribution p and another distribution q is:</p><script type="math/tex; mode=display">-\sum_i p_i log(q_i)</script><p>So that the naive-softmax loss for word2vec given in following equation is the same as the cross-entropy loss between $y$ and $\hat{y}$:</p><script type="math/tex; mode=display">-\sum_{w \in Vocab} y_w log(\hat{y}_w) = -log(\hat{y}_o)</script><p>For the backpropagation, lets introduce the intermediate variable $p$, which is a vector of the (normalized) probabilities. The loss for one example is:</p><script type="math/tex; mode=display">p_k = \frac{e^{f_k}}{ \sum_j e^{f_j} } \hspace{1in} L_i =-\log\left(p_{y_i}\right)</script><p>We now wish to understand how the computed scores inside $f$ should change to decrease the loss $L_i$  that this example contributes to the full objective. In other words, we want to derive the gradient $\frac{\partial L_i}{\partial f_k}$. The loss $L_i$ is computed from $p$ which in turn depends on $f$.</p><script type="math/tex; mode=display">\frac{\partial L_i }{ \partial f_k } = p_k - \mathbb{1}(y_i = k)</script><p>Notice how elegant and simple this expression is. Suppose the probabilities we computed were <code>p = [0.2, 0.3, 0.5]</code>, and that the correct class was the middle one (with probability <code>0.3</code>). According to this derivation the gradient on the scores would be <code>df = [0.2, -0.7, 0.5]</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naiveSoftmaxLossAndGradient</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    centerWordVec,</span></span></span><br><span class="line"><span class="function"><span class="params">    outsideWordIdx,</span></span></span><br><span class="line"><span class="function"><span class="params">    outsideVectors,</span></span></span><br><span class="line"><span class="function"><span class="params">    dataset</span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    <span class="string">""" Naive Softmax loss &amp; gradient function for word2vec models</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Implement the naive softmax loss and gradients between a center word's </span></span><br><span class="line"><span class="string">    embedding and an outside word's embedding. This will be the building block</span></span><br><span class="line"><span class="string">    for our word2vec models.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    centerWordVec -- numpy ndarray, center word's embedding</span></span><br><span class="line"><span class="string">                    (v_c in the pdf handout)</span></span><br><span class="line"><span class="string">    outsideWordIdx -- integer, the index of the outside word</span></span><br><span class="line"><span class="string">                    (o of u_o in the pdf handout)</span></span><br><span class="line"><span class="string">    outsideVectors -- outside vectors (rows of matrix) for all words in vocab</span></span><br><span class="line"><span class="string">                      (U in the pdf handout)</span></span><br><span class="line"><span class="string">    dataset -- needed for negative sampling, unused here.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    loss -- naive softmax loss</span></span><br><span class="line"><span class="string">    gradCenterVec -- the gradient with respect to the center word vector</span></span><br><span class="line"><span class="string">                     (dJ / dv_c in the pdf handout)</span></span><br><span class="line"><span class="string">    gradOutsideVecs -- the gradient with respect to all the outside word vectors</span></span><br><span class="line"><span class="string">                    (dJ / dU)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### Please use the provided softmax function (imported earlier in this file)</span></span><br><span class="line">    <span class="comment">### This numerically stable implementation helps you avoid issues pertaining</span></span><br><span class="line">    <span class="comment">### to integer overflow. </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># centerWordVec:  (embedding_dim,1)</span></span><br><span class="line">    <span class="comment"># outsideVectors: (vocab_size,embedding_dim)</span></span><br><span class="line"></span><br><span class="line">    scores = np.matmul(outsideVectors, centerWordVec)   <span class="comment"># (vocab_size,1)</span></span><br><span class="line">    <span class="comment"># print(scores.shape)</span></span><br><span class="line">    probs = softmax(scores)                          <span class="comment"># (vocab_size,1)  y_hat</span></span><br><span class="line"></span><br><span class="line">    loss = -np.log(probs[outsideWordIdx])</span><br><span class="line"></span><br><span class="line">    dscores = probs.copy()   <span class="comment"># (vocab_size,1)</span></span><br><span class="line">    dscores[outsideWordIdx] = dscores[outsideWordIdx] - <span class="number">1</span>   <span class="comment">#  y_hat minus y</span></span><br><span class="line">    gradCenterVec = np.matmul(outsideVectors.T, dscores)  <span class="comment"># (embedding_dim,1)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(dscores.shape) # (5,)</span></span><br><span class="line">    <span class="comment"># print(centerWordVec.shape) # (3,)</span></span><br><span class="line">    <span class="comment"># exit()</span></span><br><span class="line"></span><br><span class="line">    gradOutsideVecs = np.outer(dscores, centerWordVec) <span class="comment"># (vocab_size,embedding_dim)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, gradCenterVec, gradOutsideVecs</span><br></pre></td></tr></table></figure><h2 id="Negative-Sampling-Loss-And-Its-Gradient"><a href="#Negative-Sampling-Loss-And-Its-Gradient" class="headerlink" title="Negative Sampling Loss And Its Gradient"></a>Negative Sampling Loss And Its Gradient</h2><p>Now we shall consider the Negative Sampling loss, which is an alternative to the Naive Softmax loss. Assume that K negative samples (words) are drawn from the vocabulary. For simplicity of notation we shall refer to them as $w_1,w_2,\cdots,w_k$ and their outside vectors as $u_1,\cdots,u_k$. Note that $o \in {w_1, \cdots, w_k}$. For a center word c and an outside word o, the negative sampling loss function is given by:</p><script type="math/tex; mode=display">J_{neg-sample}(v_c,o,U) = -log(\sigma(u_o^{T}v_c)) - \sum_{k=1}^{k}log(\sigma(-u_k^{T},v_c))</script><p>The sigmoid function and its gradient is as follows:</p><script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+e^{-x}} \\\\\rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right) = \left( 1 - \sigma(x) \right) \sigma(x)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">negSamplingLossAndGradient</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    centerWordVec,</span></span></span><br><span class="line"><span class="function"><span class="params">    outsideWordIdx,</span></span></span><br><span class="line"><span class="function"><span class="params">    outsideVectors,</span></span></span><br><span class="line"><span class="function"><span class="params">    dataset,</span></span></span><br><span class="line"><span class="function"><span class="params">    K=<span class="number">10</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    <span class="string">""" Negative sampling loss function for word2vec models</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Implement the negative sampling loss and gradients for a centerWordVec</span></span><br><span class="line"><span class="string">    and a outsideWordIdx word vector as a building block for word2vec</span></span><br><span class="line"><span class="string">    models. K is the number of negative samples to take.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note: The same word may be negatively sampled multiple times. For</span></span><br><span class="line"><span class="string">    example if an outside word is sampled twice, you shall have to</span></span><br><span class="line"><span class="string">    double count the gradient with respect to this word. Thrice if</span></span><br><span class="line"><span class="string">    it was sampled three times, and so forth.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Negative sampling of words is done for you. Do not modify this if you</span></span><br><span class="line">    <span class="comment"># wish to match the autograder and receive points!</span></span><br><span class="line">    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)</span><br><span class="line">    indices = [outsideWordIdx] + negSampleWordIndices</span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### Please use your implementation of sigmoid in here.</span></span><br><span class="line">    gradCenterVec   = np.zeros(centerWordVec.shape)</span><br><span class="line">    gradOutsideVecs = np.zeros(outsideVectors.shape)</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    u_o = outsideVectors[outsideWordIdx]</span><br><span class="line">    z = sigmoid(np.dot(u_o,centerWordVec))</span><br><span class="line">    loss -= np.log(z)</span><br><span class="line">    gradCenterVec += u_o*(z<span class="number">-1</span>)</span><br><span class="line">    gradOutsideVecs[outsideWordIdx] = centerWordVec*(z<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(K):</span><br><span class="line">        neg_id = indices[i+<span class="number">1</span>]</span><br><span class="line">        u_k = outsideVectors[neg_id]</span><br><span class="line">        z = sigmoid(-np.dot(u_k,centerWordVec))</span><br><span class="line">        loss -= np.log(z)</span><br><span class="line">        gradCenterVec += u_k*(<span class="number">1</span>-z)</span><br><span class="line">        gradOutsideVecs[neg_id] += centerWordVec*(<span class="number">1</span>-z)</span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, gradCenterVec, gradOutsideVecs</span><br></pre></td></tr></table></figure><h2 id="SkipGram"><a href="#SkipGram" class="headerlink" title="SkipGram"></a>SkipGram</h2><p>Suppose the center word is $c = w_t$ and the context window is $[w_{t-m},\cdots,w_{t-1},\cdots, w_{t}, \cdots, w_{t+1}, \cdots,w_{t+m} ]$, where m is the context window size. Recall that for the skip-gram version of word2vec, the total loss for the context window is:</p><script type="math/tex; mode=display">J_{skip-gram} (v_c,w_{t−m},\cdots,w_{t+m}, U) = \sum_{-m \leq j \leq m, j \neq 0} J(v_c,w_{t+j},U)</script><p>Here, $J(v_c,w_{t+j},U)$ represents an arbitrary loss term for the center word $c = w_t$ and outside word</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">skipgram</span><span class="params">(currentCenterWord, windowSize, outsideWords, word2Ind,</span></span></span><br><span class="line"><span class="function"><span class="params">             centerWordVectors, outsideVectors, dataset,</span></span></span><br><span class="line"><span class="function"><span class="params">             word2vecLossAndGradient=naiveSoftmaxLossAndGradient)</span>:</span></span><br><span class="line">    <span class="string">""" Skip-gram model in word2vec</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Implement the skip-gram model in this function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    currentCenterWord -- a string of the current center word</span></span><br><span class="line"><span class="string">    windowSize -- integer, context window size</span></span><br><span class="line"><span class="string">    outsideWords -- list of no more than 2*windowSize strings, the outside words</span></span><br><span class="line"><span class="string">    word2Ind -- a dictionary that maps words to their indices in</span></span><br><span class="line"><span class="string">              the word vector list</span></span><br><span class="line"><span class="string">    centerWordVectors -- center word vectors (as rows) for all words in vocab</span></span><br><span class="line"><span class="string">                        (V in pdf handout)</span></span><br><span class="line"><span class="string">    outsideVectors -- outside word vectors (as rows) for all words in vocab</span></span><br><span class="line"><span class="string">                    (U in pdf handout)</span></span><br><span class="line"><span class="string">    word2vecLossAndGradient -- the loss and gradient function for</span></span><br><span class="line"><span class="string">                               a prediction vector given the outsideWordIdx</span></span><br><span class="line"><span class="string">                               word vectors, could be one of the two</span></span><br><span class="line"><span class="string">                               loss functions you implemented above.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    loss -- the loss function value for the skip-gram model</span></span><br><span class="line"><span class="string">            (J in the pdf handout)</span></span><br><span class="line"><span class="string">    gradCenterVecs -- the gradient with respect to the center word vectors</span></span><br><span class="line"><span class="string">            (dJ / dV in the pdf handout)</span></span><br><span class="line"><span class="string">    gradOutsideVectors -- the gradient with respect to the outside word vectors</span></span><br><span class="line"><span class="string">                        (dJ / dU in the pdf handout)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    gradCenterVecs = np.zeros(centerWordVectors.shape)</span><br><span class="line">    gradOutsideVectors = np.zeros(outsideVectors.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE</span></span><br><span class="line">    center_id = word2Ind[currentCenterWord]</span><br><span class="line">    centerWordVec = centerWordVectors[center_id]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> outsideWords:</span><br><span class="line">        outside_id = word2Ind[word]</span><br><span class="line">        loss_mini, gradCenter_mini, gradOutside_mini= \</span><br><span class="line">        word2vecLossAndGradient(centerWordVec=centerWordVec,</span><br><span class="line">            outsideWordIdx=outside_id,outsideVectors=outsideVectors,dataset=dataset)</span><br><span class="line">        loss += loss_mini</span><br><span class="line">        <span class="comment"># print(gradCenterVecs[center_id].shape, gradCenter_mini.shape)</span></span><br><span class="line">        <span class="comment"># exit()</span></span><br><span class="line">        gradCenterVecs[center_id] += gradCenter_mini</span><br><span class="line">        gradOutsideVectors += gradOutside_mini</span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, gradCenterVecs, gradOutsideVectors</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      The details of the assignments 2
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Computational Graph</title>
    <link href="https://zhangruochi.com/Computational-Graph/2019/12/06/"/>
    <id>https://zhangruochi.com/Computational-Graph/2019/12/06/</id>
    <published>2019-12-07T01:21:51.000Z</published>
    <updated>2019-12-19T18:10:04.656Z</updated>
    
    <content type="html"><![CDATA[<h2 id="General-Computation-Graph"><a href="#General-Computation-Graph" class="headerlink" title="General Computation Graph"></a>General Computation Graph</h2><ol><li>Fprop: visit nodes in <strong>topological sort</strong> order<ul><li>Compute value of node given predecessors</li></ul></li><li>Bprop:<ul><li>initialize output gradient = 1 </li><li>visit nodes in reverse order:<ul><li>Compute gradient wrt each node using gradient wrt successors<script type="math/tex; mode=display">{y_1,y_2, \cdots, y_n} = successors of x</script><script type="math/tex; mode=display">\frac{\partial z}{\partial x} = \sum_{i=1}^{n}\frac{\partial z}{\partial y_i}\frac{\partial y_i}{\partial x}</script>Done correctly, big O() complexity of fprop and bprop is the same</li></ul></li></ul></li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture note of cs224n</div></center><h2 id="Automatic-Differentiation"><a href="#Automatic-Differentiation" class="headerlink" title="Automatic Differentiation"></a>Automatic Differentiation</h2><ul><li>The gradient computation canbe automatically inferred from the symbolic expression of the fprop</li><li>Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output</li><li>Modern DL frameworks(Tensorflow, PyTorch, etc.) do backpropagation for you but mainly leave layer/node writer to hand-calculate the local derivative</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture note of cs224n</div></center><h2 id="Example-Sigmoid"><a href="#Example-Sigmoid" class="headerlink" title="Example: Sigmoid"></a>Example: Sigmoid</h2><p>The gates we introduced above are relatively arbitrary. Any kind of <strong>differentiable function</strong> can act as a gate, and we can group multiple gates into a single gate, or decompose a function into multiple gates whenever it is convenient. Lets look at another expression that illustrates this point:</p><script type="math/tex; mode=display">f(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}</script><p>We have the knowlege of derivatives</p><script type="math/tex; mode=display">\begin{aligned}f(x) = \frac{1}{x} \hspace{1in} & \rightarrow \hspace{1in} \frac{df}{dx} = -1/x^2 \\f_c(x) = c + x\hspace{1in}  & \rightarrow  \hspace{1in} \frac{df}{dx} = 1 \\f(x) = e^x\hspace{1in}  & \rightarrow  \hspace{1in} \frac{df}{dx} = e^x \\f_a(x) = ax\hspace{1in} & \rightarrow  \hspace{1in} \frac{df}{dx} = a \\\end{aligned}</script><p>the picture blow shows the visual representation of the computation. The forward pass computes values from inputs to output (shown in green). The backward pass then performs backpropagation which starts at the end and recursively applies the chain rule to compute the gradients (shown in red) all the way to the inputs of the circuit. The gradients can be thought of as flowing backwards through the circuit.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="80%" height="80%">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture note of cs231n</div></center><p>It turns out that the derivative of the sigmoid function with respect to its input simplifies if you perform the derivation (after a fun tricky part where we add and subtract a 1 in the numerator):</p><script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+e^{-x}} \\\\\rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right) = \left( 1 - \sigma(x) \right) \sigma(x)</script><p>As we see, the gradient turns out to simplify and becomes surprisingly simple. For example, the sigmoid expression receives the input 1.0 and computes the output 0.73 during the forward pass. The derivation above shows that the local gradient would simply be (1 - 0.73) * 0.73 = 0.2, as the circuit computed before (see the image above), except this way it would be done with a single, simple and efficient expression (and with less numerical issues). Therefore, in any real practical application it would be very useful to group these operations into a single gate. Lets see the backprop for this neuron in code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">w = [<span class="number">2</span>,<span class="number">-3</span>,<span class="number">-3</span>] <span class="comment"># assume some random weights and data</span></span><br><span class="line">x = [<span class="number">-1</span>, <span class="number">-2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># forward pass</span></span><br><span class="line">dot = w[<span class="number">0</span>]*x[<span class="number">0</span>] + w[<span class="number">1</span>]*x[<span class="number">1</span>] + w[<span class="number">2</span>]</span><br><span class="line">f = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-dot)) <span class="comment"># sigmoid function</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># backward pass through the neuron (backpropagation)</span></span><br><span class="line">ddot = (<span class="number">1</span> - f) * f <span class="comment"># gradient on dot variable, using the sigmoid gradient derivation</span></span><br><span class="line">dx = [w[<span class="number">0</span>] * ddot, w[<span class="number">1</span>] * ddot] <span class="comment"># backprop into x</span></span><br><span class="line">dw = [x[<span class="number">0</span>] * ddot, x[<span class="number">1</span>] * ddot, <span class="number">1.0</span> * ddot] <span class="comment"># backprop into w</span></span><br><span class="line"><span class="comment"># we're done! we have the gradients on the inputs to the circuit</span></span><br></pre></td></tr></table></figure><h2 id="Staged-computation"><a href="#Staged-computation" class="headerlink" title="Staged computation"></a>Staged computation</h2><p>Lets see this with another example. Suppose that we have a function of the form:</p><script type="math/tex; mode=display">f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}</script><p>We don’t need to have an explicit function written down that evaluates the gradient. We only have to know how to compute it. Here is how we would structure the <strong>forward pass</strong> of such expression:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">3</span> <span class="comment"># example values</span></span><br><span class="line">y = <span class="number">-4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># forward pass</span></span><br><span class="line">sigy = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-y)) <span class="comment"># sigmoid in numerator   #(1)</span></span><br><span class="line">num = x + sigy <span class="comment"># numerator                               #(2)</span></span><br><span class="line">sigx = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-x)) <span class="comment"># sigmoid in denominator #(3)</span></span><br><span class="line">xpy = x + y                                              <span class="comment">#(4)</span></span><br><span class="line">xpysqr = xpy**<span class="number">2</span>                                          <span class="comment">#(5)</span></span><br><span class="line">den = sigx + xpysqr <span class="comment"># denominator                        #(6)</span></span><br><span class="line">invden = <span class="number">1.0</span> / den                                       <span class="comment">#(7)</span></span><br><span class="line">f = num * invden <span class="comment"># done!                                 #(8)</span></span><br></pre></td></tr></table></figure><p>Computing the backprop pass is easy: We’ll go backwards and for every variable along the way in the forward pass (sigy, num, sigx, xpy, xpysqr, den, invden) we will have the same variable, but one that begins with a <code>d</code>, which will hold the gradient of the output of the circuit with respect to that variable.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># backprop f = num * invden</span></span><br><span class="line">dnum = invden <span class="comment"># gradient on numerator                             #(8)</span></span><br><span class="line">dinvden = num                                                     <span class="comment">#(8)</span></span><br><span class="line"><span class="comment"># backprop invden = 1.0 / den </span></span><br><span class="line">dden = (<span class="number">-1.0</span> / (den**<span class="number">2</span>)) * dinvden                                <span class="comment">#(7)</span></span><br><span class="line"><span class="comment"># backprop den = sigx + xpysqr</span></span><br><span class="line">dsigx = (<span class="number">1</span>) * dden                                                <span class="comment">#(6)</span></span><br><span class="line">dxpysqr = (<span class="number">1</span>) * dden                                              <span class="comment">#(6)</span></span><br><span class="line"><span class="comment"># backprop xpysqr = xpy**2</span></span><br><span class="line">dxpy = (<span class="number">2</span> * xpy) * dxpysqr                                        <span class="comment">#(5)</span></span><br><span class="line"><span class="comment"># backprop xpy = x + y</span></span><br><span class="line">dx = (<span class="number">1</span>) * dxpy                                                   <span class="comment">#(4)</span></span><br><span class="line">dy = (<span class="number">1</span>) * dxpy                                                   <span class="comment">#(4)</span></span><br><span class="line"><span class="comment"># backprop sigx = 1.0 / (1 + math.exp(-x))</span></span><br><span class="line">dx += ((<span class="number">1</span> - sigx) * sigx) * dsigx <span class="comment"># Notice += !! See notes below  #(3)</span></span><br><span class="line"><span class="comment"># backprop num = x + sigy</span></span><br><span class="line">dx += (<span class="number">1</span>) * dnum                                                  <span class="comment">#(2)</span></span><br><span class="line">dsigy = (<span class="number">1</span>) * dnum                                                <span class="comment">#(2)</span></span><br><span class="line"><span class="comment"># backprop sigy = 1.0 / (1 + math.exp(-y))</span></span><br><span class="line">dy += ((<span class="number">1</span> - sigy) * sigy) * dsigy                                 <span class="comment">#(1)</span></span><br><span class="line"><span class="comment"># done! phew</span></span><br></pre></td></tr></table></figure><h2 id="Patterns"><a href="#Patterns" class="headerlink" title="Patterns"></a>Patterns</h2><ol><li><strong>add distributes the upstream gradient</strong>: The add gate always takes the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass. This follows from the fact that the local gradient for the add operation is simply +1.0, so the gradients on all inputs will exactly equal the gradients on the output because it will be multiplied by x1.0 (and remain unchanged).</li><li><strong>max “routes” the upstream gradient</strong>: The max gate routes the gradient. Unlike the add gate which distributed the gradient unchanged to all its inputs, the max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass). This is because the local gradient for a max gate is 1.0 for the highest value, and 0.0 for all other values.</li><li><strong>mul switches the upstream gradient</strong>: The multiply gate is a little less easy to interpret. Its local gradients are the input values (except switched), and this is multiplied by the gradient on its output during the chain rule. </li></ol><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="30%" height="30%">    <br><!--     <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;"></div> --></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiplyGate</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x,y)</span>:</span></span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">        z = x*y</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(dz)</span>:</span></span><br><span class="line">        dx = self.y * dz <span class="comment"># [dz/dz * dL/dz]</span></span><br><span class="line">        dy = self.x * dz <span class="comment"># [dz/dy * dL/dz]</span></span><br><span class="line">        <span class="keyword">return</span> [dx,dy]</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>lecture notes and slides from <a href="http://cs231n.github.io/optimization-2/" target="_blank" rel="noopener">http://cs231n.github.io/optimization-2/</a></li><li>lecture notes and slides from <a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/</a></li></ol>]]></content>
    
    <summary type="html">
    
      What is computational graph, how to use computationl graph ?
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Word Vectors</title>
    <link href="https://zhangruochi.com/Word-Vectors/2019/12/04/"/>
    <id>https://zhangruochi.com/Word-Vectors/2019/12/04/</id>
    <published>2019-12-04T21:14:11.000Z</published>
    <updated>2019-12-19T18:10:37.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Why-we-need-Word-Vectors"><a href="#Why-we-need-Word-Vectors" class="headerlink" title="Why we need Word Vectors ?"></a>Why we need Word Vectors ?</h2><p>We want to encode word tokens each into some vector that represents a point in some sort of “word” space. This is paramount for a number of reasons but the most intuitive reason is that perhaps there actually exists some N-dimensional space (such that N &lt;&lt; 13 million) that is sufficient to encode all semantics of our language. Each dimension would encode some meaning that we transfer using speech. For instance, semantic dimensions might indicate tense (past vs. present vs. future), count (singular vs. plural), and gender (masculine vs. feminine).</p><h2 id="One-hot-vector"><a href="#One-hot-vector" class="headerlink" title="One-hot vector"></a>One-hot vector</h2><p>Represent every word as an $\mathbb{R}^{|v|\cdot 1}$ vector with all 0s and one 1 at the index of that word in the sorted english language. $|V|$ is the size of our vocabulary. Word vectors in this type of encoding would appear as the following:</p><script type="math/tex; mode=display">W^{abandon} = \begin{bmatrix} 1 \\0 \\0 \\0 \\\vdots \\0 \\\end{bmatrix}</script><p>We represent each word as a completely independent entity. This word representation <strong>does not</strong> give us directly any notion of similarity. For instance,</p><script type="math/tex; mode=display">(W^{hotel})^{T}W^{motel} =(W^{hotel})^{T}W^{cat} = 0</script><h2 id="SVD-Based-Methods"><a href="#SVD-Based-Methods" class="headerlink" title="SVD Based Methods"></a>SVD Based Methods</h2><p>For this class of methods to find word embeddings (otherwise known as word vectors), we first loop over a massive dataset and accumulate word co-occurrence counts in some form of a matrix X, and then perform Singular Value Decomposition on X to get a<br>$USV^{T}$ decomposition. We then use the rows of U as the word embeddings for all words in our dictionary. Let us discuss a few choices of X.</p><h3 id="Word-Document-Matrix"><a href="#Word-Document-Matrix" class="headerlink" title="Word-Document Matrix"></a>Word-Document Matrix</h3><p>As our first attempt, we make the bold conjecture that words thatare related will often appear in the <strong>same documents</strong>. We use this fact to build a word-document matrix, $X$ in the following manner: Loop over billions of documents and for each time word $i$ appears in document $j$, we add one to entry $X_{ij}$. This is obviously a very large matrix $\mathbb{R}^{|v|\cdot M}$ and it scales with the number of documents (M). So perhaps we can try something better.</p><h3 id="Window-based-Co-occurrence-Matrix"><a href="#Window-based-Co-occurrence-Matrix" class="headerlink" title="Window based Co-occurrence Matrix"></a>Window based Co-occurrence Matrix</h3><p>A co-occurrence matrix counts how often things co-occur in some environment. Given some word $w_i$ occurring in the document, we consider the <em>context window</em> surrounding $w_i$. Supposing our fixed window size is $n$, then this is the $n$ preceding and $n$ subsequent words in that document, i.e. words $w_{i-n} \dots w_{i-1}$ and $w_{i+1} \dots w_{i+n}$. We build a <em>co-occurrence matrix</em> $M$, which is a symmetric word-by-word matrix in which $M_{ij}$ is the number of times $w_j$ appears inside $w_i$’s window.</p><p><strong>Example: Co-Occurrence with Fixed Window of n=1</strong>:</p><ul><li>Document 1: “all that glitters is not gold”</li><li>Document 2: “all is well that ends well”</li></ul><div class="table-container"><table><thead><tr><th>*</th><th>START</th><th>all</th><th>that</th><th>glitters</th><th>is</th><th>not</th><th>gold</th><th>well</th><th>ends</th><th>END</th></tr></thead><tbody><tr><td>START</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>all</td><td>2</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>that</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>glitters</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>is</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>not</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>gold</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>well</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td>ends</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>END</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td></tr></tbody></table></div><p><strong>Note:</strong> In NLP, we often add START and END tokens to represent the beginning and end of sentences, paragraphs or documents. In thise case we imagine START and END tokens encapsulating each document, e.g., “START All that glitters is not gold END”, and include these tokens in our co-occurrence counts.</p><p>The rows (or columns) of this matrix provide one type of word vectors (those based on word-word co-occurrence), but the vectors will be large in general (linear in the number of distinct words in a corpus). Thus, our next step is to run <em>dimensionality reduction</em>. In particular, we will run <em>SVD (Singular Value Decomposition)</em>, which is a kind of generalized <em>PCA (Principal Components Analysis)</em> to select the top $k$ principal components. Here’s a visualization of dimensionality reduction with SVD. In this picture our co-occurrence matrix is $A$ with $n$ rows corresponding to $n$ words. We obtain a full matrix decomposition, with the singular values ordered in the diagonal $S$ matrix, and our new, shorter length-$k$ word vectors in $U_k$.</p><h3 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h3><p><strong>Eigenvalues</strong> quantify the importance of information along the line of <strong>eigenvectors</strong>. Equipped with this information, we know what part of the information can be ignored and how to compress information (SVD, Dimension reduction &amp; PCA). It also helps us to extract features in developing machine learning models. Sometimes, it makes the model easier to train because of the reduction of tangled information. It also serves the purpose to visualize tangled raw data.</p><p>for Eigenvalues $\lambda$ and Eigenvector $V$, we have:</p><script type="math/tex; mode=display">AV = \lambda V</script><p>the dimension of A is $\mathbb{R}^{n\cdot n}$ and $V$ is a $\mathbb{R}^{n\cdot 1}$ vector.</p><h4 id="Diagonalizable"><a href="#Diagonalizable" class="headerlink" title="Diagonalizable"></a>Diagonalizable</h4><p>Let’s assume a matrix A has two eigenvalues and eigenvectors.</p><script type="math/tex; mode=display">Av_1 = \lambda_1 v_1</script><script type="math/tex; mode=display">Av_2 = \lambda_2 v_2</script><p>We can concatenate them together and rewrite the equations in the matrix form.</p><script type="math/tex; mode=display">A \begin{bmatrix} v1 & v2 \end{bmatrix} = \begin{bmatrix} \lambda_1 v_1 & \lambda_2 v_2 \end{bmatrix} = \begin{bmatrix} v1 & v2 \end{bmatrix} \begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix}</script><p>We can generalize it into any number of eigenvectors as</p><script type="math/tex; mode=display">AV = V\land</script><p>A square matrix A is diagonalizable if we can convert it into a diagonal matrix, like</p><script type="math/tex; mode=display">V^{-1} A V = \land</script><p>An n × n square matrix is diagonalizable if it has n linearly independent eigenvectors. If a matrix is symmetric, it is diagonalizable. If a matrix does not have repeated eigenvalue, it always generates enough linearly independent eigenvectors to diagonalize a vector. If it has repeated eigenvalues, there is no guarantee we have enough eigenvectors. Some will not be diagonalizable.</p><p>If $A$ is a square matrix with $N$ linearly independent eigenvectors ($v_1$, $v_2$, $\cdots$, $v_n$) and corresponding eigenvalues ($\lambda_1$, $\lambda_2$, $\cdots$, $\lambda_n$), we can rearrange</p><script type="math/tex; mode=display">V^{-1} A V = \land</script><p>into </p><script type="math/tex; mode=display">A = V \land V^{-1}</script><p>For example,</p><p><img src="diagonalizable.png" alt></p><h4 id="Singular-vectors-amp-singular-values"><a href="#Singular-vectors-amp-singular-values" class="headerlink" title="Singular vectors &amp; singular values"></a>Singular vectors &amp; singular values</h4><p>However, the above method is possible only if $A$ is a square matrix and $A$ has n linearly independent eigenvectors. Now, it is time to develop a solution for all matrices using SVD.</p><p>The matrix $AA^{T}$ and $A^{T}A$ are very special in linear algebra. Consider any m × n matrix A, we can multiply it with $A^{T}$ to form $AA^{T}$ and $A^{T}A$ separately. These matrices are</p><ul><li>symmetrical,</li><li>square,</li><li>at least positive semidefinite (eigenvalues are zero or positive),</li><li>both matrices have the same positive eigenvalues, and</li><li>both have the same rank r as A.</li></ul><p>We name the eigenvectors for $AA^{T}$ as $u_i$ and $A^{T}A$ as $v_i$ here and call these sets of eigenvectors $u$ and $v$ the <strong>singular vectors</strong> of A. Both matrices have the same positive eigenvalues. The square roots of these eigenvalues are called <strong>singular values</strong>. We concatenate vectors $u_i$ into $U$ and $v_i$ into $V$ to form orthogonal matrices.</p><p><strong>SVD states that any matrix A can be factorized as</strong>:</p><script type="math/tex; mode=display">A_{m\cdot n} = U_{m\cdot m} S_{m\cdot n} V_{n\cdot n}^{T}</script><p>S is a diagonal matrix with r elements equal to the root of the positive eigenvalues of $AA^{T}$ or $A^{T}A$ (both matrics have the same positive eigenvalues anyway).</p><p><img src="usv.png" alt></p><h4 id="Applying-SVD-to-the-cooccurrence-matrix"><a href="#Applying-SVD-to-the-cooccurrence-matrix" class="headerlink" title="Applying SVD to the cooccurrence matrix"></a>Applying SVD to the cooccurrence matrix</h4><p><img src="svd.png" alt="Picture of an SVD"></p><p>This reduced-dimensionality co-occurrence representation preserves semantic relationships between words, e.g. <em>doctor</em> and <em>hospital</em> will be closer than <em>doctor</em> and <em>dog</em>. </p><p>Although these methods give us word vectors that are more than sufficient to encode semantic and syntactic (part of speech) information but are associated with many other problems:</p><ul><li>The dimensions of the matrix change very often (new words are added very frequently and corpus changes in size).</li><li>The matrix is extremely sparse since most words do not co-occur.</li><li>The matrix is very high dimensional in general (≈ 10e6 × 10e6)</li><li>Quadratic cost to train (i.e. to perform SVD)</li><li>Requires the incorporation of some hacks on X to account for the drastic imbalance in word frequency</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_co_occurrence_matrix</span><span class="params">(corpus, window_size=<span class="number">4</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Compute co-occurrence matrix for the given corpus and window_size (default of 4).</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller</span></span><br><span class="line"><span class="string">              number of co-occurring words.</span></span><br><span class="line"><span class="string">              </span></span><br><span class="line"><span class="string">              For example, if we take the document "START All that glitters is not gold END" with window size of 4,</span></span><br><span class="line"><span class="string">              "All" will co-occur with "START", "that", "glitters", "is", and "not".</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            corpus (list of list of strings): corpus of documents</span></span><br><span class="line"><span class="string">            window_size (int): size of context window</span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            M (numpy matrix of shape (number of corpus words, number of corpus words)): </span></span><br><span class="line"><span class="string">                Co-occurence matrix of word counts. </span></span><br><span class="line"><span class="string">                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.</span></span><br><span class="line"><span class="string">            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    words, num_words = distinct_words(corpus)</span><br><span class="line">    M = <span class="keyword">None</span></span><br><span class="line">    word2Ind = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    word2Ind = &#123;word:idx <span class="keyword">for</span> word,idx <span class="keyword">in</span> zip(words, range(num_words))&#125;</span><br><span class="line">    M = np.zeros((num_words,num_words))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> corpus:</span><br><span class="line">        <span class="keyword">for</span> index, central_word <span class="keyword">in</span> enumerate(doc):</span><br><span class="line">            left = max(<span class="number">0</span>,index - window_size)</span><br><span class="line">            right = min(num_words, index + window_size+<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">for</span> context_word <span class="keyword">in</span> doc[left:right]:</span><br><span class="line">                <span class="keyword">if</span> context_word == central_word:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                M[word2Ind[central_word]][word2Ind[context_word]] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> M, word2Ind</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_to_k_dim</span><span class="params">(M, k=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)</span></span><br><span class="line"><span class="string">        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:</span></span><br><span class="line"><span class="string">            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            M (numpy matrix of shape (number of corpus words, number of corpus words)): co-occurence matrix of word counts</span></span><br><span class="line"><span class="string">            k (int): embedding size of each word after dimension reduction</span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.</span></span><br><span class="line"><span class="string">                    In terms of the SVD from math class, this actually returns U * S</span></span><br><span class="line"><span class="string">    """</span>    </span><br><span class="line">    n_iters = <span class="number">10</span>     <span class="comment"># Use this parameter in your call to `TruncatedSVD`</span></span><br><span class="line">    M_reduced = <span class="keyword">None</span></span><br><span class="line">    print(<span class="string">"Running Truncated SVD over %i words..."</span> % (M.shape[<span class="number">0</span>]))</span><br><span class="line">    </span><br><span class="line">    t_svd = TruncatedSVD(n_components=k, n_iter = n_iters)</span><br><span class="line">    M_reduced = t_svd.fit_transform(M)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ------------------</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Done."</span>)</span><br><span class="line">    <span class="keyword">return</span> M_reduced</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_embeddings</span><span class="params">(M_reduced, word2Ind, words)</span>:</span></span><br><span class="line">    <span class="string">""" Plot in a scatterplot the embeddings of the words specified in the list "words".</span></span><br><span class="line"><span class="string">        NOTE: do not plot all the words listed in M_reduced / word2Ind.</span></span><br><span class="line"><span class="string">        Include a label next to each point.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            M_reduced (numpy matrix of shape (number of unique words in the corpus , k)): matrix of k-dimensioal word embeddings</span></span><br><span class="line"><span class="string">            word2Ind (dict): dictionary that maps word to indices for matrix M</span></span><br><span class="line"><span class="string">            words (list of strings): words whose embeddings we want to visualize</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    print(M_reduced.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i,word <span class="keyword">in</span> enumerate(words):</span><br><span class="line">        x = M_reduced[i][<span class="number">0</span>]</span><br><span class="line">        y = M_reduced[i][<span class="number">1</span>]</span><br><span class="line">        plt.scatter(x, y, marker=<span class="string">'x'</span>, color=<span class="string">'red'</span>)</span><br><span class="line">        plt.text(x, y, word, fontsize=<span class="number">9</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h2 id="Iteration-Based-Methods-Word2vec"><a href="#Iteration-Based-Methods-Word2vec" class="headerlink" title="Iteration Based Methods - Word2vec"></a>Iteration Based Methods - Word2vec</h2><p>Instead of computing and storing global information about some huge dataset (which might be billions of sentences), we can try to create a model that will be able to learn one iteration at a time and eventually be able to encode the probability of a word given its context. The idea is to design a model whose parameters are the word vec- tors. Then, train the model on a certain objective. At every iteration we run our model, evaluate the errors, and follow an update rule that has some notion of penalizing the model parameters that caused the error. Thus, we learn our word vectors.</p><p>Word2vec is a software package that actually includes :</p><ul><li><strong>2 algorithms</strong>: continuous bag-of-words (CBOW) and skip-gram. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word.</li><li><strong>2 training methods</strong>: negative sampling and hierarchical softmax. Negative sampling defines an objective by sampling negative exam- ples, while hierarchical softmax defines an objective using an efficient tree structure to compute probabilities for all the vocabulary.</li></ul><h3 id="Language-Models"><a href="#Language-Models" class="headerlink" title="Language Models"></a>Language Models</h3><p>First, we need to create such a model that will assign a probability to a sequence of tokens. Let us start with an example:</p><blockquote><p>“The cat jumped over the puddle.”</p></blockquote><p>A good language model will give this sentence a high probability because this is a completely valid sentence, syntactically and semantically. Mathematically, we can call this probability on any given sequence of n words:</p><script type="math/tex; mode=display">P(w_1,w_2,\cdots,w_n)</script><p>We can take the unary language model approach and break apart this probability by assuming the word occurrences are completely independent:</p><script type="math/tex; mode=display">P(w_1,w_2,\cdots,w_n) = \prod_{i=1}^{n} P(w_i)</script><p>However, we know this is a bit ludicrous because we know the next word is highly contingent upon the previous sequence of words. And the silly sentence example might actually score highly. So perhaps we let the probability of the sequence depend on the pairwise probability of a word in the sequence and the word next to it. We call this the bigram model and represent it as: </p><script type="math/tex; mode=display">P(w_1,w_2,\cdots,w_n) = \prod_{i=1}^{n} P(w_i|w_{i-1})</script><p>Again this is certainly a bit naive since we are only concerning ourselves with pairs of neighboring words rather than evaluating a whole sentence, but as we will see, this representation gets us pretty far along.</p><h3 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h3><p>One approach is to create a model such that given the center word “jumped”, the model will be able to predict or generate the surrounding words “The”, “cat”, “over”, “the”, “puddle”. Here we call the word “jumped” the context. We call this type of model a Skip-Gram model.<br><img src="Skip.png" alt><br><img src="Skip_2.png" alt></p><p>We breakdown the way this model works in these 6 steps:</p><ol><li>We generate our one hot input vector $x \in \mathbb{R}^{|v|}$ of the center word.</li><li>We get our embedded word vector for the center word<script type="math/tex; mode=display">v_c = Vx  \qquad \in \mathbb{R}^{|v|}</script></li><li>Generate a score vector<script type="math/tex; mode=display">z = Uv_c  \qquad \in \mathbb{R}^{|v|}</script></li><li>Turn the score vector into probabilities,$\hat{y} = softmax(z)$<script type="math/tex; mode=display">\hat y_{c-m}, \cdots, \hat y_{c-1}, \cdots, \hat y_{c+m}</script></li><li>We desire our probability vector generated to match the true prob- abilities which is the one hot vectors of the actual output.<script type="math/tex; mode=display">y_{c-m}, \cdots, y_{c-1}, \cdots, y_{c+m}</script></li></ol><h4 id="Objective-function"><a href="#Objective-function" class="headerlink" title="Objective function"></a>Objective function</h4><p><img src="objective.png" alt></p><p><strong> How to calculate $P(o|c)$? We will use two vectors per word w</strong>:</p><ul><li>$V_w$ when w is a center word</li><li>$U_w$ when w is a context word</li></ul><p>Then for a center word c and a context word o:</p><script type="math/tex; mode=display">P(o|c) = \frac{exp^{u_o^{T}v_c}}{\sum_{w\in v}exp^{u_w^{T}v_c}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SkipGram</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""x --&gt; batch_size x word_index</span></span><br><span class="line"><span class="string">       output --&gt; batch_size x context_predicted x vocabulary"""</span>    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocabulary_size, embedding_features, context_len, padding_idx=<span class="number">0</span> )</span>:</span></span><br><span class="line">        super(SkipGram, self).__init__()</span><br><span class="line">        self.context_len = context_len</span><br><span class="line">        self.embedding = nn.Embedding(num_embeddings = vocabulary_size, embedding_dim=embedding_features, padding_idx=padding_idx)</span><br><span class="line">        self.fc = nn.Linear(in_features = embedding_features, out_features = vocabulary_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        context_out = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.context_len):</span><br><span class="line">            wordvec_x = self.embedding(x)</span><br><span class="line">            context_word_i = self.fc(wordvec_x)</span><br><span class="line">            context_out.append(context_word_i)</span><br><span class="line">        log_prob = F.log_softmax(torch.stack(context_out, dim=<span class="number">1</span>).squeeze(), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> log_prob</span><br><span class="line"></span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">model = SkipGram()</span><br><span class="line">log_prob = model(centre_word)</span><br><span class="line">loss=<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(log_prob.shape[<span class="number">1</span>]):</span><br><span class="line">    loss_i = loss_function(log_prob[:,i,], context_word_i[:,i])</span><br><span class="line">    loss *= loss_i</span><br><span class="line">loss = loss/(i+<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Continuous-Bag-of-Words-Model-CBOW"><a href="#Continuous-Bag-of-Words-Model-CBOW" class="headerlink" title="Continuous Bag of Words Model (CBOW)"></a>Continuous Bag of Words Model (CBOW)</h3><p>Another approach is to treat {“The”, “cat”, ’over”, “the’, “puddle”} as a <strong>context</strong> and from these words, be able to predict or generate the <strong>center word</strong> “jumped”. This type of model we call a Continuous Bag of Words (CBOW) Model.</p><p><img src="CBOW.png" alt></p><p>We breakdown the way this model works in these steps:</p><ol><li>We generate our one hot word vectors for the input context of size m:<script type="math/tex; mode=display">x^{(c−m)},\cdots,x^{(c−1)},x^{(c+1)},\cdots,x^{(c+m)}\in\mathbb{R}^{|v|}</script></li><li>We get our embedded word vectors for the context:<script type="math/tex; mode=display">V_{c-m} = Vx^{(c−m)},V_{c-m+1} = Vx^{(c−m+1)},\cdots,V_{c+m} = Vx^{(c+m)}</script></li><li>Average these vectors to get <script type="math/tex; mode=display">\hat{v} = \frac{v_{c-m} + v_{c-m+1} + \cdots + v_{c+m}}{2m}</script></li><li>Generate a score vector <script type="math/tex; mode=display">z = U\hat{v}  \qquad \in \mathbb{R}^{|v|}</script>As dot product of similar vectors is higher, it will push similar words close to each other in order to achieve a high score.</li><li>Turnthescoresintoprobabilities <script type="math/tex; mode=display">\hat{y} = softmax(z)  \qquad \in \mathbb{R}^{|v|}</script></li><li>We desire our probabilities generated, $\hat{y} \in \mathbb{R}^{|v|}$, to match the true probabilities, $y \in \mathbb{R}^{|v|}$ which also happens to be the one hot vector of the actual word.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""input  -- &gt; batch_size x context_size</span></span><br><span class="line"><span class="string">       output --&gt; batch_size x vocabulary"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocabulary_size, embedding_features, padding_idx=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""nn.Embedding holds a tensor of dimmension (vocabulary_size, feature_size)--&gt;N(0,1)"""</span></span><br><span class="line">        super(CBOW, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(num_embeddings = vocabulary_size, embedding_dim = embedding_features, padding_idx=padding_idx)</span><br><span class="line">        self.fc = nn.Linear(in_features = embedding_features, out_features = vocabulary_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, context_words)</span>:</span></span><br><span class="line">        wordvecs = self.embedding(context_words)</span><br><span class="line">        mean_wordvecs = wordvecs.sum(dim=<span class="number">1</span>)/x.shape[<span class="number">1</span>] </span><br><span class="line">        log_prob = F.log_softmax(self.fc(mean_wordvecs), dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> log_prob</span><br><span class="line"></span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">model  = CBOW()</span><br><span class="line">log_prob = model(context_words)</span><br><span class="line">loss = loss_function(log_prob.squeeze(), centre_word.squeeze())</span><br></pre></td></tr></table></figure><h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>Lets take a second to look at the objective function. Note that the summation over |V| is computationally huge! Any update we do or evaluation of the objective function would take O(|V|) time which if we recall is in the millions. A simple idea is we could instead just approximate it.</p><p><strong>For every training step, instead of looping over the entire vocabulary, we can just sample several negative examples!</strong> We “sample” from a noise distribution $P_n(w)$ whose probabilities match the ordering of the frequency of the vocabulary. Unlike the probabilistic model of Word2Vec where for each input word probability is computed from all the target words in the vocabulary, here for each input word has only few target words (few true and rest randomly selected false targets). <strong>The key difference compared to the probabilistic model is the use of sigmoid activation as final discriminator replacing softmax function in the probabilistic model.</strong></p><p>Given this example(We get positive example by using the same skip-grams technique, a fixed window that goes around):</p><blockquote><p>“I want a glass of orange juice to go along with my cereal”</p></blockquote><p>The sampling will look like this:</p><div class="table-container"><table><thead><tr><th style="text-align:left">Context</th><th style="text-align:left">Word</th><th style="text-align:left">target</th></tr></thead><tbody><tr><td style="text-align:left">orange</td><td style="text-align:left">juice</td><td style="text-align:left">1</td></tr><tr><td style="text-align:left">orange</td><td style="text-align:left">king</td><td style="text-align:left">0</td></tr><tr><td style="text-align:left">orange</td><td style="text-align:left">book</td><td style="text-align:left">0</td></tr><tr><td style="text-align:left">orange</td><td style="text-align:left">the</td><td style="text-align:left">0</td></tr><tr><td style="text-align:left">orange</td><td style="text-align:left">of</td><td style="text-align:left">0</td></tr></tbody></table></div><p>So the steps to generate the samples are:</p><ol><li>Pick a positive context</li><li>Pick a k negative contexts from the dictionary.<br>We will have a k negative examples to 1 positive ones in the data we are collecting.</li></ol><p><img src="negative_sampling.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Merge</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Reshape</span><br><span class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"></span><br><span class="line"><span class="comment"># build skip-gram architecture</span></span><br><span class="line">word_model = Sequential()</span><br><span class="line">word_model.add(Embedding(vocab_size, embed_size,</span><br><span class="line">                         embeddings_initializer=<span class="string">"glorot_uniform"</span>,</span><br><span class="line">                         input_length=<span class="number">1</span>))</span><br><span class="line">word_model.add(Reshape((embed_size, )))</span><br><span class="line"></span><br><span class="line">context_model = Sequential()</span><br><span class="line">context_model.add(Embedding(vocab_size, embed_size,</span><br><span class="line">                  embeddings_initializer=<span class="string">"glorot_uniform"</span>,</span><br><span class="line">                  input_length=<span class="number">1</span>))</span><br><span class="line">context_model.add(Reshape((embed_size,)))</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Merge([word_model, context_model], mode=<span class="string">"dot"</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, kernel_initializer=<span class="string">"glorot_uniform"</span>, activation=<span class="string">"sigmoid"</span>))</span><br><span class="line">model.compile(loss=<span class="string">"mean_squared_error"</span>, optimizer=<span class="string">"rmsprop"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, elem <span class="keyword">in</span> enumerate(skip_grams):</span><br><span class="line">        pair_first_elem = np.array(list(zip(*elem[<span class="number">0</span>]))[<span class="number">0</span>], dtype=<span class="string">'int32'</span>)</span><br><span class="line">        pair_second_elem = np.array(list(zip(*elem[<span class="number">0</span>]))[<span class="number">1</span>], dtype=<span class="string">'int32'</span>)</span><br><span class="line">        labels = np.array(elem[<span class="number">1</span>], dtype=<span class="string">'int32'</span>)</span><br><span class="line">        X = [pair_first_elem, pair_second_elem]</span><br><span class="line">        Y = labels</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Processed &#123;&#125; (skip_first, skip_second, relevance) pairs'</span>.format(i))</span><br><span class="line">        loss += model.train_on_batch(X,Y)  </span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Epoch:'</span>, epoch, <span class="string">'Loss:'</span>, loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## get word embedding</span></span><br><span class="line">merge_layer = model.layers[<span class="number">0</span>]</span><br><span class="line">word_model = merge_layer.layers[<span class="number">0</span>]</span><br><span class="line">word_embed_layer = word_model.layers[<span class="number">0</span>]</span><br><span class="line">weights = word_embed_layer.get_weights()[<span class="number">0</span>][<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">print(weights.shape)</span><br><span class="line">pd.DataFrame(weights, index=id2word.values()).head()</span><br></pre></td></tr></table></figure><h3 id="hierarchical-softmax"><a href="#hierarchical-softmax" class="headerlink" title="hierarchical softmax"></a>hierarchical softmax</h3><p>hierarchical softmax is a much more efficient alternative to the normal softmax. In practice, hierarchical softmax tends to be better for infrequent words, while negative sampling works better for frequent words and lower dimensional vectors.</p><p>Hierarchical softmax uses a binary tree to represent all words in the vocabulary. Each leaf of the tree is a word, and there is a unique path from root to leaf. In this model, there is no output representation for words. Instead, each node of the graph (except the root and the leaves) is associated to a vector that the model is going to learn.</p><p>In this model, the probability of a word w given a vector $w_i$, p(w|w_i),is equal to the probability of a random walk starting in the root and ending in the leaf node corresponding to w. The main advantage in computing the probability this way is that the cost is only O(log(|V|)), corresponding to the length of the path.<br><img src="hafftree.png" alt></p><p>Taking $w_2$ in above figure, we must take two left edges and<br>then a right edge to reach w2 from the root, so</p><script type="math/tex; mode=display">p(w_2) = p(n(w_2,1),left) \cdot p(n(w_2,2),left) \cdot p(n(w_2,3),right) \\ = \sigma({\theta_{n(w_2,1)}}^T \cdot h) \cdot \sigma({\theta_{n(w_2,2)}}^T \cdot h) \cdot \sigma({-\theta_{n(w_2,3)}}^T \cdot h)</script><p>Therefore,</p><script type="math/tex; mode=display">p(w)=\prod_{j=1}^{L(w)-1}\sigma( sign(w,j)\cdot {\theta_{n(w,j)}}^Th )</script><script type="math/tex; mode=display">sign(w,j)= \begin{cases} 1, & \text{if n(w,j+1) is the left child of n(w,j)} \\ -1,& \text{if n(w,j+1) is the right child of n(w,j)}\end{cases}</script><ul><li>$\theta_{n(w,j)}$ is the vector representation of $n(w,j)$</li><li>$h$ is the output of hidden layer</li></ul><h3 id="Global-Vectors-for-Word-Representation-GloVe"><a href="#Global-Vectors-for-Word-Representation-GloVe" class="headerlink" title="Global Vectors for Word Representation (GloVe)"></a>Global Vectors for Word Representation (GloVe)</h3><p>So far, we have looked at two main classes of methods to find word embeddings. </p><ul><li>The first set are count-based and rely on matrix factor- ization (e.g. LSA, HAL). While these methods effectively leverage global statistical information, they are primarily used to capture word similarities and do poorly on tasks such as word analogy, indi- cating a sub-optimal vector space structure. </li><li>The other set of methods are shallow window-based (e.g. the skip-gram and the CBOW mod- els), which learn word embeddings by making predictions in local context windows. These models demonstrate the capacity to capture complex linguistic patterns beyond word similarity, but fail to make use of the global co-occurrence statistics.</li></ul><p>In comparison, GloVe consists of a weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics. The model produces a word vector space with meaningful sub-structure. It shows state-of-the-art per- formance on the word analogy task, and outperforms other current methods on several word similarity tasks.</p><ol><li>Construct co-occurrence Matrix</li><li>Construct relationships between word vectors and co-occurrence Matrix<ul><li>Let X denote the word-word co-occurrence matrix, where $X_{ij}$ indicates the number of times word j occur in the context of word i</li><li>$w_{i}$,$\tilde{w_{j}}$ is the word vector</li><li>$b_i,b_j$ is the bias term<script type="math/tex; mode=display">w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} = \log(X_{ij}) \tag{1}</script></li></ul></li><li>Construct loss function: Mean Square Loss<script type="math/tex; mode=display">J = \sum_{i,j=1}^{V} f(X_{ij})(w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} – \log(X_{ij}) )^2</script><script type="math/tex; mode=display">f(x)=\begin{equation} \begin{cases} (x/x_{max})^{\alpha}  & \text{if} \ x < x_{max} \\ 1 & \text{otherwise} \end{cases} \end{equation}</script></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_func</span><span class="params">(x, x_max, alpha)</span>:</span></span><br><span class="line">    wx = (x/x_max)**alpha</span><br><span class="line">    wx = torch.min(wx, torch.ones_like(wx))</span><br><span class="line">    <span class="keyword">return</span> wx</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wmse_loss</span><span class="params">(weights, inputs, targets)</span>:</span></span><br><span class="line">    loss = weights * F.mse_loss(inputs, targets, reduction=<span class="string">'none'</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.mean(loss)</span><br><span class="line"></span><br><span class="line">EMBED_DIM = <span class="number">300</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GloveModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_embeddings, embedding_dim)</span>:</span></span><br><span class="line">        super(GloveModel, self).__init__()</span><br><span class="line">        self.wi = nn.Embedding(num_embeddings, embedding_dim)</span><br><span class="line">        self.wj = nn.Embedding(num_embeddings, embedding_dim)</span><br><span class="line">        self.bi = nn.Embedding(num_embeddings, <span class="number">1</span>)</span><br><span class="line">        self.bj = nn.Embedding(num_embeddings, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.wi.weight.data.uniform_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        self.wj.weight.data.uniform_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        self.bi.weight.data.zero_()</span><br><span class="line">        self.bj.weight.data.zero_()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, i_indices, j_indices)</span>:</span></span><br><span class="line">        w_i = self.wi(i_indices)</span><br><span class="line">        w_j = self.wj(j_indices)</span><br><span class="line">        b_i = self.bi(i_indices).squeeze()</span><br><span class="line">        b_j = self.bj(j_indices).squeeze()</span><br><span class="line">        </span><br><span class="line">        x = torch.sum(w_i * w_j, dim=<span class="number">1</span>) + b_i + b_j</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">glove = GloveModel(dataset._vocab_len, EMBED_DIM)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">outputs = glove(i_idx, j_idx)</span><br><span class="line">weights_x = weight_func(x_ij, X_MAX, ALPHA)</span><br><span class="line">loss = wmse_loss(weights_x, outputs, torch.log(x_ij))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure><p>In conclusion, the GloVe model efficiently leverages global statistical information by training only on the nonzero elements in a word- word co-occurrence matrix, and produces a vector space with mean- ingful sub-structure. It consistently outperforms word2vec on the word analogy task, given the same corpus, vocabulary, window size, and training time. It achieves better results faster, and also obtains the best results irrespective of speed.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Course note and slides of <a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">cs224n</a></li><li><a href="https://medium.com/@jonathan_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491" target="_blank" rel="noopener">Machine Learning — Singular Value Decomposition (SVD) &amp; Principal Component Analysis (PCA)</a></li><li><a href="https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html" target="_blank" rel="noopener">https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html</a></li><li><a href="https://zhuanlan.zhihu.com/p/42651829" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/42651829</a></li><li><a href="https://nlpython.com/implementing-glove-model-with-pytorch/" target="_blank" rel="noopener">https://nlpython.com/implementing-glove-model-with-pytorch/</a></li></ul>]]></content>
    
    <summary type="html">
    
      Word Vectors Summary
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Composition</title>
    <link href="https://zhangruochi.com/Composition/2019/12/01/"/>
    <id>https://zhangruochi.com/Composition/2019/12/01/</id>
    <published>2019-12-02T01:25:39.000Z</published>
    <updated>2019-12-02T02:36:34.481Z</updated>
    
    <content type="html"><![CDATA[<h2 id="What-is-Composition"><a href="#What-is-Composition" class="headerlink" title="What is Composition?"></a>What is Composition?</h2><ol><li>组合是指将不同的部分结合成一个整体的行为。使用面向对象的组合技术，可以将简单的、独立的对象组合成更大更复杂的整体。</li><li>从本质上讲，参与组合的那些对象都很小，他们在结构上都是独立的。这使得他们能够无缝低转换为可插入、可互换的组件。</li></ol><h2 id="重构"><a href="#重构" class="headerlink" title="重构"></a>重构</h2><h3 id="创建零件"><a href="#创建零件" class="headerlink" title="创建零件"></a>创建零件</h3><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bicycle</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:size</span>, <span class="symbol">:parts</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(args = &#123;&#125;)</span></span></span><br><span class="line">        @size = args[<span class="symbol">:size</span>]</span><br><span class="line">        @parts = args[<span class="symbol">:parts</span>]</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spares</span></span></span><br><span class="line">        parts.spares</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Parts</span> &lt; Array</span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:parts</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(parts)</span></span></span><br><span class="line">        @parts = parts</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spares</span></span></span><br><span class="line">        parts.select &#123;<span class="params">|part|</span> part.needs_spare&#125;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span></span></span><br><span class="line">        parts.size</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Part</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:name</span>, <span class="symbol">:description</span>, <span class="symbol">:needs_spare</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(args)</span></span></span><br><span class="line">        @name = args[<span class="symbol">:name</span>]</span><br><span class="line">        @description = args[<span class="symbol">:description</span>]</span><br><span class="line">        @needs_spare = args.fetch(<span class="symbol">:needs_spare</span>,<span class="literal">true</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">chain = Part.new(<span class="symbol">name:</span> <span class="string">"chain"</span>, <span class="symbol">description:</span> <span class="string">'10-speed'</span>)</span><br><span class="line">road_tire = Part.new(<span class="symbol">name:</span> <span class="string">"tire_size"</span>, <span class="symbol">description:</span> <span class="string">'23'</span>)</span><br><span class="line">tape = Part.new(<span class="symbol">name:</span> <span class="string">"tape_color"</span>, <span class="symbol">description:</span> <span class="string">'red'</span>)</span><br><span class="line">mountain_tire = Part.new(<span class="symbol">name:</span> <span class="string">"tire_size"</span>, <span class="symbol">description:</span> <span class="string">'2.1'</span>)</span><br><span class="line">rear_shock = Part.new(<span class="symbol">name:</span> <span class="string">"rear_shock"</span>, <span class="symbol">description:</span> <span class="string">'Fox'</span>)</span><br><span class="line">front_shock = Part.new(<span class="symbol">name:</span> <span class="string">'front_shock'</span>, <span class="symbol">description:</span> <span class="string">'Manitou'</span>, <span class="symbol">needs_spare:</span> <span class="literal">false</span>)</span><br></pre></td></tr></table></figure><h3 id="组装"><a href="#组装" class="headerlink" title="组装"></a>组装</h3><p>此时<code>size</code>能正常响应，但是不能执行数组之间的加法会导致问题. 尽管<code>+</code>连接的是 <code>Parts</code> 对象，但是<code>+</code> 返回的对象是 <code>Array</code>实例。<code>Array</code>并不知道如何响应<code>spares</code></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">road_bike_parts = Parts.new([chain, road_tire, tape])</span><br><span class="line">p road_bike_parts.spares</span><br><span class="line">p road_bike_parts.size</span><br></pre></td></tr></table></figure><h3 id="让Parts对象更像一个数组"><a href="#让Parts对象更像一个数组" class="headerlink" title="让Parts对象更像一个数组"></a>让Parts对象更像一个数组</h3><ol><li>The Forwardable module<br>Forwardable is a module that can be used to add behavior to all the <strong>instances</strong> of a given class. This module is included to the singleton class using the extend keyword in order to add methods at class-level (to keep it simple).</li><li>The Forwardable#def_delegator method allows an object to forward a message to a defined receiver.<ul><li>The first argument correspond to the receiver of the message forwarding.</li><li>The second argument is the message to forward.</li><li>And finally the third argument is an alias of the message.</li></ul></li><li>The def_delegators method</li><li>The delegate method<br> The delegate method accepts a hash as argument where:<ul><li>the key is one or more messages</li><li>the value is the receiver of the messages defined as key<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in forwardable.rb</span></span><br><span class="line"><span class="keyword">require</span> <span class="string">'forwardable'</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Computer</span></span></span><br><span class="line">  attr <span class="symbol">:cores</span>, <span class="symbol">:screens</span></span><br><span class="line">  extend Forwardable</span><br><span class="line">  delegate %I[size]   =&gt; <span class="symbol">:</span>@cores,</span><br><span class="line">           %I[length] =&gt; <span class="symbol">:</span>@screens</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span></span><br><span class="line">    @cores  = (<span class="number">1</span>..<span class="number">8</span>).to_a</span><br><span class="line">    @screens = [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">macrosoft = Computer.new</span><br><span class="line">puts <span class="string">"Cores:   <span class="subst">#&#123;macrosoft.size&#125;</span>"</span></span><br><span class="line">puts <span class="string">"Screens: <span class="subst">#&#123;macrosoft.length&#125;</span>"</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><p>The 2 main differences with the def_delegator method is that it takes a set of methods to forward and the methods cannot be aliased<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">require</span> <span class="string">'forwardable'</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bicycle</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:size</span>, <span class="symbol">:parts</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(args = &#123;&#125;)</span></span></span><br><span class="line">        @size = args[<span class="symbol">:size</span>]</span><br><span class="line">        @parts = args[<span class="symbol">:parts</span>]</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spares</span></span></span><br><span class="line">        parts.spares</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Parts</span></span></span><br><span class="line">    extend Forwardable</span><br><span class="line">    def_delegators <span class="symbol">:</span>@parts, <span class="symbol">:size</span>, <span class="symbol">:each</span></span><br><span class="line">    <span class="keyword">include</span> Enumerable</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(parts)</span></span></span><br><span class="line">        @parts = parts</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spares</span></span></span><br><span class="line">        select &#123;<span class="params">|part|</span> part.needs_spare&#125;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Part</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:name</span>, <span class="symbol">:description</span>, <span class="symbol">:needs_spare</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(args)</span></span></span><br><span class="line">        @name = args[<span class="symbol">:name</span>]</span><br><span class="line">        @description = args[<span class="symbol">:description</span>]</span><br><span class="line">        @needs_spare = args.fetch(<span class="symbol">:needs_spare</span>,<span class="literal">true</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">chain = Part.new(<span class="symbol">name:</span> <span class="string">"chain"</span>, <span class="symbol">description:</span> <span class="string">'10-speed'</span>)</span><br><span class="line">road_tire = Part.new(<span class="symbol">name:</span> <span class="string">"tire_size"</span>, <span class="symbol">description:</span> <span class="string">'23'</span>)</span><br><span class="line">tape = Part.new(<span class="symbol">name:</span> <span class="string">"tape_color"</span>, <span class="symbol">description:</span> <span class="string">'red'</span>)</span><br><span class="line">mountain_tire = Part.new(<span class="symbol">name:</span> <span class="string">"tire_size"</span>, <span class="symbol">description:</span> <span class="string">'2.1'</span>)</span><br><span class="line">rear_shock = Part.new(<span class="symbol">name:</span> <span class="string">"rear_shock"</span>, <span class="symbol">description:</span> <span class="string">'Fox'</span>)</span><br><span class="line">front_shock = Part.new(<span class="symbol">name:</span> <span class="string">'front_shock'</span>, <span class="symbol">description:</span> <span class="string">'Manitou'</span>, <span class="symbol">needs_spare:</span> <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">mountain_bike = Bicycle.new(</span><br><span class="line">    <span class="symbol">size:</span> <span class="string">'L'</span>,</span><br><span class="line">    <span class="symbol">parts:</span> Parts.new([chain, mountain_tire, front_shock, rear_shock])</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">road_bike = Bicycle.new(</span><br><span class="line">    <span class="symbol">size:</span> <span class="string">'L'</span>,</span><br><span class="line">    <span class="symbol">parts:</span> Parts.new([chain, road_tire, tape])</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">p mountain_bike.size</span><br><span class="line">p road_bike.size</span><br></pre></td></tr></table></figure></p><h3 id="创建零件工厂"><a href="#创建零件工厂" class="headerlink" title="创建零件工厂"></a>创建零件工厂</h3><p>对象如何创建的知识，最好放在<code>工厂里面</code>。这样你就只需要一个说明书，就能创建对象。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">road_config = [[<span class="string">'chain'</span>,<span class="string">'10-speed'</span>],</span><br><span class="line">               [<span class="string">'tire_size'</span>,<span class="string">'23'</span>],</span><br><span class="line">               [<span class="string">'tape_color'</span>,<span class="string">'red'</span>]</span><br><span class="line">           ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">module</span> <span class="title">PartsFactory</span></span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">self</span>.<span class="title">build</span><span class="params">(config, part_class = Part, parts_class = Parts)</span></span></span><br><span class="line">        parts_class.new(</span><br><span class="line">            config.collect &#123;<span class="params">|part_config|</span></span><br><span class="line">                part_class.new (&#123;</span><br><span class="line">                    <span class="symbol">name:</span> part_config[<span class="number">0</span>],</span><br><span class="line">                    <span class="symbol">description:</span> part_config[<span class="number">1</span>],</span><br><span class="line">                    <span class="symbol">needs_spare:</span> part_config.fetch(<span class="number">2</span>,<span class="literal">true</span>)&#125;)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">road_parts = PartsFactory.build(road_config)</span><br><span class="line">p road_parts.spares</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;What-is-Composition&quot;&gt;&lt;a href=&quot;#What-is-Composition&quot; class=&quot;headerlink&quot; title=&quot;What is Composition?&quot;&gt;&lt;/a&gt;What is Composition?&lt;/h2&gt;&lt;ol
      
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
      <category term="Object Oriented Design" scheme="https://zhangruochi.com/categories/Programming-Language/Object-Oriented-Design/"/>
    
    
  </entry>
  
  <entry>
    <title>Role</title>
    <link href="https://zhangruochi.com/Role/2019/12/01/"/>
    <id>https://zhangruochi.com/Role/2019/12/01/</id>
    <published>2019-12-01T22:18:45.000Z</published>
    <updated>2019-12-02T00:04:00.870Z</updated>
    
    <content type="html"><![CDATA[<h2 id="理解角色"><a href="#理解角色" class="headerlink" title="理解角色"></a>理解角色</h2><p>有些问题需要在其他不相关的对象之间共享行为。这种公共行为对类来说是正交的，他是对象所扮演的角色。当之前无关的对象开始扮演某个公共的角色时，他们便与自己扮演的角色所对应的对象之间建立了一层关系，这些关系与经典继承要求的子类/父类关系有所不同，他们不可见，但又确实存在。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Schedule</span></span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scheduled?</span><span class="params">(schedulable, start_date, end_date)</span></span></span><br><span class="line">        puts <span class="string">"this <span class="subst">#&#123;schedulable.<span class="keyword">class</span>&#125;</span> is not schedulable between <span class="subst">#&#123;start_date&#125;</span> and <span class="subst">#&#123;end_date&#125;</span>"</span></span><br><span class="line">        <span class="literal">false</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">module</span> <span class="title">Schedulable</span></span></span><br><span class="line">    <span class="keyword">attr_writer</span> <span class="symbol">:schedule</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">schedulable?</span><span class="params">(start_date, end_date)</span></span></span><br><span class="line">        !scheduled?(start_date - lead_days, end_date)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scheduled?</span><span class="params">(start_date, end_date)</span></span></span><br><span class="line">        schedule.scheduled?(<span class="keyword">self</span>, start_date, end_date)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lead_days</span></span></span><br><span class="line">        <span class="number">0</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bicycle</span></span></span><br><span class="line">    <span class="keyword">include</span> Schedulable</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lead_days</span></span></span><br><span class="line">        <span class="number">1</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ...... </span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vehicle</span></span></span><br><span class="line">    <span class="keyword">include</span> Schedulable</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lead_days</span></span></span><br><span class="line">        <span class="number">3</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="编写可替代性的代码"><a href="#编写可替代性的代码" class="headerlink" title="编写可替代性的代码"></a>编写可替代性的代码</h2><h3 id="识别出反模式"><a href="#识别出反模式" class="headerlink" title="识别出反模式"></a>识别出反模式</h3><ol><li>使用类似 <code>type</code> 和 <code>category</code> 这类名字的变量来确定发送给<code>self</code>的是何种消息的对象，会包含两个高度相关但又有所不同的对象。</li><li>当某个发送对象要检查接受对象的类以确定所发送的消息时，你一定忽略了某个鸭子类型的存在。</li><li>除了共享接口，鸭子类型也可能共享行为。</li></ol><h3 id="坚持抽象"><a href="#坚持抽象" class="headerlink" title="坚持抽象"></a>坚持抽象</h3><ol><li>抽象父类里的所有代码都应该适用于每一个继承它的类；</li><li>错误的抽象会导致继承对象不正确的行为；</li></ol><h3 id="重视契约"><a href="#重视契约" class="headerlink" title="重视契约"></a>重视契约</h3><p><strong>里氏替换原则</strong>: 对于一个健全的类型系统，其子类必须能够替换他的父类型。</p><h3 id="使用模板方法模式"><a href="#使用模板方法模式" class="headerlink" title="使用模板方法模式"></a>使用模板方法模式</h3><p>用于创建可继承代码的基本编码技术是模板方法模式。这种模式可以让你将抽象与具体分离开来。抽象代码用于定义算法，具体代码可以继承这个抽象，并通过改写这些模板方法来提供特殊化。</p><h3 id="预先将类解耦"><a href="#预先将类解耦" class="headerlink" title="预先将类解耦"></a>预先将类解耦</h3><p>尽量避免编写继承者需要发送super消息的代码。可以通过钩子消息让子类参与进来，同时还可免除它们要知道抽象算法的职责。</p><h3 id="创建浅层结构"><a href="#创建浅层结构" class="headerlink" title="创建浅层结构"></a>创建浅层结构</h3><p>钩子方法的局限性在于它仅适用于创建浅层结构。深层次结构的问题在于，他们定义了一条很长的用于消息查找的搜索路径，并且为对象提供了大量的机会，让他们在那条路径上可以随着消息的传递添加行为。因为对象依赖于它之上的所有事物，所以深层次的结构拥有一个很大的内奸依赖关系集合。深层次结构的另外一个问题在于程序员往往只是对处于顶端和底部的类比较熟悉。</p>]]></content>
    
    <summary type="html">
    
      Using module to share behaviors of objects
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
      <category term="Object Oriented Design" scheme="https://zhangruochi.com/categories/Programming-Language/Object-Oriented-Design/"/>
    
    
  </entry>
  
  <entry>
    <title>Inheritance</title>
    <link href="https://zhangruochi.com/Inheritance/2019/12/01/"/>
    <id>https://zhangruochi.com/Inheritance/2019/12/01/</id>
    <published>2019-12-01T20:59:25.000Z</published>
    <updated>2019-12-01T21:45:22.203Z</updated>
    
    <content type="html"><![CDATA[<h2 id="What-is-inheritance"><a href="#What-is-inheritance" class="headerlink" title="What is inheritance ?"></a>What is inheritance ?</h2><ol><li>继承的核心是 <code>实现消息的自动委托</code>。</li></ol><h2 id="重构"><a href="#重构" class="headerlink" title="重构"></a>重构</h2><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bicycle</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:style</span>,<span class="symbol">:size</span>,<span class="symbol">:tape_color</span>,<span class="symbol">:front_shock</span>,<span class="symbol">:rear_shock</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(args)</span></span></span><br><span class="line">        @style = args[<span class="symbol">:style</span>]</span><br><span class="line">        @size = args[<span class="symbol">:size</span>]</span><br><span class="line">        @tape_color = args[<span class="symbol">:tape_color</span>]</span><br><span class="line">        @front_shock = args[<span class="symbol">:front_shock</span>]</span><br><span class="line">        @rear_shock = args[<span class="symbol">:rear_shock</span>]</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spares</span></span></span><br><span class="line">        <span class="keyword">if</span> style == <span class="symbol">:read</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="symbol">chain:</span> <span class="string">'10-speed'</span>,</span><br><span class="line">                <span class="symbol">tire_size:</span> <span class="string">'23'</span>, </span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="symbol">chain:</span> <span class="string">'10-speed'</span>,</span><br><span class="line">                <span class="symbol">tire_size:</span> <span class="string">'2.1'</span>,</span><br><span class="line">                <span class="symbol">rear_shock:</span> rear_shock</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><ol><li><strong>找出抽象</strong>：子类是其父类的特殊化。子类应该包含父类的一切内容，外加更多内容。任何期待父类的对象应该都能够与其子类进行交互。</li><li><p><strong>创建抽象父类</strong>先将所有的代码下放到子类然后逐渐地、部分地提升是重构操作的重要组成部分。因为一开始只将部分行为下放到子类，然后试图将某个现有的类从具体转换为抽象，那么有可能会不小心残留下具体的行为。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bicycle</span></span></span><br><span class="line">    <span class="comment"># 这个类为空</span></span><br><span class="line">    <span class="comment"># 所有的代码都移动到了 RoadBike</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RoadBike</span> &lt; Bicycle</span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:style</span>,<span class="symbol">:size</span>,<span class="symbol">:tape_color</span>,<span class="symbol">:front_shock</span>,<span class="symbol">:rear_shock</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(args)</span></span></span><br><span class="line">        @style = args[<span class="symbol">:style</span>]</span><br><span class="line">        @size = args[<span class="symbol">:size</span>]</span><br><span class="line">        @tape_color = args[<span class="symbol">:tape_color</span>]</span><br><span class="line">        @front_shock = args[<span class="symbol">:front_shock</span>]</span><br><span class="line">        @rear_shock = args[<span class="symbol">:rear_shock</span>]</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spares</span></span></span><br><span class="line">        <span class="keyword">if</span> style == <span class="symbol">:read</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="symbol">chain:</span> <span class="string">'10-speed'</span>,</span><br><span class="line">                <span class="symbol">tire_size:</span> <span class="string">'23'</span>, </span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="symbol">chain:</span> <span class="string">'10-speed'</span>,</span><br><span class="line">                <span class="symbol">tire_size:</span> <span class="string">'2.1'</span>,</span><br><span class="line">                <span class="symbol">rear_shock:</span> rear_shock</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mountain</span> &lt; Bicycle</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p>提升抽象行为</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bicycle</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:size</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(args = &#123;&#125;)</span></span></span><br><span class="line">        @size = args[<span class="symbol">:size</span>]</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RoadBike</span> &lt; Bicycle</span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:tape_color</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(args)</span></span></span><br><span class="line">        @tape_color = args[<span class="symbol">:tape_color</span>]</span><br><span class="line">        <span class="keyword">super</span>(args)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mountain</span> &lt; Bicycle</span></span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p>使用模板方法模式<br>在负类定义好几本结构，并通过发送消息来获取子类特有的实现，这种方法称为<code>模板方法模式</code>。使用模板方法模式的类都必须给它所发送的每一条消息提供一个实现。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bicycle</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:size</span>,<span class="symbol">:chain</span>,<span class="symbol">:tire_size</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(args = &#123;&#125;)</span></span></span><br><span class="line">        @size = args[<span class="symbol">:size</span>]</span><br><span class="line">        @chain = args[<span class="symbol">:chain</span>] <span class="params">||</span> default_chain</span><br><span class="line">        @tire_size = args[<span class="symbol">:tire_size</span>] <span class="params">||</span> default_tire_size</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">default_chain</span></span></span><br><span class="line">        <span class="string">'10-speed'</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">default_tire_size</span></span></span><br><span class="line">        raise NotImplememtedError</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RoadBike</span> &lt; Bicycle</span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:tape_color</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(args)</span></span></span><br><span class="line">        @tape_color = args[<span class="symbol">:tape_color</span>]</span><br><span class="line">        <span class="keyword">super</span>(args)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">default_chain</span></span></span><br><span class="line">        <span class="string">'9-speed'</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mountain</span> &lt; Bicycle</span></span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p>管理父类和子类之间的耦合<br>一如既往，当知道其他类相关的事情，变回创建依赖关系。发送<code>super</code>消息的子类，不仅需要知道自己在干什么，还需要知道自己与父类是如何交互的。可以使用<strong>钩子消息</strong>解耦子类。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bicycle</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:size</span>,<span class="symbol">:chain</span>,<span class="symbol">:tire_size</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(args = &#123;&#125;)</span></span></span><br><span class="line">        @size = args[<span class="symbol">:size</span>]</span><br><span class="line">        @chain = args[<span class="symbol">:chain</span>] <span class="params">||</span> default_chain</span><br><span class="line">        @tire_size = args[<span class="symbol">:tire_size</span>] <span class="params">||</span> default_tire_size</span><br><span class="line"></span><br><span class="line">        post_initialize(args)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spares</span></span></span><br><span class="line">        &#123;<span class="symbol">tire_size:</span> tire_size,</span><br><span class="line">            <span class="symbol">chain:</span> chain&#125;.mege(local_spares)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">local_spares</span></span></span><br><span class="line">        &#123;&#125;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">post_initialize</span><span class="params">(args)</span></span></span><br><span class="line">        <span class="literal">nil</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">default_chain</span></span></span><br><span class="line">        <span class="string">'10-speed'</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">default_tire_size</span></span></span><br><span class="line">        raise NotImplememtedError</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RoadBike</span> &lt; Bicycle</span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:tape_color</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">post_initialize</span><span class="params">(args)</span></span></span><br><span class="line">        @tape_color = args[<span class="symbol">:tape_color</span>]</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">local_spares</span></span></span><br><span class="line">        &#123;<span class="symbol">tape_color:</span> tape_color&#125;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">default_tire_size</span></span></span><br><span class="line">        <span class="string">'23'</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mountain</span> &lt; Bicycle</span></span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      Inheritance and Reconstruction
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
      <category term="Object Oriented Design" scheme="https://zhangruochi.com/categories/Programming-Language/Object-Oriented-Design/"/>
    
    
  </entry>
  
  <entry>
    <title>Interface Segmentation Principle</title>
    <link href="https://zhangruochi.com/Interface-Segmentation-Principle/2019/12/01/"/>
    <id>https://zhangruochi.com/Interface-Segmentation-Principle/2019/12/01/</id>
    <published>2019-12-01T15:32:56.000Z</published>
    <updated>2019-12-01T17:38:20.104Z</updated>
    
    <content type="html"><![CDATA[<h2 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h2><p>接口有很多不同的概念，在这里，这个术语指的是类里的接口。类实现了许多方法，有些方法旨在被其他对象使用，这些方法就组成了它的公共接口。那些组成类的公共接口的方法组成了这个类呈现给外部世界的全貌。他们：</p><ol><li>暴露了其主要的职责</li><li>期望被其它对象调用</li><li>不会因一时兴起而改变</li><li>对其他依赖它的对象来说很安全</li><li>在测试里被详细记录</li></ol><h2 id="领域对象"><a href="#领域对象" class="headerlink" title="领域对象"></a>领域对象</h2><p>Domain Object 显而易见，因为他们代表了这个应用程序，代表了现实世界里的很大的、易于发现的事物。应该要注意的是，Domain Object 往往是给粗心大意设立的陷进。因为如果过度关注他们，就会倾向于给他们强加上行为。<strong>我们应该重点关注的是他们之间的消息传递</strong>，这些消息会引导你去发现其他的对象，而这些对象可远没有这么明显。</p><h2 id="时序图"><a href="#时序图" class="headerlink" title="时序图"></a>时序图</h2><p>时序图的价值在于，他们明确制定了对象之间消息的传递。因为对象间只应使用公共接口进行通信，时序图便是一种用于暴露、实验并最终定义这些接口的工具。</p><h2 id="请询问“要什么”，别告知“如何做”"><a href="#请询问“要什么”，别告知“如何做”" class="headerlink" title="请询问“要什么”，别告知“如何做”"></a>请询问“要什么”，别告知“如何做”</h2><p>如下图， <code>Trip</code> 告知 <code>Mechanic</code> 如何去准备每一辆 bicycle.<br><img src="1.jpg" alt></p><p>下图里，<code>Trip</code> 要求 <code>Mechanic</code> 去准备每一个 bicycle</p><p><img src="2.jpg" alt></p><p>消息的这种变化是代码可维护性的一个巨大改进。因为它大大缩短了公共接口，大大减少出现“违背其承诺，然后强迫许多类进行更改”的情况。</p><h2 id="最小化上下文（context）"><a href="#最小化上下文（context）" class="headerlink" title="最小化上下文（context）"></a>最小化上下文（context）</h2><p>对象所了解到的关于其他对象的那些事情便构成了它的上下文。对象所期待的上下文会直接影响到它的重用难度。具有简单上下文的对象易于使用，也易于测试，他们对周边的环境期望很少。最好的情况是对象与他的上下文完全独立。如果某个对象在与其他对象进行合作时，不知道他们是谁，也不知道他们所做的事情，那么这个对象便可以按各种千奇百怪和完全无法预测的方式重用。</p><h2 id="信任其他对象"><a href="#信任其他对象" class="headerlink" title="信任其他对象"></a>信任其他对象</h2><p><code>我知道我需要什么，并且我相信你会做好你的本职工作。</code> 这种盲目的信任是面向对象设计的基石。在允许对象进行合作的同时，它无需将自己束缚在上下文里，并且在任何期望壮大和变化的应用程序里它都是必不可少的。</p><h2 id="创建显式接口"><a href="#创建显式接口" class="headerlink" title="创建显式接口"></a>创建显式接口</h2><ol><li>被明确标识；</li><li>多与 <code>做什么</code> 有关，少于 <code>怎么做</code> 有关；</li><li>尽可能让这些名字都稳定不变；</li><li>将散列表作为参数</li></ol><h2 id="迪米特法则"><a href="#迪米特法则" class="headerlink" title="迪米特法则"></a>迪米特法则</h2><p>迪米特法则会限制可以向某个方法发送消息的对象集合。它会禁止这样的做法：将某条消息通过第二个不同类型的对象转发给第三个对象。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不合理</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trip</span></span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">depart</span></span></span><br><span class="line">        customer.bicycle.wheel.tire</span><br><span class="line">        customer.bicycle.wheel.rotate</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 合理</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trip</span></span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">depart</span></span></span><br><span class="line">        customer.go</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="鸭子类型"><a href="#鸭子类型" class="headerlink" title="鸭子类型"></a>鸭子类型</h2><p><code>Ducking Type</code> 指的是不会绑定到任何特定类的公共接口。这种跨类的接口能为应用程序带来巨大的灵活性，所采用的方式是利用更加宽容的消息依赖取代昂贵的类依赖。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 不好的写法</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trip</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:bicycles</span>, <span class="symbol">:customers</span>, <span class="symbol">:vehicle</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare</span><span class="params">(preparers)</span></span></span><br><span class="line">        preparers.each &#123;<span class="params">|preparer|</span></span><br><span class="line">            <span class="keyword">case</span> preparer</span><br><span class="line">            <span class="keyword">when</span> Mechanic</span><br><span class="line">                preparer.prepare_bicycles(bicycles)</span><br><span class="line">            <span class="keyword">when</span> TripCoordinator</span><br><span class="line">                preparer.buy_food(customers)</span><br><span class="line">            <span class="keyword">when</span> Driver</span><br><span class="line">                preparer.gas_up(vehicle)</span><br><span class="line">                preparer.fill_water_tank(vehicle)</span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>删除依赖关系的关键是要意识到：<code>Trip</code>和<code>Prepare</code>方法只服务于单个目的，因此它的参数出现在这里是希望可以协作完成同一个目标。每一个参数都因同样的理由出现在这里，具体的原因与这些参数的底层类无关。<br>设计鸭子类型的挑战是：<strong>要注意到你需要一个鸭子类型，并且要将其接口抽象出来</strong></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 好的写法</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trip</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:bicycles</span>, <span class="symbol">:customers</span>, <span class="symbol">:vehicle</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare</span><span class="params">(preparers)</span></span></span><br><span class="line">        preparers.each &#123;<span class="params">|preparer|</span> preparer.prepare_trip(<span class="keyword">self</span>)&#125;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mechanic</span></span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_trip</span><span class="params">(trip)</span></span></span><br><span class="line">        trip.bicycles.each &#123;<span class="params">|bicycle|</span> prepare_bicycles(bicycle)&#125;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      How to create flexible interface and how to use ducking type
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
      <category term="Object Oriented Design" scheme="https://zhangruochi.com/categories/Programming-Language/Object-Oriented-Design/"/>
    
    
  </entry>
  
  <entry>
    <title>Dependency Management Principle</title>
    <link href="https://zhangruochi.com/Dependency-Management-Principle/2019/11/28/"/>
    <id>https://zhangruochi.com/Dependency-Management-Principle/2019/11/28/</id>
    <published>2019-11-28T19:49:09.000Z</published>
    <updated>2019-11-28T20:50:56.402Z</updated>
    
    <content type="html"><![CDATA[<p>通过精心设计的对象都具有单一的职责，因此他们实际上是通过<strong>合作</strong>来完成复杂的任务。这种合作强大而危险。为了实现一个合作，一个对象必须知道其他对象的某些情况。这种<strong>知道</strong>便创建了一种依赖关系。当两三个对象耦合在一起，它们变回表现得像一个整体，不可能只重用其中的一个。如果对依赖关系不仔细加以管理，那么这些依赖关系将毁掉整个应用程序。</p><p><img src="dependency.jpg" alt></p><h2 id="理解依赖关系"><a href="#理解依赖关系" class="headerlink" title="理解依赖关系"></a>理解依赖关系</h2><ol><li>知道另外一个类的名字<blockquote><p><code>Gear</code> 对 <code>Wheel</code> 的引用深入到 <code>gear_inches</code> 方法里，并将其硬编码，那么这便是明确说明它只愿意为 <code>Wheel.gear</code> 的实例计算齿轮英寸数，从而拒绝与其他任何类型的对象合作。</p></blockquote></li><li>消息的名字<blockquote><p><code>Gear</code> 需要访问可以相应 <code>diameter</code> 的对象。实际上，对 <code>Gear</code> 来说，计算 <code>gear_inches</code> 并不需要知道这个对象的类，只要这个对象可以相应 <code>diameter</code> 就行了。我们可以把这个对象理解成鸭子类型。</p></blockquote></li><li>消息所要求的参数<blockquote><p><code>Gear</code> 也并不需要知道 <code>Wheel</code> 需要使用 <code>rim</code> 和 <code>tire</code> 进行初始化</p></blockquote></li><li>参数的顺序<blockquote><p><code>Gear</code> 不需要知道 <code>Wheel</code> 初始化时参数的顺序</p></blockquote></li></ol><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Gear</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:chainring</span>, <span class="symbol">:cog</span>, <span class="symbol">:rim</span>, <span class="symbol">:tire</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(chainring,cog,rim,tire)</span></span></span><br><span class="line">        @chainring = chainring</span><br><span class="line">        @cog = cog</span><br><span class="line">        @rim = rim</span><br><span class="line">        @tire = tire</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ratio</span></span></span><br><span class="line">        chainring / cog.to_f   </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gear_inches</span></span></span><br><span class="line">        ratio * Wheel.new(rim,tire).diameter   <span class="comment"># &lt;---- 知道类的名字，知道消息的名字，知道参数，知道参数的顺序</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Wheel</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:rim</span>,<span class="symbol">:tire</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(rim,tire)</span></span></span><br><span class="line">        @rim = rim</span><br><span class="line">        @tire = tire</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">diameter</span></span></span><br><span class="line">        rim + (tire * <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">circumference</span></span></span><br><span class="line">        diameter * Math::PI</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="隔离依赖关系"><a href="#隔离依赖关系" class="headerlink" title="隔离依赖关系"></a>隔离依赖关系</h2><h3 id="隔离实例的创建"><a href="#隔离实例的创建" class="headerlink" title="隔离实例的创建"></a>隔离实例的创建</h3><p>虽然此时 <code>Gear</code> 类仍然知道得太多，但是已经有所改进。如下的编码风格减少了 <code>gear_inches</code> 的依赖关系数量，同时公开暴露了 <code>Gear</code> 对 <code>Wheel</code> 的依赖。它们没有将依赖关系<strong>隐藏</strong>起来，而是将它们<strong>显露</strong>出来。这样降低了重用这段代码的门槛。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Gear</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:chainring</span>, <span class="symbol">:cog</span>, <span class="symbol">:rim</span>, <span class="symbol">:tire</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(chainring,cog,rim,tire)</span></span></span><br><span class="line">        @chainring = chainring</span><br><span class="line">        @cog = cog</span><br><span class="line">        @rim = rim</span><br><span class="line">        @tire = tire</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ratio</span></span></span><br><span class="line">        chainring / cog.to_f   </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gear_inches</span></span></span><br><span class="line">        ratio * wheel.diameter</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wheel</span>        <span class="comment">## &lt;--- 隔离实例的创建</span></span></span><br><span class="line">        @wheel <span class="params">||</span>= Wheel.new(rim,tire)   </span><br><span class="line">    <span class="keyword">end</span> </span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="隔离外部消息"><a href="#隔离外部消息" class="headerlink" title="隔离外部消息"></a>隔离外部消息</h3><p>如下的编码方式，<code>wheel.diameter</code>被深度嵌套在 <code>gear_inches</code> 方法里。这个方法依赖 <code>Gear</code> 才能相应 Wheel，并且依赖<code>Wheel</code>才能响应diameter。完全没有必要在<code>gear_inches</code>里嵌入这种外部依赖，这使得 <code>Gear</code> 更加脆弱。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gear_inches</span></span></span><br><span class="line">    <span class="comment"># .... 假设存在额外的数学运算</span></span><br><span class="line">    ratio * wheel.diameter</span><br><span class="line">    <span class="comment"># .... 假设还存在额外的数学运算</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>现在我们将<code>wheel.gear</code>隔离在一个单独的方法里，并且 <code>gear_inches</code>可以依赖于某条发送给自己的消息。如果 <code>Wheel</code> 更改了<code>diameter</code>的名字或者签名，那么对<code>Gear</code>的副作用将会限定在一个简单的包裹方法里。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gear_inches</span></span></span><br><span class="line">    <span class="comment"># .... 假设存在额外的数学运算</span></span><br><span class="line">    ratio * diameter</span><br><span class="line">    <span class="comment"># .... 假设还存在额外的数学运算</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">diameter</span></span></span><br><span class="line">    wheel.diameter</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="移除参数顺序依赖关系"><a href="#移除参数顺序依赖关系" class="headerlink" title="移除参数顺序依赖关系"></a>移除参数顺序依赖关系</h3><ol><li><p>使用散列表来进行初始化同时显示地指定默认值</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Gear</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:chainring</span>, <span class="symbol">:cog</span>, <span class="symbol">:wheel</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(args)</span></span></span><br><span class="line">        <span class="comment"># args = defaults.merge(args)</span></span><br><span class="line">        @chainring = args.fetch(<span class="symbol">:chainring</span>,<span class="number">40</span>)</span><br><span class="line">        @cog = args.fetch(<span class="symbol">:cog</span>,<span class="number">18</span>)</span><br><span class="line">        @wheel = args[<span class="symbol">:wheel</span>]</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># def defaults</span></span><br><span class="line">    <span class="comment">#     &#123;:chainring =&gt; 40, :cog =&gt; 18&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># .......</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p>隔离多参数初始化操作<br>有时候我们会被迫依赖某个要求参数初始化顺序的固定方法，由于那里不属于我们的地盘，我们无法更改这个方法。此时可以创建一个单一的方法，将外部接口包裹起来。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">module</span> <span class="title">SomeFramework</span></span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Gear</span></span></span><br><span class="line">        <span class="keyword">attr_reader</span> <span class="symbol">:chainring</span>, <span class="symbol">:cog</span>, <span class="symbol">:wheel</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(chainring,cog,wheel)</span></span></span><br><span class="line">            @chainring = chainring</span><br><span class="line">            @cog = cog</span><br><span class="line">            @wheel = wheel</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">ratio</span></span></span><br><span class="line">            chainring / cog.to_f   </span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">gear_inches</span></span></span><br><span class="line">            ratio * wheel.diameter</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Wheel</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:rim</span>,<span class="symbol">:tire</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(rim,tire)</span></span></span><br><span class="line">        @rim = rim</span><br><span class="line">        @tire = tire</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">diameter</span></span></span><br><span class="line">        rim + (tire * <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">circumference</span></span></span><br><span class="line">        diameter * Math::PI</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">module</span> <span class="title">GearWrapper</span></span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">self</span>.<span class="title">gear</span><span class="params">(args)</span></span></span><br><span class="line">        SomeFramework::Gear.new(args[<span class="symbol">:chainring</span>],args[<span class="symbol">:cog</span>],args[<span class="symbol">:wheel</span>])</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">p GearWrapper.gear(<span class="symbol">:chainring</span> =&gt; <span class="number">52</span>, <span class="symbol">:cog</span> =&gt; <span class="number">11</span>, <span class="symbol">:wheel</span> =&gt; Wheel.new(<span class="number">16</span>,<span class="number">1.5</span>)).gear_inches</span><br></pre></td></tr></table></figure></li><li><p>反转依赖关系</p></li></ol><ul><li>有些类比其他类更容易管理</li><li>具体类比抽象类更容易发生变化</li><li>更改拥有许多关系的类会造成广泛的变化<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Gear</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:chainring</span>, <span class="symbol">:cog</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(chainring,cog)</span></span></span><br><span class="line">        @chainring = chainring</span><br><span class="line">        @cog = cog</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ratio</span></span></span><br><span class="line">        chainring / cog.to_f   </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gear_inches</span><span class="params">(diameter)</span></span></span><br><span class="line">        ratio * diameter</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Wheel</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:rim</span>,<span class="symbol">:tire</span>, <span class="symbol">:gear</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(rim, tire, chainring, cog)</span></span></span><br><span class="line">        @rim = rim</span><br><span class="line">        @tire = tire</span><br><span class="line">        @gear = Gear.new(chainring, cog)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">diameter</span></span></span><br><span class="line">        rim + (tire * <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gear_inches</span></span></span><br><span class="line">        gear.gear_inches diameter</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">p Wheel.new(<span class="number">16</span>,<span class="number">1.5</span>, <span class="number">52</span>,<span class="number">11</span>).gear_inches</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      怎样管理程序中的依赖关系
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
      <category term="Object Oriented Design" scheme="https://zhangruochi.com/categories/Programming-Language/Object-Oriented-Design/"/>
    
    
  </entry>
  
  <entry>
    <title>Single Responsibility Principle</title>
    <link href="https://zhangruochi.com/Single-Responsibility-Principle/2019/11/27/"/>
    <id>https://zhangruochi.com/Single-Responsibility-Principle/2019/11/27/</id>
    <published>2019-11-27T21:50:20.000Z</published>
    <updated>2019-11-27T21:51:05.069Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center">设计是保留可变性的艺术，而非达到完美性的行为</blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Gear</span></span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(chainring, cog, rim, tire)</span></span></span><br><span class="line">        @chainring = chainring</span><br><span class="line">        @cog = cog</span><br><span class="line">        @rim = rim</span><br><span class="line">        @tire = tire</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ratio</span></span></span><br><span class="line">        @chainring / @cog.to_f      <span class="comment"># &lt;------- 依赖数据</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gear_inches</span> </span></span><br><span class="line">        @ratio * (@rim + (@tire * <span class="number">2</span>))   <span class="comment"># &lt;------- 不符合单一职责，依赖数据</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">puts Gear.new(<span class="number">52</span>,<span class="number">11</span>,<span class="number">26</span>,<span class="number">1.5</span>).gear_inches</span><br></pre></td></tr></table></figure><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ObscuringReference</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:data</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(data)</span></span></span><br><span class="line">        @data = data</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">diameters</span></span></span><br><span class="line">        data.collect &#123;<span class="params">|cell|</span></span><br><span class="line">            cell[<span class="number">0</span>] + (cell[<span class="number">1</span>] * <span class="number">2</span>)   <span class="comment"># &lt;---- 依赖数据结构</span></span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="如何确定单一职责"><a href="#如何确定单一职责" class="headerlink" title="如何确定单一职责"></a>如何确定单一职责</h2><ol><li>假设它存在意识，然后质询它<ul><li>齿轮先生，请问你的比率是多少？ ## 靠谱的询问</li><li>齿轮先生，请问你的轮胎尺寸是多少？## 荒唐可笑的询问</li></ul></li><li>尝试用一句话来描述类，如果出现 and 或者 or，则说明不遵循单一职责原则</li><li>高内聚(cohesion)<br>OO的设计者使用内聚来描述某个类的所有内容都与其<strong>中心目标</strong>相关联的情况。<strong>一个类有责任设计其目标。</strong></li></ol><h2 id="依赖行为，不要依赖数据"><a href="#依赖行为，不要依赖数据" class="headerlink" title="依赖行为，不要依赖数据"></a>依赖行为，不要依赖数据</h2><ol><li>实例变量被引用多次，如果需要进行调整，则需要大量重改。</li><li>将数据处理成行为，对行为的一次更改，可以作用在被引用的所有数据上。</li><li>隐藏数据结构<br>方法 diameters 不仅知道如何计算直径，他还知道在哪里找到钢圈(cell[0])和轮胎(cell[1]).</li></ol><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ol><li><p>将数据结构的知识，封装在单一方法内部。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Wheel = Struct.new(<span class="symbol">:rim</span>,<span class="symbol">:tire</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wheelify</span><span class="params">(data)</span></span></span><br><span class="line">    data.collect &#123;</span><br><span class="line">        <span class="params">|cell|</span> Wheel.new(cell[<span class="number">0</span>],cell[<span class="number">1</span>])</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">diameters</span></span></span><br><span class="line">    wheels.collect &#123;<span class="params">|wheel|</span> wheel.rim * (wheel.tire * <span class="number">2</span>)&#125;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p>将额外的责任从方法里提取出来</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">diameters</span></span></span><br><span class="line">    wheels.collect &#123;<span class="params">|wheel|</span> diameter wheel&#125;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">diameter</span><span class="params">(wheel)</span></span></span><br><span class="line">    wheel.rim + (wheel.tire * <span class="number">2</span>)</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gear_inches</span></span></span><br><span class="line">    ration * diameter</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p>将类里的职责隔离<br>暂时缺乏足够的信息证明Wheel需要一个独立的类。有的时候，我们是否需要创建一个新的对象并不是有明确界限的。<strong>隔离起来总是不错的选择</strong>。同时要注意，不要讲无关的职责混入自己的类。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Gear</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:chainring</span>, <span class="symbol">:cog</span>, <span class="symbol">:wheel</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(chainring,cog,rim,tire)</span></span></span><br><span class="line">        @chainring = chainring</span><br><span class="line">        @cog = cog</span><br><span class="line">        @wheel = Wheel.new(rim,tire)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ratio</span></span></span><br><span class="line">        chainring / cog.to_f   </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gear_inches</span></span></span><br><span class="line">        ratio * wheel.diameter</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    Wheel = Struct.new(<span class="symbol">:rim</span>,<span class="symbol">:tire</span>) <span class="keyword">do</span>  <span class="comment"># &lt;----- 虽然Wheel存在于Gear里不算很好的选择，但是隔离Wheel的职责是不错的选择</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">diameter</span></span></span><br><span class="line">            rim + (tire * <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">puts Gear.new(<span class="number">52</span>,<span class="number">11</span>,<span class="number">26</span>,<span class="number">1.5</span>).gear_inches</span><br></pre></td></tr></table></figure></li></ol><h2 id="需求的更改"><a href="#需求的更改" class="headerlink" title="需求的更改"></a>需求的更改</h2><p>如果此时，我们需要计算轮子的周长。<strong>这正是我们一直等待的信息，因为它提供了做下一步设计决定时所需要的信息。</strong> 注意敏捷开发里设计和代码迭代互相交互的原则。</p><p>现在我们有理由将Wheel独立成一个类了。因为我们将Wheel的职责隔离过，此时独立出它是很容易的。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Gear</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:chainring</span>, <span class="symbol">:cog</span>, <span class="symbol">:wheel</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(chainring,cog,wheel = <span class="literal">nil</span>)</span></span></span><br><span class="line">        @chainring = chainring</span><br><span class="line">        @cog = cog</span><br><span class="line">        @wheel = wheel</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ratio</span></span></span><br><span class="line">        chainring / cog.to_f   </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gear_inches</span></span></span><br><span class="line">        ratio * wheel.diameter</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Wheel</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:rim</span>,<span class="symbol">:tire</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(rim,tire)</span></span></span><br><span class="line">        @rim = rim</span><br><span class="line">        @tire = tire</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">diameter</span></span></span><br><span class="line">        rim + (tire * <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">circumference</span></span></span><br><span class="line">        diameter * Math::PI</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">@wheel = Wheel.new(<span class="number">12</span>,<span class="number">1.5</span>)</span><br><span class="line">puts @wheel.circumference</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">puts Gear.new(<span class="number">52</span>,<span class="number">11</span>,@wheel).gear_inches</span><br><span class="line">puts Gear.new(<span class="number">52</span>,<span class="number">11</span>).ratio</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      单一职责原则
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
      <category term="Object Oriented Design" scheme="https://zhangruochi.com/categories/Programming-Language/Object-Oriented-Design/"/>
    
    
  </entry>
  
  <entry>
    <title>Object Oriented Design</title>
    <link href="https://zhangruochi.com/Object-Oriented-Design/2019/11/27/"/>
    <id>https://zhangruochi.com/Object-Oriented-Design/2019/11/27/</id>
    <published>2019-11-27T06:32:23.000Z</published>
    <updated>2019-12-02T02:40:23.544Z</updated>
    
    <content type="html"><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>应用程序面对的最大的问题是将如何应对变化。将代码有效编排应对变化是设计的事情。最常见的设计要素是原则和模式。</p><h2 id="什么是面向对象设计"><a href="#什么是面向对象设计" class="headerlink" title="什么是面向对象设计"></a>什么是面向对象设计</h2><p>面向对象设计(OOD)认为世界是由多个对象以及对象之间的一系列消息传递构成的。</p><h2 id="设计赞歌"><a href="#设计赞歌" class="headerlink" title="设计赞歌"></a>设计赞歌</h2><h3 id="设计要解决的问题"><a href="#设计要解决的问题" class="headerlink" title="设计要解决的问题"></a>设计要解决的问题</h3><p>事情总是会发生变化，这是<strong>永恒不变</strong>的。客户并不知道他们自己想要什么，他们也说不清楚自己的意图。变化的需求是编程中的摩擦力与重力，这些力会作用到周密的设计上，从而形成出人意料的压力，这是这种意外的变化使得设计变得如此重要。因此，<strong>设计为了解决意外变化的问题</strong>。</p><h3 id="为何程序难以更改"><a href="#为何程序难以更改" class="headerlink" title="为何程序难以更改"></a>为何程序难以更改</h3><p>面向对象的应用程序由交互产生整体行为，对象是产生交互的零件，而交互则体现在他们之间传递的消息里。想要获得发送给正确目标的正确消息，需要消息的发送者对接受者有所了解。这一点会在两者之间创建许多依赖于联系，并且这些联系还在不断变化中。<br>面向对象设计与依赖关系管理相关，它是一套对依赖关系进行编排，以便各个对象能够容忍更改的编码技术。在缺乏设计的情形里，非托管的依赖关系很容易造成严重的破坏，因为这些对象互相之间了解太多，更改其中之一就会强制合作者也要随之发生变化。</p><h3 id="设计的定义"><a href="#设计的定义" class="headerlink" title="设计的定义"></a>设计的定义</h3><p>设计是一门艺术，一门编排代码的艺术。设计的工作具有高度的复杂性：需要对应用程序的需求有总体的理解，与各种设计原则的利与弊的知识组合起来，然后设计出当前算是成本最低，而在将来也能继续保持那个样子的代码编排。</p><h2 id="设计工具"><a href="#设计工具" class="headerlink" title="设计工具"></a>设计工具</h2><h3 id="设计原则"><a href="#设计原则" class="headerlink" title="设计原则"></a>设计原则</h3><ol><li>单一职责</li><li>开闭原则</li><li>里氏替换原则</li><li>接口隔离原则</li><li>依赖倒置原则</li><li>不重复原则</li></ol><p>优秀的设计原则代表的是可测量的真理，遵循它们能改善你的代码。</p><h3 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h3><p>设计模式是在面向对象的软件设计过程中，<strong>针对特定问题的简单而优雅的解决方案</strong>。他是每位设计师的工具箱里的工具。</p><h2 id="设计行为"><a href="#设计行为" class="headerlink" title="设计行为"></a>设计行为</h2><h3 id="设计失败"><a href="#设计失败" class="headerlink" title="设计失败"></a>设计失败</h3><ol><li>缺乏设计知识</li><li>过度设计</li><li>设计与编程分开 </li></ol><h3 id="设计时机"><a href="#设计时机" class="headerlink" title="设计时机"></a>设计时机</h3><p>敏捷开发方法相信，大规模预先设计是完全没有意义的，因为它不可能正确。也没有人知道什么时候应用程序会完成。用户在看到具体的软件之前，对所想要的软件是没有概念的，所以向他们展示软件的时机宜早不宜迟。基于这个逻辑，我们的程序应该以<strong>微增量</strong>的方式来构建软件，逐步将你的方法迭代成满足客户真正需求的应用程序。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;综述&quot;&gt;&lt;a href=&quot;#综述&quot; class=&quot;headerlink&quot; title=&quot;综述&quot;&gt;&lt;/a&gt;综述&lt;/h2&gt;&lt;p&gt;应用程序面对的最大的问题是将如何应对变化。将代码有效编排应对变化是设计的事情。最常见的设计要素是原则和模式。&lt;/p&gt;
&lt;h2 id=&quot;什么是
      
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
      <category term="Object Oriented Design" scheme="https://zhangruochi.com/categories/Programming-Language/Object-Oriented-Design/"/>
    
    
  </entry>
  
  <entry>
    <title>Basic Calculator</title>
    <link href="https://zhangruochi.com/Basic-Calculator/2019/11/26/"/>
    <id>https://zhangruochi.com/Basic-Calculator/2019/11/26/</id>
    <published>2019-11-26T21:56:37.000Z</published>
    <updated>2019-11-26T22:12:47.372Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Leetcode-224-Basic-Calculator"><a href="#Leetcode-224-Basic-Calculator" class="headerlink" title="Leetcode 224. Basic Calculator"></a>Leetcode 224. <a href="https://leetcode.com/problems/basic-calculator/" target="_blank" rel="noopener">Basic Calculator</a></h2><p><img src="tree.png" alt></p><h3 id="中缀表达式"><a href="#中缀表达式" class="headerlink" title="中缀表达式"></a>中缀表达式</h3><p>中缀表达式是一种通用的算术或逻辑公式表示方法，操作符以中缀形式处于操作数的中间。中缀表达式是人们常用的算术表示方法，它是由相应的语法树的中序遍历的结果得到的。</p><blockquote><p>A + B * ( C - D ) - E * F</p></blockquote><h3 id="后缀表达式"><a href="#后缀表达式" class="headerlink" title="后缀表达式"></a>后缀表达式</h3><p>首先要知道什么是后缀表达式，后缀表达式又叫做逆波兰式吗它是由相应的语法树的后序遍历的结果得到的。如下图的后缀表达式为： </p><blockquote><p>A B C D - * + E F * -</p></blockquote><h3 id="中缀表达式转化为后缀表达式"><a href="#中缀表达式转化为后缀表达式" class="headerlink" title="中缀表达式转化为后缀表达式"></a>中缀表达式转化为后缀表达式</h3><ol><li>从左到右遍历中缀表达式的每个数字和符号;</li><li>若是数字就输出，即成为后缀表达式的一部分;</li><li>若是符号：<ol><li>如果是左括号，入栈；</li><li>如果是右括号，输出栈顶元素直到左括号为止，括号不输出；</li><li>如果不是括号，判断其与栈顶符号的优先级，优先级<strong>低于</strong>栈顶符号（乘除优先加减），则栈顶元素依次出栈并输出，并将当前符号进栈。优先级<strong>高于</strong>栈顶符号，入栈。</li></ol></li></ol><h3 id="根据后缀表达式计算算式的值"><a href="#根据后缀表达式计算算式的值" class="headerlink" title="根据后缀表达式计算算式的值"></a>根据后缀表达式计算算式的值</h3><ol><li>维护一个栈，遍历后缀表达式</li><li>遇到数字入栈</li><li>遇到操作符，弹出两个栈顶元素，计算结果，将结果入栈</li><li>最后栈顶元素即为所求 </li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    opr = &#123;<span class="string">"+"</span>:<span class="number">0</span>,<span class="string">"-"</span>:<span class="number">0</span>,<span class="string">"*"</span>:<span class="number">1</span>,<span class="string">"/"</span>:<span class="number">1</span>,<span class="string">"("</span>: <span class="number">-1</span>,<span class="string">")"</span>: <span class="number">-1</span>&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_postfix</span><span class="params">(self,s)</span>:</span></span><br><span class="line">        </span><br><span class="line">        stack = []</span><br><span class="line">        res = []</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; len(s):</span><br><span class="line">            <span class="keyword">if</span> s[i] == <span class="string">" "</span>:</span><br><span class="line">                i+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> s[i] <span class="keyword">not</span> <span class="keyword">in</span> self.opr:</span><br><span class="line">                num = <span class="string">""</span></span><br><span class="line">                <span class="keyword">while</span> i &lt; len(s) <span class="keyword">and</span> s[i] <span class="keyword">not</span> <span class="keyword">in</span> self.opr:</span><br><span class="line">                    num += s[i]</span><br><span class="line">                    i+=<span class="number">1</span></span><br><span class="line">                res.append(int(num))</span><br><span class="line">                num =<span class="string">""</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> s[i] == <span class="string">"("</span>:</span><br><span class="line">                    stack.append(s[i])</span><br><span class="line">                <span class="keyword">elif</span> s[i] == <span class="string">")"</span>:</span><br><span class="line">                    <span class="keyword">while</span> stack[<span class="number">-1</span>] != <span class="string">"("</span>:</span><br><span class="line">                        op = stack.pop()</span><br><span class="line">                        res += op</span><br><span class="line">                    stack.pop()</span><br><span class="line">                <span class="keyword">elif</span> (<span class="keyword">not</span> stack) <span class="keyword">or</span> self.opr[stack[<span class="number">-1</span>]] &lt; self.opr[s[i]]: <span class="comment">## 栈顶操作符优先级低，直接入栈</span></span><br><span class="line">                    stack.append(s[i])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment">## 栈顶操作符优先级大于等于当前操作符，一直弹出，直到栈顶元素的优先级小于当前元素为止</span></span><br><span class="line">                    <span class="keyword">while</span> stack <span class="keyword">and</span> self.opr[stack[<span class="number">-1</span>]] &gt;= self.opr[s[i]]:</span><br><span class="line">                        op = stack.pop()</span><br><span class="line">                        res += op</span><br><span class="line">                    stack.append(s[i])</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> stack:</span><br><span class="line">            res += stack.pop()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_postfix</span><span class="params">(self,s)</span>:</span></span><br><span class="line"></span><br><span class="line">        print(s)</span><br><span class="line"></span><br><span class="line">        add = <span class="keyword">lambda</span> x,y: x+y</span><br><span class="line">        sub = <span class="keyword">lambda</span> x,y: x-y</span><br><span class="line">        mul = <span class="keyword">lambda</span> x,y: x*y</span><br><span class="line">        div = <span class="keyword">lambda</span> x,y: x//y</span><br><span class="line"></span><br><span class="line">        self.opr_func = &#123;<span class="string">"+"</span>:add,<span class="string">"-"</span>:sub,<span class="string">"*"</span>:mul,<span class="string">"/"</span>:div&#125;</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        stack = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> s:</span><br><span class="line">            <span class="keyword">if</span> char <span class="keyword">not</span> <span class="keyword">in</span> self.opr:</span><br><span class="line">                stack.append(int(char))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                second = stack.pop()</span><br><span class="line">                first = stack.pop()</span><br><span class="line">                stack.append(self.opr_func[char](first,second))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> stack[<span class="number">-1</span>]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span><span class="params">(self, s: str)</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> s: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.calculate_postfix(self.to_postfix(s))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Calculator design
    
    </summary>
    
    
      <category term="Data Structure and Algorithm" scheme="https://zhangruochi.com/categories/Data-Structure-and-Algorithm/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Leetcode" scheme="https://zhangruochi.com/tags/Leetcode/"/>
    
  </entry>
  
  <entry>
    <title>Longest common subsequence &amp; subarray &amp; substring</title>
    <link href="https://zhangruochi.com/Longest-common-subsequence-subarray-substring/2019/11/26/"/>
    <id>https://zhangruochi.com/Longest-common-subsequence-subarray-substring/2019/11/26/</id>
    <published>2019-11-26T07:27:02.000Z</published>
    <updated>2019-11-26T22:12:28.910Z</updated>
    
    <content type="html"><![CDATA[<h2 id="LeetCode-718-Maximum-Length-of-Repeated-Subarray"><a href="#LeetCode-718-Maximum-Length-of-Repeated-Subarray" class="headerlink" title="LeetCode 718. Maximum Length of Repeated Subarray"></a>LeetCode 718. <a href="https://leetcode.com/problems/maximum-length-of-repeated-subarray/" target="_blank" rel="noopener">Maximum Length of Repeated Subarray</a></h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Given two <span class="built_in">integer</span> arrays A and B, <span class="built_in">return</span> the maximum length of an subarray that appears <span class="keyword">in</span> both arrays.</span><br><span class="line"></span><br><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">Input:</span><br><span class="line">A: [1,2,3,2,1]</span><br><span class="line">B: [3,2,1,4,7]</span><br><span class="line">Output: 3</span><br><span class="line">Explanation: </span><br><span class="line">The repeated subarray with maximum length is [3, 2, 1].</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">Note:</span><br><span class="line"></span><br><span class="line">1 &lt;= len(A), len(B) &lt;= 1000</span><br><span class="line">0 &lt;= A[i], B[i] &lt; 100</span><br></pre></td></tr></table></figure><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>这道题给了我们两个数组A和B，让返回连个数组的最长重复子数组。那么如果将数组换成字符串，实际这道题就是求 Longest Common Substring 的问题了。<strong>注意需要跟最长子序列 Longest Common Subsequence 区分开</strong>。 既然是子数组，那么重复的地方一定是连续的。对于这种求极值的问题，动态规划 Dynamic Programming 一直都是一个很好的选择，这里使用一个二维的 DP 数组，其中 dp[i][j] 表示数组A的前i个数字和数组B的前j个数字的最长子数组的长度，如果 dp[i][j] 不为0，则A中第i个数组和B中第j个数字必须相等，比对于这两个数组 [1,2,2] 和 [3,1,2]，dp 数组为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  3 1 2</span><br><span class="line">1 0 1 0</span><br><span class="line">2 0 0 2</span><br><span class="line">2 0 0 1</span><br></pre></td></tr></table></figure><ul><li>dp[i][j] := max length of (A[0:i], B[0:j])</li><li>dp[i][j] = dp[i – 1][j – 1] + 1 if A[i-1] == B[j-1] else 0</li><li>Time complexity: O(m*n)</li><li>Space complexity: O(m*n) -&gt; O(n)</li></ul><h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findLength</span><span class="params">(self, A: List[int], B: List[int])</span> -&gt; int:</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># A --&gt; j</span></span><br><span class="line">        <span class="comment"># B --&gt; i</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> A <span class="keyword">or</span> <span class="keyword">not</span> B:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        dp = [[<span class="number">0</span>]*len(A) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(B))]</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(A)):</span><br><span class="line">            <span class="keyword">if</span> A[j] == B[<span class="number">0</span>]:</span><br><span class="line">                dp[<span class="number">0</span>][j] = <span class="number">1</span></span><br><span class="line">                res = max(res,dp[<span class="number">0</span>][j])</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(B)):</span><br><span class="line">            <span class="keyword">if</span> B[i] == A[<span class="number">0</span>]:</span><br><span class="line">                dp[i][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">                res = max(res,dp[i][<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(A)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,len(B)):</span><br><span class="line">                <span class="keyword">if</span> B[i] == A[j]:</span><br><span class="line">                    dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>] + <span class="number">1</span></span><br><span class="line">                    res = max(res,dp[i][j])</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><h2 id="Leetoce-1143-Longest-Common-Subsequence"><a href="#Leetoce-1143-Longest-Common-Subsequence" class="headerlink" title="Leetoce 1143. Longest Common Subsequence"></a>Leetoce 1143. <a href="https://leetcode.com/problems/longest-common-subsequence/" target="_blank" rel="noopener">Longest Common Subsequence</a></h2><blockquote><p>A subsequence of a string is a new string generated from the original string with some characters(can be none) deleted without changing the relative order of the remaining characters. (eg, “ace” is a subsequence of “abcde” while “aec” is not). A common subsequence of two strings is a subsequence that is common to both strings.</p></blockquote><h3 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">Given two strings text1 and text2, <span class="built_in">return</span> the length of their longest common subsequence.</span><br><span class="line"></span><br><span class="line">A subsequence of a string is a new string generated from the original string with some characters(can be none) deleted without changing the relative order of the remaining characters. (eg, <span class="string">"ace"</span> is a subsequence of <span class="string">"abcde"</span> <span class="keyword">while</span> <span class="string">"aec"</span> is not). A common subsequence of two strings is a subsequence that is common to both strings.</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">If there is no common subsequence, <span class="built_in">return</span> 0.</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">Input: text1 = <span class="string">"abcde"</span>, text2 = <span class="string">"ace"</span> </span><br><span class="line">Output: 3  </span><br><span class="line">Explanation: The longest common subsequence is <span class="string">"ace"</span> and its length is 3.</span><br><span class="line">Example 2:</span><br><span class="line"></span><br><span class="line">Input: text1 = <span class="string">"abc"</span>, text2 = <span class="string">"abc"</span></span><br><span class="line">Output: 3</span><br><span class="line">Explanation: The longest common subsequence is <span class="string">"abc"</span> and its length is 3.</span><br><span class="line">Example 3:</span><br><span class="line"></span><br><span class="line">Input: text1 = <span class="string">"abc"</span>, text2 = <span class="string">"def"</span></span><br><span class="line">Output: 0</span><br><span class="line">Explanation: There is no such common subsequence, so the result is 0.</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">Constraints:</span><br><span class="line"></span><br><span class="line">1 &lt;= text1.length &lt;= 1000</span><br><span class="line">1 &lt;= text2.length &lt;= 1000</span><br><span class="line">The input strings consist of lowercase English characters only.</span><br></pre></td></tr></table></figure><h3 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h3><ul><li>dp[i][j] stands for length of LCS between text1 up to i and text2 up to j.</li><li>dp[i][j] = dp[i-1][j-1] + 1 if text1[i] == text2[j] </li><li>Otherwise, dp[i][j] = max(dp[i][j-1], dp[i-1][j])</li></ul><h3 id="code-1"><a href="#code-1" class="headerlink" title="code"></a>code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonSubsequence</span><span class="params">(self, text1: str, text2: str)</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> text1 <span class="keyword">or</span> <span class="keyword">not</span> text2:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        dp = [[<span class="number">0</span>] * len(text1) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(text2))]</span><br><span class="line">        <span class="comment">## text1 --&gt;j; text2--&gt;i</span></span><br><span class="line">        </span><br><span class="line">        flag = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(text2)):</span><br><span class="line">            <span class="keyword">if</span> text2[i] == text1[<span class="number">0</span>]:</span><br><span class="line">                flag = <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">if</span> flag:</span><br><span class="line">                dp[i][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        flag = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(text1)):</span><br><span class="line">            <span class="keyword">if</span> text1[j] == text2[<span class="number">0</span>]:</span><br><span class="line">                flag = <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">if</span> flag:</span><br><span class="line">                dp[<span class="number">0</span>][j] = <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(text2)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,len(text1)):</span><br><span class="line">                <span class="keyword">if</span> text2[i] == text1[j]:</span><br><span class="line">                    dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>] + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = max(dp[i<span class="number">-1</span>][j],dp[i][j<span class="number">-1</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dp[i][j]</span><br></pre></td></tr></table></figure><h2 id="Leetoce-583-Delete-Operation-for-Two-Strings"><a href="#Leetoce-583-Delete-Operation-for-Two-Strings" class="headerlink" title="Leetoce 583. Delete Operation for Two Strings"></a>Leetoce 583. <a href="https://leetcode.com/problems/delete-operation-for-two-strings/" target="_blank" rel="noopener">Delete Operation for Two Strings</a></h2><h3 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a>问题描述</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Given two words word1 and word2, find the minimum number of steps required to make word1 and word2 the same, <span class="built_in">where</span> <span class="keyword">in</span> each step you can delete one character <span class="keyword">in</span> either string.</span><br><span class="line"></span><br><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">Input: <span class="string">"sea"</span>, <span class="string">"eat"</span></span><br><span class="line">Output: 2</span><br><span class="line">Explanation: You need one step to make <span class="string">"sea"</span> to <span class="string">"ea"</span> and another step to make <span class="string">"eat"</span> to <span class="string">"ea"</span>.</span><br><span class="line">Note:</span><br><span class="line"></span><br><span class="line">The length of given words won<span class="string">'t exceed 500.</span></span><br><span class="line"><span class="string">Characters in given words can only be lower-case letters.</span></span><br></pre></td></tr></table></figure><h3 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a>思路</h3><p>这题实际上是求两个字符串的 Longest Common Subsequence. 然后用两个字符串的长度和减去 2*LCS. DP的思路与上题一样。</p><h3 id="code-2"><a href="#code-2" class="headerlink" title="code"></a>code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minDistance</span><span class="params">(self, word1: str, word2: str)</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> word1 <span class="keyword">or</span> <span class="keyword">not</span> word2:</span><br><span class="line">            same = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dp = [[<span class="number">0</span>] * len(word1) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word2))]</span><br><span class="line">            <span class="comment">## text1 --&gt;j; text2--&gt;i</span></span><br><span class="line"></span><br><span class="line">            flag = <span class="keyword">False</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word2)):</span><br><span class="line">                <span class="keyword">if</span> word2[i] == word1[<span class="number">0</span>]:</span><br><span class="line">                    flag = <span class="keyword">True</span></span><br><span class="line">                <span class="keyword">if</span> flag:</span><br><span class="line">                    dp[i][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            flag = <span class="keyword">False</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(word1)):</span><br><span class="line">                <span class="keyword">if</span> word1[j] == word2[<span class="number">0</span>]:</span><br><span class="line">                    flag = <span class="keyword">True</span></span><br><span class="line">                <span class="keyword">if</span> flag:</span><br><span class="line">                    dp[<span class="number">0</span>][j] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(word2)):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,len(word1)):</span><br><span class="line">                    <span class="keyword">if</span> word2[i] == word1[j]:</span><br><span class="line">                        dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>] + <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        dp[i][j] = max(dp[i<span class="number">-1</span>][j],dp[i][j<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">            same = dp[i][j]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> len(word1) + len(word2) - same*<span class="number">2</span></span><br></pre></td></tr></table></figure><h2 id="Leetcode-1092-Shortest-Common-Supersequence"><a href="#Leetcode-1092-Shortest-Common-Supersequence" class="headerlink" title="Leetcode 1092. Shortest Common Supersequence"></a>Leetcode 1092. <a href="https://leetcode.com/problems/shortest-common-supersequence/" target="_blank" rel="noopener">Shortest Common Supersequence</a></h2><h3 id="问题描述-3"><a href="#问题描述-3" class="headerlink" title="问题描述"></a>问题描述</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Given two strings str1 and str2, <span class="built_in">return</span> the shortest string that has both str1 and str2 as subsequences.  If multiple answers exist, you may <span class="built_in">return</span> any of them.</span><br><span class="line"></span><br><span class="line">(A string S is a subsequence of string T <span class="keyword">if</span> deleting some number of characters from T (possibly 0, and the characters are chosen anywhere from T) results <span class="keyword">in</span> the string S.)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">Input: str1 = <span class="string">"abac"</span>, str2 = <span class="string">"cab"</span></span><br><span class="line">Output: <span class="string">"cabac"</span></span><br><span class="line">Explanation: </span><br><span class="line">str1 = <span class="string">"abac"</span> is a subsequence of <span class="string">"cabac"</span> because we can delete the first <span class="string">"c"</span>.</span><br><span class="line">str2 = <span class="string">"cab"</span> is a subsequence of <span class="string">"cabac"</span> because we can delete the last <span class="string">"ac"</span>.</span><br><span class="line">The answer provided is the shortest such string that satisfies these properties.</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">Note:</span><br><span class="line"></span><br><span class="line">1 &lt;= str1.length, str2.length &lt;= 1000</span><br><span class="line">str1 and str2 consist of lowercase English letters.</span><br></pre></td></tr></table></figure><h3 id="思路-3"><a href="#思路-3" class="headerlink" title="思路"></a>思路</h3><p><img src="lcs.png" alt></p><h3 id="code-3"><a href="#code-3" class="headerlink" title="code"></a>code</h3><pre><code class="lang-python">class Solution:    def shortestCommonSupersequence(self, str1: str, str2: str) -&gt; str:        if not str1 or not str2:            return &quot;&quot;        dp = [[0] * (len(str2)+1) for i in range(len(str1)+1)]        # dp[i][j] --&gt;  i: str1, j: str2        for i in range(1,len(str1)+1):            for j in range(1,len(str2)+1):                if str1[i-1] == str2[j-1]:                    dp[i][j] = dp[i-1][j-1] + 1                else:                    dp[i][j] = max(dp[i-1][j],dp[i][j-1])        l1 = len(str1); l2 = len(str2)        res = []        while l1 or l2:            if not l1:                char = str2[l2-1]                l2 -= 1            elif not l2:                char = str1[l1-1]                l1 -= 1            elif str1[l1-1] == str2[l2-1]:                char = str1[l1-1]                l1 -= 1                l2 -= 1            elif dp[l1-1][l2] == dp[l1][l2]:                char = str1[l1-1]                l1 -= 1            elif dp[l1][l2-1] == dp[l1][l2]:                char = str2[l2-1]                l2 -= 1            res.append(char)        return &quot;&quot;.join(res[::-1])</code></pre><blockquote><p>reference from <a href="https://zxi.mytechroad.com/blog/dynamic-programming/leetcode-1092-shortest-common-supersequence/" target="_blank" rel="noopener">https://zxi.mytechroad.com/blog/dynamic-programming/leetcode-1092-shortest-common-supersequence/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      the summary of Longest common subsequence subarray substring base on leetcode questions
    
    </summary>
    
    
      <category term="Data Structure and Algorithm" scheme="https://zhangruochi.com/categories/Data-Structure-and-Algorithm/"/>
    
    
      <category term="Leetcode" scheme="https://zhangruochi.com/tags/Leetcode/"/>
    
  </entry>
  
  <entry>
    <title>Ruby Hightlights</title>
    <link href="https://zhangruochi.com/Ruby-Hightlights/2019/11/24/"/>
    <id>https://zhangruochi.com/Ruby-Hightlights/2019/11/24/</id>
    <published>2019-11-25T01:11:56.000Z</published>
    <updated>2019-11-25T20:14:53.587Z</updated>
    
    <content type="html"><![CDATA[<h2 id="标识符"><a href="#标识符" class="headerlink" title="标识符"></a>标识符</h2><div class="table-container"><table><thead><tr><th style="text-align:left">标识符</th><th style="text-align:left">例子</th></tr></thead><tbody><tr><td style="text-align:left">局部变量</td><td style="text-align:left">first_name</td></tr><tr><td style="text-align:left">实例变量</td><td style="text-align:left">@first_name</td></tr><tr><td style="text-align:left">类变量</td><td style="text-align:left">@@first_name</td></tr><tr><td style="text-align:left">全局变量</td><td style="text-align:left">$FIRST_NAME</td></tr><tr><td style="text-align:left">常量</td><td style="text-align:left">FIRST_NAME</td></tr><tr><td style="text-align:left">关键字</td><td style="text-align:left">class</td></tr><tr><td style="text-align:left">方法名</td><td style="text-align:left">同局部变量</td></tr></tbody></table></div><h3 id="类变量"><a href="#类变量" class="headerlink" title="类变量"></a>类变量</h3><p>类变量的作用域并不属于类作用域，而是输入类层级的作用域。它提供了一种可以在类和类的实例之间共享数据的存储机制，但是对其他对象不可见。<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Car</span></span></span><br><span class="line">    @@makes = []</span><br><span class="line">    @@cars = &#123;&#125;</span><br><span class="line">    @@total_count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:make</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">self</span>.<span class="title">add_make</span><span class="params">(make)</span></span></span><br><span class="line">        <span class="keyword">unless</span> @@makes.<span class="keyword">include</span>?(make)</span><br><span class="line">            @@makes &lt;&lt; make</span><br><span class="line">            @@cars[make] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">self</span>.<span class="title">total_count</span></span></span><br><span class="line">        puts <span class="string">"<span class="subst">#&#123;@@total_count&#125;</span>"</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(make)</span></span></span><br><span class="line">        <span class="keyword">if</span> @@makes.<span class="keyword">include</span>? make</span><br><span class="line">            puts <span class="string">"Create a new make <span class="subst">#&#123;make&#125;</span>"</span></span><br><span class="line">            @make = make</span><br><span class="line">            @@cars[make] += <span class="number">1</span></span><br><span class="line">            @@total_count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            raise <span class="string">"no such make"</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_mates</span></span></span><br><span class="line">        @@cars[<span class="keyword">self</span>.make]</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">total_count</span></span></span><br><span class="line">        puts <span class="string">"<span class="subst">#&#123;@@total_count&#125;</span>"</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># my_car = Car.new "xx"</span></span><br><span class="line">Car.add_make(<span class="string">"BMW"</span>)</span><br><span class="line">my_car = Car.new <span class="string">"BMW"</span></span><br><span class="line">Car.total_count</span><br><span class="line">puts my_car.make</span><br><span class="line">my_car.total_count</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">=begin</span></span><br><span class="line"><span class="comment">Create a new make BMW</span></span><br><span class="line"><span class="comment">1</span></span><br><span class="line"><span class="comment">BMW</span></span><br><span class="line"><span class="comment">1</span></span><br><span class="line"><span class="comment">=end</span></span><br></pre></td></tr></table></figure></p><h3 id="父类和子类共享类变量"><a href="#父类和子类共享类变量" class="headerlink" title="父类和子类共享类变量"></a>父类和子类共享类变量</h3><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Parent</span></span></span><br><span class="line">    @@value = <span class="number">100</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Child</span> &lt; Parent</span></span><br><span class="line">    @@value = <span class="number">200</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Parent</span></span></span><br><span class="line">    puts @@value</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">Parent.new <span class="comment"># 200</span></span><br></pre></td></tr></table></figure><h3 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h3><p>定义在类中的实例方法可以看到同样定义在类中的常量，但其他程序也可以看到这些常量</p><h3 id="私有方法"><a href="#私有方法" class="headerlink" title="私有方法"></a>私有方法</h3><p>私有方法不能显式地被接收者调用。因为当没有明确接收对象时，接收对象是self。所以如果接收对象是Test的一个实例时，是可以调用成功的。<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span></span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">a</span></span></span><br><span class="line">        puts <span class="string">"a"</span> </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">b</span></span></span><br><span class="line">        <span class="comment"># self.a error</span></span><br><span class="line">        a</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    private <span class="symbol">:a</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">test = Test.new</span><br><span class="line">test.a <span class="comment"># error!</span></span><br><span class="line">test.b <span class="comment"># "a"</span></span><br></pre></td></tr></table></figure></p><h3 id="保护方法"><a href="#保护方法" class="headerlink" title="保护方法"></a>保护方法</h3><p>只能在此对象的实例方法或此类(或子类)的另一个对象中直接调用受保护的方法。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span></span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">a</span></span></span><br><span class="line">        puts <span class="string">"a"</span> </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">b</span></span></span><br><span class="line">        a</span><br><span class="line">        puts <span class="string">"<span class="subst">#&#123;<span class="keyword">self</span>&#125;</span>"</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    protected <span class="symbol">:a</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">test = Test.new</span><br><span class="line">test.b</span><br></pre></td></tr></table></figure><h2 id="Class-and-object"><a href="#Class-and-object" class="headerlink" title="Class and object"></a>Class and object</h2><h3 id="查看对象的原生方法"><a href="#查看对象的原生方法" class="headerlink" title="查看对象的原生方法"></a>查看对象的原生方法</h3><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p Object.new.methods.sort</span><br><span class="line"><span class="comment">=begin</span></span><br><span class="line"><span class="comment">[:!, :!=, :!~, :&lt;=&gt;, :==, :===, :=~, :__id__, :__send__, :class, :clone, :define_singleton_method, :display, :dup, :enum_for, :eql?, :equal?, :extend, :freeze, :frozen?, :hash, :inspect, :instance_eval, :instance_exec, :instance_of?, :instance_variable_defined?, :instance_variable_get, :instance_variable_set, :instance_variables, :is_a?, :itself, :kind_of?, :method, :methods, :nil?, :object_id, :private_methods, :protected_methods, :public_method, :public_methods, :public_send, :remove_instance_variable, :respond_to?, :send, :singleton_class, :singleton_method, :singleton_methods, :taint, :tainted?, :tap, :then, :to_enum, :to_s, :trust, :untaint, :untrust, :untrusted?, :yield_self]</span></span><br><span class="line"><span class="comment">=end</span></span><br></pre></td></tr></table></figure><h3 id="重开类"><a href="#重开类" class="headerlink" title="重开类"></a>重开类</h3><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span></span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">m</span></span></span><br><span class="line">        puts <span class="string">"method m"</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span></span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">y</span></span></span><br><span class="line">        puts <span class="string">"methods y"</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">p C.new.methods.sort</span><br><span class="line"></span><br><span class="line"><span class="comment"># [...... :m, ...... :y, ......]</span></span><br></pre></td></tr></table></figure><h3 id="单例方法"><a href="#单例方法" class="headerlink" title="单例方法"></a>单例方法</h3><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">obj = Object.new</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">obj</span>.<span class="title">test</span></span></span><br><span class="line">    p <span class="string">"test"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">cla = Class.new</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cla</span>.<span class="title">test</span></span></span><br><span class="line">    p <span class="string">"test"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">obj.test</span><br><span class="line">cla.test</span><br><span class="line"></span><br><span class="line"><span class="comment"># "test"</span></span><br><span class="line"><span class="comment"># "test"</span></span><br></pre></td></tr></table></figure><h3 id="对象方法查找规则"><a href="#对象方法查找规则" class="headerlink" title="对象方法查找规则"></a>对象方法查找规则</h3><p><img src="methods.jpg" alt></p><h3 id="类对象怎样查找方法"><a href="#类对象怎样查找方法" class="headerlink" title="类对象怎样查找方法"></a>类对象怎样查找方法</h3><ul><li>类是对象</li><li>类的实例也是对象</li><li>类对象有自己的方法、自己的状态和唯一标识</li></ul><p>因此，类的对象方法查找规则是</p><ol><li>从他的类</li><li>从超类和他们类更早的祖先</li><li>从存储在他们自身的单例方法</li></ol><h3 id="顶层方法"><a href="#顶层方法" class="headerlink" title="顶层方法"></a>顶层方法</h3><p>顶层方法作为Object类的私有方法被保存下来，它在任何地方可用，但是不能现式地指定接收者。</p><h2 id="Self"><a href="#Self" class="headerlink" title="Self"></a>Self</h2><p><img src="self.jpg" alt></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">puts <span class="string">"In top level, self is:  <span class="subst">#&#123;<span class="keyword">self</span>&#125;</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">module</span> <span class="title">A</span></span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">n</span></span></span><br><span class="line">        puts <span class="string">"In module A, self is:  <span class="subst">#&#123;<span class="keyword">self</span>&#125;</span>"</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span></span></span><br><span class="line">    puts <span class="string">"In Test, self is: <span class="subst">#&#123;<span class="keyword">self</span>&#125;</span>"</span>  <span class="comment"># 类对象</span></span><br><span class="line">    <span class="keyword">include</span> A</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">self</span>.<span class="title">x</span></span></span><br><span class="line">        puts <span class="string">"In x, self is: <span class="subst">#&#123;<span class="keyword">self</span>&#125;</span>"</span>   <span class="comment"># 类对象</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">m</span></span></span><br><span class="line">        puts <span class="string">"In m, self is: <span class="subst">#&#123;<span class="keyword">self</span>&#125;</span>"</span>  <span class="comment"># 实例对象</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">test = Test.new</span><br><span class="line">Test.x</span><br><span class="line">test.m</span><br><span class="line">test.n</span><br><span class="line"></span><br><span class="line"><span class="comment">=begin</span></span><br><span class="line"><span class="comment">In top level, self is:  main</span></span><br><span class="line"><span class="comment">In Test, self is: Test</span></span><br><span class="line"><span class="comment">In x, self is: Test</span></span><br><span class="line"><span class="comment">In m, self is: #&lt;Test:0x00007fa01a04cbc8&gt;</span></span><br><span class="line"><span class="comment">In module A, self is:  #&lt;Test:0x00007fa01a04cbc8&gt;</span></span><br><span class="line"><span class="comment">=end</span></span><br></pre></td></tr></table></figure><h2 id="Scope"><a href="#Scope" class="headerlink" title="Scope"></a>Scope</h2><p><img src="scope.jpg" alt></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">t</span></span></span><br><span class="line">    puts <span class="string">"Top level method t"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span></span></span><br><span class="line">    a = <span class="number">1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">self</span>.<span class="title">x</span></span></span><br><span class="line">        a = <span class="number">2</span></span><br><span class="line">        puts <span class="string">"C.x, a = <span class="subst">#&#123;a&#125;</span>"</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">m</span></span></span><br><span class="line">        a = <span class="number">3</span></span><br><span class="line">        puts <span class="string">"C#m; a = <span class="subst">#&#123;a&#125;</span>"</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">n</span></span></span><br><span class="line">        a = <span class="number">4</span></span><br><span class="line">        puts <span class="string">"C#n; a = <span class="subst">#&#123;a&#125;</span>"</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    puts <span class="string">"class scope: a = <span class="subst">#&#123;a&#125;</span>"</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">C.x</span><br><span class="line">c = C.new</span><br><span class="line">c.m</span><br><span class="line">c.n</span><br><span class="line"></span><br><span class="line"><span class="comment">=begin</span></span><br><span class="line"><span class="comment">class scope: a = 1</span></span><br><span class="line"><span class="comment">C.x, a = 2</span></span><br><span class="line"><span class="comment">C#m; a = 3</span></span><br><span class="line"><span class="comment">C#n; a = 4</span></span><br><span class="line"><span class="comment">=end</span></span><br></pre></td></tr></table></figure><h2 id="控制流"><a href="#控制流" class="headerlink" title="控制流"></a>控制流</h2><h3 id="case-子句"><a href="#case-子句" class="headerlink" title="case 子句"></a>case 子句</h3><p>每个 ruby 对象都有一个 case 相等性判断方法: <strong>===</strong></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span></span></span><br><span class="line">    <span class="keyword">attr_accessor</span> <span class="symbol">:age</span>, <span class="symbol">:height</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(age,height)</span></span></span><br><span class="line">        <span class="keyword">self</span>.age = age</span><br><span class="line">        <span class="keyword">self</span>.height = height</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">===</span><span class="params">(other_obj)</span></span></span><br><span class="line">        (<span class="keyword">self</span>.age == other_obj.age) &amp; (<span class="keyword">self</span>.height == other_obj.height)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$person1 = Person.new(<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">$person2 = Person.new(<span class="number">10</span>,<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(person)</span></span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> person</span><br><span class="line">    <span class="keyword">when</span> $person1</span><br><span class="line">        puts <span class="string">"person 1"</span></span><br><span class="line">    <span class="keyword">when</span> $person2</span><br><span class="line">        puts <span class="string">"person"</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        puts <span class="string">"not matching"</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">person = Person.new(<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">test(person) <span class="comment"># person 1</span></span><br></pre></td></tr></table></figure><h3 id="比较和-Comparable-模块"><a href="#比较和-Comparable-模块" class="headerlink" title="比较和 Comparable 模块"></a>比较和 Comparable 模块</h3><p>如果类需要有完整的比较方法，可以由如下解决方案：</p><ul><li>混合名为 Comparable 的模块</li><li>定义 <strong>&lt;=&gt;</strong> 的比较方法做为实例方法，在这个方法内部需要定义”小于”，”等于”，”大于”的含义。</li></ul><h3 id="代码块（block-和-关键字-yield"><a href="#代码块（block-和-关键字-yield" class="headerlink" title="代码块（block) 和 关键字 (yield)"></a>代码块（block) 和 关键字 (yield)</h3><p>ruby 中的每个方法调用都遵循如下语法：</p><ol><li>接收者对象或者变量</li><li>点（如果有明确的接收者则为必要条件）</li><li>方法名</li><li>参数列表（可选，默认为（））</li><li>代码块 （可选，没有默认）</li></ol><ul><li>代码块可以使用 {}, 也可以使用 do while 提供；</li><li>如果提供的代码块在方法中不能提供，并不会发生错误；</li><li>代码块和方法一样，可以接受参数；</li><li>yeild 的实际总用是转移控制权与作用域到代码块（可以跳回来）。</li><li>代码块是一个闭包(closure)</li></ul><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">block_scope</span></span></span><br><span class="line">    x = <span class="number">10</span></span><br><span class="line">    <span class="number">1</span>.times &#123; puts x&#125;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">block_scope <span class="comment"># 10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">block_scope</span></span></span><br><span class="line">    x = <span class="number">10</span></span><br><span class="line">    <span class="number">1</span>.times <span class="keyword">do</span></span><br><span class="line">        x = <span class="number">20</span></span><br><span class="line">        puts x</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">block_scope <span class="comment"># 20</span></span><br></pre></td></tr></table></figure><h2 id="String"><a href="#String" class="headerlink" title="String"></a>String</h2><ol><li><p>here-doc</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">text = &lt;&lt;EOM.to_i</span><br><span class="line"><span class="number">10</span></span><br><span class="line">EOM</span><br><span class="line"></span><br><span class="line">puts text <span class="comment"># 10</span></span><br></pre></td></tr></table></figure></li><li><p>索引</p></li></ol><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">string = <span class="string">"zhangruochi"</span></span><br><span class="line"></span><br><span class="line">puts string[<span class="number">0</span>,<span class="number">2</span>] <span class="comment"># zh</span></span><br><span class="line">puts string[<span class="number">0</span>..<span class="number">2</span>] <span class="comment"># zha</span></span><br><span class="line">puts string[<span class="number">0</span>...<span class="number">2</span>] <span class="comment"># zh</span></span><br><span class="line">puts string[<span class="number">0</span>..-<span class="number">1</span>] <span class="comment"># zhangruochi</span></span><br><span class="line">puts string[<span class="string">"zhang"</span>] <span class="comment">#zhang</span></span><br><span class="line">puts string[<span class="string">"xxx"</span>] <span class="comment">#nil</span></span><br></pre></td></tr></table></figure><h2 id="Enumerable"><a href="#Enumerable" class="headerlink" title="Enumerable"></a>Enumerable</h2><p>在ruby中，对象之间共有的特性通常都被放在模块里，集合类型也不例外：Ruby中的集合类型对象通常都包含 Enumerable 模块。</p><h2 id="Each"><a href="#Each" class="headerlink" title="Each"></a>Each</h2><p>任何枚举类型都包含一个each方法，其作用是将其中的元素逐个地作为参数传递给代码块。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里的每个方法都是基于each构建</span></span><br><span class="line">p Enumerable.instance_methods(<span class="literal">false</span>).sort</span><br><span class="line"><span class="comment">=begin</span></span><br><span class="line"><span class="comment">[:all?, :any?, :chain, :chunk, :chunk_while, :collect, :collect_concat, :count, :cycle, :detect, :drop, :drop_while, :each_cons, :each_entry, :each_slice, :each_with_index, :each_with_object, :entries, :filter, :find, :find_all, :find_index, :first, :flat_map, :grep, :grep_v, :group_by, :include?, :inject, :lazy, :map, :max, :max_by, :member?, :min, :min_by, :minmax, :minmax_by, :none?, :one?, :partition, :reduce, :reject, :reverse_each, :select, :slice_after, :slice_before, :slice_when, :sort, :sort_by, :sum, :take, :take_while, :to_a, :to_h, :uniq, :zip]</span></span><br><span class="line"><span class="comment">=end</span></span><br></pre></td></tr></table></figure><p><strong>Rainbow 混合了Enumerable模块，则自动赋予了整套基于 each 方法构建的方法。</strong><br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rainbow</span></span></span><br><span class="line">    <span class="keyword">include</span> Enumerable</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">each</span></span></span><br><span class="line">        <span class="keyword">yield</span> <span class="string">"Red"</span></span><br><span class="line">        <span class="keyword">yield</span> <span class="string">"Orange"</span></span><br><span class="line">        <span class="keyword">yield</span> <span class="string">"Green"</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">a = Rainbow.new</span><br><span class="line">p a.map(&amp;<span class="symbol">:upcase</span>)</span><br><span class="line"><span class="comment"># ["RED", "ORANGE", "GREEN"]</span></span><br></pre></td></tr></table></figure></p><h2 id="符号参数"><a href="#符号参数" class="headerlink" title="符号参数"></a>符号参数</h2><p>可以在方法参数的位置使用如<strong>:upcase</strong>这样的符号并在前面加上&amp;，它的效果与在代码块中调用每个元素同符号名称相等的方法的作用相同。<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p [<span class="string">"A"</span>,<span class="string">"B"</span>,<span class="string">"C"</span>].map(&amp;<span class="symbol">:downcase</span>)</span><br></pre></td></tr></table></figure></p><h2 id="Enumerator-枚举器"><a href="#Enumerator-枚举器" class="headerlink" title="Enumerator(枚举器)"></a>Enumerator(枚举器)</h2><ol><li>迭代器本质上是一个方法，它将一个或多个元素传递到代码块中。</li><li>枚举器本质上一枚举器是一个简单的可枚举对象。它有 each 方法，并使用Enumerable模块定义其所有常用的方法。</li><li>枚举器只需要有一个“迭代每一个”的逻辑，它已经知道后面的事情可以一一展开。<ul><li>Enumerator.new 伴随一个包含稍后会用到的 each 逻辑的代码块</li><li>创建一个基于可存在枚举对象的枚举器，这样枚举器的 each 方法就会从那个枚举对象的特定方法中抽取它的元素用于迭代</li></ul></li></ol><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">e = Enumerator.new <span class="keyword">do</span> <span class="params">|y|</span></span><br><span class="line">    y &lt;&lt; <span class="number">1</span></span><br><span class="line">    y &lt;&lt; <span class="number">2</span></span><br><span class="line">    y &lt;&lt; <span class="number">3</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">p e.to_a</span><br><span class="line">p e.map &#123;<span class="params">|x|</span> x * <span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Some important concepts in Ruby
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
    
  </entry>
  
  <entry>
    <title>Topological Sorting</title>
    <link href="https://zhangruochi.com/Topological-Sorting/2019/11/20/"/>
    <id>https://zhangruochi.com/Topological-Sorting/2019/11/20/</id>
    <published>2019-11-20T13:54:11.000Z</published>
    <updated>2019-11-20T14:25:40.986Z</updated>
    
    <content type="html"><![CDATA[<p>在图论中，拓扑排序（Topological Sorting）是一个有向无环图（DAG, Directed Acyclic Graph）的所有顶点的线性序列。且该序列必须满足下面两个条件：</p><ol><li>每个顶点出现且只出现一次。</li><li>若存在一条从顶点 A 到顶点 B 的路径，那么在序列中顶点 A 出现在顶点 B 的前面。</li></ol><p>有向无环图（DAG）才有拓扑排序，非DAG图没有拓扑排序一说。例如，下面这个图：</p><p><img src="1.png" alt></p><p>它是一个 DAG 图，那么如何写出它的拓扑排序呢？这里说一种比较常用的方法：</p><ol><li>从 DAG 图中选择一个 没有前驱（即入度为0）的顶点并输出。</li><li>从图中删除该顶点和所有以它为起点的有向边。</li><li>重复 1 和 2 直到当前的 DAG 图为空。</li><li><strong>当前图中不存在无前驱的顶点说明有向图中必然存在环</strong></li></ol><p><img src="2.png" alt></p><p>于是，得到拓扑排序后的结果是[1, 2, 4, 3, 5]。 通常，一个有向无环图可以有一个或多个拓扑排序序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topoSort</span><span class="params">(G)</span>:</span></span><br><span class="line">    in_degree = &#123;node:<span class="number">0</span> <span class="keyword">for</span> node <span class="keyword">in</span> G&#125;</span><br><span class="line">    <span class="keyword">for</span> u <span class="keyword">in</span> G:</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> G[u]:</span><br><span class="line">            in_degree[v] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    queue = [u <span class="keyword">for</span> u <span class="keyword">in</span> in_degree <span class="keyword">if</span> in_degree[u] == <span class="number">0</span>]</span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">while</span> queue:</span><br><span class="line">        s = queue.pop()</span><br><span class="line">        res.append(s)</span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> G[s]:</span><br><span class="line">            in_degree[u] -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> in_degree[u] == <span class="number">0</span>:</span><br><span class="line">                queue.append(u)</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">G=&#123;</span><br><span class="line">    <span class="number">1</span>: [<span class="number">2</span>,<span class="number">4</span>],</span><br><span class="line">    <span class="number">2</span>: [<span class="number">3</span>,<span class="number">4</span>],</span><br><span class="line">    <span class="number">3</span>: [<span class="number">5</span>],</span><br><span class="line">    <span class="number">4</span>: [<span class="number">3</span>,<span class="number">5</span>],</span><br><span class="line">    <span class="number">5</span>: [],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">res=topoSort(G)</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure><h2 id="Leetcode-207-Course-Schedule"><a href="#Leetcode-207-Course-Schedule" class="headerlink" title="Leetcode 207 Course Schedule"></a>Leetcode 207 <a href="https://leetcode.com/problems/course-schedule/" target="_blank" rel="noopener">Course Schedule</a></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">There are a total of n courses you have to take, labeled from 0 to n-1.</span><br><span class="line"></span><br><span class="line">Some courses may have prerequisites, for example to take course 0 you have to first take course 1, which is expressed as a pair: [0,1]</span><br><span class="line"></span><br><span class="line">Given the total number of courses and a list of prerequisite pairs, is it possible for you to finish all courses?</span><br><span class="line"></span><br><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">Input: 2, [[1,0]] </span><br><span class="line">Output: true</span><br><span class="line">Explanation: There are a total of 2 courses to take. </span><br><span class="line">             To take course 1 you should have finished course 0. So it is possible.</span><br><span class="line">Example 2:</span><br><span class="line"></span><br><span class="line">Input: 2, [[1,0],[0,1]]</span><br><span class="line">Output: false</span><br><span class="line">Explanation: There are a total of 2 courses to take. </span><br><span class="line">             To take course 1 you should have finished course 0, and to take course 0 you should</span><br><span class="line">             also have finished course 1. So it is impossible.</span><br><span class="line">Note:</span><br><span class="line"></span><br><span class="line">The input prerequisites is a graph represented by a list of edges, not adjacency matrices. Read more about how a graph is represented.</span><br><span class="line">You may assume that there are no duplicate edges in the input prerequisites.</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">canFinish</span><span class="params">(self, numCourses: int, prerequisites: List[List[int]])</span> -&gt; bool:</span></span><br><span class="line">        G = &#123;node:[] <span class="keyword">for</span> node <span class="keyword">in</span> range(numCourses)&#125;</span><br><span class="line">        <span class="keyword">for</span> edge <span class="keyword">in</span> prerequisites:</span><br><span class="line">            G[edge[<span class="number">1</span>]].append(edge[<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">top_sort</span><span class="params">(G)</span>:</span></span><br><span class="line">            in_degree = &#123;node: <span class="number">0</span> <span class="keyword">for</span> node <span class="keyword">in</span> G&#125;</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> u <span class="keyword">in</span> G:</span><br><span class="line">                <span class="keyword">for</span> v <span class="keyword">in</span> G[u]:</span><br><span class="line">                    in_degree[v] += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            queue = [node <span class="keyword">for</span> node <span class="keyword">in</span> in_degree <span class="keyword">if</span> in_degree[node] == <span class="number">0</span>]</span><br><span class="line">            res = []</span><br><span class="line">            <span class="keyword">while</span> queue:</span><br><span class="line">                cur_node = queue.pop()</span><br><span class="line">                res.append(cur_node)</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">for</span> v <span class="keyword">in</span> G[cur_node]:</span><br><span class="line">                    in_degree[v] -= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> in_degree[v] == <span class="number">0</span>:</span><br><span class="line">                        queue.append(v)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span> <span class="keyword">if</span> len(res) == len(G) <span class="keyword">else</span> <span class="keyword">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> top_sort(G)</span><br></pre></td></tr></table></figure><blockquote><p>reference <a href="https://blog.csdn.net/lisonglisonglisong/article/details/45543451" target="_blank" rel="noopener">https://blog.csdn.net/lisonglisonglisong/article/details/45543451</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在图论中，拓扑排序（Topological Sorting）是一个有向无环图（DAG, Directed Acyclic Graph）的所有顶点的线性序列。且该序列必须满足下面两个条件：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每个顶点出现且只出现一次。&lt;/li&gt;
&lt;li&gt;若存在一条从顶
      
    
    </summary>
    
    
      <category term="Data Structure and Algorithm" scheme="https://zhangruochi.com/categories/Data-Structure-and-Algorithm/"/>
    
    
  </entry>
  
</feed>
