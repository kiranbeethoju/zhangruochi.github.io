<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RUOCHI.AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhangruochi.com/"/>
  <updated>2020-01-18T23:01:40.510Z</updated>
  <id>https://zhangruochi.com/</id>
  
  <author>
    <name>Ruochi Zhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>rails tutorials: Association</title>
    <link href="https://zhangruochi.com/rails-tutorials-Association/2020/01/18/"/>
    <id>https://zhangruochi.com/rails-tutorials-Association/2020/01/18/</id>
    <published>2020-01-18T22:06:30.000Z</published>
    <updated>2020-01-18T23:01:40.510Z</updated>
    
    <content type="html"><![CDATA[<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">ER</div></center><h2 id="One-to-One-Association"><a href="#One-to-One-Association" class="headerlink" title="One-to-One Association"></a>One-to-One Association</h2><ul><li>One person has exactly one personal_info entry</li><li>One personal_info entry belongs to exactly one person</li><li>The “belongs to” side is the one with a foreign key</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rails g model personal_info height:<span class="built_in">float</span> weight:<span class="built_in">float</span> person:references</span><br></pre></td></tr></table></figure><blockquote><p>db/migrate/_create_personal_info.rb</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CreatePersonalInfos</span> &lt; ActiveRecord::Migration[6.0]</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">change</span></span></span><br><span class="line">    create_table <span class="symbol">:personal_infos</span> <span class="keyword">do</span> <span class="params">|t|</span></span><br><span class="line">      t.float <span class="symbol">:height</span></span><br><span class="line">      t.float <span class="symbol">:weight</span></span><br><span class="line">      t.references <span class="symbol">:person</span>, <span class="symbol">null:</span> <span class="literal">false</span>, <span class="symbol">foreign_key:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">      t.timestamps</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rake db:migrate</span><br></pre></td></tr></table></figure><blockquote><p>app/models/person.rb</p></blockquote><figure class="highlight rb"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> &lt; ApplicationRecord</span></span><br><span class="line">    has_one <span class="symbol">:personal_info</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><blockquote><p>app/models/personal_info.rb</p></blockquote><figure class="highlight rb"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PersonalInfo</span> &lt; ApplicationRecord</span></span><br><span class="line">  belongs_to <span class="symbol">:person</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="More-Methods"><a href="#More-Methods" class="headerlink" title="More Methods"></a>More Methods</h3><p>you have <code>build_personal_info(hash)</code> and <code>create_personal_info(hash)</code> methods on a person instance</p><h3 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zhang = Person.first</span><br><span class="line">pi1 = PersonalInfo.create height: 6.5 weight: 220</span><br><span class="line">zhang.personal_info = pi1</span><br><span class="line"></span><br><span class="line">zhang.build_personal_info height: 6.5 weight: 220</span><br></pre></td></tr></table></figure><h2 id="One-to-Many-Association"><a href="#One-to-Many-Association" class="headerlink" title="One-to-Many Association"></a>One-to-Many Association</h2><ul><li>One person has one or more jobs</li><li>One job entry belongs to exactly one person</li><li>The “belongs to” side is the one with a foreign key</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rails g model job title company position_id person:references</span><br></pre></td></tr></table></figure><blockquote><p>db/migrate/_create_jobs.rb</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CreateJobs</span> &lt; ActiveRecord::Migration[6.0]</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">change</span></span></span><br><span class="line">    create_table <span class="symbol">:jobs</span> <span class="keyword">do</span> <span class="params">|t|</span></span><br><span class="line">      t.string <span class="symbol">:title</span></span><br><span class="line">      t.string <span class="symbol">:company</span></span><br><span class="line">      t.string <span class="symbol">:position_id</span></span><br><span class="line">      t.references <span class="symbol">:person</span>, <span class="symbol">null:</span> <span class="literal">false</span>, <span class="symbol">foreign_key:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">      t.timestamps</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rake db:migrate</span><br></pre></td></tr></table></figure><blockquote><p>app/models/person.rb</p></blockquote><figure class="highlight rb"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> &lt; ApplicationRecord</span></span><br><span class="line">    has_one <span class="symbol">:personal_info</span></span><br><span class="line">    has_many <span class="symbol">:jobs</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><blockquote><p>app/models/job.rb</p></blockquote><figure class="highlight rb"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Job</span> &lt; ApplicationRecord</span></span><br><span class="line">  belongs_to <span class="symbol">:person</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="More-Methods-1"><a href="#More-Methods-1" class="headerlink" title="More Methods"></a>More Methods</h3><ul><li>person.jobs = jobs</li><li>person.jobs &lt;&lt; job(s)</li><li>person.jobs.clear</li></ul><h2 id="Many-to-Many"><a href="#Many-to-Many" class="headerlink" title="Many-to-Many"></a>Many-to-Many</h2><ul><li>One person can have many hobbies</li><li>One hobby can be shared by many people</li><li>Need to create an extra (a.k.a. join) table (<strong>without</strong> a model, i.e. just a migration)</li><li>Convention: Plural model names separated by an underscore in alphabetical order</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rails g model hobby name</span><br><span class="line">rails g migration create_hobbies_people person:references hobby:references</span><br></pre></td></tr></table></figure><blockquote><p>db/migrate/_create_habbies_people.rb</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CreateHabbiesPeople</span> &lt; ActiveRecord::Migration[6.0]</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">change</span></span></span><br><span class="line">    create_table <span class="symbol">:habbies_people</span>, <span class="symbol">id:</span><span class="literal">false</span> <span class="keyword">do</span> <span class="params">|t|</span></span><br><span class="line">      t.references <span class="symbol">:person</span>, <span class="symbol">null:</span> <span class="literal">false</span>, <span class="symbol">index:</span> <span class="literal">false</span>, <span class="symbol">foreign_key:</span> <span class="literal">true</span></span><br><span class="line">      t.references <span class="symbol">:hobby</span>, <span class="symbol">null:</span> <span class="literal">false</span>, <span class="symbol">index:</span> <span class="literal">false</span>, <span class="symbol">foreign_key:</span> <span class="literal">true</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rake db:migrate</span><br></pre></td></tr></table></figure><blockquote><p>app/models/person.rb</p></blockquote><figure class="highlight rb"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> &lt; ApplicationRecord</span></span><br><span class="line">    has_one <span class="symbol">:personal_info</span></span><br><span class="line">    has_many <span class="symbol">:jobs</span></span><br><span class="line">    has_and_belongs_to_many <span class="symbol">:hobbies</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><blockquote><p>app/models/hobby.rb</p></blockquote><figure class="highlight rb"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Hobby</span> &lt; ApplicationRecord</span></span><br><span class="line">    has_and_belongs_to_many <span class="symbol">:people</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
    &lt;img style=&quot;border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);&quot; src=&quot;1.pn
      
    
    </summary>
    
    
      <category term="Full Stack" scheme="https://zhangruochi.com/categories/Full-Stack/"/>
    
      <category term="Ruby on Rails" scheme="https://zhangruochi.com/categories/Full-Stack/Ruby-on-Rails/"/>
    
    
  </entry>
  
  <entry>
    <title>rails tutorials: Active Record CURD</title>
    <link href="https://zhangruochi.com/rails-tutorials-Active-Record-CURD/2020/01/18/"/>
    <id>https://zhangruochi.com/rails-tutorials-Active-Record-CURD/2020/01/18/</id>
    <published>2020-01-18T17:57:50.000Z</published>
    <updated>2020-01-18T20:02:27.046Z</updated>
    
    <content type="html"><![CDATA[<p>ActiveRecord is the name of Rails’ default ORM</p><blockquote><p>ORM (Object-Relational Mapping): Bridges the gap between <code>relational databases</code>, which are designed around mathematical Set Theory and Object-Oriented programming languages that deal with objects and their behavior. Greatly simplifies writing code for accessing the database.</p></blockquote><h3 id="Three-Prerequisites"><a href="#Three-Prerequisites" class="headerlink" title="Three Prerequisites"></a>Three Prerequisites</h3><ol><li>ActiveRecord has to know how to find your database (when Rails<br>is loaded, this info is read from config/database.yml file)</li><li>(Convention) There is a table with a <code>plural name</code> that corresponds<br>to ActiveRecord::Base subclass with a <code>singular name</code></li><li>(Convention) Expects the table to have a primary key named <code>id</code></li></ol><h3 id="CURD"><a href="#CURD" class="headerlink" title="CURD"></a>CURD</h3><h4 id="Create"><a href="#Create" class="headerlink" title="Create"></a>Create</h4><ol><li><p>Use an empty constructor and (ghost) attributes to set the values and then call save.</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p1 = Person.new</span><br><span class="line">p1.first_name = <span class="string">"zhang"</span></span><br><span class="line">p1.email = <span class="string">"zrc720@gmail.com"</span></span><br><span class="line">p1.save</span><br></pre></td></tr></table></figure></li><li><p>Pass a hash of attributes into the constructor and then call save.</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p2 = Person.new(<span class="symbol">name:</span> <span class="string">"zhang"</span>, <span class="symbol">email:</span> <span class="string">"zrc720@gmail.com"</span>)</span><br><span class="line">p2.save</span><br></pre></td></tr></table></figure></li><li><p>Use create method with a hash to create an object and save it to the database in one step.</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p3 = Person.create(<span class="symbol">name:</span> <span class="string">"zhang"</span>, <span class="symbol">email:</span> <span class="string">"zrc720@gmail.com"</span>)</span><br></pre></td></tr></table></figure></li></ol><h4 id="Retrieve"><a href="#Retrieve" class="headerlink" title="Retrieve"></a>Retrieve</h4><ol><li><p><code>find(id)</code> or <code>find(id1, id2)</code><br>Throws a RecordNotFound exception if not found</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Person.find(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p><code>first</code>, <code>last</code>, <code>take</code>, <code>all</code><br>Return the results you expect or nil if nothing is found</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Person.first</span><br><span class="line">Person.last</span><br><span class="line">Person.take</span><br><span class="line">Person.all</span><br></pre></td></tr></table></figure></li><li><p><code>order(:column)</code> or <code>order(column: :desc)</code><br>Allows ordering of the results. Ascending or descending</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Person.all.order(<span class="symbol">first_name:</span> <span class="symbol">:desc</span>)</span><br><span class="line">Person.all.order(<span class="symbol">:first_name</span>)</span><br></pre></td></tr></table></figure></li><li><p><code>pluck</code><br>Use pluck as a shortcut to select one or more attributes without loading a bunch of records just to grab the attributes you want.</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Person.pluck(<span class="symbol">:id</span>, <span class="symbol">:name</span>)</span><br><span class="line"><span class="comment"># SELECT people.id, people.name FROM people</span></span><br><span class="line"><span class="comment"># =&gt; [[1, 'David'], [2, 'Jeremy'], [3, 'Jose']]</span></span><br></pre></td></tr></table></figure></li><li><p><code>take</code><br>Gives a record (or N records if a parameter is supplied) without any implied order. The order will depend on the database implementation. If an order is supplied it will be respected.</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Person.take <span class="comment"># returns an object fetched by SELECT * FROM people LIMIT 1</span></span><br><span class="line">Person.take(<span class="number">5</span>) <span class="comment"># returns 5 objects fetched by SELECT * FROM people LIMIT 5</span></span><br></pre></td></tr></table></figure></li><li><p><code>where(hash)</code><br>Enables you to supply conditions for your search</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Person.where(<span class="symbol">name:</span> <span class="string">"zhangruochi"</span>)</span><br><span class="line">Person.where([<span class="string">"name = ? and email = ?"</span>, <span class="string">"Joe"</span>, <span class="string">"joe@example.com"</span>])</span><br><span class="line"><span class="string">``</span><span class="string">` </span></span><br><span class="line"><span class="string">7. `</span>find_by<span class="string">`</span></span><br><span class="line"><span class="string">Same as where, but returns a single result or nil if a record with the specified conditions is not found</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span>ruby</span><br><span class="line">User.find_by(<span class="symbol">name:</span> <span class="string">"zhangruochi"</span>)</span><br></pre></td></tr></table></figure></li><li><p><code>limit</code><br>Enables you to limit how many records come back</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Person.offset(<span class="number">1</span>).limit(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p><code>offset(n)</code><br>Don’t start from the beginning; skip a few</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Person.offset(<span class="number">1</span>).limit(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ol><h4 id="Update"><a href="#Update" class="headerlink" title="Update"></a>Update</h4><ol><li><p>Retrieve a record, modify the values and then call save</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zhang = User.where(<span class="symbol">name:</span> <span class="string">"zhangruochi"</span>)</span><br><span class="line">zhang.email = <span class="string">"lvduzhen@gmail.com"</span></span><br><span class="line">zhang.save</span><br></pre></td></tr></table></figure></li><li><p>Retrieve a record and then call update method passing in a hash of attributes with new values</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zhang = User.where(<span class="symbol">name:</span> <span class="string">"zhangruochi"</span>)</span><br><span class="line">zhang.update(<span class="symbol">email:</span> <span class="string">"lvduzhen@gmail.com"</span>)</span><br></pre></td></tr></table></figure></li><li><p>There is also <code>update_all</code> for batch updates</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">User.where(<span class="symbol">email:</span> <span class="string">'zrc720@gmail.com'</span>).update_all(<span class="symbol">name:</span> <span class="string">'ruochi'</span>)</span><br></pre></td></tr></table></figure></li></ol><h4 id="Delete"><a href="#Delete" class="headerlink" title="Delete"></a>Delete</h4><ol><li>destroy(id) or destroy<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zhang = User.first</span><br><span class="line">zhang.destroy</span><br><span class="line"></span><br><span class="line">User.destroy(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ActiveRecord is the name of Rails’ default ORM&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ORM (Object-Relational Mapping): Bridges the gap between &lt;code&gt;relatio
      
    
    </summary>
    
    
      <category term="Full Stack" scheme="https://zhangruochi.com/categories/Full-Stack/"/>
    
      <category term="Ruby on Rails" scheme="https://zhangruochi.com/categories/Full-Stack/Ruby-on-Rails/"/>
    
    
  </entry>
  
  <entry>
    <title>Metaprogramming in Ruby</title>
    <link href="https://zhangruochi.com/Metaprogramming-in-Ruby/2020/01/17/"/>
    <id>https://zhangruochi.com/Metaprogramming-in-Ruby/2020/01/17/</id>
    <published>2020-01-17T19:38:13.000Z</published>
    <updated>2020-01-18T17:58:30.315Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Dynamic-Dispatch"><a href="#Dynamic-Dispatch" class="headerlink" title="Dynamic Dispatch"></a>Dynamic Dispatch</h2><ul><li>There is another way to call a method in Ruby - using the <code>send</code> method</li><li>First parameter is the method name/symbol; the rest (if any) are method arguments</li></ul><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span> </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bark</span></span></span><br><span class="line">        puts <span class="string">"Woof, woof!"</span> </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">greet</span><span class="params">(greeting)</span></span> </span><br><span class="line">        puts greeting</span><br><span class="line">    <span class="keyword">end</span> </span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">dog = Dog.new</span><br><span class="line">dog.bark <span class="comment"># =&gt; Woof, woof! dog.send("bark") # =&gt; Woof, woof! dog.send(:bark) # =&gt; Woof, woof! method_name = :bark</span></span><br><span class="line">dog.send method_name <span class="comment"># =&gt; Woof, woof!</span></span><br><span class="line">dog.send(<span class="symbol">:greet</span>, <span class="string">"hello"</span>) <span class="comment"># =&gt; hello</span></span><br></pre></td></tr></table></figure><h2 id="Dynamic-Method"><a href="#Dynamic-Method" class="headerlink" title="Dynamic Method"></a>Dynamic Method</h2><ul><li>Not only can you call methods dynamically (with send), you can also define methods dynamically</li><li><code>define_method :method_name</code> and a <code>block</code> which<br>contains the method definition</li></ul><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Whatever</span></span></span><br><span class="line">    define_method <span class="symbol">:make_it_up</span> <span class="keyword">do</span></span><br><span class="line">        puts <span class="string">"Whatever..."</span> </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">whatever = Whatever.new whatever.make_it_up <span class="comment"># =&gt; Whatever...</span></span><br></pre></td></tr></table></figure><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">require_relative <span class="string">'store'</span> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReportingSystem</span></span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span></span><br><span class="line">        @store = Store.new</span><br><span class="line">        @store.methods.grep(<span class="regexp">/^get_(.*)_desc/</span>) &#123; ReportingSystem.define_report_methods_for $1 &#125;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">self</span>.<span class="title">define_report_methods_for</span> <span class="params">(item)</span></span> </span><br><span class="line">        define_method(<span class="string">"get_<span class="subst">#&#123;item&#125;</span>_desc"</span>) &#123; @store.send(<span class="string">"get_<span class="subst">#&#123;item&#125;</span>_desc"</span>)&#125; define_method(<span class="string">"get_<span class="subst">#&#123;item&#125;</span>_price"</span>) &#123; @store.send(<span class="string">"get_<span class="subst">#&#123;item&#125;</span>_price"</span>)&#125;</span><br><span class="line">    <span class="keyword">end</span> </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">rs = ReportingSystem.new</span><br><span class="line">puts <span class="string">"<span class="subst">#&#123;rs.get_piano_desc&#125;</span> costs <span class="subst">#&#123;rs.get_piano_price.to_s.ljust(<span class="number">6</span>, <span class="string">'0'</span>)&#125;</span>"</span> <span class="comment"># =&gt; Excellent piano costs 120.00</span></span><br></pre></td></tr></table></figure><h2 id="Ghost-Methods"><a href="#Ghost-Methods" class="headerlink" title="Ghost Methods"></a>Ghost Methods</h2><ul><li>method_missing gives you the power to “fake” the methods</li><li>Called “ghost methods” because the methods don’t really exist</li></ul><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mystery</span></span></span><br><span class="line"><span class="comment"># no_methods defined</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">method_missing</span> <span class="params">(method, *args)</span></span></span><br><span class="line">        puts <span class="string">"Looking for..."</span></span><br><span class="line">        puts <span class="string">"\"<span class="subst">#&#123;method&#125;</span>\" with params (<span class="subst">#&#123;args.join(<span class="string">','</span>)&#125;</span>) ?"</span> puts <span class="string">"Sorry... He is on vacation..."</span></span><br><span class="line">        <span class="keyword">yield</span> <span class="string">"Ended up in method_missing"</span> <span class="keyword">if</span> block_given?</span><br><span class="line">    <span class="keyword">end</span> </span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">m = Mystery.new</span><br><span class="line">m.solve_mystery(<span class="string">"abc"</span>, <span class="number">123123</span>) <span class="keyword">do</span> <span class="params">|answer|</span></span><br><span class="line">    puts <span class="string">"And the answer is: <span class="subst">#&#123;answer&#125;</span>"</span> </span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="Struct-and-OpenStruct"><a href="#Struct-and-OpenStruct" class="headerlink" title="Struct and OpenStruct"></a>Struct and OpenStruct</h2><ul><li><code>Struct</code>: Generator of specific classes, each one of which is defined to hold a set of variables and their accessors (“Dynamic Methods”)</li><li><code>OpenStruct</code>: Object (similar to Struct) whose attributes are created dynamically when first assigned (“Ghost methods”)</li></ul><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Customer = Struct.new(<span class="symbol">:name</span>, <span class="symbol">:address</span>) <span class="keyword">do</span> <span class="comment"># block is optional </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_s</span></span></span><br><span class="line">        <span class="string">"<span class="subst">#&#123;name&#125;</span> lives at <span class="subst">#&#123;address&#125;</span>"</span> </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">jim = Customer.new(<span class="string">"Jim"</span>, <span class="string">"-1000 Wall Street"</span>) puts jim <span class="comment"># =&gt; Jim lives at -1000 Wall Street</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">require</span> <span class="string">'ostruct'</span> <span class="comment"># =&gt; need to require ostruct for OpenStruct</span></span><br><span class="line">some_obj = OpenStruct.new(<span class="symbol">name:</span> <span class="string">"Joe"</span>, <span class="symbol">age:</span> <span class="number">15</span>) </span><br><span class="line">some_obj.sure = <span class="string">"three"</span></span><br><span class="line">some_obj.really = <span class="string">"yes, it is true"</span> some_obj.not_only_strings = <span class="number">10</span></span><br><span class="line">puts <span class="string">"<span class="subst">#&#123;some_obj.name&#125;</span> <span class="subst">#&#123;some_obj.age&#125;</span> <span class="subst">#&#123;some_obj.really&#125;</span>"</span> <span class="comment"># =&gt; Joe 15 yes, it is true</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Dynamic Dispatch, Dynamic Method, Ghost Methods, Struct and OpenStruct
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
      <category term="Metaprogramming" scheme="https://zhangruochi.com/categories/Programming-Language/Metaprogramming/"/>
    
    
  </entry>
  
  <entry>
    <title>rails tutorils: signup</title>
    <link href="https://zhangruochi.com/rails-tutorils-signup/2020/01/17/"/>
    <id>https://zhangruochi.com/rails-tutorils-signup/2020/01/17/</id>
    <published>2020-01-17T16:38:19.000Z</published>
    <updated>2020-01-17T17:28:35.003Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Users-资源"><a href="#Users-资源" class="headerlink" title="Users 资源"></a>Users 资源</h3><blockquote><p>config/routes.rb</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Rails.application.routes.draw <span class="keyword">do</span></span><br><span class="line">    root <span class="string">'static_pages#home'</span></span><br><span class="line">    get <span class="string">'/help'</span>, <span class="symbol">to:</span> <span class="string">'static_pages#help'</span> </span><br><span class="line">    get <span class="string">'/about'</span>, <span class="symbol">to:</span> <span class="string">'static_pages#about'</span> </span><br><span class="line">    get <span class="string">'/contact'</span>, <span class="symbol">to:</span> <span class="string">'static_pages#contact'</span> </span><br><span class="line">    get <span class="string">'/signup'</span>, <span class="symbol">to:</span> <span class="string">'users#new'</span></span><br><span class="line">    </span><br><span class="line">    resources <span class="symbol">:users</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="users.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Users Resources</div></center><h3 id="Gravatar-头像和侧边栏"><a href="#Gravatar-头像和侧边栏" class="headerlink" title="Gravatar 头像和侧边栏"></a>Gravatar 头像和侧边栏</h3><blockquote><p>app/views/users/show.html.erb</p></blockquote><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">%</span> <span class="attr">provide</span>(<span class="attr">:title</span>, @<span class="attr">user.name</span>) %&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"row"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">aside</span> <span class="attr">class</span>=<span class="string">"col-md-4"</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">section</span> <span class="attr">class</span>=<span class="string">"user_info"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">h1</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">%=</span> <span class="attr">gravatar_for</span> @<span class="attr">user</span> %&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">%=</span> @<span class="attr">user.name</span> %&gt;</span> <span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">section</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">aside</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p>app/assets/stylesheets/custom.scss</p></blockquote><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* sidebar */</span></span><br><span class="line"><span class="selector-tag">aside</span> &#123; </span><br><span class="line">    section.user_info &#123;</span><br><span class="line">        <span class="selector-tag">margin-top</span>: 20<span class="selector-tag">px</span>; &#125;</span><br><span class="line">    <span class="selector-tag">section</span> &#123;</span><br><span class="line">        padding: 10px 0; margin-top: 20px; &amp;:first-child &#123;</span><br><span class="line">            <span class="selector-tag">border</span>: 0;</span><br><span class="line">            <span class="selector-tag">padding-top</span>: 0; &#125;</span><br><span class="line">        <span class="selector-tag">span</span> &#123;</span><br><span class="line">            <span class="attribute">display</span>: block; <span class="attribute">margin-bottom</span>: <span class="number">3px</span>; <span class="attribute">line-height</span>: <span class="number">1</span>;&#125;</span><br><span class="line">        <span class="selector-tag">h1</span> &#123;</span><br><span class="line">            <span class="attribute">font-size</span>: <span class="number">1.4em</span>; <span class="attribute">text-align</span>: left; <span class="attribute">letter-spacing</span>: -<span class="number">1px</span>; <span class="attribute">margin-bottom</span>: <span class="number">3px</span>; <span class="attribute">margin-top</span>: <span class="number">0px</span>;</span><br><span class="line">        &#125; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.gravatar</span> &#123;</span><br><span class="line">    <span class="attribute">float</span>: left; <span class="attribute">margin-right</span>: <span class="number">10px</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.gravatar_edit</span> &#123; </span><br><span class="line">    <span class="attribute">margin-top</span>: <span class="number">15px</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>app/helpers/users_helper.rb</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">module</span> <span class="title">UsersHelper</span></span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gravatar_for</span><span class="params">(user, options = &#123; <span class="symbol">size:</span> <span class="number">80</span> &#125;)</span></span></span><br><span class="line">        size = options[<span class="symbol">:size</span>]</span><br><span class="line">        gravatar_id = Digest::MD5::hexdigest(user.email.downcase)</span><br><span class="line">        gravatar_url = <span class="string">"https://secure.gravatar.com/avatar/<span class="subst">#&#123;gravatar_id&#125;</span>?s=<span class="subst">#&#123;size&#125;</span>"</span> </span><br><span class="line">        image_tag(gravatar_url, <span class="symbol">alt:</span> user.name, <span class="class"><span class="keyword">class</span>: "<span class="title">gravatar</span>")</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="用户注册表单"><a href="#用户注册表单" class="headerlink" title="用户注册表单"></a>用户注册表单</h3><blockquote><p>app/views/users/new.html.erb</p></blockquote><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">%</span> <span class="attr">provide</span>(<span class="attr">:title</span>, '<span class="attr">Sign</span> <span class="attr">up</span>') %&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">h1</span>&gt;</span>Sign up<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"row"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"col-md-6 col-md-offset-3"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">%=</span> <span class="attr">form_with</span>(<span class="attr">model:</span> @<span class="attr">user</span>, <span class="attr">local:</span> <span class="attr">true</span>) <span class="attr">do</span> |<span class="attr">f</span>| %&gt;</span> <span class="tag">&lt;<span class="name">%=</span> <span class="attr">render</span> '<span class="attr">shared</span>/<span class="attr">error_messages</span>' %&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%=</span> <span class="attr">f.label</span> <span class="attr">:name</span> %&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%=</span> <span class="attr">f.text_field</span> <span class="attr">:name</span>, <span class="attr">class:</span> '<span class="attr">form-control</span>' %&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%=</span> <span class="attr">f.label</span> <span class="attr">:email</span> %&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%=</span> <span class="attr">f.email_field</span> <span class="attr">:email</span>, <span class="attr">class:</span> '<span class="attr">form-control</span>' %&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%=</span> <span class="attr">f.label</span> <span class="attr">:password</span> %&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%=</span> <span class="attr">f.password_field</span> <span class="attr">:password</span>, <span class="attr">class:</span> '<span class="attr">form-control</span>' %&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%=</span> <span class="attr">f.label</span> <span class="attr">:password_confirmation</span>, "<span class="attr">Confirmation</span>" %&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%=</span> <span class="attr">f.password_field</span> <span class="attr">:password_confirmation</span>, <span class="attr">class:</span> '<span class="attr">form-control</span>' %&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%=</span> <span class="attr">f.submit</span> "<span class="attr">Create</span> <span class="attr">my</span> <span class="attr">account</span>", <span class="attr">class:</span> "<span class="attr">btn</span> <span class="attr">btn-primary</span>" %&gt;</span> <span class="tag">&lt;<span class="name">%</span> <span class="attr">end</span> %&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p>app/assets/stylesheets/custom.scss</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/* forms *<span class="regexp">/</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">input, textarea, select, .uneditable-input &#123; </span></span><br><span class="line"><span class="regexp">    border: 1px solid #bbb;</span></span><br><span class="line"><span class="regexp">    width: 100%;</span></span><br><span class="line"><span class="regexp">    margin-bottom: 15px;</span></span><br><span class="line"><span class="regexp">    @include box_sizing; </span></span><br><span class="line"><span class="regexp">&#125;</span></span><br><span class="line"><span class="regexp">    input &#123;</span></span><br><span class="line"><span class="regexp">    height: auto !important;</span></span><br><span class="line"><span class="regexp">&#125;</span></span><br></pre></td></tr></table></figure><p>注意，在上面的代码中，渲染的局部视图名为 ‘shared/error_messages’，这里用到了 Rails 的一个约定:如 果局部视图要在多个控制器中使用(10.1.1 节)，则把它存放在专门的 shared/ 目录中。</p><blockquote><p>app/views/shared/_error_messages.html.erb</p></blockquote><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">%</span> <span class="attr">if</span> @<span class="attr">user.errors.any</span>? %&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"error_explanation"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"alert alert-danger"</span>&gt;</span></span><br><span class="line">            The form contains <span class="tag">&lt;<span class="name">%=</span> <span class="attr">pluralize</span>(@<span class="attr">user.errors.count</span>, "<span class="attr">error</span>") %&gt;</span>.</span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%</span> @<span class="attr">user.errors.full_messages.each</span> <span class="attr">do</span> |<span class="attr">msg</span>| %&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">%=</span> <span class="attr">msg</span> %&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span> </span><br><span class="line">            <span class="tag">&lt;<span class="name">%</span> <span class="attr">end</span> %&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">%</span> <span class="attr">end</span> %&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* forms */</span></span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line"><span class="selector-id">#error_explanation</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: red; </span><br><span class="line">    ul &#123;</span><br><span class="line">        <span class="selector-tag">color</span>: <span class="selector-tag">red</span>;</span><br><span class="line">        <span class="selector-tag">margin</span>: 0 0 30<span class="selector-tag">px</span> 0;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.field_with_errors</span> &#123; </span><br><span class="line">    @extend .has-error; </span><br><span class="line">    <span class="selector-class">.form-control</span> &#123;</span><br><span class="line">        <span class="attribute">color</span>: $state-danger-text; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UsersController</span> &lt; ApplicationController .</span></span><br><span class="line">    .</span><br><span class="line">    .</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create</span></span></span><br><span class="line">        @user = User.new(user_params) </span><br><span class="line">        <span class="keyword">if</span> @user.save</span><br><span class="line">            <span class="comment"># 处理注册成功的情况 else</span></span><br><span class="line">            redirect_to @user</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    private</span><br><span class="line">    </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">user_params</span></span></span><br><span class="line">            params.<span class="keyword">require</span>(<span class="symbol">:user</span>).permit(<span class="symbol">:name</span>, <span class="symbol">:email</span>, <span class="symbol">:password</span>,</span><br><span class="line">            <span class="symbol">:password_confirmation</span>)</span><br><span class="line">        <span class="keyword">end</span> </span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="闪现消息"><a href="#闪现消息" class="headerlink" title="闪现消息"></a>闪现消息</h3><blockquote><p>app/controllers/users_controller.rb</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UsersController</span> &lt; ApplicationController .</span></span><br><span class="line">    .</span><br><span class="line">    .</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create</span></span></span><br><span class="line">        @user = User.new(user_params) </span><br><span class="line">        <span class="keyword">if</span> @user.save</span><br><span class="line">            flash[<span class="symbol">:success</span>] = <span class="string">"Welcome to the Sample App!"</span></span><br><span class="line">            <span class="comment"># 处理注册成功的情况 else</span></span><br><span class="line">            redirect_to @user</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    private</span><br><span class="line">    </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">user_params</span></span></span><br><span class="line">            params.<span class="keyword">require</span>(<span class="symbol">:user</span>).permit(<span class="symbol">:name</span>, <span class="symbol">:email</span>, <span class="symbol">:password</span>,</span><br><span class="line">            <span class="symbol">:password_confirmation</span>)</span><br><span class="line">        <span class="keyword">end</span> </span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><blockquote><p>app/views/layouts/application.html.erb</p></blockquote><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">    .</span><br><span class="line">    .</span><br><span class="line">    .</span><br><span class="line">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">%=</span> <span class="attr">render</span> '<span class="attr">layouts</span>/<span class="attr">header</span>' %&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"container"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%</span> <span class="attr">flash.each</span> <span class="attr">do</span> |<span class="attr">message_type</span>, <span class="attr">message</span>| %&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"alert alert-&lt;%= message_type %&gt;"</span>&gt;</span><span class="tag">&lt;<span class="name">%=</span> <span class="attr">message</span> %&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%</span> <span class="attr">end</span> %&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%=</span> <span class="attr">yield</span> %&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%=</span> <span class="attr">render</span> '<span class="attr">layouts</span>/<span class="attr">footer</span>' %&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%=</span> <span class="attr">debug</span>(<span class="attr">params</span>) <span class="attr">if</span> <span class="attr">Rails.env.development</span>? %&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">            .</span><br><span class="line">            .</span><br><span class="line">            .</span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="专业部署方案"><a href="#专业部署方案" class="headerlink" title="专业部署方案"></a>专业部署方案</h3><h4 id="在生产环境中使用-SSL"><a href="#在生产环境中使用-SSL" class="headerlink" title="在生产环境中使用 SSL"></a>在生产环境中使用 SSL</h4><blockquote><p>config/environments/production.rb</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Rails.application.configure <span class="keyword">do</span> .</span><br><span class="line">    .</span><br><span class="line">    .</span><br><span class="line">    <span class="comment"># Force all access to the app over SSL, use Strict-Transport-Security,</span></span><br><span class="line">    <span class="comment"># and use secure cookies. </span></span><br><span class="line">    config.force_ssl = <span class="literal">true</span></span><br><span class="line">    .</span><br><span class="line">    .</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h4 id="生产环境中的-Web-服务器"><a href="#生产环境中的-Web-服务器" class="headerlink" title="生产环境中的 Web 服务器"></a>生产环境中的 Web 服务器</h4><blockquote><p>config/puma.rb</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Puma configuration file.</span></span><br><span class="line">max_threads_count = ENV.fetch(<span class="string">"RAILS_MAX_THREADS"</span>) &#123; <span class="number">5</span> &#125; min_threads_count = ENV.fetch(<span class="string">"RAILS_MIN_THREADS"</span>) &#123; max_threads_count &#125; threads min_threads_count, max_threads_count</span><br><span class="line">port ENV.fetch(<span class="string">"PORT"</span>) &#123; <span class="number">3000</span> &#125;</span><br><span class="line">environment ENV.fetch(<span class="string">"RAILS_ENV"</span>) &#123; ENV[<span class="string">'RACK_ENV'</span>] <span class="params">||</span> <span class="string">"development"</span> &#125; pidfile ENV.fetch(<span class="string">"PIDFILE"</span>) &#123; <span class="string">"tmp/pids/server.pid"</span> &#125;</span><br><span class="line">workers ENV.fetch(<span class="string">"WEB_CONCURRENCY"</span>) &#123; <span class="number">2</span> &#125;</span><br><span class="line">preload_app!</span><br><span class="line">plugin <span class="symbol">:tmp_restart</span></span><br></pre></td></tr></table></figure><p>最后，我们要新建一个 Procfile 文件，告诉 Heroku 在生产环境运行一个 Puma 进程。这个文件的内容如代 码清单 7.36 所示。Procfile 文件和 Gemfile 文件一样，应该放在应用的根目录中。</p><blockquote><p>./Procfile</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web: bundle <span class="built_in">exec</span> puma -C config/puma.rb</span><br></pre></td></tr></table></figure><h4 id="配置生产数据库"><a href="#配置生产数据库" class="headerlink" title="配置生产数据库"></a>配置生产数据库</h4><blockquote><p>config/database.yml</p></blockquote><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">default:</span> <span class="meta">&amp;default</span></span><br><span class="line"><span class="attr">      adapter:</span> <span class="string">sqlite3</span></span><br><span class="line"><span class="attr">      pool:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">      timeout:</span> <span class="number">5000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">development:</span></span><br><span class="line">    <span class="string">&lt;&lt;:</span> <span class="meta">*default</span></span><br><span class="line"><span class="attr">    database:</span> <span class="string">db/development.sqlite3</span></span><br><span class="line"><span class="comment"># Warning: The database defined as "test" will be erased and</span></span><br><span class="line"><span class="comment"># re-generated from your development database when you run "rake".</span></span><br><span class="line"><span class="comment"># Do not set this db to the same as development or production. </span></span><br><span class="line"></span><br><span class="line"><span class="attr">test:</span></span><br><span class="line">    <span class="string">&lt;&lt;:</span> <span class="meta">*default</span></span><br><span class="line"><span class="attr">    database:</span> <span class="string">db/test.sqlite3</span></span><br><span class="line"></span><br><span class="line"><span class="attr">production:</span></span><br><span class="line"><span class="attr">    adapter:</span> <span class="string">postgresql</span></span><br><span class="line"><span class="attr">    encoding:</span> <span class="string">unicode</span></span><br><span class="line">    <span class="comment"># For details on connection pooling, see Rails configuration guide # https://guides.rubyonrails.org/configuring.html#database-pooling pool: &lt;%= ENV.fetch("RAILS_MAX_THREADS") &#123; 5 &#125; %&gt;</span></span><br><span class="line"><span class="attr">    database:</span> <span class="string">sample_app_production</span></span><br><span class="line"><span class="attr">    username:</span> <span class="string">sample_app</span></span><br><span class="line"><span class="attr">    password:</span> &lt;%=<span class="ruby"> ENV[<span class="string">'SAMPLE_APP_DATABASE_PASSWORD'</span>] </span>%&gt;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      how to signup in website using ruby on rails
    
    </summary>
    
    
      <category term="Full Stack" scheme="https://zhangruochi.com/categories/Full-Stack/"/>
    
      <category term="Ruby on Rails" scheme="https://zhangruochi.com/categories/Full-Stack/Ruby-on-Rails/"/>
    
    
  </entry>
  
  <entry>
    <title>rails tutorials: data model</title>
    <link href="https://zhangruochi.com/rails-tutorials-data-model/2020/01/17/"/>
    <id>https://zhangruochi.com/rails-tutorials-data-model/2020/01/17/</id>
    <published>2020-01-17T15:19:10.000Z</published>
    <updated>2020-01-17T15:42:17.179Z</updated>
    
    <content type="html"><![CDATA[<h3 id="生成-User-Model"><a href="#生成-User-Model" class="headerlink" title="生成 User Model"></a>生成 User Model</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rails g model User name:string email:string</span><br></pre></td></tr></table></figure><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><blockquote><p>app/models/user.rb</p></blockquote><h4 id="存在性验证"><a href="#存在性验证" class="headerlink" title="存在性验证"></a>存在性验证</h4><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> &lt; ApplicationRecord </span></span><br><span class="line">    validates <span class="symbol">:name</span>, <span class="symbol">presence:</span> <span class="literal">true</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h4 id="长度验证"><a href="#长度验证" class="headerlink" title="长度验证"></a>长度验证</h4><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> &lt; ApplicationRecord</span></span><br><span class="line">    validates <span class="symbol">:name</span>, <span class="symbol">presence:</span> <span class="literal">true</span>, <span class="symbol">length:</span> &#123; <span class="symbol">maximum:</span> <span class="number">50</span> &#125; </span><br><span class="line">    validates <span class="symbol">:email</span>, <span class="symbol">presence:</span> <span class="literal">true</span>, <span class="symbol">length:</span> &#123; <span class="symbol">maximum:</span> <span class="number">255</span> &#125;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h4 id="电子邮件格式验证"><a href="#电子邮件格式验证" class="headerlink" title="电子邮件格式验证"></a>电子邮件格式验证</h4><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> &lt; ApplicationRecord</span></span><br><span class="line">    VALID_EMAIL_REGEX = <span class="regexp">/\A[\w+\-.]+@[a-z\d\-.]+\.[a-z]+\z/i</span></span><br><span class="line">    validates <span class="symbol">:name</span>, <span class="symbol">presence:</span> <span class="literal">true</span>, <span class="symbol">length:</span> &#123; <span class="symbol">maximum:</span> <span class="number">50</span> &#125; </span><br><span class="line">    validates <span class="symbol">:email</span>, <span class="symbol">presence:</span> <span class="literal">true</span>, <span class="symbol">length:</span> &#123; <span class="symbol">maximum:</span> <span class="number">255</span> &#125;,</span><br><span class="line">                      <span class="symbol">format:</span> &#123;<span class="symbol">with:</span> VALID_EMAIL_REGEX&#125;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h4 id="唯一性验证"><a href="#唯一性验证" class="headerlink" title="唯一性验证"></a>唯一性验证</h4><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> &lt; ApplicationRecord</span></span><br><span class="line">    VALID_EMAIL_REGEX = <span class="regexp">/\A[\w+\-.]+@[a-z\d\-.]+\.[a-z]+\z/i</span></span><br><span class="line">    validates <span class="symbol">:name</span>, <span class="symbol">presence:</span> <span class="literal">true</span>, <span class="symbol">length:</span> &#123; <span class="symbol">maximum:</span> <span class="number">50</span> &#125; </span><br><span class="line">    validates <span class="symbol">:email</span>, <span class="symbol">presence:</span> <span class="literal">true</span>, <span class="symbol">length:</span> &#123; <span class="symbol">maximum:</span> <span class="number">255</span> &#125;,</span><br><span class="line">                      <span class="symbol">format:</span> &#123;<span class="symbol">with:</span> VALID_EMAIL_REGEX&#125;,</span><br><span class="line">                      <span class="symbol">uniqueness:</span> <span class="literal">true</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="数据库层唯一性验证"><a href="#数据库层唯一性验证" class="headerlink" title="数据库层唯一性验证"></a>数据库层唯一性验证</h3><blockquote><p>rails generate migration add_index_to_users_email</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AddIndexToUsersEmail</span> &lt; ActiveRecord::Migration[6.0] </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">change</span></span></span><br><span class="line">        add_index <span class="symbol">:users</span>, <span class="symbol">:email</span>, <span class="symbol">unique:</span> <span class="literal">true</span> </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rails db:migrate</span><br></pre></td></tr></table></figure><h3 id="回调"><a href="#回调" class="headerlink" title="回调"></a>回调</h3><p>保证存储在数据库中的电子邮件都是小写字母的形式</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> &lt; ApplicationRecord</span></span><br><span class="line">    before_save &#123; <span class="keyword">self</span>.email = email.downcase &#125;</span><br><span class="line">    validates <span class="symbol">:name</span>, <span class="symbol">presence:</span> <span class="literal">true</span>, <span class="symbol">length:</span> &#123; <span class="symbol">maximum:</span> <span class="number">50</span> &#125; </span><br><span class="line">    VALID_EMAIL_REGEX = <span class="regexp">/\A[\w+\-.]+@[a-z\d\-.]+\.[a-z]+\z/i</span> </span><br><span class="line">    validates <span class="symbol">:email</span>, <span class="symbol">presence:</span> <span class="literal">true</span>, <span class="symbol">length:</span> &#123; <span class="symbol">maximum:</span> <span class="number">255</span> &#125;,</span><br><span class="line">                      <span class="symbol">format:</span> &#123; <span class="symbol">with:</span> VALID_EMAIL_REGEX &#125;, <span class="symbol">uniqueness:</span> <span class="literal">true</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="添加安全密码"><a href="#添加安全密码" class="headerlink" title="添加安全密码"></a>添加安全密码</h3><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> &lt; ApplicationRecord</span></span><br><span class="line">    ......</span><br><span class="line">    has_secure_password</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>在模型中调用这个方法后，会自动添加如下功能:</p><ol><li>在数据库中的password_digest列存储安全的密码哈希值;</li><li>获得一对虚拟属性，20password和password_confirmation，而且创建用户对象时会执行存在性验证和 匹配验证;</li><li>获得authenticate方法，如果密码正确，返回对应的用户对象，否则返回false。</li></ol><p>has_secure_password 发挥功效的唯一要求是，对应的模型中有个名为 password_digest 的属性。因此，创建一个适当的迁移文件，添加 password_digest 列。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rails generate migration add_password_digest_to_users password_digest:string</span><br></pre></td></tr></table></figure><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AddPasswordDigestToUsers</span> &lt; ActiveRecord::Migration[6.0] </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">change</span></span></span><br><span class="line">        add_column <span class="symbol">:users</span>, <span class="symbol">:password_digest</span>, <span class="symbol">:string</span> </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rails db:migrate</span><br></pre></td></tr></table></figure><p>has_secure_password 方法使用先进的 bcrypt 哈希算法计算密码摘要。使用 bcrypt 计算密码哈希值，就算攻击者设法获得了数据库副本也无法登录网站。我们要把 bcrypt gem 添加到 Gemfile 文件中。</p>]]></content>
    
    <summary type="html">
    
      How to create data model and store the data
    
    </summary>
    
    
      <category term="Full Stack" scheme="https://zhangruochi.com/categories/Full-Stack/"/>
    
      <category term="Ruby on Rails" scheme="https://zhangruochi.com/categories/Full-Stack/Ruby-on-Rails/"/>
    
    
  </entry>
  
  <entry>
    <title>rails-tutorials: static page and automated testing</title>
    <link href="https://zhangruochi.com/rails-tutorials-static-page-and-automated-testing/2020/01/11/"/>
    <id>https://zhangruochi.com/rails-tutorials-static-page-and-automated-testing/2020/01/11/</id>
    <published>2020-01-11T20:28:09.000Z</published>
    <updated>2020-01-11T09:11:13.679Z</updated>
    
    <content type="html"><![CDATA[<h3 id="生成静态页面的控制器"><a href="#生成静态页面的控制器" class="headerlink" title="生成静态页面的控制器"></a>生成静态页面的控制器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rails generate controller StaticPages home <span class="built_in">help</span></span><br></pre></td></tr></table></figure><h3 id="撤销操作"><a href="#撤销操作" class="headerlink" title="撤销操作"></a>撤销操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">rails generate controller StaticPages home <span class="built_in">help</span></span><br><span class="line">rails destroy  controller StaticPages home <span class="built_in">help</span></span><br><span class="line"></span><br><span class="line">rails generate model User name:string email:string</span><br><span class="line">rails destroy model User</span><br><span class="line"></span><br><span class="line">rails db:migrate</span><br><span class="line">rails db:rollback</span><br><span class="line">rails db:migrate VERSION=0</span><br></pre></td></tr></table></figure><h3 id="TDD"><a href="#TDD" class="headerlink" title="TDD"></a>TDD</h3><blockquote><p>test/controllers/static_pages_controller_test.rb</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StaticPagesControllerTest</span> &lt; ActionDispatch::IntegrationTest</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setup</span></span></span><br><span class="line">        @base_title = <span class="string">"Ruby on Rails Tutorial Sample App"</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    test <span class="string">"should get home"</span> <span class="keyword">do</span></span><br><span class="line">        get static_pages_home_url</span><br><span class="line">        assert_response <span class="symbol">:success</span></span><br><span class="line">        assert_select <span class="string">"title"</span>, <span class="string">"Home | <span class="subst">#&#123;@base_title&#125;</span>"</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    test <span class="string">"should get help"</span> <span class="keyword">do</span></span><br><span class="line">        get static_pages_help_url</span><br><span class="line">        assert_response <span class="symbol">:success</span></span><br><span class="line">        assert_select <span class="string">"title"</span>, <span class="string">"Help | <span class="subst">#&#123;@base_title&#125;</span>"</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    test <span class="string">"should get about"</span> <span class="keyword">do</span></span><br><span class="line">        get static_pages_about_url</span><br><span class="line">        assert_response <span class="symbol">:success</span></span><br><span class="line">        assert_select <span class="string">"title"</span>, <span class="string">"About | <span class="subst">#&#123;@base_title&#125;</span>"</span></span><br><span class="line">    <span class="keyword">end</span> </span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><blockquote><p>app/views/static_pages/home.html.erb</p></blockquote><p>通过&lt;% … %&gt;调用Rails提供的<code>provide</code>函数，把字符串”Home”赋给:title。11然后，在标题中，我们使<br>用类似的符号&lt;%= … %&gt;，通过Ruby的<code>yield</code>函数把标题插入模板中。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">%</span> <span class="attr">provide</span>(<span class="attr">:title</span>, "<span class="attr">Home</span>") %&gt;</span> </span><br><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span><span class="tag">&lt;<span class="name">%=</span> <span class="attr">yield</span>(<span class="attr">:title</span>) %&gt;</span> | Ruby on Rails Tutorial Sample App<span class="tag">&lt;/<span class="name">title</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">h1</span>&gt;</span>Sample App<span class="tag">&lt;/<span class="name">h1</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span>&gt;</span></span><br><span class="line">        This is the home page for the</span><br><span class="line">        <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"https://www.railstutorial.org/"</span>&gt;</span>Ruby on Rails Tutorial<span class="tag">&lt;/<span class="name">a</span>&gt;</span> sample application.</span><br><span class="line">        <span class="tag">&lt;/<span class="name">p</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>为了提取出共用的结构，Rails 提供了一个特别的布局文件，名为 application.html.erb。</p><blockquote><p>app/views/layouts/application.html.erb</p></blockquote><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> <span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">title</span>&gt;</span><span class="tag">&lt;<span class="name">%=</span> <span class="attr">yield</span>(<span class="attr">:title</span>) %&gt;</span> | Ruby on Rails Tutorial Sample App<span class="tag">&lt;/<span class="name">title</span>&gt;</span> </span><br><span class="line">            <span class="tag">&lt;<span class="name">%=</span> <span class="attr">csrf_meta_tags</span> %&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%=</span> <span class="attr">csp_meta_tag</span> %&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%=</span> <span class="attr">stylesheet_link_tag</span> '<span class="attr">application</span>', <span class="attr">media:</span> '<span class="attr">all</span>', '<span class="attr">data-turbolinks-track</span>'<span class="attr">:</span> '<span class="attr">reload</span>' %&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">%=</span> <span class="attr">javascript_pack_tag</span> '<span class="attr">application</span>', '<span class="attr">data-turbolinks-track</span>'<span class="attr">:</span> '<span class="attr">reload</span>' %&gt;</span> </span><br><span class="line">        <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">%=</span> <span class="attr">yield</span> %&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这几行代码的作用是，引入应用的样式表和 JavaScript 文件; Rails 提供的 csp_meta_tag 方法实现内容安全策略(Content Security Policy，CSP)，避免遭受跨站脚本(cross-site scripting，XSS)攻击;Rails 提供的 csrf_meta_tags 方法用于避免跨站请求伪造(Cross-Site Request Forgery，CSRF)攻击。</p><blockquote><p>app/views/static_pages/home.html.erb</p></blockquote><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">%</span> <span class="attr">provide</span>(<span class="attr">:title</span>, "<span class="attr">Home</span>") %&gt;</span> </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">h1</span>&gt;</span>Sample App<span class="tag">&lt;/<span class="name">h1</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span></span><br><span class="line">    This is the home page for the</span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"https://www.railstutorial.org/"</span>&gt;</span>Ruby on Rails Tutorial<span class="tag">&lt;/<span class="name">a</span>&gt;</span> </span><br><span class="line">    sample application.</span><br><span class="line"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>&lt;%= yield %&gt;</code>这行代码的作用是，把每个页面的内容插入布局中。在布局 中使用这行代码后，访问 /static_pages/home 时会把 home.html.erb 中的内容转换成 HTML，然后插入 <code>&lt;%= yield %&gt;</code> 所在的位置。</p>]]></content>
    
    <summary type="html">
    
      how to use static page and automated testing in ROR
    
    </summary>
    
    
      <category term="Full Stack" scheme="https://zhangruochi.com/categories/Full-Stack/"/>
    
      <category term="Ruby on Rails" scheme="https://zhangruochi.com/categories/Full-Stack/Ruby-on-Rails/"/>
    
    
  </entry>
  
  <entry>
    <title>rails tutorials - toy_app</title>
    <link href="https://zhangruochi.com/rails-tutorials-toy-app/2020/01/11/"/>
    <id>https://zhangruochi.com/rails-tutorials-toy-app/2020/01/11/</id>
    <published>2020-01-11T19:51:49.000Z</published>
    <updated>2020-01-17T15:19:48.617Z</updated>
    
    <content type="html"><![CDATA[<h3 id="创建应用"><a href="#创建应用" class="headerlink" title="创建应用"></a>创建应用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rails new toy_app</span><br><span class="line"><span class="built_in">cd</span> toy_app</span><br></pre></td></tr></table></figure><h3 id="修改-Gemfile"><a href="#修改-Gemfile" class="headerlink" title="修改 Gemfile"></a>修改 Gemfile</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">group :development, :<span class="built_in">test</span> <span class="keyword">do</span></span><br><span class="line">    gem <span class="string">'sqlite3'</span>, <span class="string">'1.4.1'</span></span><br><span class="line">    gem <span class="string">'byebug'</span>, <span class="string">'11.0.1'</span>, platforms: [:mri, :mingw, :x64_mingw]</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">group :production <span class="keyword">do</span> </span><br><span class="line">    gem <span class="string">'pg'</span>, <span class="string">'1.1.4'</span></span><br><span class="line">end</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装gem时要指定--without production选项，禁止安装生产环境使用的gem </span></span><br><span class="line">bundle install --without production</span><br></pre></td></tr></table></figure><h3 id="使用-git-版本控制系统"><a href="#使用-git-版本控制系统" class="headerlink" title="使用 git 版本控制系统"></a>使用 git 版本控制系统</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br><span class="line">git add -A</span><br><span class="line">git commit -m <span class="string">"Initialize repository"</span></span><br><span class="line"></span><br><span class="line">git remote add origin https://github.com/&lt;username&gt;/toy_app.git</span><br><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure><h3 id="修改-root-route"><a href="#修改-root-route" class="headerlink" title="修改 root route"></a>修改 root route</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="route.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">routes</div></center><blockquote><p>app/controllers/application_controller.rb</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ApplicationController</span> &lt; ActionController::Base</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hello</span></span></span><br><span class="line">        render <span class="symbol">html:</span> <span class="string">"hello, world!"</span></span><br><span class="line">    <span class="keyword">end</span> </span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><blockquote><p>config/routes.rb</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Rails.application.routes.draw <span class="keyword">do</span> </span><br><span class="line">    root <span class="string">'application#hello'</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="创建数据资源"><a href="#创建数据资源" class="headerlink" title="创建数据资源"></a>创建数据资源</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="users.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">User model</div></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="microposts.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">microposts model</div></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="relation.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">relation between user and micropost</div></center><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rails generate scaffold User name:string email:string</span><br><span class="line">rails generate scaffold Micropost content:text user_id:<span class="built_in">integer</span></span><br></pre></td></tr></table></figure><h3 id="修改根路由"><a href="#修改根路由" class="headerlink" title="修改根路由"></a>修改根路由</h3><blockquote><p>config/routes.rb</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Rails.application.routes.draw <span class="keyword">do</span> </span><br><span class="line">    resources <span class="symbol">:users</span></span><br><span class="line">    root <span class="string">'users#index'</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="模型验证"><a href="#模型验证" class="headerlink" title="模型验证"></a>模型验证</h3><blockquote><p>app/models/micropost.rb</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Micropost</span> &lt; ApplicationRecord</span></span><br><span class="line">    belongs_to <span class="symbol">:user</span></span><br><span class="line">    validates <span class="symbol">:content</span>, <span class="symbol">length:</span> &#123; <span class="symbol">maximum:</span> <span class="number">140</span> &#125;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> &lt; ApplicationRecord </span></span><br><span class="line">    has_many <span class="symbol">:microposts</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="数据库迁移"><a href="#数据库迁移" class="headerlink" title="数据库迁移"></a>数据库迁移</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rails db:migrate</span><br></pre></td></tr></table></figure><h3 id="部署到heroku"><a href="#部署到heroku" class="headerlink" title="部署到heroku"></a>部署到<code>heroku</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git add -A</span><br><span class="line">git commit -m <span class="string">"Finish toy app"</span></span><br><span class="line">git push</span><br><span class="line"></span><br><span class="line">heroku run rails db:migrate</span><br><span class="line">git push heroku</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      a simple tutorial of ror
    
    </summary>
    
    
      <category term="Full Stack" scheme="https://zhangruochi.com/categories/Full-Stack/"/>
    
      <category term="Ruby on Rails" scheme="https://zhangruochi.com/categories/Full-Stack/Ruby-on-Rails/"/>
    
    
  </entry>
  
  <entry>
    <title>Fastai Data Block API</title>
    <link href="https://zhangruochi.com/Fastai-Data-Block-API/2019/12/29/"/>
    <id>https://zhangruochi.com/Fastai-Data-Block-API/2019/12/29/</id>
    <published>2019-12-29T21:08:18.000Z</published>
    <updated>2019-12-29T13:03:57.484Z</updated>
    
    <content type="html"><![CDATA[<p>Essentially steps:</p><ol><li><p>Define the source of your inputs(X values)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ImageItemList.from_folder(path) </span><br><span class="line"></span><br><span class="line"><span class="comment">## This step generate the `ItemList` class. </span></span><br><span class="line"></span><br><span class="line"><span class="comment">## ItemBase: The ItemBase class defines what an “item” in your inputs or your targets looks like. </span></span><br><span class="line"></span><br><span class="line"><span class="comment">## ItemList: An ItemList defines a collections of `items` (e.g., ItemBase objects) including how they are individually fetched and displayed.</span></span><br></pre></td></tr></table></figure></li><li><p>Define how you want to split your inputs into training and validation datasets using one of the built-in mechanisms for doing so.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ImageItemList.from_folder(path)</span><br><span class="line">             .split_by_folder()</span><br><span class="line"></span><br><span class="line"><span class="comment">## This step generate the `ItemLists` class. </span></span><br><span class="line"></span><br><span class="line"><span class="comment">## ItemLists: A collection of ItemList instances for your inputs or targets. the `split` function above will return a separate ItemList instance for both your training and validation sets in an `ItemLists` object.</span></span><br></pre></td></tr></table></figure></li><li><p>Define the source of your targets (that is your y values) and combine them with the inputs of your training and validation datasets in the form of fastai <code>LabelList</code> objects. LabelList subclasses the PyTorch <code>Dataset</code> class.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ImageItemList.from_folder(path)</span><br><span class="line">             .split_by_folder()</span><br><span class="line">             .label_from_folder()</span><br><span class="line"></span><br><span class="line"><span class="comment">## This step generate the `LabelLists` class</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## LabelList: A LabelList is a PyTorch Dataset that combines your input and target ItemList classes (an inputs ItemList + a targets ItemList = a LabelList). </span></span><br><span class="line"></span><br><span class="line"><span class="comment">## LabelLists: A collection of LabelList instances you get as a result of your `labeling` function. Again, a LabelList` is a PyTorch Dataset and essentially defines the things, your inputs and optionally targets, fed into the forward function of your model.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Pre-Processing: This is also where any PreProcessor classes you’ve passed into your ItemList class run. These classes define things you want done to your data once before they are turned into PyTorch Datasets/DataLoaders. Examples include things like tokenizing and numericalizing text, filling in missing values in tabular, etc…. You can define a default `PreProcessor` or collection of PreProcessors you want ran by overloading the _processor class variable in your custom ItemList.</span></span><br></pre></td></tr></table></figure></li><li><p>Add a test dataset (optional).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data = (ImageItemList.from_folder(path) </span><br><span class="line">                     .split_by_folder()</span><br><span class="line">                     .label_from_folder()</span><br><span class="line">                     .add_test_folder())</span><br><span class="line"></span><br><span class="line"><span class="comment">## If you add a test set, like we do above, the same pre-processing applied to your validation set will be applied to your test.</span></span><br></pre></td></tr></table></figure></li><li><p>Add transforms to your <code>LabelList</code> objects (optional). Here you can apply data augmentation to either, or both, your inputs and targets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data = (ImageItemList.from_folder(path) </span><br><span class="line">                     .split_by_folder()</span><br><span class="line">                     .label_from_folder()</span><br><span class="line">                     .add_test_folder()</span><br><span class="line">                     .transform(tfms, size=<span class="number">64</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">## Transforms define data augmentation you want done to either, or both, of your inputs and targets datasets.</span></span><br></pre></td></tr></table></figure></li><li><p>Build PyTorch DataLoaders from the Datasets defined above and package them up into a fastai <code>DataBunch</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = (ImageItemList.from_folder(path) </span><br><span class="line">                     .split_by_folder()</span><br><span class="line">                     .label_from_folder()</span><br><span class="line">                     .add_test_folder()</span><br><span class="line">                     .transform(tfms, size=<span class="number">64</span>)</span><br><span class="line">                     .databunch())</span><br><span class="line"></span><br><span class="line"><span class="comment">## The step generate the `DataBunch` class</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## A DataBunch is a collection of PyTorch DataLoaders returned when you call the databunch function. It also defines how they are created from your training, validation, and optionally test LabelList instances.</span></span><br></pre></td></tr></table></figure></li></ol><p>Once this is done, you’ll have everything you need to train, validate, and test any PyTorch nn.Module using the fastai library. You’ll also have everything you need to later do inference on future data.</p><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImageTuple</span><span class="params">(ItemBase)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    we need to create a custom type of items since we feed the model tuples of images.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, img1, img2)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">         The basis is to code the data attribute that is what will be </span></span><br><span class="line"><span class="string">         given to the model. Note that we still keep track of the </span></span><br><span class="line"><span class="string">         initial object (usuall in an obj attribute) to be able to show </span></span><br><span class="line"><span class="string">         nice representations later on. </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.img1 = img1</span><br><span class="line">        self.img2 = img2</span><br><span class="line">        self.obj = (img1,img2)</span><br><span class="line">        self.data = [<span class="number">-1</span>+<span class="number">2</span>*img1.data,<span class="number">-1</span>+<span class="number">2</span>*img2.data]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply_tfms</span><span class="params">(self, tfms, **kwargs)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Then we want to apply data augmentation to our tuple of images. </span></span><br><span class="line"><span class="string">        That's done by writing an apply_tfms method as we saw before. </span></span><br><span class="line"><span class="string">        Here we pass that call to the two underlying images then update the data.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.img1 = self.img1.apply_tfms(tfms, **kwargs)</span><br><span class="line">        self.img2 = self.img2.apply_tfms(tfms, **kwargs)</span><br><span class="line">        self.data = [<span class="number">-1</span>+<span class="number">2</span>*self.img1.data,<span class="number">-1</span>+<span class="number">2</span>*self.img2.data]</span><br><span class="line">        <span class="keyword">return</span> self   </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_one</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        We define a last method to stack the two images next to each other, which we </span></span><br><span class="line"><span class="string">        will use later for a customized show_batch / show_results behavior.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> Image(<span class="number">0.5</span>+torch.cat(self.data,<span class="number">2</span>)/<span class="number">2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TargetTupleList</span><span class="params">(ItemList)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reconstruct</span><span class="params">(self, t:Tensor)</span>:</span> </span><br><span class="line">        <span class="keyword">if</span> len(t.size()) == <span class="number">0</span>: <span class="keyword">return</span> t</span><br><span class="line">        <span class="keyword">return</span> ImageTuple(Image(t[<span class="number">0</span>]/<span class="number">2</span>+<span class="number">0.5</span>),Image(t[<span class="number">1</span>]/<span class="number">2</span>+<span class="number">0.5</span>))</span><br></pre></td></tr></table></figure><ul><li><code>_bunch</code> contains the name of the class that will be used to create a DataBunch</li><li><code>_processor</code> contains a class (or a list of classes) of PreProcessor that will then be used as the default to create processor for this ItemList</li><li><code>_label_cls</code> contains the class that will be used to create the labels by default</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImageTupleList</span><span class="params">(ImageList)</span>:</span></span><br><span class="line">    _label_cls=TargetTupleList</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, items, itemsB=None, **kwargs)</span>:</span></span><br><span class="line">        super().__init__(items, **kwargs)</span><br><span class="line">        self.itemsB = itemsB</span><br><span class="line">        self.copy_new.append(<span class="string">'itemsB'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, i)</span>:</span></span><br><span class="line">        img1 = super().get(i)</span><br><span class="line">        fn = self.itemsB[random.randint(<span class="number">0</span>, len(self.itemsB)<span class="number">-1</span>)]</span><br><span class="line">        <span class="keyword">return</span> ImageTuple(img1, open_image(fn))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reconstruct</span><span class="params">(self, t:Tensor)</span>:</span> </span><br><span class="line">        <span class="keyword">return</span> ImageTuple(Image(t[<span class="number">0</span>]/<span class="number">2</span>+<span class="number">0.5</span>),Image(t[<span class="number">1</span>]/<span class="number">2</span>+<span class="number">0.5</span>))</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_folders</span><span class="params">(cls, path, folderA, folderB, **kwargs)</span>:</span></span><br><span class="line">        itemsB = ImageList.from_folder(path/folderB).items</span><br><span class="line">        res = super().from_folder(path/folderA, itemsB=itemsB, **kwargs)</span><br><span class="line">        res.path = path</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show_xys</span><span class="params">(self, xs, ys, figsize:Tuple[int,int]=<span class="params">(<span class="number">12</span>,<span class="number">6</span>)</span>, **kwargs)</span>:</span></span><br><span class="line">        <span class="string">"Show the `xs` and `ys` on a figure of `figsize`. `kwargs` are passed to the show method."</span></span><br><span class="line">        rows = int(math.sqrt(len(xs)))</span><br><span class="line">        fig, axs = plt.subplots(rows,rows,figsize=figsize)</span><br><span class="line">        <span class="keyword">for</span> i, ax <span class="keyword">in</span> enumerate(axs.flatten() <span class="keyword">if</span> rows &gt; <span class="number">1</span> <span class="keyword">else</span> [axs]):</span><br><span class="line">            xs[i].to_one().show(ax=ax, **kwargs)</span><br><span class="line">        plt.tight_layout()</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show_xyzs</span><span class="params">(self, xs, ys, zs, figsize:Tuple[int,int]=None, **kwargs)</span>:</span></span><br><span class="line">        <span class="string">"""Show `xs` (inputs), `ys` (targets) and `zs` (predictions) on a figure of `figsize`.</span></span><br><span class="line"><span class="string">        `kwargs` are passed to the show method."""</span></span><br><span class="line">        figsize = ifnone(figsize, (<span class="number">12</span>,<span class="number">3</span>*len(xs)))</span><br><span class="line">        fig,axs = plt.subplots(len(xs), <span class="number">2</span>, figsize=figsize)</span><br><span class="line">        fig.suptitle(<span class="string">'Ground truth / Predictions'</span>, weight=<span class="string">'bold'</span>, size=<span class="number">14</span>)</span><br><span class="line">        <span class="keyword">for</span> i,(x,z) <span class="keyword">in</span> enumerate(zip(xs,zs)):</span><br><span class="line">            x.to_one().show(ax=axs[i,<span class="number">0</span>], **kwargs)</span><br><span class="line">            z.to_one().show(ax=axs[i,<span class="number">1</span>], **kwargs)</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://blog.usejournal.com/finding-data-block-nirvana-a-journey-through-the-fastai-data-block-api-c38210537fe4" target="_blank" rel="noopener">https://blog.usejournal.com/finding-data-block-nirvana-a-journey-through-the-fastai-data-block-api-c38210537fe4</a></li><li><a href="https://docs.fast.ai/tutorial.itemlist.html" target="_blank" rel="noopener">https://docs.fast.ai/tutorial.itemlist.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      a journey through the fastai data block API
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Face Blindness Saver</title>
    <link href="https://zhangruochi.com/Face-Blindness-Saver/2019/12/25/"/>
    <id>https://zhangruochi.com/Face-Blindness-Saver/2019/12/25/</id>
    <published>2019-12-25T06:20:49.000Z</published>
    <updated>2020-01-09T06:28:53.828Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Creating-your-own-dataset-from-Google-Images"><a href="#Creating-your-own-dataset-from-Google-Images" class="headerlink" title="Creating your own dataset from Google Images"></a>Creating your own dataset from Google Images</h1><p><em>by: Francisco Ingham and Jeremy Howard. Inspired by <a href="https://www.pyimagesearch.com/2017/12/04/how-to-create-a-deep-learning-dataset-using-google-images/" target="_blank" rel="noopener">Adrian Rosebrock</a></em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastai.vision <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><h2 id="Get-a-list-of-URLs"><a href="#Get-a-list-of-URLs" class="headerlink" title="Get a list of URLs"></a>Get a list of URLs</h2><h3 id="Search-and-scroll"><a href="#Search-and-scroll" class="headerlink" title="Search and scroll"></a>Search and scroll</h3><p>Go to <a href="http://images.google.com" target="_blank" rel="noopener">Google Images</a> and search for the images you are interested in. The more specific you are in your Google Search, the better the results and the less manual pruning you will have to do.</p><p>Scroll down until you’ve seen all the images you want to download, or until you see a button that says ‘Show more results’. All the images you scrolled past are now available to download. To get more, click on the button, and continue scrolling. The maximum number of images Google Images shows is 700.</p><p>It is a good idea to put things you want to exclude into the search query, for instance if you are searching for the Eurasian wolf, “canis lupus lupus”, it might be a good idea to exclude other variants:</p><pre><code>&quot;canis lupus lupus&quot; -dog -arctos -familiaris -baileyi -occidentalis</code></pre><p>You can also limit your results to show only photos by clicking on Tools and selecting Photos from the Type dropdown.</p><h3 id="Download-into-file"><a href="#Download-into-file" class="headerlink" title="Download into file"></a>Download into file</h3><p>Now you must run some Javascript code in your browser which will save the URLs of all the images you want for you dataset.</p><p>In Google Chrome press <kbd>Ctrl</kbd><kbd>Shift</kbd><kbd>j</kbd> on Windows/Linux and <kbd>Cmd</kbd><kbd>Opt</kbd><kbd>j</kbd> on macOS, and a small window the javascript ‘Console’ will appear. In Firefox press <kbd>Ctrl</kbd><kbd>Shift</kbd><kbd>k</kbd> on Windows/Linux or <kbd>Cmd</kbd><kbd>Opt</kbd><kbd>k</kbd> on macOS. That is where you will paste the JavaScript commands.</p><p>You will need to get the urls of each of the images. Before running the following commands, you may want to disable ad blocking extensions (uBlock, AdBlockPlus etc.) in Chrome. Otherwise the window.open() command doesn’t work. Then you can run the following commands:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">urls=<span class="built_in">Array</span>.from(<span class="built_in">document</span>.querySelectorAll(<span class="string">'.rg_i'</span>)).map(<span class="function"><span class="params">el</span>=&gt;</span> el.hasAttribute(<span class="string">'data-src'</span>)?el.getAttribute(<span class="string">'data-src'</span>):el.getAttribute(<span class="string">'data-iurl'</span>));</span><br><span class="line"><span class="built_in">window</span>.open(<span class="string">'data:text/csv;charset=utf-8,'</span> + <span class="built_in">escape</span>(urls.join(<span class="string">'\n'</span>)));</span><br></pre></td></tr></table></figure><h3 id="Create-directory-and-upload-urls-file-into-your-server"><a href="#Create-directory-and-upload-urls-file-into-your-server" class="headerlink" title="Create directory and upload urls file into your server"></a>Create directory and upload urls file into your server</h3><p>Choose an appropriate name for your labeled images. You can run these steps multiple times to create different labels.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">help(download_images)</span><br></pre></td></tr></table></figure><pre><code>Help on function download_images in module fastai.vision.data:download_images(urls:Collection[str], dest:Union[pathlib.Path, str], max_pics:int=1000, max_workers:int=8, timeout=4)    Download images listed in text file `urls` to path `dest`, at most `max_pics`</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">path = Path(<span class="string">'data/dogs'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">folder = <span class="string">'akita'</span></span><br><span class="line">file = <span class="string">'akita.csv'</span></span><br><span class="line">dest = path/folder</span><br><span class="line">dest.mkdir(parents=<span class="keyword">True</span>, exist_ok=<span class="keyword">True</span>)</span><br><span class="line">urls = path/file</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">download_images(urls=urls, dest=dest, max_pics=<span class="number">200</span>)</span><br></pre></td></tr></table></figure><pre><code>Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">folder = <span class="string">'husky'</span></span><br><span class="line">file = <span class="string">'husky.csv'</span></span><br><span class="line">dest = path/folder</span><br><span class="line">dest.mkdir(parents=<span class="keyword">True</span>, exist_ok=<span class="keyword">True</span>)</span><br><span class="line">urls = path/file</span><br><span class="line">download_images(urls=urls, dest=dest, max_pics=<span class="number">200</span>)</span><br></pre></td></tr></table></figure><pre><code>Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">folder = <span class="string">'shibaInu'</span></span><br><span class="line">file = <span class="string">'shibaInu.csv'</span></span><br><span class="line">dest = path/folder</span><br><span class="line">dest.mkdir(parents=<span class="keyword">True</span>, exist_ok=<span class="keyword">True</span>)</span><br><span class="line">urls = path/file</span><br><span class="line">download_images(urls=urls, dest=dest, max_pics=<span class="number">200</span>)</span><br></pre></td></tr></table></figure><pre><code>Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">folder = <span class="string">'alaska'</span></span><br><span class="line">file = <span class="string">'alaska.csv'</span></span><br><span class="line">dest = path/folder</span><br><span class="line">dest.mkdir(parents=<span class="keyword">True</span>, exist_ok=<span class="keyword">True</span>)</span><br><span class="line">urls = path/file</span><br><span class="line">download_images(urls=urls, dest=dest, max_pics=<span class="number">200</span>)</span><br></pre></td></tr></table></figure><pre><code>Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?Error  Invalid URL &#39;&#39;: No schema supplied. Perhaps you meant http://?</code></pre><h3 id="View-data"><a href="#View-data" class="headerlink" title="View data"></a>View data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">help(DataBunch)</span><br></pre></td></tr></table></figure><pre><code>Help on class DataBunch in module fastai.basic_data:class DataBunch(builtins.object) |  Bind `train_dl`,`valid_dl` and `test_dl` in a data object. |   |  Methods defined here: |   |  __getattr__(self, k:int) -&gt; Any |   |  __init__(self, train_dl:torch.utils.data.dataloader.DataLoader, valid_dl:torch.utils.data.dataloader.DataLoader, fix_dl:torch.utils.data.dataloader.DataLoader=None, test_dl:Union[torch.utils.data.dataloader.DataLoader, NoneType]=None, device:torch.device=None, dl_tfms:Union[Collection[Callable], NoneType]=None, path:Union[pathlib.Path, str]=&#39;.&#39;, collate_fn:Callable=&lt;function data_collate at 0x7f14501736a8&gt;, no_check:bool=False) |      Initialize self.  See help(type(self)) for accurate signature. |   |  __repr__(self) -&gt; str |      Return repr(self). |   |  __setstate__(self, data:Any) |   |  add_test(self, items:Iterator, label:Any=None, tfms=None, tfm_y=None) -&gt; None |      Add the `items` as a test set. Pass along `label` otherwise label them with `EmptyLabel`. |   |  add_tfm(self, tfm:Callable) -&gt; None |   |  dl(self, ds_type:fastai.basic_data.DatasetType=&lt;DatasetType.Valid: 2&gt;) -&gt; fastai.basic_data.DeviceDataLoader |      Returns appropriate `Dataset` for validation, training, or test (`ds_type`). |   |  export(self, file:Union[pathlib.Path, str, _io.BufferedWriter, _io.BytesIO]=&#39;export.pkl&#39;) |      Export the minimal state of `self` for inference in `self.path/file`. `file` can be file-like (file or buffer) |   |  one_batch(self, ds_type:fastai.basic_data.DatasetType=&lt;DatasetType.Train: 1&gt;, detach:bool=True, denorm:bool=True, cpu:bool=True) -&gt; Collection[torch.Tensor] |      Get one batch from the data loader of `ds_type`. Optionally `detach` and `denorm`. |   |  one_item(self, item, detach:bool=False, denorm:bool=False, cpu:bool=False) |      Get `item` into a batch. Optionally `detach` and `denorm`. |   |  pre_transform = _db_pre_transform(self, train_tfm:List[Callable], valid_tfm:List[Callable]) |      Call `train_tfm` and `valid_tfm` after opening image, before converting from `PIL.Image` |   |  presize = _presize(self, size:int, val_xtra_size:int=32, scale:Tuple[float]=(0.08, 1.0), ratio:Tuple[float]=(0.75, 1.3333333333333333), interpolation:int=2) |      Resize images to `size` using `RandomResizedCrop`, passing along `kwargs` to train transform |   |  remove_tfm(self, tfm:Callable) -&gt; None |   |  sanity_check(self) |      Check the underlying data in the training set can be properly loaded. |   |  save(self, file:Union[pathlib.Path, str, _io.BufferedWriter, _io.BytesIO]=&#39;data_save.pkl&#39;) -&gt; None |      Save the `DataBunch` in `self.path/file`. `file` can be file-like (file or buffer) |   |  show_batch(self, rows:int=5, ds_type:fastai.basic_data.DatasetType=&lt;DatasetType.Train: 1&gt;, reverse:bool=False, **kwargs) -&gt; None |      Show a batch of data in `ds_type` on a few `rows`. |   |  ---------------------------------------------------------------------- |  Class methods defined here: |   |  create(train_ds:torch.utils.data.dataset.Dataset, valid_ds:torch.utils.data.dataset.Dataset, test_ds:Union[torch.utils.data.dataset.Dataset, NoneType]=None, path:Union[pathlib.Path, str]=&#39;.&#39;, bs:int=64, val_bs:int=None, num_workers:int=6, dl_tfms:Union[Collection[Callable], NoneType]=None, device:torch.device=None, collate_fn:Callable=&lt;function data_collate at 0x7f14501736a8&gt;, no_check:bool=False, **dl_kwargs) -&gt; &#39;DataBunch&#39; from builtins.type |      Create a `DataBunch` from `train_ds`, `valid_ds` and maybe `test_ds` with a batch size of `bs`. Passes `**dl_kwargs` to `DataLoader()` |   |  load_empty = _databunch_load_empty(path, fname:str=&#39;export.pkl&#39;) from builtins.type |      Load an empty `DataBunch` from the exported file in `path/fname` with optional `tfms`. |   |  ---------------------------------------------------------------------- |  Data descriptors defined here: |   |  __dict__ |      dictionary for instance variables (if defined) |   |  __weakref__ |      list of weak references to the object (if defined) |   |  batch_size |   |  dls |      Returns a list of all DeviceDataLoaders. If you need a specific DeviceDataLoader, access via the relevant property (`train_dl`, `valid_dl`, etc) as the index of DLs in this list is not guaranteed to remain constant. |   |  empty_val |   |  fix_ds |   |  is_empty |   |  loss_func |   |  single_ds |   |  test_ds |   |  train_ds |   |  valid_ds</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">help(ImageDataBunch.from_folder)</span><br></pre></td></tr></table></figure><pre><code>Help on method from_folder in module fastai.vision.data:from_folder(path:Union[pathlib.Path, str], train:Union[pathlib.Path, str]=&#39;train&#39;, valid:Union[pathlib.Path, str]=&#39;valid&#39;, test:Union[pathlib.Path, str, NoneType]=None, valid_pct=None, seed:int=None, classes:Collection=None, **kwargs:Any) -&gt; &#39;ImageDataBunch&#39; method of builtins.type instance    Create from imagenet style dataset in `path` with `train`,`valid`,`test` subfolders (or provide `valid_pct`).</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">help(ImageDataBunch.from_folder)</span><br></pre></td></tr></table></figure><pre><code>Help on method from_folder in module fastai.vision.data:from_folder(path:Union[pathlib.Path, str], train:Union[pathlib.Path, str]=&#39;train&#39;, valid:Union[pathlib.Path, str]=&#39;valid&#39;, test:Union[pathlib.Path, str, NoneType]=None, valid_pct=None, seed:int=None, classes:Collection=None, **kwargs:Any) -&gt; &#39;ImageDataBunch&#39; method of builtins.type instance    Create from imagenet style dataset in `path` with `train`,`valid`,`test` subfolders (or provide `valid_pct`).</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">7</span>)</span><br><span class="line">data = ImageDataBunch.from_folder(path,train = <span class="string">"."</span>, valid_pct = <span class="number">0.2</span>, size = <span class="number">224</span>, ds_tfms=get_transforms()).normalize(imagenet_stats)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.show_batch(rows = <span class="number">5</span>)</span><br></pre></td></tr></table></figure><p><img src="output_16_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.classes,data.c, len(data.train_ds), len(data.valid_ds)</span><br></pre></td></tr></table></figure><pre><code>([&#39;akita&#39;, &#39;alaska&#39;, &#39;husky&#39;, &#39;shibaInu&#39;], 4, 512, 128)</code></pre><h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn = cnn_learner(data,models.resnet34, metrics = error_rate)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn.fit_one_cycle(<span class="number">4</span>)</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: left;">      <th>epoch</th>      <th>train_loss</th>      <th>valid_loss</th>      <th>error_rate</th>      <th>time</th>    </tr>  </thead>  <tbody>    <tr>      <td>0</td>      <td>1.609119</td>      <td>0.857839</td>      <td>0.328125</td>      <td>00:05</td>    </tr>    <tr>      <td>1</td>      <td>1.124259</td>      <td>0.611583</td>      <td>0.250000</td>      <td>00:05</td>    </tr>    <tr>      <td>2</td>      <td>0.894434</td>      <td>0.618738</td>      <td>0.218750</td>      <td>00:05</td>    </tr>    <tr>      <td>3</td>      <td>0.738053</td>      <td>0.625707</td>      <td>0.242188</td>      <td>00:05</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lean.load(<span class="string">"stage-1"</span>)</span><br></pre></td></tr></table></figure><pre><code>Learner(data=ImageDataBunch;Train: LabelList (512 items)x: ImageListImage (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)y: CategoryListshibaInu,shibaInu,shibaInu,shibaInu,shibaInuPath: data/dogs;Valid: LabelList (128 items)x: ImageListImage (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)y: CategoryListshibaInu,alaska,akita,shibaInu,huskyPath: data/dogs;Test: None, model=Sequential(  (0): Sequential(    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    (2): ReLU(inplace)    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)    (4): Sequential(      (0): BasicBlock(        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (relu): ReLU(inplace)        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )      (1): BasicBlock(        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (relu): ReLU(inplace)        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )      (2): BasicBlock(        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (relu): ReLU(inplace)        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (5): Sequential(      (0): BasicBlock(        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (relu): ReLU(inplace)        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (downsample): Sequential(          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        )      )      (1): BasicBlock(        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (relu): ReLU(inplace)        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )      (2): BasicBlock(        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (relu): ReLU(inplace)        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )      (3): BasicBlock(        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (relu): ReLU(inplace)        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (6): Sequential(      (0): BasicBlock(        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (relu): ReLU(inplace)        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (downsample): Sequential(          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        )      )      (1): BasicBlock(        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (relu): ReLU(inplace)        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )      (2): BasicBlock(        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (relu): ReLU(inplace)        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )      (3): BasicBlock(        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (relu): ReLU(inplace)        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )      (4): BasicBlock(        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (relu): ReLU(inplace)        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )      (5): BasicBlock(        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (relu): ReLU(inplace)        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (7): Sequential(      (0): BasicBlock(        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (relu): ReLU(inplace)        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (downsample): Sequential(          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        )      )      (1): BasicBlock(        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (relu): ReLU(inplace)        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )      (2): BasicBlock(        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (relu): ReLU(inplace)        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )  )  (1): Sequential(    (0): AdaptiveConcatPool2d(      (ap): AdaptiveAvgPool2d(output_size=1)      (mp): AdaptiveMaxPool2d(output_size=1)    )    (1): Flatten()    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    (3): Dropout(p=0.25)    (4): Linear(in_features=1024, out_features=512, bias=True)    (5): ReLU(inplace)    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    (7): Dropout(p=0.5)    (8): Linear(in_features=512, out_features=4, bias=True)  )), opt_func=functools.partial(&lt;class &#39;torch.optim.adam.Adam&#39;&gt;, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[&lt;function error_rate at 0x7f144e518e18&gt;], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath(&#39;data/dogs&#39;), model_dir=&#39;models&#39;, callback_fns=[functools.partial(&lt;class &#39;fastai.basic_train.Recorder&#39;&gt;, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (2): ReLU(inplace)  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (6): ReLU(inplace)  (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (11): ReLU(inplace)  (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (14): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (16): ReLU(inplace)  (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (19): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)  (20): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (21): ReLU(inplace)  (22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (23): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (24): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)  (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (26): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (27): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (28): ReLU(inplace)  (29): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (30): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (31): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (32): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (33): ReLU(inplace)  (34): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (35): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (36): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (37): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (38): ReLU(inplace)  (39): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (40): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), Sequential(  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (2): ReLU(inplace)  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (5): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)  (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (9): ReLU(inplace)  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (14): ReLU(inplace)  (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (19): ReLU(inplace)  (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (23): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (24): ReLU(inplace)  (25): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (26): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (29): ReLU(inplace)  (30): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (32): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)  (33): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (34): ReLU(inplace)  (35): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (36): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (37): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)  (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (39): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (41): ReLU(inplace)  (42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (43): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (44): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (45): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (46): ReLU(inplace)  (47): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  (48): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), Sequential(  (0): AdaptiveAvgPool2d(output_size=1)  (1): AdaptiveMaxPool2d(output_size=1)  (2): Flatten()  (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (4): Dropout(p=0.25)  (5): Linear(in_features=1024, out_features=512, bias=True)  (6): ReLU(inplace)  (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (8): Dropout(p=0.5)  (9): Linear(in_features=512, out_features=4, bias=True))], add_time=True, silent=False)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lean.unfreeze()</span><br><span class="line">lean.lr_find()</span><br></pre></td></tr></table></figure><pre><code>LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lean.recorder.plot()</span><br></pre></td></tr></table></figure><p><img src="output_23_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lean.fit_one_cycle(<span class="number">4</span>,max_lr=slice(<span class="number">1e-4</span>,<span class="number">3e-3</span>))</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: left;">      <th>epoch</th>      <th>train_loss</th>      <th>valid_loss</th>      <th>error_rate</th>      <th>time</th>    </tr>  </thead>  <tbody>    <tr>      <td>0</td>      <td>0.676978</td>      <td>0.840263</td>      <td>0.406250</td>      <td>00:06</td>    </tr>    <tr>      <td>1</td>      <td>0.629555</td>      <td>0.816797</td>      <td>0.437500</td>      <td>00:06</td>    </tr>    <tr>      <td>2</td>      <td>0.582286</td>      <td>0.740251</td>      <td>0.335938</td>      <td>00:06</td>    </tr>    <tr>      <td>3</td>      <td>0.535621</td>      <td>0.722407</td>      <td>0.289062</td>      <td>00:06</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn.save(<span class="string">'stage-2'</span>)</span><br></pre></td></tr></table></figure><h2 id="Intepretation"><a href="#Intepretation" class="headerlink" title="Intepretation"></a>Intepretation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">interp = ClassificationInterpretation.from_learner(lean)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">interp.plot_confusion_matrix()</span><br></pre></td></tr></table></figure><p><img src="output_28_0.png" alt="png"></p><h2 id="Cleaning-Up"><a href="#Cleaning-Up" class="headerlink" title="Cleaning Up"></a>Cleaning Up</h2><p>Some of our top losses aren’t due to bad performance by our model. There are images in our data set that shouldn’t be.</p><p>Using the <code>ImageCleaner</code> widget from <code>fastai.widgets</code> we can prune our top losses, removing photos that don’t belong.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastai.widgets <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><p>First we need to get the file paths from our top_losses. We can do this with <code>.from_toplosses</code>. We then feed the top losses indexes and corresponding dataset to <code>ImageCleaner</code>.</p><p>Notice that the widget will not delete images directly from disk but it will create a new csv file <code>cleaned.csv</code> from where you can create a new ImageDataBunch with the corrected labels to continue training your model.</p><p>In order to clean the entire set of images, we need to create a new dataset without the split. The video lecture demostrated the use of the <code>ds_type</code> param which no longer has any effect. See <a href="https://forums.fast.ai/t/duplicate-widget/30975/10" target="_blank" rel="noopener">the thread</a> for more details.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">db = (ImageList.from_folder(path)</span><br><span class="line">                   .split_none()</span><br><span class="line">                   .label_from_folder()</span><br><span class="line">                   .transform(get_transforms(), size=<span class="number">224</span>)</span><br><span class="line">                   .databunch()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn_cln = cnn_learner(db, models.resnet34, metrics=error_rate)</span><br><span class="line">learn_cln.load(<span class="string">'stage-2'</span>);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ds, idxs = DatasetFormatter().from_toplosses(learn_cln)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImageCleaner(ds, idxs, path)</span><br></pre></td></tr></table></figure><pre><code>&#39;No images to show :)&#39;</code></pre><p>You can also find duplicates in your dataset and delete them! To do this, you need to run <code>.from_similars</code> to get the potential duplicates’ ids and then run <code>ImageCleaner</code> with <code>duplicates=True</code>. The API works in a similar way as with misclassified images: just choose the ones you want to delete and click ‘Next Batch’ until there are no more images left.</p><p>Make sure to recreate the databunch and <code>learn_cln</code> from the <code>cleaned.csv</code> file. Otherwise the file would be overwritten from scratch, losing all the results from cleaning the data from toplosses.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">doc(ImageDataBunch.from_csv)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_cleand = ImageDataBunch.from_csv(path,csv_labels = <span class="string">"cleaned.csv"</span>,ds_tfms=get_transforms(),valid_pct = <span class="number">0.2</span>, size = <span class="number">224</span>, bs = <span class="number">32</span>).normalize()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_cleand.show_batch(rows=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p><img src="output_39_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">final_learn = cnn_learner(data_cleand, models.resnet50, metrics = error_rate)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">final_learn.fit_one_cycle(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: left;">      <th>epoch</th>      <th>train_loss</th>      <th>valid_loss</th>      <th>error_rate</th>      <th>time</th>    </tr>  </thead>  <tbody>    <tr>      <td>0</td>      <td>1.187646</td>      <td>0.361647</td>      <td>0.114943</td>      <td>00:07</td>    </tr>    <tr>      <td>1</td>      <td>0.830483</td>      <td>0.527104</td>      <td>0.149425</td>      <td>00:06</td>    </tr>    <tr>      <td>2</td>      <td>0.620431</td>      <td>0.356172</td>      <td>0.126437</td>      <td>00:06</td>    </tr>    <tr>      <td>3</td>      <td>0.493722</td>      <td>0.324358</td>      <td>0.114943</td>      <td>00:06</td>    </tr>    <tr>      <td>4</td>      <td>0.417320</td>      <td>0.308305</td>      <td>0.103448</td>      <td>00:06</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">final_learn.save(<span class="string">"stage-1"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># final_learn.unfreeze()</span></span><br><span class="line">final_learn.lr_find()</span><br></pre></td></tr></table></figure><pre><code>LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">final_learn.recorder.plot()</span><br></pre></td></tr></table></figure><p><img src="output_44_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">final_learn.fit_one_cycle(<span class="number">1</span>, max_lr=slice(<span class="number">1e-3</span>,<span class="number">3e-3</span>))</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: left;">      <th>epoch</th>      <th>train_loss</th>      <th>valid_loss</th>      <th>error_rate</th>      <th>time</th>    </tr>  </thead>  <tbody>    <tr>      <td>0</td>      <td>0.218457</td>      <td>0.279203</td>      <td>0.080460</td>      <td>00:06</td>    </tr>  </tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">final_learn.save(<span class="string">"stage-final"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">interp = ClassificationInterpretation.from_learner(final_learn)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">interp.plot_confusion_matrix()</span><br></pre></td></tr></table></figure><p><img src="output_48_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">interp.plot_top_losses(<span class="number">9</span>, figsize=(<span class="number">15</span>,<span class="number">11</span>))</span><br></pre></td></tr></table></figure><p><img src="output_49_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install starlette</span><br></pre></td></tr></table></figure><pre><code>Collecting starlette[?25l  Downloading https://files.pythonhosted.org/packages/e1/22/360e07bf7852cc83fecf701bae502b90b62fa8584c4b70ee7bb1ea216dfe/starlette-0.10.2.tar.gz (45kB)[K     |████████████████████████████████| 51kB 3.4MB/s  eta 0:00:01[?25hBuilding wheels for collected packages: starlette  Building wheel for starlette (setup.py) ... [?25ldone[?25h  Created wheel for starlette: filename=starlette-0.10.2-cp35-none-any.whl size=58484 sha256=5b3216465d63bb24678d9e6b54680ce1f8d208bafe509c2df635df50a522e26a  Stored in directory: /home/zhangruochi/.cache/pip/wheels/28/c6/04/886febad73c854ba4e8a5b4f40aeaec3a97a9ab77795d141fbSuccessfully built starletteInstalling collected packages: starletteSuccessfully installed starlette-0.10.2</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">final_learn.export()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">defaults.device = torch.device(<span class="string">'cpu'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">img = open_image(path/<span class="string">'akita'</span>/<span class="string">'00000100.jpg'</span>)</span><br><span class="line">img</span><br></pre></td></tr></table></figure><p><img src="output_53_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">serving_model = load_learner(path)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_class,pred_idx,outputs = learn.predict(img)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_class</span><br></pre></td></tr></table></figure><pre><code>Category akita</code></pre><h2 id="Model-Serving"><a href="#Model-Serving" class="headerlink" title="Model Serving"></a>Model Serving</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> starlette.applications <span class="keyword">import</span> Starlette</span><br><span class="line"><span class="keyword">from</span> starlette.responses <span class="keyword">import</span> JSONResponse</span><br><span class="line"><span class="comment"># from starlette.routing import Route</span></span><br><span class="line"><span class="keyword">import</span> uvicorn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> fastai</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fastai.vision <span class="keyword">import</span> *</span><br><span class="line">defaults.device = torch.device(<span class="string">'cpu'</span>)</span><br><span class="line">learner = load_learner(<span class="string">"."</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">app = Starlette(debug=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_bytes</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> session.get(url) <span class="keyword">as</span> response:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">await</span> response.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route("/", methods=["GET"])</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">homepage</span><span class="params">(request)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> JSONResponse(&#123;<span class="string">'hello'</span>: <span class="string">'world'</span>&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route("/classify-url", methods=["GET"])</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">classify_url</span><span class="params">(request)</span>:</span></span><br><span class="line">    bytes = <span class="keyword">await</span> get_bytes(request.query_params[<span class="string">"url"</span>])</span><br><span class="line">    img = open_image(BytesIO(bytes))</span><br><span class="line">    pred_class,pred_idx,outputs = learner.predict(img)</span><br><span class="line">    <span class="keyword">return</span> JSONResponse(&#123;</span><br><span class="line">        <span class="string">"predictions"</span>:  str(pred_class)&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    uvicorn.run(app, host=<span class="string">"0.0.0.0"</span>, port=<span class="number">8080</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      I want to make a classifier to classify the `akita` and `shiba inu`, `alaska` and `husky`.
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Project" scheme="https://zhangruochi.com/tags/Project/"/>
    
      <category term="Computer Vision" scheme="https://zhangruochi.com/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>ELMo,OpenAI GPT,BERT</title>
    <link href="https://zhangruochi.com/ELMo-OpenAI-GPT-BERT/2019/12/21/"/>
    <id>https://zhangruochi.com/ELMo-OpenAI-GPT-BERT/2019/12/21/</id>
    <published>2019-12-22T03:31:19.000Z</published>
    <updated>2019-12-22T05:26:06.946Z</updated>
    
    <content type="html"><![CDATA[<h2 id="The-problems-of-RNN"><a href="#The-problems-of-RNN" class="headerlink" title="The problems of RNN"></a>The problems of RNN</h2><ol><li>Sequential computation inhibit parallelization</li><li>No explicit modeling of long and short range</li><li>We want to model hierarchy (RNNs seem wasteful)</li></ol><h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><p>ELMo means <code>Embeddings from Language Models</code>. the original paper is from <a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noopener">https://arxiv.org/abs/1802.05365</a></p><ol><li>Breakout version of word token vectors or contextual word vectors</li><li>Learn word token vectors using long contexts not context windows (here, whole sentence, could be longer)</li><li>Learn a deep Bi-NLM and use all its layers in prediction</li></ol><h3 id="What’s-ELMo’s-secret"><a href="#What’s-ELMo’s-secret" class="headerlink" title="What’s ELMo’s secret?"></a>What’s ELMo’s secret?</h3><p>ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called <code>Language Modeling</code>. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="elmo2.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">char cnn embedding from ELMo</div></center><p>A step in the pre-training process of ELMo: Given “Let’s stick to” as input, predict the next most likely word – a language modeling task. When trained on a large dataset, the model starts to pick up on language patterns. It’s unlikely it’ll accurately guess the next word in this example. More realistically, after a word such as “hang”, it will assign a higher probability to a word like “out” (to spell “hang out”) than to “camera”.</p><h3 id="bilstm-LM"><a href="#bilstm-LM" class="headerlink" title="bilstm LM"></a>bilstm LM</h3><p>ELMo actually goes a step further and trains a bi-directional LSTM – so that its language model doesn’t only have a sense of the next word, but also the previous word.</p><ol><li>前向LSTM结构:<script type="math/tex; mode=display">p(t_1,t_2,...,t_N) = \prod^N_{k=1}p(t_k|t_1,t_2,...,t_{k-1})</script></li><li>反向LSTM结构:<script type="math/tex; mode=display">p(t_1,t_2,...,t_N) = \prod^N_{k=1}p(t_k|t_{k+1},t_{k+2},...,t_{N})</script></li><li>最大似然函数:<script type="math/tex; mode=display">\sum_{k=1}^N(logp(t_k|t_1,t_2,...,t_{k-1}) + logp(t_k|t_{k+1},t_{k+2},...,t_{N}))</script></li><li>线性组合公式：<script type="math/tex; mode=display">\textrm{ELMo}_k^{task} = E(R_k;\Theta^{task}) = \gamma^{task}\sum_{j=0}^L s_j^{task}h_{k,j}^{LM} \tag{1}</script></li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="elmo.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">bilstm from ELMo</div></center><h3 id="Char-cnn-embedding"><a href="#Char-cnn-embedding" class="headerlink" title="Char cnn embedding"></a>Char cnn embedding</h3><p>The input of elmo is char embedding, see the details from <a href="https://zhangruochi.com/Subword-Models/2019/12/19/">https://zhangruochi.com/Subword-Models/2019/12/19/</a></p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="char_cnn.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">char cnn embedding from ELMo</div></center><h2 id="How-to-use-ELMo-when-after-pre-training"><a href="#How-to-use-ELMo-when-after-pre-training" class="headerlink" title="How to use ELMo when after pre-training"></a>How to use ELMo when after pre-training</h2><p>We can feed our input data to the pre-trained ELMo and get the representation of <code>dynamic word vectors</code>. And then we use them to our specific tasks.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="elmo3.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">ELMo used in a sequence tagger</div></center><h2 id="OpenAI-Transformer-Pre-training-a-Transformer-Decoder-for-Language-Modeling"><a href="#OpenAI-Transformer-Pre-training-a-Transformer-Decoder-for-Language-Modeling" class="headerlink" title="OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling"></a>OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling</h2><p>It turns out we don’t need an entire Transformer to adopt transfer learning and a fine-tunable language model for NLP tasks. We can do with just the decoder of the transformer. The decoder is a good choice because it’s a natural choice for language modeling (predicting the next word) since it’s built to <code>mask future tokens</code> – a valuable feature when it’s generating a translation word by word.</p><p>The model stacked twelve decoder layers. Since there is no encoder in this set up, these decoder layers would not have the encoder-decoder attention sublayer that vanilla transformer decoder layers have. It would still have the self-attention layer, however (masked so it doesn’t peak at future tokens).</p><p>With this structure, we can proceed to train the model on the same language modeling task: predict the next word using massive (unlabeled) datasets. Just, throw the text of 7,000 books at it and have it learn! Books are great for this sort of task since it allows the model to learn to associate related information even if they’re separated by a lot of text – something you don’t get for example, when you’re training with tweets, or articles.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="openai.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;"></div></center><script type="math/tex; mode=display">% <![CDATA[\begin{split}h_0 & =UW_e+W_p \\h_l & = transformer\_block(h_{l-1}) \\P(u) & = softmax(h_n W_e^T)\end{split} %]]></script><p>$W_e$ is the embedding matrix, $W_p$ is the positional embedding matrix(Note that it is different with classicial transformer)</p><h3 id="Fine-Tuning-with-OpenAI"><a href="#Fine-Tuning-with-OpenAI" class="headerlink" title="Fine-Tuning with OpenAI"></a>Fine-Tuning with OpenAI</h3><p>Now that the OpenAI transformer is pre-trained and its layers have been tuned to reasonably handle language, we can start using it for downstream tasks. Let’s first look at sentence classification (classify an email message as “spam” or “not spam”):</p><p>If our input sequence is $x_1,\cdots,x_m$, and the label is y. We can add a <code>softmax layer</code> to do classification and use the cross entrophy to calculate the loss.</p><script type="math/tex; mode=display">L_2(\mathcal{C})=\sum{x,y}logP(y|x^1,...,x^m)</script><p>In general, we should update the parameters to minimize the $L_2$, but we can use <code>Multi-task Learning</code> to get a more generalize model. Therefore we can get the max likelihood of $L3$</p><script type="math/tex; mode=display">L_3(\mathcal{C})=L_2(\mathcal{C})+\lambda \times L_1(\mathcal{C})</script><p>$L_1$ if the loss of previous language model.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="openai2.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">How to use a pre-trained OpenAI transformer to do sentence clasification</div></center><p>The OpenAI paper outlines a number of input transformations to handle the inputs for different types of tasks. The following image from the paper shows the structures of the models and input transformations to carry out different tasks.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="openai3.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">How to use a pre-trained OpenAI transformer to do different tasks</div></center><h2 id="BERT-From-Decoders-to-Encoders"><a href="#BERT-From-Decoders-to-Encoders" class="headerlink" title="BERT: From Decoders to Encoders"></a>BERT: From Decoders to Encoders</h2><p>The openAI transformer gave us a fine-tunable pre-trained model based on the Transformer. But something went missing in this transition from LSTMs to Transformers. ELMo’s language model was bi-directional, but the openAI transformer only trains a forward language model. Could we build a transformer-based model whose language model looks both forward and backwards (in the technical jargon – “is conditioned on both left and right context”)?</p><h3 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h3><p>The input representation of BERT is shown in the figure below. For example, the two sentences “my dog ​​is cute” and “he likes playing” are entered. I’ll explain why two sentences are needed later. Here, the two sentences similar to GPT are used. First, a special Token <code>[CLS]</code> is added at the beginning of the first sentence, and a <code>[SEP]</code> is added after the cute to indicate the end of the first sentence. After ##ing, A <code>[SEP]</code> will be added later. Note that the word segmentation here will divide “playing” into “play” and “##ing” two tokens. This method of dividing words into more fine-grained Word Pieces was introduced in the previous machine translation section. This is a kind of Common methods to resolve unregistered words. Then perform 3 Embeddings on each Token: </p><ol><li>Embedding of words;</li><li>Embedding of positions; </li><li>Embedding of segments. </li></ol><p>The word Embedding is familiar to everyone, and the position Embedding is similar to the word embedding, mapping a position (such as 2) into a low-dimensional dense vector. And Segment embedding has only two, either belong to the first sentence (segment) or belong to the second sentence. Segment Embedding of the same sentence is shared so that it can learn information belonging to different segments. For tasks such as sentiment classification, there is only one sentence, so the Segment id is always 0; for the Entailment task, the input is two sentences, so the Segment is 0 or 1.</p><p>The BERT model requires a fixed sequence length, such as 128. If it is not enough, then padding in the back, otherwise it will intercept the excess Token, so as to ensure that the input is a fixed-length Token sequence. The first token is always special <code>[CLS]</code>. It does not have any semantics, so it will (must) encode the semantics of the entire sentence (other words).</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="bert.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Input of Bert/div></div></center><h3 id="Masked-Language-Model"><a href="#Masked-Language-Model" class="headerlink" title="Masked Language Model"></a>Masked Language Model</h3><p>Finding the right task to train a Transformer stack of encoders is a complex hurdle that BERT resolves by adopting a <code>masked language model</code> concept from earlier literature (where it’s called a Cloze task).</p><p>Beyond masking 15% of the input, BERT also mixes things a bit in order to improve how the model later fine-tunes. Sometimes it randomly replaces a word with another word and asks the model to predict the correct word in that position.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="bert2.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">masked language model</div></center><h3 id="Two-sentence-Tasks"><a href="#Two-sentence-Tasks" class="headerlink" title="Two-sentence Tasks"></a>Two-sentence Tasks</h3><p>If you look back up at the input transformations the OpenAI transformer does to handle different tasks, you’ll notice that some tasks require the model to say something intelligent about two sentences (e.g. are they simply paraphrased versions of each other? Given a wikipedia entry as input, and a question regarding that entry as another input, can we answer that question?).</p><p>To make BERT better at handling relationships between multiple sentences, the pre-training process includes an additional task: Given two sentences (A and B), is B likely to be the sentence that follows A, or not?</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="bert3.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">The second task BERT is pre-trained on is a two-sentence classification task.</div></center><h3 id="Task-specific-Models"><a href="#Task-specific-Models" class="headerlink" title="Task specific-Models"></a>Task specific-Models</h3><p>The BERT paper shows a number of ways to use BERT for different tasks.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="bert4.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">different ways to use BERT</div></center><ol><li>For common classification tasks, the input is a sequence, as shown in the upper right of the figure. All tokens belong to the same Segment (Id = 0). We use the last layer of the first special token <code>[CLS]</code> to connect it. Softmax is used for classification, and classified data is used for Fine-Tuning.</li><li>For tasks such as similarity calculation that are input as two sequences, the process is shown in the upper left. The tokens of the two sequences correspond to different segments (Id = 0/1). We also use the last layer output of the first special token [CLS] to connect with softmax for classification, and then use the classification data for Fine-Tuning.</li><li>The third type is a question-and-answer type question, such as the SQuAD v1.1 dataset. The input is a question and a long paragraph containing the answer (Paragraph), and the output finds the answer to the question in this paragraph.</li><li>The forth type of task is sequence labeling, such as named entity recognition. The input is a sentence (Token sequence). Except for [CLS] and [SEP], there will be output tags at each moment. For example, B-PER indicates the beginning of a person’s name. </li></ol><h3 id="BERT-for-feature-extraction"><a href="#BERT-for-feature-extraction" class="headerlink" title="BERT for feature extraction"></a>BERT for feature extraction</h3><p>The fine-tuning approach isn’t the only way to use BERT. Just like ELMo, you can use the pre-trained BERT to create contextualized word embeddings. Then you can feed these embeddings to your existing model – a process the paper shows yield results not far behind fine-tuning BERT on a task such as named-entity recognition.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="bert5.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Feature extraction</div></center><p>Which vector works best as a contextualized embedding? I would think it depends on the task. The paper examines six choices (Compared to the fine-tuned model which achieved a score of 96.4):</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="bert6.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Feature extraction</div></center><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://fancyerii.github.io/2019/03/09/bert-theory/#词汇扩展" target="_blank" rel="noopener">http://fancyerii.github.io/2019/03/09/bert-theory/#词汇扩展</a></li><li><a href="https://zhuanlan.zhihu.com/p/63115885" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/63115885</a></li><li><a href="http://jalammar.github.io/illustrated-bert/" target="_blank" rel="noopener">http://jalammar.github.io/illustrated-bert/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;The-problems-of-RNN&quot;&gt;&lt;a href=&quot;#The-problems-of-RNN&quot; class=&quot;headerlink&quot; title=&quot;The problems of RNN&quot;&gt;&lt;/a&gt;The problems of RNN&lt;/h2&gt;&lt;ol&gt;

      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Transformer</title>
    <link href="https://zhangruochi.com/Transformer/2019/12/20/"/>
    <id>https://zhangruochi.com/Transformer/2019/12/20/</id>
    <published>2019-12-20T17:24:40.000Z</published>
    <updated>2019-12-21T18:20:04.625Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention.</p><p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations. End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and<br>language modeling tasks.</p><p>To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution. </p><h2 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h2><p>Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\text{model}}$.  We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.  In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to <a href="https://arxiv.org/abs/1608.05859" target="_blank" rel="noopener">(cite)</a>. In the embedding layers, we multiply those weights by $\sqrt{d_{\text{model}}}$.           </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure><h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>Since our model contains no recurrence and no convolution, <strong>in order for the model to make use of the order of the sequence</strong>, we must inject some information about the relative or absolute position of the tokens in the sequence.  To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks.  The positional encodings have the same dimension $d_{\text{model}}$ as the embeddings, so that the two can be summed.   There are many choices of positional encodings, learned and fixed <a href="https://arxiv.org/pdf/1705.03122.pdf" target="_blank" rel="noopener">(cite)</a>. </p><p>In this work, we use sine and cosine functions of different frequencies:    </p><script type="math/tex; mode=display">PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\text{model}}})</script><script type="math/tex; mode=display">PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\text{model}}})</script><p>where $pos$ is the position and $i$ is the dimension.  That is, each dimension of the positional encoding corresponds to a sinusoid.  The wavelengths form a geometric progression from $2\pi$ to $10000 \cdot 2\pi$.  We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. </p><p>In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.  For the base model, we use a rate of $P_{drop}=0.1$. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><h2 id="Layer-Norm-and-residual-connection"><a href="#Layer-Norm-and-residual-connection" class="headerlink" title="Layer Norm and residual connection"></a>Layer Norm and residual connection</h2><p>We employ a residual connection <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">(cite)</a> around each of the two sub-layers, followed by layer normalization <a href="https://arxiv.org/abs/1607.06450" target="_blank" rel="noopener">(cite)</a>.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure><p>That is, the output of each sub-layer is $\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))$, where $\mathrm{Sublayer}(x)$ is the function implemented by the sub-layer itself.  We apply dropout <a href="http://jmlr.org/papers/v15/srivastava14a.html" target="_blank" rel="noopener">(cite)</a> to the output of each sub-layer, before it is added to the sub-layer input and normalized.  </p><p>To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{\text{model}}=512$.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="string">"Apply residual connection to any sublayer with the same size."</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure><p>Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.</p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p>An attention function can be described as mapping a <code>query</code> and a set of <code>key</code>-<code>value</code> pairs to an output, where the query, keys, values, and output are all vectors.  The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p><p>We call our particular attention <code>Scaled Dot-Product Attention</code>.   The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.  We compute the dot products of the query with all keys, divide each by $\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="ModalNet-19.png" width="20%" height="20%"></center><p>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.   The keys and values are also packed together into matrices $K$ and $V$.  We compute the matrix of outputs as:                      </p><script type="math/tex; mode=display">\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V</script><p>The two most commonly used attention functions are additive attention <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">(cite)</a>, and dot-product (multiplicative) attention.  Dot-product attention is identical to our algorithm, except for the scaling factor of $\frac{1}{\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.  While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.</p><p>While for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ <a href="https://arxiv.org/abs/1703.03906" target="_blank" rel="noopener">(cite)</a>. We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients  (To illustrate why the dot products get large, assume that the components of $q$ and $k$ are independent random variables with mean $0$ and variance $1$.  Then their dot product, $q \cdot k = \sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance $d_k$.). To counteract this effect, we scale the dot products by $\frac{1}{\sqrt{d_k}}$.          </p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="attention.png" width="50%" height="50%"></center><h3 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h3><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="ModalNet-20.png" width="50%" height="50%"></center><script type="math/tex; mode=display">\mathrm{MultiHead}(Q, K, V) = \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O    \\                                               \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)</script><p>Where the projections are parameter matrices $W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}$ and $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$.                                                                                                                                                                                            In this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_k=d_v=d_{\text{model}}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="keyword">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">"Implements Figure 2"</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) "Concat" using a view and apply a final linear. </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br></pre></td></tr></table></figure><h3 id="Applications-of-Attention-in-our-Model"><a href="#Applications-of-Attention-in-our-Model" class="headerlink" title="Applications of Attention in our Model"></a>Applications of Attention in our Model</h3><p>The Transformer uses multi-head attention in three different ways:                                                        </p><ol><li><p>In <strong>encoder-decoder attention</strong> layers, the queries come from the previous decoder layer, and the memory keys and values come from the <code>output of the encoder</code>.   This allows every position in the decoder to attend over all positions in the input sequence.  This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as <a href="https://arxiv.org/abs/1609.08144" target="_blank" rel="noopener">(cite)</a>.    </p></li><li><p>The encoder contains self-attention layers.  In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.   Each position in the encoder can attend to all positions in the previous layer of the encoder.                                                   </p></li><li>Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.  We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.  We implement this inside of scaled dot-product attention by masking out (setting to $-\infty$) all values in the input of the softmax which correspond to illegal connections. </li></ol><h2 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h2><p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.  This consists of two linear transformations with a ReLU activation in between.</p><script type="math/tex; mode=display">\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2</script><p>While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.  The dimensionality of input and output is $d_{\text{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implements FFN equation."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>Most competitive neural sequence transduction models have an encoder-decoder structure <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">(cite)</a>. Here, the encoder maps an input sequence of symbol representations $(x_1, …, x_n)$ to a sequence of continuous representations $\mathbf{z} = (z_1, …, z_n)$. Given $\mathbf{z}$, the decoder then generates an output sequence $(y_1,…,y_m)$ of symbols one element at a time. At each step the model is auto-regressive <a href="https://arxiv.org/abs/1308.0850" target="_blank" rel="noopener">(cite)</a>, consuming the previously generated symbols as additional input when generating the next. </p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="ModalNet-21.png" width="50%" height="50%"></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many </span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Take in and process masked src and target sequences."</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Define standard linear + softmax generation step."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="Encoder-and-Decoder-Stacks"><a href="#Encoder-and-Decoder-Stacks" class="headerlink" title="Encoder and Decoder Stacks"></a>Encoder and Decoder Stacks</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>The encoder is composed of a stack of $N=6$ identical layers. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span><span class="params">(module, N)</span>:</span></span><br><span class="line">    <span class="string">"Produce N identical layers."</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Pass the input (and mask) through each layer in turn."</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Encoder is made up of self-attn and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (left) for connections."</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>The decoder is also composed of a stack of $N=6$ identical layers.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Generic N layer decoder with masking."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><p>In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.  Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (right) for connections."</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure></p><p>We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.  This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="string">"Mask out subsequent positions."</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br></pre></td></tr></table></figure><h2 id="Full-Model"><a href="#Full-Model" class="headerlink" title="Full Model"></a>Full Model</h2><p>Here we define a function from hyperparameters to a full model. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></li><li><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-transformer/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h2&gt;&lt;p&gt;The goal of reducing sequential comput
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Subword Models</title>
    <link href="https://zhangruochi.com/Subword-Models/2019/12/19/"/>
    <id>https://zhangruochi.com/Subword-Models/2019/12/19/</id>
    <published>2019-12-19T18:07:57.000Z</published>
    <updated>2019-12-19T21:04:03.603Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Character-Level-Models"><a href="#Character-Level-Models" class="headerlink" title="Character-Level Models"></a>Character-Level Models</h2><ol><li>Word embeddings can be composed from character embeddings<ul><li>Generates embeddings for unknown words</li><li>Similar spellings share similar embeddings</li><li>Solves OOV problem</li></ul></li><li>Motivation<ul><li>Derive a powerful,robust language model effective across a variety of languages.</li><li>Encode subword relatedness:eventful,eventfully, uneventful…</li><li>Address rare-word problem of prior models. </li><li>Obtain comparable expressivity with fewer parameters.</li></ul></li></ol><h2 id="Two-trends"><a href="#Two-trends" class="headerlink" title="Two trends"></a>Two trends</h2><ol><li>Same architecture as forword-level model but use smaller units: “word pieces”</li><li>Hybrid architectures: Main model has words; something else for characters</li></ol><h2 id="Byte-Pair-Encoding"><a href="#Byte-Pair-Encoding" class="headerlink" title="Byte Pair Encoding"></a>Byte Pair Encoding</h2><p>A word segmentation algorithm:</p><ul><li>Start with a vocabulary of characters</li><li>Most frequent ngram pairs -&gt; a new ngram</li><li>Have a target vocabulary size and stop when you reach it</li><li>Do deterministic longest piece segmentation of words</li><li>Segmentation is only within words identified by some prior tokenizer</li></ul><p>For example, all the words in our documents database and their frequency are</p><blockquote><p>{‘l o w’: 5, ‘l o w e r’: 2, ‘n e w e s t’: 6, ‘w i d e s t’: 3}</p></blockquote><p>We can initialize our vocabulary library as:  </p><blockquote><p>{ ‘l’, ‘o’, ‘w’, ‘e’, ‘r’, ‘n’, ‘w’, ‘s’, ‘t’, ‘i’, ‘d’}</p></blockquote><p>The most frequent ngram pair is (‘e’,’s’) and its count is 9. So we add the ‘es’ to our vocabulary library. </p><p>Our documents database now is:</p><blockquote><p>{‘l o w’: 5, ‘l o w e r’: 2, ‘n e w es t’: 6, ‘w i d es t’: 3}.</p></blockquote><p>Our vocabulary library now is:</p><blockquote><p>{ ‘l’, ‘o’, ‘w’, ‘e’, ‘r’, ‘n’, ‘w’, ‘s’, ‘t’, ‘i’, ‘d’, ‘es’}</p></blockquote><p><strong>Again</strong>, the most frequent ngram pair is (‘es’,’t’) and its count is 9，So we add the ‘est’ to our vocabulary library.</p><p>Our documents database now is: </p><blockquote><p>{‘l o w’: 5, ‘l o w e r’: 2, ‘n e w est’: 6, ‘w i d est’: 3}</p></blockquote><p>Our vocabulary library now is:</p><blockquote><p>{ ‘l’, ‘o’, ‘w’, ‘e’, ‘r’, ‘n’, ‘w’, ‘s’, ‘t’, ‘i’, ‘d’, ‘es’,’est’}</p></blockquote><p>the rest can be done in the same manner. We can set a threshold of total count of our vocabulary library. By doing so, we can use BPE to construct a vocabulary library to represent all the words based on subword unit.</p><p>Google NMT(GNMT) uses a variant of this:</p><ul><li>V1: wordpiece model (Word piece model tokenizes inside words)</li><li>V2: sentencepiece model (Sentence piece model works from raw text)</li></ul><h2 id="Character-level-to-build-word-level"><a href="#Character-level-to-build-word-level" class="headerlink" title="Character-level to build word-level"></a>Character-level to build word-level</h2><ol><li>Convolution over characters to generate word embeddings<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%"></center></li><li>Character-based LSTM to build word representation<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center></li></ol><h2 id="CS224n-Assignment5"><a href="#CS224n-Assignment5" class="headerlink" title="CS224n Assignment5"></a>CS224n Assignment5</h2><h3 id="Character-based-convolutional-encoder-for-NMT"><a href="#Character-based-convolutional-encoder-for-NMT" class="headerlink" title="Character-based convolutional encoder for NMT."></a>Character-based convolutional encoder for NMT.</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Figure from cs224n. Character-based convolutional encoder, which ultimately produces a word embedding of length $e_{word}$</div></center><ol><li>Convert word to character indices. We have a word $x$ (e.g. Anarchy in above figure) that we wish to represent. Assume we have a predefined ‘vocabulary’ of characters (for example, all lowercase letters, uppercase letters, numbers, and some punctuation). By looking up the index of each character, we can thus represent the length-l word x as a vector of integers:<script type="math/tex; mode=display">x = \left[ c_1,c_2,\cdots,c_l  \right ] \in \mathbb{Z}^{l}</script>where each $c_i$ is an integer index into the character vocabulary.</li><li><p>Padding and embedding lookup. Using a special <pad> ‘character’, we pad (or truncate) every word so that it has length $m_word$ (this is some predefined hyperparameter representing maximum word length):</pad></p><script type="math/tex; mode=display">x_{padded} = \left [ c_1,c_2,\cdots,c_{m_{word}}  \right ] \in \mathbb{Z}^{m_{word}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_sents_char</span><span class="params">(sents, char_pad_token)</span>:</span></span><br><span class="line">    <span class="string">""" Pad list of sentences according to the longest sentence in the batch and max_word_length.</span></span><br><span class="line"><span class="string">    @param sents (list[list[list[int]]]): list of sentences, result of `words2charindices()`</span></span><br><span class="line"><span class="string">        from `vocab.py`</span></span><br><span class="line"><span class="string">    @param char_pad_token (int): index of the character-padding token</span></span><br><span class="line"><span class="string">    @returns sents_padded (list[list[list[int]]]): list of sentences where sentences/words shorter</span></span><br><span class="line"><span class="string">        than the max length sentence/word are padded out with the appropriate pad token, such that</span></span><br><span class="line"><span class="string">        each sentence in the batch now has same number of words and each word has an equal</span></span><br><span class="line"><span class="string">        number of characters</span></span><br><span class="line"><span class="string">        Output shape: (batch_size, max_sentence_length, max_word_length)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Words longer than 21 characters should be truncated</span></span><br><span class="line">    max_word_length = <span class="number">21</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE for part 1f</span></span><br><span class="line">    <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">    <span class="comment">###     Perform necessary padding to the sentences in the batch similar to the pad_sents()</span></span><br><span class="line">    <span class="comment">###     method below using the padding character from the arguments. You should ensure all</span></span><br><span class="line">    <span class="comment">###     sentences have the same number of words and each word has the same number of</span></span><br><span class="line">    <span class="comment">###     characters.</span></span><br><span class="line">    <span class="comment">###     Set padding words to a `max_word_length` sized vector of padding characters.</span></span><br><span class="line">    <span class="comment">###</span></span><br><span class="line">    <span class="comment">###     You should NOT use the method `pad_sents()` below because of the way it handles</span></span><br><span class="line">    <span class="comment">###     padding and unknown words.</span></span><br><span class="line">    sents_padded = []</span><br><span class="line">    max_sent_len = max([len(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> sents])</span><br><span class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> sents:</span><br><span class="line">        sent = sent + [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(max_sent_len - len(sent))]</span><br><span class="line">        <span class="keyword">assert</span> len(sent) == max_sent_len</span><br><span class="line">        tmp_sent = []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sent:</span><br><span class="line">            word = word[:max_word_length]</span><br><span class="line">            diff = max_word_length - len(word)</span><br><span class="line">            word = word + [char_pad_token] * diff</span><br><span class="line">            <span class="keyword">assert</span> len(word) == max_word_length</span><br><span class="line">            tmp_sent.append(word)</span><br><span class="line">        sents_padded.append(tmp_sent)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sents_padded</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VocabEntry</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">words2charindices</span><span class="params">(self, sents)</span>:</span></span><br><span class="line">            <span class="string">""" Convert list of sentences of words into list of list of list of character indices.</span></span><br><span class="line"><span class="string">            @param sents (list[list[str]]): sentence(s) in words</span></span><br><span class="line"><span class="string">            @return word_ids (list[list[list[int]]]): sentence(s) in indices</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            <span class="comment">### YOUR CODE HERE for part 1e</span></span><br><span class="line">            <span class="comment">### <span class="doctag">TODO:</span> </span></span><br><span class="line">            <span class="comment">###     This method should convert characters in the input sentences into their </span></span><br><span class="line">            <span class="comment">###     corresponding character indices using the character vocabulary char2id </span></span><br><span class="line">            <span class="comment">###     defined above.</span></span><br><span class="line">            <span class="comment">###</span></span><br><span class="line">            <span class="comment">###     You must prepend each word with the `start_of_word` character and append </span></span><br><span class="line">            <span class="comment">###     with the `end_of_word` character. </span></span><br><span class="line"></span><br><span class="line">            word_ids = []</span><br><span class="line">            <span class="keyword">for</span> sent <span class="keyword">in</span> sents:</span><br><span class="line">                sent_chars_id = []</span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> sent:</span><br><span class="line">                    sent_chars_id.append([<span class="number">1</span>] + [ self.char2id.get(char,<span class="number">3</span>) <span class="keyword">for</span> char <span class="keyword">in</span> word ] + [<span class="number">2</span>])</span><br><span class="line">                word_ids.append(sent_chars_id)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> word_ids</span><br><span class="line"></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">to_input_tensor_char</span><span class="params">(self, sents: List[List[str]], device: torch.device)</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">""" Convert list of sentences (words) into tensor with necessary padding for </span></span><br><span class="line"><span class="string">        shorter sentences.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param sents (List[List[str]]): list of sentences (words)</span></span><br><span class="line"><span class="string">        @param device: device on which to load the tensor, i.e. CPU or GPU</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns sents_var: tensor of (max_sentence_length, batch_size, max_word_length)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 1g</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span> </span></span><br><span class="line">        <span class="comment">###     Connect `words2charindices()` and `pad_sents_char()` which you've defined in </span></span><br><span class="line">        <span class="comment">###     previous parts</span></span><br><span class="line">        char_sents =  self.words2charindices(sents)</span><br><span class="line">        padded_char_sents = pad_sents_char(char_sents, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        sents_var = torch.tensor(padded_char_sents, dtype=torch.int8, device= device)</span><br><span class="line">        sents_var = sents_var.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sents_var</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ......</span></span><br></pre></td></tr></table></figure></li><li><p>For each of these characters $c_i$, we lookup a dense character embedding (which has shape $e_{char}$). This yields a tensor $x_{emb}$:</p><script type="math/tex; mode=display">x_{emb} = CharEmbedding(X_{padded}) \in \mathbb{R}^{m_{word} \times e_{char}}</script><p>We’ll reshape $x_{emb}$ to obtain $x_{reshaped} in \mathbb{R}^{e_{char} \times m_{word}}$ before feeding into the convolutional network.</p></li><li><p><strong>Convolutional network</strong>. To combine these character embeddings, we’ll use 1-dimensional convolutions. The convolutional layer has two hyperparameters: the kernel size $k$ (also called window size), which dictates the size of the window used to compute features, and the number of filters $f$, (also called number of output features or number of output channels). The convolutional layer has a weight matrix $W \in \mathbb{R}^{f \times e_{char} \times k}$ and a bias vector $b \in \mathbb{R}^{f}$. Overall this produces output $x_{conv}$.</p><script type="math/tex; mode=display">x_{conv} = Conv1D(x_{reshaped}) \in \mathbb{R}^{f \times {m_{word} - k + 1}}</script><p>For our application, we’ll set $f$ to be equal to $e_{word}$, the size of the final word embedding for word x. Therefore,</p><script type="math/tex; mode=display">x_{conv} \in \mathbb{R}^{e_{word} \times (m_{word} - k + 1)}</script><p>Finally, we apply the <code>ReLU</code> function to $x_{conv}$, then use max-pooling to reduce this to a single vector $x_{conv_out} \in \mathbb{R}^{e_{word}}$, which is the final output of the Convolutional Network:</p><script type="math/tex; mode=display">x_{conv\_out} = MaxPool(ReLU(x_{conv})) \in \mathbb{R}^{e_{word}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### YOUR CODE HERE for part 1i</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" </span></span><br><span class="line"><span class="string">        Init CNN which is a 1-D cnn.</span></span><br><span class="line"><span class="string">        @param embed_size (int): embedding size of char (dimensionality)</span></span><br><span class="line"><span class="string">        @param k: kernel size, also called window size</span></span><br><span class="line"><span class="string">        @param f: number of filters, should be embed_size of word</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">            embed_size: int = <span class="number">50</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 m_word: int = <span class="number">21</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 k: int = <span class="number">5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 f: int = <span class="number">50</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        Init CNN which is a 1-D cnn.</span></span><br><span class="line"><span class="string">        @param embed_size (int): embedding size of char (dimensionality)</span></span><br><span class="line"><span class="string">        @param k: kernel size, also called window size</span></span><br><span class="line"><span class="string">        @param f: number of filters, should be embed_size of word</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        super(CNN, self).__init__()</span><br><span class="line">        self.conv1d = nn.Conv1d(in_channels = embed_size, </span><br><span class="line">                    out_channels = f,</span><br><span class="line">                    kernel_size = k)</span><br><span class="line"></span><br><span class="line">        self.maxpool = nn.MaxPool1d(kernel_size = m_word - k + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X_reshaped: torch.Tensor)</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        map from X_reshaped to X_conv_out</span></span><br><span class="line"><span class="string">        @param X_reshaped (Tensor): Tensor of char-level embedding with shape ( </span></span><br><span class="line"><span class="string">                                    batch_size, e_char, m_word), where e_char = embed_size of char, </span></span><br><span class="line"><span class="string">                                    m_word = max_word_length.</span></span><br><span class="line"><span class="string">        @return X_conv_out (Tensor): Tensor of word-level embedding with shape (max_sentence_length,</span></span><br><span class="line"><span class="string">                                    batch_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        X_conv = self.conv1d(X_reshaped)</span><br><span class="line">        <span class="comment"># print(X_conv.size())</span></span><br><span class="line">        X_conv_out = self.maxpool(F.relu(X_conv))</span><br><span class="line">        <span class="comment"># print(X_conv_out.size())</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X_conv_out.squeeze(<span class="number">-1</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>Highway layer and dropout</strong>. Highway Networks6 have a skip-connection controlled by a dynamic gate. Given the input $x_{conv\_out} \in \mathbb{R}^{e_{word}}$, we compute:<br><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%"> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Figure from cs224n. Highway Network (Srivastava et al. 2015)</div></center></p><script type="math/tex; mode=display">\begin{align}& x_{proj} = RELU(W_{proj}x_{conv\_cout} + b_{proj}) \quad \in \mathbb{R}^{e_{word}} \\& x_{gate} = \sigma(W_{gate}x_{conv\_out} + b_{gate}) \quad \in \mathbb{R}^{e_{word}} \\& x_{highway} = x_{gate} \circ x_{proj} + ( 1 - x_{gate}) \circ x_{conv\_out}  \quad \in \mathbb{R}^{e_{word}}\\& x_{word_emb} = Dropout(x_{highway}) \quad \in \mathbb{R}^{e_{word}} \end{align}</script><p>Where $W_{proj},W_{gate} \in \mathbb{R}^{e_{word} \times e_{word}}$, and $\circ$ denotes element-wise multiplication.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment">### YOUR CODE HERE for part 1h</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Highway</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" Highway network for ConvNN</span></span><br><span class="line"><span class="string">        - Relu</span></span><br><span class="line"><span class="string">        - Sigmoid</span></span><br><span class="line"><span class="string">        - gating mechanism from LSTM</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,embed_size)</span>:</span></span><br><span class="line">        <span class="string">""" Init Higway network</span></span><br><span class="line"><span class="string">            @param embed_size (int): Embedding size of word, in handout, </span></span><br><span class="line"><span class="string">                                     it's e_&#123;word&#125; (dimensionality)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        super(Highway, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.projection = nn.Linear(embed_size,embed_size,bias = <span class="keyword">True</span>)</span><br><span class="line">        self.gate = nn.Linear(embed_size,embed_size, bias = <span class="keyword">True</span>)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X_conv_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            Take mini-batch of sentence of ConvNN</span></span><br><span class="line"><span class="string">            @param X_conv_out (Tensor): Tensor with shape (max_sentence_length, batch_size, embed_size)</span></span><br><span class="line"><span class="string">            @return X_highway (Tensor): combinded output with shape (max_sentence_length, batch_size, embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        X_proj = F.relu(self.projection(X_conv_out))</span><br><span class="line">        X_gate = torch.sigmoid(self.gate(X_conv_out))</span><br><span class="line">        X_highway =  torch.mul(X_gate, X_proj) + torch.mul((<span class="number">1</span> - X_gate),X_conv_out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X_highway</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### END YOUR CODE</span></span><br></pre></td></tr></table></figure></li><li><p>Combine above steps together to get our <strong>Character-based word embedding model</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 5</span></span><br><span class="line"><span class="string">model_embeddings.py: Embeddings for the NMT model</span></span><br><span class="line"><span class="string">Pencheng Yin &lt;pcyin@cs.cmu.edu&gt;</span></span><br><span class="line"><span class="string">Sahil Chopra &lt;schopra8@stanford.edu&gt;</span></span><br><span class="line"><span class="string">Anand Dhoot &lt;anandd@stanford.edu&gt;</span></span><br><span class="line"><span class="string">Michael Hahn &lt;mhahn2@stanford.edu&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not change these imports; your module names should be</span></span><br><span class="line"><span class="comment">#   `CNN` in the file `cnn.py`</span></span><br><span class="line"><span class="comment">#   `Highway` in the file `highway.py`</span></span><br><span class="line"><span class="comment"># Uncomment the following two imports once you're ready to run part 1(j)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> cnn <span class="keyword">import</span> CNN</span><br><span class="line"><span class="keyword">from</span> highway <span class="keyword">import</span> Highway</span><br><span class="line"></span><br><span class="line"><span class="comment"># End "do not change" </span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelEmbeddings</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Class that converts input words to their CNN-based embeddings.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embed_size, vocab)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Init the Embedding layer for one language</span></span><br><span class="line"><span class="string">        @param embed_size (int): Embedding size (dimensionality) for the output </span></span><br><span class="line"><span class="string">        @param vocab (VocabEntry): VocabEntry object. See vocab.py for documentation.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ModelEmbeddings, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment">## A4 code</span></span><br><span class="line">        <span class="comment"># pad_token_idx = vocab.src['&lt;pad&gt;']</span></span><br><span class="line">        <span class="comment"># self.embeddings = nn.Embedding(len(vocab.src), embed_size, padding_idx=pad_token_idx)</span></span><br><span class="line">        <span class="comment">## End A4 code</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 1j</span></span><br><span class="line">        pad_token_idx = vocab.char2id[<span class="string">'&lt;pad&gt;'</span>]</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        char_embed_size = <span class="number">50</span></span><br><span class="line">        self.char_embedding = nn.Embedding(len(vocab.char2id),</span><br><span class="line">                                           char_embed_size,</span><br><span class="line">                                           pad_token_idx)</span><br><span class="line">        self.convNN = CNN(f=self.embed_size)</span><br><span class="line">        self.highway = Highway(embed_size=self.embed_size)</span><br><span class="line">        self.dropout = nn.Dropout(p=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Looks up character-based CNN embeddings for the words in a batch of sentences.</span></span><br><span class="line"><span class="string">        @param input: Tensor of integers of shape (sentence_length, batch_size, max_word_length) where</span></span><br><span class="line"><span class="string">            each integer is an index into the character vocabulary</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param output: Tensor of shape (sentence_length, batch_size, embed_size), containing the </span></span><br><span class="line"><span class="string">            CNN-based embeddings for each word of the sentences in the batch</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">## A4 code</span></span><br><span class="line">        <span class="comment"># output = self.embeddings(input)</span></span><br><span class="line">        <span class="comment"># return output</span></span><br><span class="line">        <span class="comment">## End A4 code</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 1j</span></span><br><span class="line">        X_word_emb_list = []</span><br><span class="line">        <span class="keyword">for</span> X_padded <span class="keyword">in</span> input:</span><br><span class="line">            <span class="comment"># (batch_size,max_word_length) -&gt; (batch_size,max_word_length,embed_size)</span></span><br><span class="line">            X_emb = self.char_embedding(X_padded)</span><br><span class="line">            <span class="comment"># print(X_emb.size())</span></span><br><span class="line">            X_reshaped = X_emb.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">            X_conv_out = self.convNN(X_reshaped)</span><br><span class="line">            X_highway = self.highway(X_conv_out)</span><br><span class="line">            X_word_emb = self.dropout(X_highway)</span><br><span class="line">            X_word_emb_list.append(X_word_emb)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (sentence_length, batch_size, embed_size)</span></span><br><span class="line">        X_word_emb = torch.stack(X_word_emb_list, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X_word_emb</span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="Character-based-LSTM-decoder-for-NMT"><a href="#Character-based-LSTM-decoder-for-NMT" class="headerlink" title="Character-based LSTM decoder for NMT"></a>Character-based LSTM decoder for NMT</h3><p>We will now add a LSTM-based character-level decoder to our NMT system. The main idea is that when our word-level decoder produces and <code>&lt;UNK&gt;</code> token, we run our character-level decoder (which you can think of as a character-level conditional language model) to instead generate the target word one character at a time, as shown in below figure. This will help us to produce rare and out-of-vocabulary target words.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Figure from cs224n. A character-based decoder which is triggered if the word-based decoder produces an UNK. Figure courtesy of Luong & Manning.</div></center><p>We now describe the model in three sections:</p><ol><li><p><strong>Forward computation of Character Decoder</strong>: Given a sequence of integers $x_i,\cdots,x_n \in \mathbb{Z}$ representing a sequence of characters, we lookup their character embeddings $x_i,\cdots,x_n \in \mathbb{Z}^{e_{char}}$ and pass these as input in to the(unidirectional)LSTM,obtaining hidden states $h1, \cdots, h_n$ and cell states $c_1, \cdots, c_n$</p><script type="math/tex; mode=display">h_t,c_t = CharDecoderLSTM(x_t,h_{t-1},c_{t-1}) \quad \text{where} \quad h_t,c_t \in \mathbb{R}^{h}</script><p>where h is the hidden size of the CharDecoderLSTM. The initial hidden and cell states $h_0$ and $c_0$ are both set to the <strong>combined output</strong> vector (attentioned) for the current timestep of the main word-level NMT decoder.<br>For every timestep $t \in { 1, \cdots, n }$ we compute scores (also called logits) $s_t \in \mathbb{R}^{V_{char}}$</p><script type="math/tex; mode=display">s_t = W_{dec}h_t + b_{dec} \in \mathbb{R}^{V_{char}}</script><p>Where the weight matrix $W_{dec} \in \mathbb{R}^{V_{char} \times h}$ and the bias vector $b_{dec} \in \mathbb{R}^{V_{char}}$. If we passed $s_t$ through a softmax function, we would have the probability distribution for the next character in the sequence.</p></li><li><p><strong>Training of Character Decoder</strong> When we train the NMT system, we train the character decoder on <strong>every word</strong> in the target sentence (not just the words represented by <unk>). For example, on a particular step of the main NMT decoder, if the target word is music then the input sequence for the CharDecoderLSTM is $[x_1,…,x_n]$ = [<start>,m,u,s,i,c] and the target sequence for the CharDecoderLSTM is $[x_{2}, . . . , x_{n+1}]$ = [m,u,s,i,c,<end>].<br>We pass the input sequence $x_1, \cdots, x_n$, along with the initial states $h_0$ and $c_0$ obtained from the combined output vector) into the CharDecoderLSTM, thus obtaining scores $s_1,\cdots, s_n$ which we will compare to the target sequence $x_2,\cdots, x_{n+1}$. We optimize with respect to sum of the cross-entropy loss:</end></start></unk></p><script type="math/tex; mode=display">p_t = softmax(s_t) \in \mathbb{R}^{V_{char}}</script><script type="math/tex; mode=display">loss_{char\_dec} = -\sum_{t=1}^{n}log p_t(x_{t+1})</script></li><li><p><strong>Decoding from the Character Decoder</strong> t test time, first we produce a translation from our word- based NMT system in the usual way (e.g. a decoding algorithm like beam search). If the translation contains any <unk> tokens, then for each of those positions, we use the word-based decoder’s combined output vector to initialize the CharDecoderLSTM initial $h_0$ and $c_0$, then use CharDecoderLSTM to generate a sequence of characters. To generate the sequence of characters, we use the greedy decoding algorithm, which repeatedly chooses the most probable next character, until either the <end> token is produced or we reach a predetermined max length. The algorithm is given below, for a single example (not batched).</end></unk></p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="100%" height="100%"> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Figure from cs224n. Greedy Decoding</div></center></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CharDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, char_embedding_size=<span class="number">50</span>, target_vocab=None)</span>:</span></span><br><span class="line">        <span class="string">""" Init Character Decoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param hidden_size (int): Hidden size of the decoder LSTM</span></span><br><span class="line"><span class="string">        @param char_embedding_size (int): dimensionality of character embeddings</span></span><br><span class="line"><span class="string">        @param target_vocab (VocabEntry): vocabulary for the target language. See vocab.py for documentation.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 2a</span></span><br><span class="line">        <span class="comment">### TODO - Initialize as an nn.Module.</span></span><br><span class="line">        <span class="comment">###      - Initialize the following variables:</span></span><br><span class="line">        <span class="comment">###        self.charDecoder: LSTM. Please use nn.LSTM() to construct this.</span></span><br><span class="line">        <span class="comment">###        self.char_output_projection: Linear layer, called W_&#123;dec&#125; and b_&#123;dec&#125; in the PDF</span></span><br><span class="line">        <span class="comment">###        self.decoderCharEmb: Embedding matrix of character embeddings</span></span><br><span class="line">        <span class="comment">###        self.target_vocab: vocabulary for the target language</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Hint: - Use target_vocab.char2id to access the character vocabulary for the target language.</span></span><br><span class="line">        <span class="comment">###       - Set the padding_idx argument of the embedding matrix.</span></span><br><span class="line">        <span class="comment">###       - Create a new Embedding layer. Do not reuse embeddings created in Part 1 of this assignment.</span></span><br><span class="line">        </span><br><span class="line">        super(CharDecoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.char_embedding_size = char_embedding_size</span><br><span class="line">        self.target_vocab = target_vocab</span><br><span class="line">        self.padding_idx = self.target_vocab.char2id[<span class="string">'&lt;pad&gt;'</span>]</span><br><span class="line">        self.decoderCharEmb = nn.Embedding(len(self.target_vocab.char2id),</span><br><span class="line">                                           char_embedding_size,</span><br><span class="line">                                           self.padding_idx)</span><br><span class="line">        self.charDecoder = nn.LSTM(input_size=char_embedding_size,</span><br><span class="line">                                   hidden_size=hidden_size)</span><br><span class="line">        self.char_output_projection = nn.Linear(hidden_size,</span><br><span class="line">                                                len(self.target_vocab.char2id))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, dec_hidden=None)</span>:</span></span><br><span class="line">        <span class="string">""" Forward pass of character decoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param input: tensor of integers, shape (length, batch)</span></span><br><span class="line"><span class="string">        @param dec_hidden: internal state of the LSTM before reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns scores: called s_t in the PDF, shape (length, batch, self.vocab_size)</span></span><br><span class="line"><span class="string">        @returns dec_hidden: internal state of the LSTM after reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 2b</span></span><br><span class="line">        <span class="comment">### TODO - Implement the forward pass of the character decoder.</span></span><br><span class="line">        char_embeddings = self.decoderCharEmb(input)        <span class="comment"># (length, batch, char_embed_size)</span></span><br><span class="line">        hidden_states, dec_hidden = self.charDecoder(</span><br><span class="line">            char_embeddings, dec_hidden)    <span class="comment"># (length, batch, hidden_size)</span></span><br><span class="line">        scores = self.char_output_projection(hidden_states)     <span class="comment"># (len, batch, vocab)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> scores, dec_hidden</span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_forward</span><span class="params">(self, char_sequence, dec_hidden=None)</span>:</span></span><br><span class="line">        <span class="string">""" Forward computation during training.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param char_sequence: tensor of integers, shape (length, batch). Note that "length" here and in forward() need not be the same.</span></span><br><span class="line"><span class="string">        @param dec_hidden: initial internal state of the LSTM, obtained from the output of the word-level decoder. A tuple of two tensors of shape (1, batch, hidden_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns The cross-entropy loss, computed as the *sum* of cross-entropy losses of all the words in the batch.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 2c</span></span><br><span class="line">        <span class="comment">### TODO - Implement training forward pass.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Hint: - Make sure padding characters do not contribute to the cross-entropy loss.</span></span><br><span class="line">        <span class="comment">###       - char_sequence corresponds to the sequence x_1 ... x_&#123;n+1&#125; from the handout (e.g., &lt;START&gt;,m,u,s,i,c,&lt;END&gt;).</span></span><br><span class="line">        scores, dec_hidden = self.forward(char_sequence[:<span class="number">-1</span>], dec_hidden)</span><br><span class="line">        <span class="comment"># char_embeddings = self.decoderCharEmb(char_sequence)</span></span><br><span class="line">        <span class="comment"># hidden_states, dec_hidden = self.charDecoder(char_embeddings[:-1], dec_hidden)</span></span><br><span class="line">        <span class="comment"># scores = self.char_output_projection(hidden_states)  # (len, batch, vocab)</span></span><br><span class="line">        loss = nn.CrossEntropyLoss(ignore_index=self.padding_idx,</span><br><span class="line">                                   reduction=<span class="string">'sum'</span>)</span><br><span class="line">        ce_loss = loss(scores.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">                       char_sequence[<span class="number">1</span>:].transpose(<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ce_loss</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode_greedy</span><span class="params">(self, initialStates, device, max_length=<span class="number">21</span>)</span>:</span></span><br><span class="line">        <span class="string">""" Greedy decoding</span></span><br><span class="line"><span class="string">        @param initialStates: initial internal state of the LSTM, a tuple of two tensors of size (1, batch, hidden_size)</span></span><br><span class="line"><span class="string">        @param device: torch.device (indicates whether the model is on CPU or GPU)</span></span><br><span class="line"><span class="string">        @param max_length: maximum length of words to decode</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns decodedWords: a list (of length batch) of strings, each of which has length &lt;= max_length.</span></span><br><span class="line"><span class="string">                              The decoded strings should NOT contain the start-of-word and end-of-word characters.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 2d</span></span><br><span class="line">        <span class="comment">### TODO - Implement greedy decoding.</span></span><br><span class="line">        <span class="comment">### Hints:</span></span><br><span class="line">        <span class="comment">###      - Use target_vocab.char2id and target_vocab.id2char to convert between integers and characters</span></span><br><span class="line">        <span class="comment">###      - Use torch.tensor(..., device=device) to turn a list of character indices into a tensor.</span></span><br><span class="line">        <span class="comment">###      - We use curly brackets as start-of-word and end-of-word characters. That is, use the character '&#123;' for &lt;START&gt; and '&#125;' for &lt;END&gt;.</span></span><br><span class="line">        <span class="comment">###        Their indices are self.target_vocab.start_of_word and self.target_vocab.end_of_word, respectively.</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">        output_words = []</span><br><span class="line">        decodedWords = []</span><br><span class="line">        start_idx = self.target_vocab.start_of_word</span><br><span class="line">        end_idx = self.target_vocab.end_of_word</span><br><span class="line">        dec_hidden = initialStates</span><br><span class="line">        batch_size = dec_hidden[<span class="number">0</span>].shape[<span class="number">1</span>]</span><br><span class="line">        current_char = torch.tensor([[start_idx] * batch_size],</span><br><span class="line">                                    device=device)  <span class="comment"># idx of '&lt;start&gt;' token</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(max_length):</span><br><span class="line">            scores, dec_hidden = self.forward(current_char, dec_hidden)</span><br><span class="line">            current_char = scores.argmax(<span class="number">-1</span>)</span><br><span class="line">            output_words += [current_char]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        output_words = torch.cat(output_words).t().tolist()</span><br><span class="line">        <span class="keyword">for</span> foo <span class="keyword">in</span> output_words:</span><br><span class="line">            word = <span class="string">""</span></span><br><span class="line">            <span class="keyword">for</span> bar <span class="keyword">in</span> foo:</span><br><span class="line">                <span class="keyword">if</span> bar == end_idx:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                word += self.target_vocab.id2char[bar]</span><br><span class="line">            decodedWords += [word]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> decodedWords</span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Course note and slides of <a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">cs224n</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Character-Level-Models&quot;&gt;&lt;a href=&quot;#Character-Level-Models&quot; class=&quot;headerlink&quot; title=&quot;Character-Level Models&quot;&gt;&lt;/a&gt;Character-Level Mode
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Attention</title>
    <link href="https://zhangruochi.com/Attention/2019/12/16/"/>
    <id>https://zhangruochi.com/Attention/2019/12/16/</id>
    <published>2019-12-17T01:55:07.000Z</published>
    <updated>2019-12-19T18:10:20.263Z</updated>
    
    <content type="html"><![CDATA[<h2 id="General-definition-of-attention"><a href="#General-definition-of-attention" class="headerlink" title="General definition of attention"></a>General definition of attention</h2><p>Given a set of vector <strong>values</strong>, and a vector <strong>query</strong>, attention is a technique to compute a <strong>weighted sum</strong> of the values, dependent on the query.</p><ul><li>We sometimes say that the query attends to the values.</li><li>For example, in the seq2seq + attention model, each decoder hidden state (query) attends to all the encoder hidden states<br>75 (values).<ul><li>The weighted sum is a <strong>selective</strong> summary of the information contained in the values, where the query determines which values to focus on.</li><li>Attention is a way to obtain a <strong>fixed-size representation</strong> of an arbitrary set of representations (the values), dependent on some other representation (the query).</li></ul></li></ul><h2 id="How-to-do-attention"><a href="#How-to-do-attention" class="headerlink" title="How to do attention"></a>How to do attention</h2><ol><li>We have some <strong>values</strong> $h1$,$\cdots$,$h_N$ $\in \mathbb{R}^{d_1}$ and a <strong>query</strong> $s \in \mathbb{R}^{d_2}$</li><li>Computing the attention scores (multiple ways to do this)<script type="math/tex; mode=display">e \in \mathbb{R}^{N}</script></li><li>Taking softmax to get attention distribution $\alpha$<script type="math/tex; mode=display">\alpha = softmax(e) \in \mathbb{R}^{N}</script></li><li>Using attention distribution to take weighted sum of values:<script type="math/tex; mode=display">a = \sum_{i=1}^{N}\alpha_i h_i \in \mathbb{R}^{d_1}</script>thus obtaining the attention output a (sometimes called the <strong>context vector</strong>)</li></ol><h2 id="Bidirectional-RNNs"><a href="#Bidirectional-RNNs" class="headerlink" title="Bidirectional RNNs"></a>Bidirectional RNNs</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture notes of cs224n</div>    <br>    <br></center><p>Bidirectional RNNs fix this problem by traversing a sequence in both directions and concatenating the resulting outputs (both cell outputs and final hidden states). For every RNN cell, we simply add another cell but feed inputs to it in the opposite direction; the output $O_t$ corresponding to the $t\prime$ word is the concatenated vector $\left [ o_t^{(f)}, o_t^{(b)}  \right ]$ where $o_t^{(f)}$ is the output of the forward-direction RNN on word t and $o_t^{(b)}$ is the corresponding output from the reverse-direction RNN. Similarly, the final hidden state is $h = \left [   h^{(f)}, h^{(b)}  \right ]$.</p><h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><p>Sequence-to-sequence, or “Seq2Seq”, is a relatively new paradigm,with its first published usage in 2014 for English-French translation. At a high level, a sequence-to-sequence model is an end-to-end model made up of two recurrent neural networks:<br>Sutskever et al. 2014, “Sequence to Sequence Learning with Neural Networks”</p><ul><li>an encoder, which takes the model’s input sequence as input and encodes it into a fixed-size “context vector”</li><li>a decoder, which uses the context vector from above as a “seed” from which to generate an output sequence.<br>For this reason, Seq2Seq models are often referred to as “encoder- decoder models.” We’ll look at the details of these two networks separately.</li></ul><h3 id="Seq2Seq-architecture-encoder"><a href="#Seq2Seq-architecture-encoder" class="headerlink" title="Seq2Seq architecture - encoder"></a>Seq2Seq architecture - encoder</h3><blockquote><p>Encoder RNN produces an encoding of the source sentence.</p></blockquote><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture notes of cs224n</div>    <br>    <br></center><p>The encoder network’s job is to read the input sequence to our Seq2Seq model and generate a fixed-dimensional context vector <strong>C</strong> for the sequence. To do so, the encoder will use a recurrent neural network cell – usually an LSTM – to read the input tokens one at a time. The final hidden state of the cell will then become C. However, because it’s so difficult to compress an arbitrary-length sequence into a single fixed-size vector (especially for difficult tasks like transla- tion), the encoder will usually consist of stacked LSTMs: a series of LSTM “layers” where each layer’s outputs are the input sequence to the next layer. The final layer’s LSTM hidden state will be used as <strong>C</strong>.</p><p>Seq2Seq encoders will often do something strange: they will pro- cess the input sequence in reverse. This is actually done on purpose. The idea is that, by doing this, the last thing that the encoder sees will (roughly) corresponds to the first thing that the model outputs; this makes it easier for the decoder to “get started” on the output, which makes then gives the decoder an easier time generating a proper output sentence. In the context of translation, we’re allowing the network to translate the first few words of the input as soon as it sees them; once it has the first few words translated correctly, it’s much easier to go on to construct a correct sentence than it is to do so from scratch.</p><h3 id="Seq2Seq-architecture-decoder"><a href="#Seq2Seq-architecture-decoder" class="headerlink" title="Seq2Seq architecture - decoder"></a>Seq2Seq architecture - decoder</h3><blockquote><p>Decoder RNN is a Language Model that generates target sentence, conditioned on encoding.</p></blockquote><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture notes of cs224n</div>    <br>    <br></center><p>The decoder is also an LSTM network, but its usage is a little more complex than the encoder network. Essentially, we’d like to use it as a <strong>language model</strong> that’s “aware” of the words that it’s generated so far and of the input. To that end, we’ll keep the “stacked” LSTM architecture from the encoder, but we’ll initialize the hidden state of our first layer with the context vector from above; the decoder will literally use the context of the input to generate an output.</p><p>Once the decoder is set up with its context, we’ll pass in a special token to signify the start of output generation; in literature, this is usually an <eos> token appended to the end of the input (there’s also one at the end of the output). Then, we’ll run all three layers of LSTM, one after the other, following up with a softmax on the final layer’s output to generate the first output word. Then, we pass that word into the first layer, and repeat the generation. This is how we get the LSTMs to act like a language model. See Fig. 2 for an example of a decoder network.</eos></p><p>Once we have the output sequence, we use the same learning strat- egy as usual. We define a loss, the cross entropy on the prediction sequence, and we minimize it with a gradient descent algorithm and back-propagation. Both the encoder and decoder are trained at the same time, so that they both learn the same context vector represen- tation.</p><h2 id="Training-a-Neural-Machine-Translation-system"><a href="#Training-a-Neural-Machine-Translation-system" class="headerlink" title="Training a Neural Machine Translation system"></a>Training a Neural Machine Translation system</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture notes of cs224n</div>    <br>    <br></center><h3 id="Greedy-Search"><a href="#Greedy-Search" class="headerlink" title="Greedy Search"></a>Greedy Search</h3><p>At each time step, we pick the most probable token. In other words</p><script type="math/tex; mode=display">x_t = argmax_{\tilde{x_t} \mathbb{P}(\tilde(x_t)| x_1, \cdots, x_t)}</script><p>This technique is efficient and natural, however it explores a small part of the search space and if we make a mistake at one time step, the rest of the sentence could be heavily impacted.</p><h3 id="Beam-search-decoding"><a href="#Beam-search-decoding" class="headerlink" title="Beam search decoding"></a>Beam search decoding</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture notes of cs224n</div>    <br>    <br></center><p>the idea is to maintain K candidates at each time step.</p><script type="math/tex; mode=display">H_t = \left\{ (x_1^{1}, \cdots, x_t^1), \cdots, (x_1^k, \cdots, x_t^k) \right\}</script><p>and compute $H_{t+1}$ by expanding $H_t$ and keeping the best K candi- dates. In other words, we pick the best K sequence in the following set</p><script type="math/tex; mode=display">\tilde{H_{t+1}} = \cup_{k=1}^{k}H_{t+1}^{\tilde{k}}</script><p>where</p><script type="math/tex; mode=display">\tilde{H_t} = \left\{ (x_1^{k}, \cdots, x_t^{k}, v_1), \cdots, (x_1^{k}, \cdots, x_t^{k}, V_{|v|}) \right\}</script><p>As we increase K, we gain precision and we are asymptotically exact. However, the improvement is not monotonic and we can set a K that combines reasonable performance and computational efficiency. </p><h2 id="CS224n-Assignment4"><a href="#CS224n-Assignment4" class="headerlink" title="CS224n Assignment4"></a>CS224n Assignment4</h2><p>In Machine Translation, our goal is to convert a sentence from the source language (e.g. Spanish) to the target language (e.g. English). In this assignment, we will implement a sequence-to-sequence (Seq2Seq) network with attention, to build a Neural Machine Translation (NMT) system. In this section, we describe the training procedure for the proposed NMT system, which uses a Bidirectional LSTM Encoder and a Unidirectional LSTM Decoder.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture notes of cs224n</div>    <br>    <br></center><h3 id="Initialize"><a href="#Initialize" class="headerlink" title="Initialize"></a>Initialize</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embed_size, hidden_size, vocab, dropout_rate=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        <span class="string">""" Init NMT Model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param embed_size (int): Embedding size (dimensionality)</span></span><br><span class="line"><span class="string">        @param hidden_size (int): Hidden Size (dimensionality)</span></span><br><span class="line"><span class="string">        @param vocab (Vocab): Vocabulary object containing src and tgt languages</span></span><br><span class="line"><span class="string">                              See vocab.py for documentation.</span></span><br><span class="line"><span class="string">        @param dropout_rate (float): Dropout probability, for attention</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(NMT, self).__init__()</span><br><span class="line">        self.model_embeddings = ModelEmbeddings(embed_size, vocab)</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.dropout_rate = dropout_rate</span><br><span class="line">        self.vocab = vocab</span><br><span class="line"></span><br><span class="line">        <span class="comment"># default values</span></span><br><span class="line">        self.encoder = <span class="keyword">None</span> </span><br><span class="line">        self.decoder = <span class="keyword">None</span></span><br><span class="line">        self.h_projection = <span class="keyword">None</span></span><br><span class="line">        self.c_projection = <span class="keyword">None</span></span><br><span class="line">        self.att_projection = <span class="keyword">None</span></span><br><span class="line">        self.combined_output_projection = <span class="keyword">None</span></span><br><span class="line">        self.target_vocab_projection = <span class="keyword">None</span></span><br><span class="line">        self.dropout = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~8 Lines)</span></span><br><span class="line">        <span class="comment">### TODO - Initialize the following variables:</span></span><br><span class="line">        <span class="comment">###     self.encoder (Bidirectional LSTM with bias)</span></span><br><span class="line">        <span class="comment">###     self.decoder (LSTM Cell with bias)</span></span><br><span class="line">        <span class="comment">###     self.h_projection (Linear Layer with no bias), called W_&#123;h&#125; in the PDF.</span></span><br><span class="line">        <span class="comment">###     self.c_projection (Linear Layer with no bias), called W_&#123;c&#125; in the PDF.</span></span><br><span class="line">        <span class="comment">###     self.att_projection (Linear Layer with no bias), called W_&#123;attProj&#125; in the PDF.</span></span><br><span class="line">        <span class="comment">###     self.combined_output_projection (Linear Layer with no bias), called W_&#123;u&#125; in the PDF.</span></span><br><span class="line">        <span class="comment">###     self.target_vocab_projection (Linear Layer with no bias), called W_&#123;vocab&#125; in the PDF.</span></span><br><span class="line">        <span class="comment">###     self.dropout (Dropout Layer)</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Use the following docs to properly initialize these variables:</span></span><br><span class="line">        <span class="comment">###     LSTM:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM</span></span><br><span class="line">        <span class="comment">###     LSTM Cell:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell</span></span><br><span class="line">        <span class="comment">###     Linear Layer:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear</span></span><br><span class="line">        <span class="comment">###     Dropout Layer:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout</span></span><br><span class="line"></span><br><span class="line">        self.encoder = nn.LSTM(embed_size, self.hidden_size, dropout=self.dropout_rate,bias = <span class="keyword">True</span>, bidirectional = <span class="keyword">True</span>)</span><br><span class="line">        self.decoder = nn.LSTMCell(embed_size + self.hidden_size, self.hidden_size, bias = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        self.h_projection = nn.Linear(<span class="number">2</span> * self.hidden_size, self.hidden_size, bias = <span class="keyword">False</span>)</span><br><span class="line">        self.c_projection = nn.Linear(<span class="number">2</span> * self.hidden_size, self.hidden_size, bias = <span class="keyword">False</span>)</span><br><span class="line">        self.att_projection = nn.Linear(<span class="number">2</span> * self.hidden_size, self.hidden_size, bias = <span class="keyword">False</span>)</span><br><span class="line">        self.combined_output_projection = nn.Linear(<span class="number">3</span> * self.hidden_size, self.hidden_size, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.target_vocab_projection = nn.Linear(self.hidden_size, self.model_embeddings.target.weight.shape[<span class="number">0</span>])</span><br><span class="line">        self.dropout = nn.Dropout(p = self.dropout_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br></pre></td></tr></table></figure><h3 id="Encode"><a href="#Encode" class="headerlink" title="Encode"></a>Encode</h3><p>Given a sentence in the source language, we look up the word embeddings from an embeddings matrix, yielding $x_1,\cdots,x_m | x_i \in \mathbb{R}^{e x 1}$,  where m is the length of the source sentence and e is the embedding size. We feed these embeddings to the bidirectional Encoder, yielding hidden states and cell states for both the forwards (-&gt;) and backwards (&lt;-) LSTMs. The forwards and backwards versions are concatenated<br>to give hidden states $h_i^{enc}$ and cell states $c_i^{enc}$</p><script type="math/tex; mode=display">\begin{align}& h_i^{enc} = \left [  \overleftarrow{h_i^{enc}}; \overrightarrow{h_i^{enc}} \right ] \qquad \text{where} \qquad h_i^{enc} \in \mathbb{R}^{2h x 1},  \overleftarrow{h_i^{enc}}, \overrightarrow{h_i^{enc}} \in \mathbb{R}^{h x 1}  \qquad 1 \leq i \leq m  \\& c_i^{enc} = \left [  \overleftarrow{c_i^{enc}}; \overrightarrow{c_i^{enc}} \right ] \qquad \text{where}  \qquad c_i^{enc} \in \mathbb{R}^{2h x 1},  \overleftarrow{c_i^{enc}}, \overrightarrow{c_i^{enc}} \in \mathbb{R}^{h x 1}  \qquad 1 \leq i \leq m \\ \end{align}</script><p>We then initialize the Decoder’s first hidden state $h_0^{dec}$ and cell state $c_0^{dec}$ with a linear projection of the Encoder’s final hidden state and final cell state</p><script type="math/tex; mode=display">\begin{align}& h_0^{dec} = W_h \left [  \overleftarrow{h_1^{enc}}; \overrightarrow{h_m^{enc}} \right ] \qquad \text{where} \qquad h_0^{dec} \in \mathbb{R}^{h x 1},  W_h \in \mathbb{R}^{h x 2h} \\& c_0^{dec} = W_h \left [  \overleftarrow{c_1^{enc}}; \overrightarrow{c_m^{enc}} \right ] \qquad \text{where} \qquad c_0^{dec} \in \mathbb{R}^{h x 1},  W_c \in \mathbb{R}^{h x 2h} \\\end{align}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, source_padded: torch.Tensor, source_lengths: List[int])</span> -&gt; Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:</span></span><br><span class="line">        <span class="string">""" Apply the encoder to source sentences to obtain encoder hidden states.</span></span><br><span class="line"><span class="string">            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where</span></span><br><span class="line"><span class="string">                                        b = batch_size, src_len = maximum source sentence length. Note that </span></span><br><span class="line"><span class="string">                                       these have already been sorted in order of longest to shortest sentence.</span></span><br><span class="line"><span class="string">        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch</span></span><br><span class="line"><span class="string">        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where</span></span><br><span class="line"><span class="string">                                        b = batch size, src_len = maximum source sentence length, h = hidden size.</span></span><br><span class="line"><span class="string">        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial</span></span><br><span class="line"><span class="string">                                                hidden state and cell.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        enc_hiddens, dec_init_state = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~ 8 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1. Construct Tensor `X` of source sentences with shape (src_len, b, e) using the source model embeddings.</span></span><br><span class="line">        <span class="comment">###         src_len = maximum source sentence length, b = batch size, e = embedding size. Note</span></span><br><span class="line">        <span class="comment">###         that there is no initial hidden state or cell for the decoder.</span></span><br><span class="line">        <span class="comment">###     2. Compute `enc_hiddens`, `last_hidden`, `last_cell` by applying the encoder to `X`.</span></span><br><span class="line">        <span class="comment">###         - Before you can apply the encoder, you need to apply the `pack_padded_sequence` function to X.</span></span><br><span class="line">        <span class="comment">###         - After you apply the encoder, you need to apply the `pad_packed_sequence` function to enc_hiddens.</span></span><br><span class="line">        <span class="comment">###         - Note that the shape of the tensor returned by the encoder is (src_len b, h*2) and we want to</span></span><br><span class="line">        <span class="comment">###           return a tensor of shape (b, src_len, h*2) as `enc_hiddens`.</span></span><br><span class="line">        <span class="comment">###     3. Compute `dec_init_state` = (init_decoder_hidden, init_decoder_cell):</span></span><br><span class="line">        <span class="comment">###         - `init_decoder_hidden`:</span></span><br><span class="line">        <span class="comment">###             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.</span></span><br><span class="line">        <span class="comment">###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).</span></span><br><span class="line">        <span class="comment">###             Apply the h_projection layer to this in order to compute init_decoder_hidden.</span></span><br><span class="line">        <span class="comment">###             This is h_0^&#123;dec&#125; in the PDF. Here b = batch size, h = hidden size</span></span><br><span class="line">        <span class="comment">###         - `init_decoder_cell`:</span></span><br><span class="line">        <span class="comment">###             `last_cell` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.</span></span><br><span class="line">        <span class="comment">###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).</span></span><br><span class="line">        <span class="comment">###             Apply the c_projection layer to this in order to compute init_decoder_cell.</span></span><br><span class="line">        <span class="comment">###             This is c_0^&#123;dec&#125; in the PDF. Here b = batch size, h = hidden size</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### See the following docs, as you may need to use some of the following functions in your implementation:</span></span><br><span class="line">        <span class="comment">###     Pack the padded sequence X before passing to the encoder:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence</span></span><br><span class="line">        <span class="comment">###     Pad the packed sequence, enc_hiddens, returned by the encoder:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence</span></span><br><span class="line">        <span class="comment">###     Tensor Concatenation:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.cat</span></span><br><span class="line">        <span class="comment">###     Tensor Permute:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute</span></span><br><span class="line"></span><br><span class="line">        X = self.model_embeddings.source(source_padded)</span><br><span class="line">        output, (h_enc, c_enc) = self.encoder(</span><br><span class="line">            pack_padded_sequence(X, source_lengths))</span><br><span class="line">        enc_hiddens,sequence_length = pad_packed_sequence(output, batch_first = <span class="keyword">True</span>) <span class="comment"># output of shape (batch, seq_len, num_directions * hidden_size)</span></span><br><span class="line">        h_0_dec = self.h_projection(torch.cat((h_enc[<span class="number">0</span>,:],h_enc[<span class="number">1</span>,:]), <span class="number">1</span>))</span><br><span class="line">        c_0_dec = self.c_projection(torch.cat((c_enc[<span class="number">0</span>,:],c_enc[<span class="number">1</span>,:]), <span class="number">1</span>))</span><br><span class="line">        dec_init_state = (h_0_dec,c_0_dec)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> enc_hiddens, dec_init_state</span><br></pre></td></tr></table></figure><h3 id="Decode"><a href="#Decode" class="headerlink" title="Decode"></a>Decode</h3><p>With the Decoder initialized, we must now feed it a matching sentence in the target language. On the $t^{th}$ step, we look up the embedding for the $t^{th}$ word, $y_t \in \mathbb{R}^{e x 1}$, we then concatenate $y_t$ with the combined-output vector $O_{t-1} \in \mathbb{R}^{h x 1}$ from the previous step to produce $\bar{y_t} \in \mathbb{R}^{(e+h) x 1}$. Note that for the first target word $O_0$ is zero-vector. We then fedd $\bar{y_t}$ as input to the Decoder LSTM.</p><script type="math/tex; mode=display">h_t^{dec}, c_t^{dec} = Decoder(\bar{y_t},h_{t-1}^{dec}, c_{t-1}^{dec} ) \quad \text{where} \quad h_t^{dec} \in \mathbb{R}^{h x 1}</script><p><strong>We then use $h_t^{dec}$ to compute multiplicative attention ovev $h_t^{enc}, \cdots, h_m^{enc}$</strong></p><script type="math/tex; mode=display">\begin{align}& e_{t_i} = (h_t^{dec})^{T}W_{attProj}h_i^{enc} \quad \text{where} \quad e_t \in \mathbb{R}^{m x 1}, W_{attProj} \in \mathbb{R}^{h x 2h} \\ & \alpha_{t} = Softmax(e_t) \quad \text{where} \quad \alpha_t \in \mathbb{R}^{m x 1} \\ & a_t = \sum_i^{m} \alpha_{t,i}h_i^{enc}  \quad \text{where} \quad a_t \in \mathbb{R}^{2h x 1}\\\end{align}</script><p>We now <strong>concatenate</strong> the attention output $a_t$ with the decoder hidden state $h_t^{dec}$ and pass this through a linear layer, Tanh, and Dropout to attain the <strong>combined-output vector</strong> $o_t$</p><script type="math/tex; mode=display">\begin{align}& u_t = \left[ a_t; h_t^{dec} \right ]  \quad \text{where} \quad u_t \in \mathbb{R}^{3h x 1} \\& v_t = W_u u_t \quad \text{where} \quad v_t \in \mathbb{R}^{h x 1}, W_u \in \mathbb{R}^{h x 1} \\& O_t = Dropout(Tanh(v_t)) \quad \text{where} \quad o_t \in \mathbb{R}^{h x 1} \\\end{align}</script><p>Then, we produce a probability distribution $P_t$ over target words at the $t^{th}$ timestep:</p><script type="math/tex; mode=display">P_t = Softmax(W_{vocab}O_t)  \quad \text{where} \quad P_t \in \mathbb{R}^{v_t x h}</script><p>Here, $V_t$ is the size of  the target vocabulary. Finally, to train the network we then compute the softmax cross entropy loss between $P_t$ and $g_t$, where $g_t$ is the 1-hot vector of the target word at timestep t:</p><script type="math/tex; mode=display">J(\theta) = CE(P_t, g_t)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor)</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">"""Compute combined output vectors for a batch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where</span></span><br><span class="line"><span class="string">                                     b = batch size, src_len = maximum source sentence length, h = hidden size.</span></span><br><span class="line"><span class="string">        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where</span></span><br><span class="line"><span class="string">                                     b = batch size, src_len = maximum source sentence length.</span></span><br><span class="line"><span class="string">        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder</span></span><br><span class="line"><span class="string">        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where</span></span><br><span class="line"><span class="string">                                       tgt_len = maximum target sentence length, b = batch size. </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where</span></span><br><span class="line"><span class="string">                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Chop of the &lt;END&gt; token for max length sentences.</span></span><br><span class="line">        target_padded = target_padded[:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize the decoder state (hidden and cell)</span></span><br><span class="line">        dec_state = dec_init_state</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize previous combined output vector o_&#123;t-1&#125; as zero</span></span><br><span class="line">        batch_size = enc_hiddens.size(<span class="number">0</span>)</span><br><span class="line">        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize a list we will use to collect the combined output o_t on each step</span></span><br><span class="line">        combined_outputs = []</span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~9 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1. Apply the attention projection layer to `enc_hiddens` to obtain `enc_hiddens_proj`,</span></span><br><span class="line">        <span class="comment">###         which should be shape (b, src_len, h),</span></span><br><span class="line">        <span class="comment">###         where b = batch size, src_len = maximum source length, h = hidden size.</span></span><br><span class="line">        <span class="comment">###         This is applying W_&#123;attProj&#125; to h^enc, as described in the PDF.</span></span><br><span class="line">        <span class="comment">###     2. Construct tensor `Y` of target sentences with shape (tgt_len, b, e) using the target model embeddings.</span></span><br><span class="line">        <span class="comment">###         where tgt_len = maximum target sentence length, b = batch size, e = embedding size.</span></span><br><span class="line">        <span class="comment">###     3. Use the torch.split function to iterate over the time dimension of Y.</span></span><br><span class="line">        <span class="comment">###         Within the loop, this will give you Y_t of shape (1, b, e) where b = batch size, e = embedding size.</span></span><br><span class="line">        <span class="comment">###             - Squeeze Y_t into a tensor of dimension (b, e). </span></span><br><span class="line">        <span class="comment">###             - Construct Ybar_t by concatenating Y_t with o_prev.</span></span><br><span class="line">        <span class="comment">###             - Use the step function to compute the the Decoder's next (cell, state) values</span></span><br><span class="line">        <span class="comment">###               as well as the new combined output o_t.</span></span><br><span class="line">        <span class="comment">###             - Append o_t to combined_outputs</span></span><br><span class="line">        <span class="comment">###             - Update o_prev to the new o_t.</span></span><br><span class="line">        <span class="comment">###     4. Use torch.stack to convert combined_outputs from a list length tgt_len of</span></span><br><span class="line">        <span class="comment">###         tensors shape (b, h), to a single tensor shape (tgt_len, b, h)</span></span><br><span class="line">        <span class="comment">###         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Note:</span></span><br><span class="line">        <span class="comment">###    - When using the squeeze() function make sure to specify the dimension you want to squeeze</span></span><br><span class="line">        <span class="comment">###      over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.</span></span><br><span class="line">        <span class="comment">###   </span></span><br><span class="line">        <span class="comment">### Use the following docs to implement this functionality:</span></span><br><span class="line">        <span class="comment">###     Zeros Tensor:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.zeros</span></span><br><span class="line">        <span class="comment">###     Tensor Splitting (iteration):</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.split</span></span><br><span class="line">        <span class="comment">###     Tensor Dimension Squeezing:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.squeeze</span></span><br><span class="line">        <span class="comment">###     Tensor Concatenation:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.cat</span></span><br><span class="line">        <span class="comment">###     Tensor Stacking:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.stack</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#   (b, src_len, h*2) * [2h , h]  = (b, src_len, h)</span></span><br><span class="line">        enc_hiddens_proj = self.att_projection(enc_hiddens)</span><br><span class="line">        <span class="comment">#   (tgt_len, b, e)</span></span><br><span class="line">        Y = self.model_embeddings.target(target_padded)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> Y_t <span class="keyword">in</span> torch.split(Y, split_size_or_sections = <span class="number">1</span>, dim = <span class="number">0</span>):</span><br><span class="line">            squeezed_Y_t = torch.squeeze(Y_t) <span class="comment"># (b, e) + (b,h) = (b,e+h)</span></span><br><span class="line">            Ybar_t = torch.cat((o_prev,squeezed_Y_t), dim = <span class="number">1</span>)</span><br><span class="line">            dec_state, o_t, _ = self.step(Ybar_t,dec_state,enc_hiddens,enc_hiddens_proj,enc_masks)</span><br><span class="line">            combined_outputs.append(o_t)</span><br><span class="line">            o_prev = o_t</span><br><span class="line"></span><br><span class="line">        <span class="comment">#  (b, h) -&gt; (tgt_len, b, h)</span></span><br><span class="line">        combined_outputs = torch.stack(combined_outputs,dim = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined_outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, Ybar_t: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            dec_state: Tuple[torch.Tensor, torch.Tensor],</span></span></span><br><span class="line"><span class="function"><span class="params">            enc_hiddens: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            enc_hiddens_proj: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">            enc_masks: torch.Tensor)</span> -&gt; Tuple[Tuple, torch.Tensor, torch.Tensor]:</span></span><br><span class="line">        <span class="string">""" Compute one forward step of the LSTM decoder, including the attention computation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,</span></span><br><span class="line"><span class="string">                                where b = batch size, e = embedding size, h = hidden size.</span></span><br><span class="line"><span class="string">        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.</span></span><br><span class="line"><span class="string">                First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.</span></span><br><span class="line"><span class="string">        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,</span></span><br><span class="line"><span class="string">                                    src_len = maximum source length, h = hidden size.</span></span><br><span class="line"><span class="string">        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),</span></span><br><span class="line"><span class="string">                                    where b = batch size, src_len = maximum source length, h = hidden size.</span></span><br><span class="line"><span class="string">        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),</span></span><br><span class="line"><span class="string">                                    where b = batch size, src_len is maximum source length. </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.</span></span><br><span class="line"><span class="string">                First tensor is decoder's new hidden state, second tensor is decoder's new cell.</span></span><br><span class="line"><span class="string">        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.</span></span><br><span class="line"><span class="string">        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.</span></span><br><span class="line"><span class="string">                                Note: You will not use this outside of this function.</span></span><br><span class="line"><span class="string">                                      We are simply returning this value so that we can sanity check</span></span><br><span class="line"><span class="string">                                      your implementation.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        combined_output = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~3 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1. Apply the decoder to `Ybar_t` and `dec_state`to obtain the new dec_state.</span></span><br><span class="line">        <span class="comment">###     2. Split dec_state into its two parts (dec_hidden, dec_cell)</span></span><br><span class="line">        <span class="comment">###     3. Compute the attention scores e_t, a Tensor shape (b, src_len). </span></span><br><span class="line">        <span class="comment">###        Note: b = batch_size, src_len = maximum source length, h = hidden size.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">###       Hints:</span></span><br><span class="line">        <span class="comment">###         - dec_hidden is shape (b, h) and corresponds to h^dec_t in the PDF (batched)</span></span><br><span class="line">        <span class="comment">###         - enc_hiddens_proj is shape (b, src_len, h) and corresponds to W_&#123;attProj&#125; h^enc (batched).</span></span><br><span class="line">        <span class="comment">###         - Use batched matrix multiplication (torch.bmm) to compute e_t.</span></span><br><span class="line">        <span class="comment">###         - To get the tensors into the right shapes for bmm, you will need to do some squeezing and unsqueezing.</span></span><br><span class="line">        <span class="comment">###         - When using the squeeze() function make sure to specify the dimension you want to squeeze</span></span><br><span class="line">        <span class="comment">###             over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Use the following docs to implement this functionality:</span></span><br><span class="line">        <span class="comment">###     Batch Multiplication:</span></span><br><span class="line">        <span class="comment">###        https://pytorch.org/docs/stable/torch.html#torch.bmm</span></span><br><span class="line">        <span class="comment">###     Tensor Unsqueeze:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.unsqueeze</span></span><br><span class="line">        <span class="comment">###     Tensor Squeeze:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.squeeze</span></span><br><span class="line"></span><br><span class="line">        dec_state = self.decoder(Ybar_t, dec_state)</span><br><span class="line">        h_t_dec, c_t_dec = dec_state</span><br><span class="line">        <span class="comment">#  enc_hiddens_proj(b, src_len, h) * h_t_dec (b,h,1) = (b,src_len)</span></span><br><span class="line">        e_t = torch.squeeze(torch.bmm(enc_hiddens_proj, torch.unsqueeze(h_t_dec,<span class="number">2</span>)),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set e_t to -inf where enc_masks has 1</span></span><br><span class="line">        <span class="keyword">if</span> enc_masks <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            e_t.data.masked_fill_(enc_masks.byte(), -float(<span class="string">'inf'</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~6 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1. Apply softmax to e_t to yield alpha_t</span></span><br><span class="line">        <span class="comment">###     2. Use batched matrix multiplication between alpha_t and enc_hiddens to obtain the</span></span><br><span class="line">        <span class="comment">###         attention output vector, a_t.</span></span><br><span class="line">        <span class="comment">#$$     Hints:</span></span><br><span class="line">        <span class="comment">###           - alpha_t is shape (b, src_len)</span></span><br><span class="line">        <span class="comment">###           - enc_hiddens is shape (b, src_len, 2h)</span></span><br><span class="line">        <span class="comment">###           - a_t should be shape (b, 2h)</span></span><br><span class="line">        <span class="comment">###           - You will need to do some squeezing and unsqueezing.</span></span><br><span class="line">        <span class="comment">###     Note: b = batch size, src_len = maximum source length, h = hidden size.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">###     3. Concatenate dec_hidden with a_t to compute tensor U_t</span></span><br><span class="line">        <span class="comment">###     4. Apply the combined output projection layer to U_t to compute tensor V_t</span></span><br><span class="line">        <span class="comment">###     5. Compute tensor O_t by first applying the Tanh function and then the dropout layer.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Use the following docs to implement this functionality:</span></span><br><span class="line">        <span class="comment">###     Softmax:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.functional.softmax</span></span><br><span class="line">        <span class="comment">###     Batch Multiplication:</span></span><br><span class="line">        <span class="comment">###        https://pytorch.org/docs/stable/torch.html#torch.bmm</span></span><br><span class="line">        <span class="comment">###     Tensor View:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view</span></span><br><span class="line">        <span class="comment">###     Tensor Concatenation:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.cat</span></span><br><span class="line">        <span class="comment">###     Tanh:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.tanh</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># (b,src_len)</span></span><br><span class="line">        alpha_t = nn.functional.softmax(e_t, dim = <span class="number">1</span>) </span><br><span class="line">        <span class="comment"># alpha_t(b,src_len) - (b,1,src_len) * enc_hiddens(b, src_len, h * 2) = (b, 1, h * 2) -&gt; (b,2h)</span></span><br><span class="line">        a_t = torch.squeeze(torch.bmm(torch.unsqueeze(alpha_t,<span class="number">1</span>),enc_hiddens),<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#(b,2h) + (b,h)</span></span><br><span class="line">        U_t = torch.cat((a_t,h_t_dec), dim = <span class="number">1</span>)</span><br><span class="line">        V_t = self.combined_output_projection(U_t)</span><br><span class="line">        O_t = self.dropout(nn.functional.tanh(V_t))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line">        combined_output = O_t</span><br><span class="line">        <span class="keyword">return</span> dec_state, combined_output, e_t</span><br></pre></td></tr></table></figure><h3 id="Helpers"><a href="#Helpers" class="headerlink" title="Helpers"></a>Helpers</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, source: List[List[str]], target: List[List[str]])</span> -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">""" Take a mini-batch of source and target sentences, compute the log-likelihood of</span></span><br><span class="line"><span class="string">        target sentences under the language models learned by the NMT system.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param source (List[List[str]]): list of source sentence tokens</span></span><br><span class="line"><span class="string">        @param target (List[List[str]]): list of target sentence tokens, wrapped by `&lt;s&gt;` and `&lt;/s&gt;`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the</span></span><br><span class="line"><span class="string">                                    log-likelihood of generating the gold-standard target sentence for</span></span><br><span class="line"><span class="string">                                    each example in the input batch. Here b = batch size.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Compute sentence lengths</span></span><br><span class="line">        source_lengths = [len(s) <span class="keyword">for</span> s <span class="keyword">in</span> source]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert list of lists into tensors</span></span><br><span class="line">        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   <span class="comment"># Tensor: (src_len, b)</span></span><br><span class="line">        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   <span class="comment"># Tensor: (tgt_len, b)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">###     Run the network forward:</span></span><br><span class="line">        <span class="comment">###     1. Apply the encoder to `source_padded` by calling `self.encode()`</span></span><br><span class="line">        <span class="comment">###     2. Generate sentence masks for `source_padded` by calling `self.generate_sent_masks()`</span></span><br><span class="line">        <span class="comment">###     3. Apply the decoder to compute combined-output by calling `self.decode()`</span></span><br><span class="line">        <span class="comment">###     4. Compute log probability distribution over the target vocabulary using the</span></span><br><span class="line">        <span class="comment">###        combined_outputs returned by the `self.decode()` function.</span></span><br><span class="line"></span><br><span class="line">        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)</span><br><span class="line">        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)</span><br><span class="line">        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)</span><br><span class="line">        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Zero out, probabilities for which we have nothing in the target text</span></span><br><span class="line">        target_masks = (target_padded != self.vocab.tgt[<span class="string">'&lt;pad&gt;'</span>]).float()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute log probability of generating true target words</span></span><br><span class="line">        target_gold_words_log_prob = torch.gather(P, index=target_padded[<span class="number">1</span>:].unsqueeze(<span class="number">-1</span>), dim=<span class="number">-1</span>).squeeze(<span class="number">-1</span>) * target_masks[<span class="number">1</span>:]</span><br><span class="line">        scores = target_gold_words_log_prob.sum(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 4</span></span><br><span class="line"><span class="string">model_embeddings.py: Embeddings for the NMT model</span></span><br><span class="line"><span class="string">Pencheng Yin &lt;pcyin@cs.cmu.edu&gt;</span></span><br><span class="line"><span class="string">Sahil Chopra &lt;schopra8@stanford.edu&gt;</span></span><br><span class="line"><span class="string">Anand Dhoot &lt;anandd@stanford.edu&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelEmbeddings</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Class that converts input words to their embeddings.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embed_size, vocab)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Init the Embedding layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param embed_size (int): Embedding size (dimensionality)</span></span><br><span class="line"><span class="string">        @param vocab (Vocab): Vocabulary object containing src and tgt languages</span></span><br><span class="line"><span class="string">                              See vocab.py for documentation.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ModelEmbeddings, self).__init__()</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># default values</span></span><br><span class="line">        self.source = <span class="keyword">None</span></span><br><span class="line">        self.target = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        src_pad_token_idx = vocab.src[<span class="string">'&lt;pad&gt;'</span>]</span><br><span class="line">        tgt_pad_token_idx = vocab.tgt[<span class="string">'&lt;pad&gt;'</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~2 Lines)</span></span><br><span class="line">        <span class="comment">### TODO - Initialize the following variables:</span></span><br><span class="line">        <span class="comment">###     self.source (Embedding Layer for source language)</span></span><br><span class="line">        <span class="comment">###     self.target (Embedding Layer for target langauge)</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Note:</span></span><br><span class="line">        <span class="comment">###     1. `vocab` object contains two vocabularies:</span></span><br><span class="line">        <span class="comment">###            `vocab.src` for source</span></span><br><span class="line">        <span class="comment">###            `vocab.tgt` for target</span></span><br><span class="line">        <span class="comment">###     2. You can get the length of a specific vocabulary by running:</span></span><br><span class="line">        <span class="comment">###             `len(vocab.&lt;specific_vocabulary&gt;)`</span></span><br><span class="line">        <span class="comment">###     3. Remember to include the padding token for the specific vocabulary</span></span><br><span class="line">        <span class="comment">###        when creating your Embedding.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Use the following docs to properly initialize these variables:</span></span><br><span class="line">        <span class="comment">###     Embedding Layer:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding</span></span><br><span class="line">        self.source = nn.Embedding(len(vocab.src),self.embed_size, padding_idx = src_pad_token_idx)</span><br><span class="line">        self.target = nn.Embedding(len(vocab.tgt), self.embed_size, padding_idx = tgt_pad_token_idx) </span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_sents</span><span class="params">(sents, pad_token)</span>:</span></span><br><span class="line">    <span class="string">""" Pad list of sentences according to the longest sentence in the batch.</span></span><br><span class="line"><span class="string">    @param sents (list[list[str]]): list of sentences, where each sentence</span></span><br><span class="line"><span class="string">                                    is represented as a list of words</span></span><br><span class="line"><span class="string">    @param pad_token (str): padding token</span></span><br><span class="line"><span class="string">    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter</span></span><br><span class="line"><span class="string">        than the max length sentence are padded out with the pad_token, such that</span></span><br><span class="line"><span class="string">        each sentences in the batch now has equal length.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    sents_padded = []</span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE (~6 Lines)</span></span><br><span class="line">    max_sentence_len = max([len(s) <span class="keyword">for</span> s <span class="keyword">in</span> sents])</span><br><span class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> sents:</span><br><span class="line">        sents_padded.append(sent + [pad_token] * (max_sentence_len - len(sent)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sents_padded</span><br></pre></td></tr></table></figure><h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ol><li>course slides and notes from cs224n (<a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/</a>)</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;General-definition-of-attention&quot;&gt;&lt;a href=&quot;#General-definition-of-attention&quot; class=&quot;headerlink&quot; title=&quot;General definition of attentio
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Gated RNN Units</title>
    <link href="https://zhangruochi.com/Gated-RNN-Units/2019/12/15/"/>
    <id>https://zhangruochi.com/Gated-RNN-Units/2019/12/15/</id>
    <published>2019-12-15T06:40:39.000Z</published>
    <updated>2019-12-19T18:09:48.113Z</updated>
    
    <content type="html"><![CDATA[<h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><p>RNNs have been found to perform better with the use of more complex units for activation. Here, we discuss the use of a gated activation function thereby modifying the RNN architecture. What motivates this? Well, although RNNs can theoretically capture long-term dependencies, they are very hard to actually train to do this. Gated recurrent units are designed in a manner to have more persistent memory thereby making it easier for RNNs to capture long-term dependencies. Let us see mathematically how a GRU uses $h_{t−1}$ and $x_t$ to generate the next hidden state ht. We will then dive into the intuition of this architecture.</p><script type="math/tex; mode=display">\begin{aligned}& z_t = \sigma(W^{(z)}x_t + U^{(z)}h_{t-1}) & \text{(Update gate)} \\ & r_t = \sigma(W^{(r)}x_t + U^{(r)}h_{t-1})  & \text{(Reset gate)}\\ & \tilde{h_t} = tanh{r_t \circ Uh_{t-1} + Wx_t} & \text{(New memory)} \\ & h_t = (1 - z_t) \circ \tilde{h_t} + z_t \circ h_{t-1} & \text{(Hidden state)}\end{aligned}</script><p>The above equations can be thought of a GRU’s four fundamental operational stages and they have intuitive interpretations that make this model much more intellectually.</p><ol><li><strong>Reset gate</strong>: controls what parts of previous hidden state are used to compute new content</li><li><strong>Update gate</strong>: controls what parts of hidden state are updated vs preserved</li><li><strong>New hidden state content</strong>: reset gate selects useful parts of prev hidden state. Use this and current input to compute new hidden content.</li><li><strong>Hidden state</strong>: update gate simultaneously controls what is kept from previous hidden state, and what is updated to new hidden state content</li></ol><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>Long-Short-Term-Memories are another type of complex activation unit that differ a little from GRUs. The motivation for using these is similar to those for GRUs however the architecture of such units does differ. Let us first take a look at the mathematical formulation of LSTM units before diving into the intuition behind this design:</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="LSTM3.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">slides of cs224n</div></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="LSTM1.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/</div></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="LSTM2.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/</div></center><ol><li>The LSTM architecture makes it easier for the RNN to preserve information over many timesteps<ul><li>e.g. if the forget gate is set to remember everything on every timestep, then the info in the cell is preserved indefinitely</li><li>By contrast, it’s harder for vanilla RNN to learn a recurrent weight matrix Wh that preserves info in hidden state</li></ul></li><li>LSTM doesn’t guarantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies.</li></ol><h2 id="LSTM-vs-GRU"><a href="#LSTM-vs-GRU" class="headerlink" title="LSTM vs GRU"></a>LSTM vs GRU</h2><ul><li>Researchers have proposed many gated RNN variants, but LSTM and GRU are the most widely-used</li><li>The biggest difference is that GRU is quicker to compute and has fewer parameters</li><li>There is no conclusive evidence that one consistently performs better than the other</li><li>LSTM is a good default choice (especially if your data has particularly long dependencies, or you have lots of training data)</li><li><strong>Rule of thumb</strong>: start with LSTM, but switch to GRU if you want something more efficient</li></ul><h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ol><li>course slides and notes from cs224n (<a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/</a>)</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;GRU&quot;&gt;&lt;a href=&quot;#GRU&quot; class=&quot;headerlink&quot; title=&quot;GRU&quot;&gt;&lt;/a&gt;GRU&lt;/h2&gt;&lt;p&gt;RNNs have been found to perform better with the use of more comple
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Human Factor</title>
    <link href="https://zhangruochi.com/Human-Factor/2019/12/12/"/>
    <id>https://zhangruochi.com/Human-Factor/2019/12/12/</id>
    <published>2019-12-12T09:00:04.000Z</published>
    <updated>2019-12-12T09:02:16.947Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Course-Description"><a href="#Course-Description" class="headerlink" title="Course Description"></a>Course Description</h2><p> This course provides an introduction to human factors research and applications with emphasis on mature areas such as sensation and perception and manual control. <strong>Each class will introduce some concrete human factors problem and explore theory and application relevant to solving it</strong>. The term long conceptual design assignment is intended to help maintain focus on applications to design.</p><h2 id="Week1"><a href="#Week1" class="headerlink" title="Week1"></a>Week1</h2><p>Learning Objectives:</p><ol><li><strong>Theoretical understanding</strong>: Develop an understanding of the historical context, disciplines, and schools of thought that led to the development of the current field of human factors engineering/psychology; Develop an acquaintance with basic elements of <strong>scientific method</strong> and <strong>decision making</strong> including tests of hypotheses, dependent and independent variables, and inferential and descriptive use of statistics. </li><li><strong>Apply theory and skills</strong>: Use scientific methods including selection of measures and experimental tasks to evaluate the utility of a variety of pointing devices</li><li><strong>Proficiency in information related skills</strong>: Learn how to conduct a critical incident-based evaluation of an interactive system and to perform a standard task analysis for such a system</li></ol><h3 id="Research-Methods"><a href="#Research-Methods" class="headerlink" title="Research Methods"></a>Research Methods</h3><ol><li>IV &amp; DV</li><li>Cause vs. Chance</li><li>Descriptive vs. Inferential Statistics</li><li>Null hypothesis(H0) &amp; Experimentalhypothesis(H1)</li><li>Hypothesis Testing</li><li>Statistical Significance &amp; Practical Significance</li><li>Graph &amp; Interpetation</li></ol><h3 id="Goals-of-System-Evaluation"><a href="#Goals-of-System-Evaluation" class="headerlink" title="Goals of System Evaluation"></a>Goals of System Evaluation</h3><ul><li>Functionality<ul><li>Can it do what it is supposed to do?</li></ul></li><li>Usability<ul><li>Does it make the task easier?</li></ul></li><li>Diagnosticity<ul><li>Does it pinpoint what is wrong?</li></ul></li></ul><ol><li>User Population<ul><li>Who are they?</li><li>What are their goals?</li><li>What do they already know how to do?</li><li>Ask, don’t assume!</li></ul></li><li>Research Objectives<ul><li>Generalizability</li><li>Precision</li><li>Realism</li></ul></li><li>Usability Testing</li><li>Design an Experiment</li></ol><h3 id="Two-Models-for-Human-Factors"><a href="#Two-Models-for-Human-Factors" class="headerlink" title="Two Models for Human Factors"></a>Two Models for Human Factors</h3><ul><li>System component (computer)</li><li>Embedded organism (cybernetic)</li></ul><h3 id="Task-Analyses"><a href="#Task-Analyses" class="headerlink" title="Task Analyses"></a>Task Analyses</h3><p>为了研究如何使系统更好地和人的能力相匹配所进行的一种描述人机交互的方法</p><ul><li><strong>Sequential</strong><br>  任务的顺序和不同任务在时间序列上的关系<ul><li>Procedural</li><li>Therp</li></ul></li><li><strong>Hierarchical</strong><br>  描述一个大的任务如何由子任务组成以及这些任务又是如何联系起来体现其功能的<ul><li>HTA</li><li>GOMS</li><li>Cognitive</li></ul></li></ul><h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul><li>Types of Evaluation<ul><li>Laboratory Studies</li><li>Field Studies</li><li>Participatory Design/Rapid Prototyping</li><li>Brainstorming</li><li>StoryBoarding/Wizard of Oz</li><li>Workshops/Role Playing</li><li>Walkthrough/Talkthrough</li></ul></li><li>Observation Evaluation</li></ul><h4 id="Function-Allocation"><a href="#Function-Allocation" class="headerlink" title="Function Allocation"></a>Function Allocation</h4><p>每个功能由人来实现还是系统来实现</p><ul><li>Mandatory</li><li>Relative value</li><li>Cost based</li><li>Cognitive/affective</li></ul><h2 id="Week2-Sensation"><a href="#Week2-Sensation" class="headerlink" title="Week2 Sensation"></a>Week2 Sensation</h2><p>Learning Objectives:</p><ul><li>Psychophysical methods &amp; the problem of separating bias from measurement </li><li>Signal Detection: Perform specific calculations      <ul><li><strong>Conceptual model</strong> (associating hits, FA’s, misses, &amp; CR with areas under the curves) </li><li><strong>Computational understanding</strong> (interpreting problems, using table, &amp; finding d’) </li><li><strong>Vigilance</strong> - what the vigilance decrement is and how signal detection has been used to better understand it </li></ul></li><li>DESIGN: understanding of the cognitive information capabilities of humans General knowledge- wide range of sensitivity, approximately logarithmic, etc. </li><li>Place encoding of frequency and its consequences for: <ul><li>Masking </li><li>Threshold shift &amp; frequency related loss, etc. </li></ul></li><li>Measurement issues<ul><li>Loudness &amp; equal loudness contours </li><li>SPL meters </li><li>Articulation index, SIL, and other speech &amp; noise issues </li><li>Noise &amp; annoyance </li></ul></li></ul><h3 id="Classical-Errors"><a href="#Classical-Errors" class="headerlink" title="Classical Errors"></a>Classical Errors</h3><p>Error of Habituation</p><ul><li>Keep on saying the same thing<br>Error of Anticipation </li><li>Shift to new response</li></ul><h3 id="Signal-Detection-Model"><a href="#Signal-Detection-Model" class="headerlink" title="Signal Detection Model"></a>Signal Detection Model</h3><p><img src="resources/E893E7BC11981E867B4A94DA85979D8E.png" alt="Screen Shot 2019-10-24 at 00.00.37.png"><br><img src="resources/C2A377503B642EB567613413A81A15E0.png" alt="Screen Shot 2019-10-24 at 00.00.54.png"></p><h3 id="Vigilance"><a href="#Vigilance" class="headerlink" title="Vigilance"></a>Vigilance</h3><p>警报的设计必须建立在对人类的听觉加工充分了解的基础上</p><ul><li>Near threshold signals</li><li>Low rate of occurrence</li><li>Extended Watch</li><li>Inspection(definitetrial)</li><li>Free response time arbitrarily brokeninto intervals</li><li>Successive(noavailablestandard)</li><li>Simultaneous(signal to standard comparison)</li></ul><h3 id="Fatigue"><a href="#Fatigue" class="headerlink" title="Fatigue"></a>Fatigue</h3><ol><li>Sustained attention leads to fatigue</li><li>Load on working memory to keep target in mind depletes resources</li></ol><h3 id="Expectancy"><a href="#Expectancy" class="headerlink" title="Expectancy"></a>Expectancy</h3><h3 id="Remedies"><a href="#Remedies" class="headerlink" title="Remedies"></a>Remedies</h3><h3 id="Sound"><a href="#Sound" class="headerlink" title="Sound"></a>Sound</h3><p>Patterns of rare faction/compression of air</p><ol><li>Physics<ul><li>Intensity = amplitude</li><li>Frequency = cycles(hz)</li></ul></li><li>Perception<ul><li>Loudness</li><li>Pitch</li></ul></li></ol><ul><li>强度决定响度</li><li>频率决定音调</li><li>位置决定听觉定位</li><li>品质由频率及其掩蔽来决定</li></ul><h3 id="DECIBEL"><a href="#DECIBEL" class="headerlink" title="DECIBEL"></a>DECIBEL</h3><ul><li>dB(a): dB weighted by threshold equal loudness curve</li><li>dB(c)/dB (spl): dB with no weighting</li><li>dB(d) weighted by equal annoyance curves</li></ul><h3 id="Encoding-Pitch-by-Place"><a href="#Encoding-Pitch-by-Place" class="headerlink" title="Encoding Pitch by Place"></a>Encoding Pitch by Place</h3><h3 id="Auditory-Masking"><a href="#Auditory-Masking" class="headerlink" title="Auditory Masking"></a>Auditory Masking</h3><p>the presence of tone that inhibits the perception of another tone that occurs before, at the same time, or after it.</p><h2 id="Week3-Vision-and-Color"><a href="#Week3-Vision-and-Color" class="headerlink" title="Week3 Vision and Color"></a>Week3 Vision and Color</h2><p>Learning Objectives:</p><ul><li>Even more than place perception of pitch vision is all about relative differences (contrasts) and adaptation</li><li>The relation between what we experience and what is physically out there isn’t direct</li><li>Good Human Factors engineering requires designing so our users don’t notice</li></ul><p>Overview of vision</p><ul><li>Peripheral/central processing</li><li>Dark Adaptation &amp; Illusions &amp; their relation to the structure of the eye &amp; vision</li><li>Visual angle &amp; spatial frequency</li><li>VDTs, visual fatigue, &amp; ergonomic effects</li><li>Color measurement Munsell color wheel vs. CIE</li></ul><h3 id="Rods-and-Cones"><a href="#Rods-and-Cones" class="headerlink" title="Rods and Cones"></a>Rods and Cones</h3><ol><li>位置 location</li><li>视敏度 acuity（解析细节的能力）</li><li>敏感性 sensitivity (即使光很少，Rods也能工作)</li><li>color sensitivity （rods 是色盲）</li><li>adaption （rods 受光刺激的影响大）</li><li>diffrrential wavelength sensitivity (cones对所有光敏感，rods对红光不敏感）</li></ol><h3 id="对比敏感度"><a href="#对比敏感度" class="headerlink" title="对比敏感度"></a>对比敏感度</h3><p>c = (L-D) / (L+D)</p><h3 id="color-sensation"><a href="#color-sensation" class="headerlink" title="color sensation"></a>color sensation</h3><p>对单色进行设计，然后将颜色作为冗余编码信息提供</p><h3 id="dark-sensation"><a href="#dark-sensation" class="headerlink" title="dark sensation"></a>dark sensation</h3><p>当照明条件比较差时，所有空间评率的对比度都会降低</p><h3 id="检测"><a href="#检测" class="headerlink" title="检测"></a>检测</h3><ul><li>在很多系统中，影响作业绩效的重要因素是密切相关的两个过程：视觉搜索和物体或时间的检测</li><li>在视觉搜索领域，一旦某个项目被确定可能是目标，就必须对它是否是真的目标进行确认。</li><li>d’ 反映了一个操作者从噪音中分辨出信号的能力，它等于好结果的数目除以所有结果的总和</li></ul><h3 id="Peripheral-Processing"><a href="#Peripheral-Processing" class="headerlink" title="Peripheral Processing"></a>Peripheral Processing</h3><ul><li>Photo receptors are interconnected and can reciprocally inhibit one another</li><li>Can be tuned for featurere cognition</li></ul><h3 id="Adaptation"><a href="#Adaptation" class="headerlink" title="Adaptation"></a>Adaptation</h3><p>Adaptation is a major characteristic of sensation</p><h2 id="Week4-Reaction-time"><a href="#Week4-Reaction-time" class="headerlink" title="Week4 Reaction time"></a>Week4 Reaction time</h2><p>Learning Objectives:</p><ul><li>Subtractive&amp;additive factors analyses of choice reaction time</li><li>Limits&amp;values of psychological experimentation</li><li>Human bottleneck in choice responses</li><li>Power law of learning</li><li>Automaticvs.controlledresponding</li><li>Information theoretic interpretations of reaction time</li></ul><h3 id="Hick’s-Law"><a href="#Hick’s-Law" class="headerlink" title="Hick’s Law"></a>Hick’s Law</h3><p>Hick’s Law holds that choice reaction time is proportional to log2 of the number of alternatives.（反应时间是log2N的函数）<br>RT = a + blog2N</p><h3 id="Conventional-Controls-amp-Displays"><a href="#Conventional-Controls-amp-Displays" class="headerlink" title="Conventional Controls &amp; Displays"></a>Conventional Controls &amp; Displays</h3><ul><li>conventional practice in design of controls</li><li>displays Acquire familiarity with human factors design principles and heuristics </li></ul><h3 id="Control"><a href="#Control" class="headerlink" title="Control"></a>Control</h3><p>Controls are used by the human operator to communicate with the machine/device in the system. It’simportantthatcontrolsservetheirfunction. Based on:</p><ul><li>Ease of operation (considering population, biomechanics, etc.).</li><li>Nature of the task (force, precision, etc.). </li><li>Arrangement.</li></ul><h3 id="Basic-dimensions"><a href="#Basic-dimensions" class="headerlink" title="Basic dimensions:"></a>Basic dimensions:</h3><ul><li>Discrete (e.g., light switch) vs. continuous (e.g., dimmer)</li><li>Linear vs. rotary</li><li>Unidimensional vs. multidimensional</li><li>Isometric vs. isotonic</li><li>Plus mass, shape, range of motion, resistance to movement.</li></ul><h3 id="Control-Features"><a href="#Control-Features" class="headerlink" title="Control Features"></a>Control Features</h3><ol><li>Control resistance<ul><li>Elastic resistance: Spring loaded.<ul><li>Resistance increases as control gets farther from neutral.</li><li>Gives proprioceptive feedback about <strong>control position</strong>.</li><li>Returns to neutral when released (deadman switch).</li></ul></li><li>Frictional resistance<ul><li>Static friction for resting state, decreases when pushed. </li><li>Sliding friction not influenced by velocity or position.</li></ul></li><li>Viscous resistance<ul><li>Increases as a function of velocity.</li><li>Gives proprioceptive feedback about speed.</li><li>Promotes smooth movement.</li></ul></li><li>Inertial resistance<ul><li>Hard to start and stop.</li><li>Users tend to overshoot (revolving doors).</li></ul></li><li>Performance and resistance<ul><li>For frictional and inertial, the JND is 10%-20% of resting state.</li><li>Lighter controls preferred to heavy.</li><li>Viscous preferred to frictional.</li><li>For continuous, inertial hurts performance; </li><li>Elastic is the best.</li><li>For all of these, hard-and-fast rules are not available. It’s a function of the system.</li></ul></li></ul></li><li>Control-display ratio: <ul><li>Ratio of magnitude of control adjustment to magnitude of change in display.</li></ul></li><li>Gain: <ul><li>Responsiveness of control.</li></ul></li></ol><h3 id="Control-Panels"><a href="#Control-Panels" class="headerlink" title="Control Panels"></a>Control Panels</h3><ol><li>Location coding<ul><li>Need to be able to reliably distinguish locations.</li><li>Vertical localization is easier than horizontal.</li><li>Overuse of location coding is still a factor in some aircraft designs.</li></ul></li><li>Labels<ul><li>Not recommended as the sole code</li></ul></li><li>General rules for labels<ul><li>Locate labels systematically with respect to controls (all above, etc.).</li><li>Make labels brief.</li><li>Avoid abstract symbols; use standards.</li><li>Attend to fonts.</li><li>Position labels so they can be seen while the control is in use.</li></ul></li><li>Coding of controls<ul><li>Color coding</li><li>Shape coding</li><li>Size coding</li><li>Texture coding.<br>– Coding by type of operation.<br>– Redundant coding: Multiple dimensions</li></ul></li><li>Control arrangements<ul><li>Grouping is important.</li><li>Population stereotypes for control arrangements can be device specific</li><li>Attend to the reach envelope</li></ul></li><li>Preventing accidental operation</li><li>Specific Controls<ul><li>Hand operated controls</li><li>Foot operated controls</li><li>Specialized controls</li></ul></li></ol><h3 id="Visual-Displays"><a href="#Visual-Displays" class="headerlink" title="Visual Displays"></a>Visual Displays</h3><p>Display - anything that conveys information</p><ol><li>Requirements:<ul><li>Compatibility to senses</li><li>Language compatibility</li><li>Right info at the right time</li></ul></li><li>Types of information to display: <ul><li>Instructional</li><li>Command - direct orders </li><li>Advisory</li><li>Historical/predictive</li><li>Answers</li></ul></li><li>Functions of Dynamic Visual<br>Displays<ul><li>Continuous System Control</li><li>System Status Monitoring</li><li>Briefing</li><li>Search and Identification</li><li>Decision Making</li></ul></li><li>Visual Display Technology<ul><li>Mechanical </li><li>Electronic</li><li>Optical Projection</li></ul></li><li>General Display Principles<ul><li>Color</li><li>Shape</li><li>Coding</li><li>Approximation<ul><li>Get attention with one display<br>– Present detailed info with another</li></ul></li><li>Integration</li></ul></li></ol><h3 id="Principles-of-Display-Design"><a href="#Principles-of-Display-Design" class="headerlink" title="Principles of Display Design"></a>Principles of Display Design</h3><ul><li>Perceptual Principles<ul><li>Avoid absolute judgments</li><li>Top-down processing （信号的显示方式尽量与人的经验相符合）</li><li>Redundancy gain</li><li>Discriminability</li><li><img src="resources/088856BA3C1FA5A601838DAD16F27045.png" alt="Screen Shot 2019-12-11 at 14.23.03.png"></li></ul></li><li>Mental Model Principles （显示方式与操作员的心理模型一致，有助于提高正确操作）<ul><li>Pictorial realism （形如其表）</li><li>Principle of the moving part （运动一致）</li></ul></li><li>Principles Based on Attention<ul><li>Minimizing information access costs （将访问信息的消耗降到最低）</li><li>Proximity compatibility principle （接近相容原则）</li><li>Principle of multiple resources （要同时对多种信息进行加工时，可以将信息的呈现方式区分开）</li></ul></li><li>Memory Principles<ul><li>Principle of predictive aiding （预测辅助原则）</li><li>Principle of knowledge in the world （利用知识降低记忆负荷）</li><li>Principle of consistency （一致性原则）</li></ul></li></ul><ul><li>Two-Valued Info</li><li>Quantitative Information</li><li>Qualitative readings</li><li>Check Reading</li><li>Situation awareness</li></ul><h3 id="Three-Heuristics"><a href="#Three-Heuristics" class="headerlink" title="Three Heuristics"></a>Three Heuristics</h3><ol><li>Minimize Information</li><li>Promote Good C-D mappings</li><li>Provide Feedback</li></ol><h2 id="Spatial-amp-Integrative-Displays"><a href="#Spatial-amp-Integrative-Displays" class="headerlink" title="Spatial &amp; Integrative Displays"></a>Spatial &amp; Integrative Displays</h2><p>Learning Objectives:</p><ul><li>6 DOF &amp; moving &amp; orienting in 3 space</li><li>Problems with viewpoint &amp; situation awareness</li><li>Applications VR, games, &amp; robotics – Attitude &amp; pose</li><li>Navigation &amp; search</li><li><p>Cues to depth &amp; distance</p></li><li><p>Use of emergent features and perceptual salience to integrate displays</p></li><li>TMI and need to provide context to events</li></ul><h3 id="6-Degrees-of-Freedom-6DOF"><a href="#6-Degrees-of-Freedom-6DOF" class="headerlink" title="6 Degrees of Freedom (6DOF)"></a>6 Degrees of Freedom (6DOF)</h3><p>– Position (X,Y, Z)<br>– Orientation (Yaw, Pitch, Roll)</p><h3 id="3d-display-on-2d-panels"><a href="#3d-display-on-2d-panels" class="headerlink" title="3d display on 2d panels"></a>3d display on 2d panels</h3><ul><li>Depth is often poorly represented &amp; less discernable than other dimensions</li></ul><h3 id="Configural-Displays"><a href="#Configural-Displays" class="headerlink" title="Configural Displays"></a>Configural Displays</h3><ul><li>Low level data: usually individual sensor data</li><li>High level relation: a more global and general display of what the data means</li><li>Emergent property or emergent feature: a pattern or shape that is created from the low level data, is recognisable and has meaning</li></ul><h3 id="Separable"><a href="#Separable" class="headerlink" title="Separable"></a>Separable</h3><p>Show each variable as a single output</p><h3 id="Separable-vs-Configural-vs-Integral"><a href="#Separable-vs-Configural-vs-Integral" class="headerlink" title="Separable vs Configural vs Integral"></a>Separable vs Configural vs Integral</h3><ul><li>Separable generally makes it easier to extract low level information</li><li>Integral Show high level information but not low level information</li><li>Configural Arrange low level data into a meaningful form,whole is greater than the sum of the parts</li><li>Configural makes it harder to extract low level information</li></ul><h2 id="Tracking-manual-control"><a href="#Tracking-manual-control" class="headerlink" title="Tracking (manual control)"></a>Tracking (manual control)</h2><p>Learning Objectives:</p><ul><li>Fitts’ Law Ability to apply theory and skills: Design Use Fitts’ Law to evaluate/predict pointing performance </li><li>Theoretical understanding: Order of control </li></ul><h3 id="Open-versus-Closed-Loop-Systems"><a href="#Open-versus-Closed-Loop-Systems" class="headerlink" title="Open versus Closed Loop Systems"></a>Open versus Closed Loop Systems</h3><h3 id="Tracking-Terms"><a href="#Tracking-Terms" class="headerlink" title="Tracking Terms"></a>Tracking Terms</h3><ul><li>Control movement</li><li>Controlled element</li><li>Target</li><li>Forcing function- disturbances to target</li></ul><h3 id="Pursuit-and-Compensatory"><a href="#Pursuit-and-Compensatory" class="headerlink" title="Pursuit and Compensatory"></a>Pursuit and Compensatory</h3><p>补偿追踪与尾随追踪</p><ul><li>Pursuit   <ul><li>Target moved</li><li>Usually more accurate</li></ul></li><li>Compensatory<ul><li>Target fixed</li><li>Target &amp; control movements confounded</li></ul></li></ul><h3 id="Fitts-Law"><a href="#Fitts-Law" class="headerlink" title="Fitts Law"></a>Fitts Law</h3><p>MT(movement time) = a + blog2(2A/W)</p><h3 id="Tracking-vs-pointing"><a href="#Tracking-vs-pointing" class="headerlink" title="Tracking vs. pointing"></a>Tracking vs. pointing</h3><p>Pointing as expressed by Fitts law is a very special case of tracking.<br>In pointing:</p><ul><li>Stationary target</li><li>No lag</li><li>Gain is only control system parameter</li></ul><h3 id="Gain-amp-C-D-ratio"><a href="#Gain-amp-C-D-ratio" class="headerlink" title="Gain &amp; C/D ratio"></a>Gain &amp; C/D ratio</h3><ul><li>Gain describes the change in the controlled element (display) corresponding to a movement of the control: gain = y/x..</li><li>C/D ratio describes the movement of a control needed for a given change in the display: C/D = x/y</li></ul><h3 id="Order-of-Control"><a href="#Order-of-Control" class="headerlink" title="Order of Control"></a>Order of Control</h3><ul><li>0 order: Position<ul><li>A 0 order system has <strong>no</strong> integrations between input and output</li></ul></li><li>1 order: Velocity<ul><li>A 1 order system has <strong>one</strong> integrations between input and output</li></ul></li><li>2 order Acceleration<ul><li>A 2 order system has <strong>two</strong> integrations between input and output</li></ul></li></ul><h2 id="Week-8-HIP-amp-Workload"><a href="#Week-8-HIP-amp-Workload" class="headerlink" title="Week 8 HIP &amp; Workload"></a>Week 8 HIP &amp; Workload</h2><p>Learning Objectives:</p><ol><li>What is mental workload?<br> – Subjective, performance, &amp; physiological measures</li><li>HIP &amp; human factors<br> – Working memory, absolute judgment, &amp; other aspects of the bottleneck</li><li>Mental representation &amp; difficulty</li></ol><h3 id="Basic-approaches-to-measuring-mental-workload"><a href="#Basic-approaches-to-measuring-mental-workload" class="headerlink" title="Basic approaches to measuring mental workload"></a>Basic approaches to measuring mental workload</h3><ul><li>Analytic<br>  – Task difficulty<ul><li>Number of simultaneous tasks</li></ul></li><li>Task performance <ul><li>Primary task</li><li>Secondary task</li></ul></li><li>Physiological (arousal/effort)<ul><li>heart rate</li><li>evoked response amplitude</li><li>……</li></ul></li><li>Subjective assessment<ul><li>Cooper-Harris</li><li>SWAT</li><li>NASA</li></ul></li></ul><p><img src="resources/B1FE7729E735139A9174D4D525B76743.png" alt="Screen Shot 2019-12-11 at 17.05.25.png"></p><h3 id="Selective-Attention"><a href="#Selective-Attention" class="headerlink" title="Selective Attention"></a>Selective Attention</h3><p>通道的选择性注意主要受下面因素的影响<br><img src="resources/505A2612DA444E6ABADCD8DA347E4794.png" alt="Screen Shot 2019-12-11 at 17.04.47.png"></p><h3 id="Three-aspects-of-perception"><a href="#Three-aspects-of-perception" class="headerlink" title="Three aspects of perception"></a>Three aspects of perception</h3><ol><li>Sensory based<ul><li><strong>Bottom-up</strong> feature analysis<ul><li>clear stimuli/minimize sensory similarities</li></ul></li></ul></li><li>Memory based<ul><li><strong>Unitization</strong><br>  – perceive grouped features as a whole (Gestalt)</li><li><strong>Top-down</strong> [correct guesses &amp; fill-ins]</li></ul></li></ol><h3 id="working-memory"><a href="#working-memory" class="headerlink" title="working memory"></a>working memory</h3><ul><li>working memory</li><li>long term memory</li></ul><p><img src="resources/8424FD27B3344B794E85F01390B2C560.png" alt="Screen Shot 2019-12-12 at 03.50.04.png"></p><h3 id="90’s-model-nods-to-Baddeley-amp-Schneider"><a href="#90’s-model-nods-to-Baddeley-amp-Schneider" class="headerlink" title="90’s model (nods to Baddeley &amp; Schneider)"></a>90’s model (nods to Baddeley &amp; Schneider)</h3><p><img src="resources/2D6E557C69F004CC508948E5AF2DA91B.png" alt="Screen Shot 2019-12-11 at 17.13.34.png"></p><ol><li>Central executive<ul><li>Coordinate multiple tasks (OS)</li><li>Hold &amp; manipulate info from LTM (RAM)</li><li>Control retrieval strategies from LTM (data<br>access)</li><li>Attend selectively to stimuli (time share)</li><li>协调两个存储子系统</li></ul></li><li>Visual sketchpad<ul><li>以模拟的，空间的形式保持正在使用的信息</li></ul></li><li>Phonological store<ul><li>存储以声音的形式存在的信息</li></ul></li></ol><h2 id="Human-Error-and-Reliability"><a href="#Human-Error-and-Reliability" class="headerlink" title="Human Error and Reliability"></a>Human Error and Reliability</h2><ol><li>Understanding Mechanisms underlying human error </li><li>What types of errors can be predicted? </li><li>Proficiency in information-related skills: Analysis Perform THERP analyses to predict errors for a design/task </li></ol><h3 id="Error"><a href="#Error" class="headerlink" title="Error"></a>Error</h3><ul><li>Any act with adverse consequences</li><li>An act resulting from an inappropriate intention</li><li>Keeping the pressurizer level under control</li></ul><h3 id="Action-Schema"><a href="#Action-Schema" class="headerlink" title="Action Schema"></a>Action Schema</h3><p><img src="resources/5CB1A89659EE40E74A848B269D451306.png" alt="Screen Shot 2019-12-11 at 19.22.21.png"></p><ol><li>Intention<ul><li>mode errors</li><li>description errors</li></ul></li><li>Activation<ul><li>capture errors</li><li>data-driven</li><li>associative activation</li><li>loss of activation</li><li>sequence error</li></ul></li><li>Faulty Triggering<ul><li>Out of sequence and “mangled” execution..</li></ul></li></ol><h3 id="Reliability-Engineering"><a href="#Reliability-Engineering" class="headerlink" title="Reliability Engineering"></a>Reliability Engineering</h3><ul><li>The <strong>Key</strong> to reliability is Redundancy</li></ul><h3 id="Components-in-Series"><a href="#Components-in-Series" class="headerlink" title="Components in Series"></a>Components in Series</h3><p><img src="resources/E81CF8AF176C7942E34394DC8A2FF674.png" alt="Screen Shot 2019-12-12 at 03.52.33.png"></p><h3 id="Components-in-Parallel"><a href="#Components-in-Parallel" class="headerlink" title="Components in Parallel"></a>Components in Parallel</h3><p><img src="resources/9C5CE086D1C126A6965F4602030EB667.png" alt="Screen Shot 2019-12-12 at 03.52.39.png"></p><h3 id="Tradeoffs-Redundant-or-not"><a href="#Tradeoffs-Redundant-or-not" class="headerlink" title="Tradeoffs (Redundant or not)"></a>Tradeoffs (Redundant or not)</h3><h3 id="How-redundancy-works"><a href="#How-redundancy-works" class="headerlink" title="How redundancy works"></a>How redundancy works</h3><p>For the mathematics to work out the probabilities of failure for redundant components or subsystems must be <strong>completely independent</strong></p><h3 id="What-Reliability-Engineers-do"><a href="#What-Reliability-Engineers-do" class="headerlink" title="What Reliability Engineers do"></a>What Reliability Engineers do</h3><ul><li>The primary task of a reliability engineer is to defend redundancy against <strong>unexpected violations of independence</strong>.</li><li>In a well designed system only the <strong>human operator</strong> bridges these islands of independence</li></ul><h3 id="THERP"><a href="#THERP" class="headerlink" title="THERP"></a>THERP</h3><ul><li>Quality control method for estimating errors</li><li>Model<ul><li><strong>Errors</strong>: such as reading or omitting an instructional step, or choosing the wrong switch, are presumed to occur at constant rates</li><li>If tasks can be broken down into subtasks for which errors can be predicted, then the probability of the successful completion of the overall task can be predicted</li><li>The probability of successfully completing the task (if its something like warhead assembly) is then simply the <strong>joint probability</strong> that everything is done correctly</li></ul></li></ul><h3 id="Points-on-THERP-analyses"><a href="#Points-on-THERP-analyses" class="headerlink" title="Points on THERP analyses"></a>Points on THERP analyses</h3><ul><li>Tree is not sacrosanct but a convenient way to organize independent tasks</li><li>Probabilities for errors and recoveries should be entered into trees at level of aggregation at which independence holds</li><li>Method is ultimately simply a way to make our commonsense about the likelihood of failing more explicit</li></ul><p><img src="resources/81B1E00736CFE5835EC5B0CB6ED3E51C.png" alt="Screen Shot 2019-12-11 at 20.05.14.png"></p><h2 id="Human-Computer-interaction"><a href="#Human-Computer-interaction" class="headerlink" title="Human-Computer interaction"></a>Human-Computer interaction</h2><ol><li>Understand basic assumptions and mechanics of constructing GOMS keystroke level model</li><li>Contrast the HIP vs. Ecological vision of problems in HCI</li><li>Standard visualizations and the problem(s) they solve- finding context for local views</li></ol><h3 id="GOMS-Models"><a href="#GOMS-Models" class="headerlink" title="GOMS Models"></a>GOMS Models</h3><p>用于设计的用户绩效模型</p><ul><li>Goals</li><li>Operators </li><li>Methods</li><li>Selection rules</li></ul><p>用户可以通过方法和选择形成他们要达到的目标和子目标。方法是一系列知觉的、认知的或行为操作的步骤。</p><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ul><li>列出目标和子目标</li><li>明确达到问题的方法</li><li>写出选择关系</li><li>揭示问题</li></ul><h3 id="Norman’s-7-Stages-amp-design"><a href="#Norman’s-7-Stages-amp-design" class="headerlink" title="Norman’s 7 Stages &amp; design"></a>Norman’s 7 Stages &amp; design</h3><p>用户导向界面设计的七阶段理论<br><img src="resources/64FB6BD735C5ACC5560D714AD40CEBF1.png" alt="Screen Shot 2019-12-11 at 20.22.00.png"></p><ol><li>实施的鸿沟：用户的目的和软件所支持的行为之间的错误匹配（通过好的人因学方案解决，input tracking position）</li><li>评价的鸿沟：用户期望与系统状态的不匹配 (好的说明性显示)</li></ol><h3 id="Mplications-of-working-memory-amp-absolute-judgment-limitations"><a href="#Mplications-of-working-memory-amp-absolute-judgment-limitations" class="headerlink" title="Mplications of working memory &amp; absolute judgment limitations"></a>Mplications of working memory &amp; absolute judgment limitations</h3><ul><li>Recognition is MUCH easier than Recall :Make the objects of working memory available to perception..</li></ul><h3 id="The-Power-Law-of-Practice"><a href="#The-Power-Law-of-Practice" class="headerlink" title="The Power Law of Practice"></a>The Power Law of Practice</h3><p>Improvement in performance is logarithmic in the N of trials</p><h3 id="Mental-Models"><a href="#Mental-Models" class="headerlink" title="Mental Models"></a>Mental Models</h3><p>跨越实施和评价的鸿沟依赖于心理模型，好的心理模型可以帮助房主错误和改进绩效</p><ul><li>Allows people to make predictions about how things will work</li><li>Mental models are often wrong</li></ul><h3 id="Conceptual-Models"><a href="#Conceptual-Models" class="headerlink" title="Conceptual Models"></a>Conceptual Models</h3><p>使用户看不见的部分变为可见 比如”房间”</p><h3 id="State-Transition-Models-of-Devices"><a href="#State-Transition-Models-of-Devices" class="headerlink" title="State Transition Models of Devices"></a>State Transition Models of Devices</h3><p><img src="resources/B8C3FE244974F3F50AE2A26BFAE36589.png" alt="Screen Shot 2019-12-11 at 20.45.26.png"></p><h2 id="Speech"><a href="#Speech" class="headerlink" title="Speech"></a>Speech</h2><ul><li>Theoretical Understanding: Problems in building applications using speech recognition Design</li><li>Strategies for getting good performance despite poor recognition </li><li>Theoretical Understanding: Computer supported cooperative work </li><li>Theoretical Understanding: Design Strategies for using sensors to augment human inputs and improve interaction </li></ul><h3 id="Basic-Speech-Parameters"><a href="#Basic-Speech-Parameters" class="headerlink" title="Basic Speech Parameters"></a>Basic Speech Parameters</h3><ul><li>Speaker Dependent/Independent</li><li>Size/Type of Vocabulary</li><li>Isolated word vs. Continuous Speech</li><li>Grammar/constraint</li><li>Environment/noise tolerance</li><li>Noise canceling unidirectional microphones<br>– Quiet environments</li></ul><h3 id="Rec-System-maximizes-chance-of-getting-things-right-by"><a href="#Rec-System-maximizes-chance-of-getting-things-right-by" class="headerlink" title="Rec System maximizes chance of getting things right by"></a>Rec System maximizes chance of getting things right by</h3><ul><li>Restricting vocabulary</li><li>Specifying order &amp; transitions for recognition</li><li>Associating actions/meaning with partial recognition of phrase</li><li>Use of context particularly within dialog to adjust constraints</li></ul><h3 id="Lombard-Effect"><a href="#Lombard-Effect" class="headerlink" title="Lombard Effect"></a>Lombard Effect</h3><p>During noise, speakers have an automatic normalization response that causes systematic speech modifications, including increased volume, reduced speaking rate, and changes in articulation and pitch.</p><h3 id="Issues-in-Ubicomp"><a href="#Issues-in-Ubicomp" class="headerlink" title="Issues in Ubicomp"></a>Issues in Ubicomp</h3><p>Issues in Ubicomp</p><ul><li>Context</li><li>Uneven conditioning</li><li>Inferring user intent</li><li>System interoperation</li></ul><h2 id="Decision-Making-amp-Diagnosis"><a href="#Decision-Making-amp-Diagnosis" class="headerlink" title="Decision Making &amp; Diagnosis"></a>Decision Making &amp; Diagnosis</h2><p>Learning Objectives</p><ul><li>Theoretical Understanding: Normative vs. Behavioral theories of decision making</li><li>Models of decision making in diagnosis Name &amp; illustrate standard fallacies in decision making </li></ul><h3 id="Rational-Decision-Making"><a href="#Rational-Decision-Making" class="headerlink" title="Rational Decision Making"></a>Rational Decision Making</h3><ul><li>A rational decision maker is one who chooses the alternative which maximizes his expected utility.</li><li>A rational decision maker is presumed to maximize her <strong>Subjective Utility</strong> which is likely to be some function of objective</li></ul><h3 id="Prospect-theory"><a href="#Prospect-theory" class="headerlink" title="Prospect theory"></a>Prospect theory</h3><p>Loss hurts more than Gain helps （抛硬币，正面赢20，反面输 10，大多数人选择不玩）</p><h3 id="Base-Rate-Fallacy"><a href="#Base-Rate-Fallacy" class="headerlink" title="Base Rate Fallacy"></a>Base Rate Fallacy</h3><p>Undervalue base rates!! </p><h3 id="Behavioral-Decision-Making"><a href="#Behavioral-Decision-Making" class="headerlink" title="Behavioral Decision Making"></a>Behavioral Decision Making</h3><ol><li>Treate extreme values as more moderate  （感知不到极端数值，就比如考试明明只剩三天，但还觉得时间很充裕不好好看hf）</li><li>Imperfections in memory （记忆缺陷， 不能收集到所有过去的信息帮助做决策，比如期中复习hf就十分紧张期末还是这样） </li><li>Inability to do complex math in our heads （做不了复杂算数） </li></ol><h3 id="Gambler’s-Fallacy"><a href="#Gambler’s-Fallacy" class="headerlink" title="Gambler’s Fallacy"></a>Gambler’s Fallacy</h3><p>Error: treating independent events as though they were dependent</p><h3 id="Availability-heuristic可得性偏差"><a href="#Availability-heuristic可得性偏差" class="headerlink" title="Availability heuristic可得性偏差"></a>Availability heuristic可得性偏差</h3><p>人们做决策总会基于 avaliability &amp; imaginability </p><h3 id="Imaginability"><a href="#Imaginability" class="headerlink" title="Imaginability"></a>Imaginability</h3><h3 id="Confirmation-bias"><a href="#Confirmation-bias" class="headerlink" title="Confirmation bias"></a>Confirmation bias</h3><p>确定偏差:简而言之就是听不进新的观点，无论怎样论证都是认为自己原本认为的是对的， 本质还是overconfidence<br>例如：给一些本身对于某件事有观点的人接受正反两面信息，人们通常都只注意到支持自己观点的理论，暗中反驳不符合自己观点的理论 </p><h3 id="Representativeness-bias-代表性偏差"><a href="#Representativeness-bias-代表性偏差" class="headerlink" title="Representativeness bias 代表性偏差"></a>Representativeness bias 代表性偏差</h3><p>人类在对事件做出判断的时候，过度关注于这个事件的某个特征，而忽略了这个事件发生的大环境概率和样本大小。<br>例如，你看到一家公司连续3年利润都翻番，然后立即对它的股票做出判断——买！错在代表性偏差。连续3年利润翻番，是一个好公司的代表性特征。但这并不意味着这家公司真的就是一家好公司，这家公司还有好多信息都被你忽略掉了。比如说，业绩可能是有意调整出来的；再比如说，这家公司未来的盈利机会消失，业绩不能持续。 </p><h3 id="Anchoring锚定效应"><a href="#Anchoring锚定效应" class="headerlink" title="Anchoring锚定效应"></a>Anchoring锚定效应</h3><p>人们在对某人某事做出判断时，易受第一印象影响从而先入为主 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Course-Description&quot;&gt;&lt;a href=&quot;#Course-Description&quot; class=&quot;headerlink&quot; title=&quot;Course Description&quot;&gt;&lt;/a&gt;Course Description&lt;/h2&gt;&lt;p&gt; This 
      
    
    </summary>
    
    
      <category term="Product Design" scheme="https://zhangruochi.com/categories/Product-Design/"/>
    
    
  </entry>
  
  <entry>
    <title>Dependency Parsing and Assignment3 of CS224n</title>
    <link href="https://zhangruochi.com/Dependency-Parsing-and-Assignment3-of-CS224n/2019/12/10/"/>
    <id>https://zhangruochi.com/Dependency-Parsing-and-Assignment3-of-CS224n/2019/12/10/</id>
    <published>2019-12-10T09:13:41.000Z</published>
    <updated>2019-12-19T18:09:55.553Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Dependency-Grammar-and-Dependency-Structure"><a href="#Dependency-Grammar-and-Dependency-Structure" class="headerlink" title="Dependency Grammar and Dependency Structure"></a>Dependency Grammar and Dependency Structure</h2><p>Parse trees in NLP, analogous to those in compilers, are used to analyze the syntactic structure of sentences. There are two main types of structures used:</p><ol><li>constituency structures</li><li>dependency structures</li></ol><p>Dependency structure of sentences shows which words depend on (modify or are arguments of) which other words. These binary asymmetric relations between the words are called dependencies and are depicted as arrows going from the <strong>head</strong> (or governor, superior, regent) to the <strong>dependent</strong> (or modifier, inferior, subordinate).  Usually these dependencies form a tree structure. They are often typed with the name of grammatical relations (subject, prepositional object, apposition, etc.). An example of such a dependency tree is shown in below</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Figure from cs224n</div></center><p>Usually some constraints:</p><ol><li>Only one word is adependent of ROOT</li><li>Don’twantcyclesA-&gt;B,B-&gt;A (tree structure)</li><li>Final issue is whether arrows can cross (non-projective) or not<ul><li>Defn: There are no crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words</li><li>Dependencies parallel to a CFG tree must be <strong>projective</strong>: Forming dependencies by taking 1 child of each category as head</li><li>But dependency theory normally does allow non-projective structures to account for displaced constituents: You can’t easily get the semantics of certain constructions right without these <strong>non-projective</strong> dependencies</li></ul></li></ol><h2 id="Parsing"><a href="#Parsing" class="headerlink" title="Parsing"></a>Parsing</h2><p>Given a parsing model M and a sentence S, derive the optimal dependency graph D for S according to M.</p><ol><li><strong>Dynamic programming</strong><br>Eisner (1996) gives a clever algorithm with complexity O(n3), by producing parse items with heads at the ends rather than in the middle</li><li><strong>Graph algorithms</strong><br>You create a Minimum Spanning Tree for a sentence McDonald et al.’s (2005) MSTParser scores dependencies independently using an ML classifier (he uses MIRA, for online learning, but it can be something else)</li><li><strong>Constraint Satisfaction</strong><br>Edges are eliminated that don’t satisfy hard constraints. Karlsson (1990), etc.</li><li><strong>Transition-based parsing</strong> or <strong>deterministic dependency parsing</strong><br>Greedy choice of attachments guided by good machine learning classifiers MaltParser (Nivre et al. 2008). Has proven highly effective.</li></ol><h2 id="Neural-Transition-Based-Dependency-Parsing"><a href="#Neural-Transition-Based-Dependency-Parsing" class="headerlink" title="Neural Transition-Based Dependency Parsing"></a>Neural Transition-Based Dependency Parsing</h2><p>A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between head words, and words which modify those heads. Your implementation will be a transition-based parser, which incrementally builds up a parse one step at a time. At every step it maintains a partial parse, which is represented as follows:</p><ul><li>A <strong>stack</strong> of words that are currently being processed. </li><li>A <strong>buffer</strong> of words yet to be processed.</li><li>A <strong>list</strong> of dependencies predicted by the parser.</li></ul><p>Initially, the stack only contains ROOT, the dependencies list is empty, and the buffer contains all words of the sentence in order. At each step, the parser applies a transition to the partial parse until its buffer is empty and the stack size is 1. The following transitions can be applied:</p><ul><li><strong>SHIFT</strong>: removes the first word from the buffer and pushes it onto the stack.</li><li><strong>LEFT-ARC</strong>: marks the second (second most recently added) item on the stack as a dependent of<br>the first item and removes the second item from the stack.</li><li><strong>RIGHT-ARC</strong>: marks the first (most recently added) item on the stack as a dependent of the second item and removes the first item from the stack.</li></ul><p>On each step, your parser will decide among the three transitions using a neural network classifier.Go through the sequence of transitions needed for parsing the sentence “I parsed this sentence correctly”. The dependency tree for the sentence is shown below. At each step, give the configuration of the stack and buffer, as well as what transition was applied this step and what new dependency was added (if any). The first three steps are provided below as an example.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="100%" height="100%">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Figure from cs224n</div></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 3</span></span><br><span class="line"><span class="string">parser_transitions.py: Algorithms for completing partial parsess.</span></span><br><span class="line"><span class="string">Sahil Chopra &lt;schopra8@stanford.edu&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PartialParse</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">        <span class="string">"""Initializes this partial parse.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param sentence (list of str): The sentence to be parsed as a list of words.</span></span><br><span class="line"><span class="string">                                        Your code should not modify the sentence.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># The sentence being parsed is kept for bookkeeping purposes. Do not alter it in your code.</span></span><br><span class="line">        self.sentence = sentence</span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (3 Lines)</span></span><br><span class="line">        <span class="comment">### Your code should initialize the following fields:</span></span><br><span class="line">        <span class="comment">###     self.stack: The current stack represented as a list with the top of the stack as the</span></span><br><span class="line">        <span class="comment">###                 last element of the list.</span></span><br><span class="line">        <span class="comment">###     self.buffer: The current buffer represented as a list with the first item on the</span></span><br><span class="line">        <span class="comment">###                  buffer as the first item of the list</span></span><br><span class="line">        <span class="comment">###     self.dependencies: The list of dependencies produced so far. Represented as a list of</span></span><br><span class="line">        <span class="comment">###             tuples where each tuple is of the form (head, dependent).</span></span><br><span class="line">        <span class="comment">###             Order for this list doesn't matter.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Note: The root token should be represented with the string "ROOT"</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line"></span><br><span class="line">        self.stack = [<span class="string">"ROOT"</span>]</span><br><span class="line">        self.buffer = sentence[:]</span><br><span class="line">        self.dependencies = []</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_step</span><span class="params">(self, transition)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single parse step by applying the given transition to this partial parse</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param transition (str): A string that equals "S", "LA", or "RA" representing the shift,</span></span><br><span class="line"><span class="string">                                left-arc, and right-arc transitions. You can assume the provided</span></span><br><span class="line"><span class="string">                                transition is a legal transition.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~7-10 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     Implement a single parsing step, i.e. the logic for the following as</span></span><br><span class="line">        <span class="comment">###     described in the pdf handout:</span></span><br><span class="line">        <span class="comment">###         1. Shift</span></span><br><span class="line">        <span class="comment">###         2. Left Arc</span></span><br><span class="line">        <span class="comment">###         3. Right Arc</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># if self.buffer and transition == "S":</span></span><br><span class="line">        <span class="comment">#     self.stack.append(self.buffer.pop(0))</span></span><br><span class="line">        <span class="comment"># elif len(self.stack) &gt;=2 and self.stack[-2] != "ROOT" and transition == "LA":</span></span><br><span class="line">        <span class="comment">#     self.dependencies.append(( self.stack[-1],self.stack[-2]))</span></span><br><span class="line">        <span class="comment">#     self.stack.pop(-2)</span></span><br><span class="line">        <span class="comment"># elif len(self.stack) &gt;= 2 and transition == "RA":</span></span><br><span class="line">        <span class="comment">#     self.dependencies.append((self.stack[-2], self.stack[-1]))</span></span><br><span class="line">        <span class="comment">#     self.stack.pop()</span></span><br><span class="line">        <span class="keyword">if</span> self.buffer <span class="keyword">and</span> transition == <span class="string">"S"</span>:</span><br><span class="line">            self.stack.append(self.buffer.pop(<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">elif</span> len(self.stack) &gt;= <span class="number">2</span> <span class="keyword">and</span> transition == <span class="string">"LA"</span>:</span><br><span class="line">            self.dependencies.append((self.stack[<span class="number">-1</span>], self.stack[<span class="number">-2</span>]))</span><br><span class="line">            self.stack.pop(<span class="number">-2</span>)</span><br><span class="line">        <span class="keyword">elif</span> len(self.stack) &gt;= <span class="number">2</span> <span class="keyword">and</span> transition == <span class="string">"RA"</span>:</span><br><span class="line">            self.dependencies.append((self.stack[<span class="number">-2</span>], self.stack[<span class="number">-1</span>]))</span><br><span class="line">            self.stack.pop(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, transitions)</span>:</span></span><br><span class="line">        <span class="string">"""Applies the provided transitions to this PartialParse</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param transitions (list of str): The list of transitions in the order they should be applied</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @return dsependencies (list of string tuples): The list of dependencies produced when</span></span><br><span class="line"><span class="string">                                                        parsing the sentence. Represented as a list of</span></span><br><span class="line"><span class="string">                                                        tuples where each tuple is of the form (head, dependent).</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> transitions:</span><br><span class="line">            self.parse_step(transition)</span><br><span class="line">        <span class="keyword">return</span> self.dependencies</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minibatch_parse</span><span class="params">(sentences, model, batch_size)</span>:</span></span><br><span class="line">    <span class="string">"""Parses a list of sentences in minibatches using a model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    @param sentences (list of list of str): A list of sentences to be parsed</span></span><br><span class="line"><span class="string">                                            (each sentence is a list of words and each word is of type string)</span></span><br><span class="line"><span class="string">    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function</span></span><br><span class="line"><span class="string">                                model.predict(partial_parses) that takes in a list of PartialParses as input and</span></span><br><span class="line"><span class="string">                                returns a list of transitions predicted for each parse. That is, after calling</span></span><br><span class="line"><span class="string">                                    transitions = model.predict(partial_parses)</span></span><br><span class="line"><span class="string">                                transitions[i] will be the next transition to apply to partial_parses[i].</span></span><br><span class="line"><span class="string">    @param batch_size (int): The number of PartialParses to include in each minibatch</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    @return dependencies (list of dependency lists): A list where each element is the dependencies</span></span><br><span class="line"><span class="string">                                                    list for a parsed sentence. Ordering should be the</span></span><br><span class="line"><span class="string">                                                    same as in sentences (i.e., dependencies[i] should</span></span><br><span class="line"><span class="string">                                                    contain the parse for sentences[i]).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dependencies = []</span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE (~8-10 Lines)</span></span><br><span class="line">    <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">    <span class="comment">###     Implement the minibatch parse algorithm as described in the pdf handout</span></span><br><span class="line">    <span class="comment">###</span></span><br><span class="line">    <span class="comment">###     Note: A shallow copy (as denoted in the PDF) can be made with the "=" sign in python, e.g.</span></span><br><span class="line">    <span class="comment">###                 unfinished_parses = partial_parses[:].</span></span><br><span class="line">    <span class="comment">###             Here `unfinished_parses` is a shallow copy of `partial_parses`.</span></span><br><span class="line">    <span class="comment">###             In Python, a shallow copied list like `unfinished_parses` does not contain new instances</span></span><br><span class="line">    <span class="comment">###             of the object stored in `partial_parses`. Rather both lists refer to the same objects.</span></span><br><span class="line">    <span class="comment">###             In our case, `partial_parses` contains a list of partial parses. `unfinished_parses`</span></span><br><span class="line">    <span class="comment">###             contains references to the same objects. Thus, you should NOT use the `del` operator</span></span><br><span class="line">    <span class="comment">###             to remove objects from the `unfinished_parses` list. This will free the underlying memory that</span></span><br><span class="line">    <span class="comment">###             is being accessed by `partial_parses` and may cause your code to crash.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> batch_size != <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    partial_parses = [PartialParse(s) <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">    unfinished_parses = partial_parses</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> unfinished_parses:</span><br><span class="line">        batch_parser = unfinished_parses[:batch_size]</span><br><span class="line">        <span class="keyword">while</span> batch_parser:</span><br><span class="line">            transitions = model.predict(batch_parser)</span><br><span class="line">            <span class="comment"># print(transitions)</span></span><br><span class="line">            <span class="keyword">for</span> parser,transition <span class="keyword">in</span> zip(batch_parser,transitions):</span><br><span class="line">                parser.parse_step(transition)</span><br><span class="line">            batch_parser = [parser <span class="keyword">for</span> parser <span class="keyword">in</span> batch_parser <span class="keyword">if</span> len(parser.stack) &gt; <span class="number">1</span> <span class="keyword">or</span> parser.buffer]</span><br><span class="line">            <span class="comment"># print(len(batch_parser))</span></span><br><span class="line">        unfinished_parses = unfinished_parses[batch_size:]</span><br><span class="line">    </span><br><span class="line">    dependencies = [parser.dependencies <span class="keyword">for</span> parser <span class="keyword">in</span> partial_parses]</span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dependencies</span><br></pre></td></tr></table></figure><p>We are now going to train a neural network to predict, given the state of the stack, buffer, and<br>dependencies, which transition should be applied next. First, the model extracts a feature vector<br>representing the current state. They can be represented as a list of integers $[w_1,w_2,\cdots,w_m]$ where m is the number of features and each $0 \leq w_i &lt; |V|$ is the index of a token in the vocabulary (|V| is the vocabulary size). First our network looks up an embedding for each word and concatenates them into a single input vector:</p><script type="math/tex; mode=display">x = [E_{w_1},\cdots,E_{w_m} ] \in \mathbb{R}^{dm}</script><p>We then compute our prediction as:</p><script type="math/tex; mode=display">\begin{aligned}& h = ReLU(xW + b_1) \\& l = hU + b_2 \\ & \hat{y} = softmax(l)\end{aligned}</script><p>where $h$ is referred to as the hidden layer,$l$ is referred to as the logits, $\hat{y}$ is referred to as the predictions. We will train the model to minimize cross-entropy loss:</p><script type="math/tex; mode=display">J(\theta) = CE(y,\hat{y}) = -\sum_{i=1}^{3}y_i log{\hat{y_i}}</script><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="100%" height="100%">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Figure from cs224n</div></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 3</span></span><br><span class="line"><span class="string">parser_model.py: Feed-Forward Neural Network for Dependency Parsing</span></span><br><span class="line"><span class="string">Sahil Chopra &lt;schopra8@stanford.edu&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ParserModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" Feedforward neural network with an embedding layer and single hidden layer.</span></span><br><span class="line"><span class="string">    The ParserModel will predict which transition should be applied to a</span></span><br><span class="line"><span class="string">    given partial parse configuration.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    PyTorch Notes:</span></span><br><span class="line"><span class="string">        - Note that "ParserModel" is a subclass of the "nn.Module" class. In PyTorch all neural networks</span></span><br><span class="line"><span class="string">            are a subclass of this "nn.Module".</span></span><br><span class="line"><span class="string">        - The "__init__" method is where you define all the layers and their respective parameters</span></span><br><span class="line"><span class="string">            (embedding layers, linear layers, dropout layers, etc.).</span></span><br><span class="line"><span class="string">        - "__init__" gets automatically called when you create a new instance of your class, e.g.</span></span><br><span class="line"><span class="string">            when you write "m = ParserModel()".</span></span><br><span class="line"><span class="string">        - Other methods of ParserModel can access variables that have "self." prefix. Thus,</span></span><br><span class="line"><span class="string">            you should add the "self." prefix layers, values, etc. that you want to utilize</span></span><br><span class="line"><span class="string">            in other ParserModel methods.</span></span><br><span class="line"><span class="string">        - For further documentation on "nn.Module" please see https://pytorch.org/docs/stable/nn.html.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embeddings, n_features=<span class="number">36</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        hidden_size=<span class="number">200</span>, n_classes=<span class="number">3</span>, dropout_prob=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        <span class="string">""" Initialize the parser model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param embeddings (Tensor): word embeddings (num_words, embedding_size)</span></span><br><span class="line"><span class="string">        @param n_features (int): number of input features</span></span><br><span class="line"><span class="string">        @param hidden_size (int): number of hidden units</span></span><br><span class="line"><span class="string">        @param n_classes (int): number of output classes</span></span><br><span class="line"><span class="string">        @param dropout_prob (float): dropout probability</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ParserModel, self).__init__()</span><br><span class="line">        self.n_features = n_features</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.dropout_prob = dropout_prob</span><br><span class="line">        self.embed_size = embeddings.shape[<span class="number">1</span>]</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.pretrained_embeddings = nn.Embedding(embeddings.shape[<span class="number">0</span>], self.embed_size)</span><br><span class="line">        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))</span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~5 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1) Construct `self.embed_to_hidden` linear layer, initializing the weight matrix</span></span><br><span class="line">        <span class="comment">###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)</span></span><br><span class="line">        <span class="comment">###     2) Construct `self.dropout` layer.</span></span><br><span class="line">        <span class="comment">###     3) Construct `self.hidden_to_logits` linear layer, initializing the weight matrix</span></span><br><span class="line">        <span class="comment">###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Note: Here, we use Xavier Uniform Initialization for our Weight initialization.</span></span><br><span class="line">        <span class="comment">###         It has been shown empirically, that this provides better initial weights</span></span><br><span class="line">        <span class="comment">###         for training networks than random uniform initialization.</span></span><br><span class="line">        <span class="comment">###         For more details checkout this great blogpost:</span></span><br><span class="line">        <span class="comment">###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization </span></span><br><span class="line">        <span class="comment">### Hints:</span></span><br><span class="line">        <span class="comment">###     - After you create a linear layer you can access the weight</span></span><br><span class="line">        <span class="comment">###       matrix via:</span></span><br><span class="line">        <span class="comment">###         linear_layer.weight</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Please see the following docs for support:</span></span><br><span class="line">        <span class="comment">###     Linear Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Linear</span></span><br><span class="line">        <span class="comment">###     Xavier Init: https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_</span></span><br><span class="line">        <span class="comment">###     Dropout: https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout</span></span><br><span class="line">        </span><br><span class="line">        self.embed_to_hidden = nn.Linear(self.embed_size * self.n_features, hidden_size)</span><br><span class="line">        self.dropout = nn.Dropout(p = self.dropout_prob)</span><br><span class="line">        self.hidden_to_logits = nn.Linear(hidden_size,self.n_classes)</span><br><span class="line">        nn.init.xavier_uniform_(self.embed_to_hidden.weight,gain=<span class="number">1</span>)</span><br><span class="line">        nn.init.xavier_uniform_(self.hidden_to_logits.weight,gain=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">embedding_lookup</span><span class="params">(self, t)</span>:</span></span><br><span class="line">        <span class="string">""" Utilize `self.pretrained_embeddings` to map input `t` from input tokens (integers)</span></span><br><span class="line"><span class="string">            to embedding vectors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            PyTorch Notes:</span></span><br><span class="line"><span class="string">                - `self.pretrained_embeddings` is a torch.nn.Embedding object that we defined in __init__</span></span><br><span class="line"><span class="string">                - Here `t` is a tensor where each row represents a list of features. Each feature is represented by an integer (input token).</span></span><br><span class="line"><span class="string">                - In PyTorch the Embedding object, e.g. `self.pretrained_embeddings`, allows you to</span></span><br><span class="line"><span class="string">                    go from an index to embedding. Please see the documentation (https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)</span></span><br><span class="line"><span class="string">                    to learn how to use `self.pretrained_embeddings` to extract the embeddings for your tensor `t`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            @param t (Tensor): input tensor of tokens (batch_size, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            @return x (Tensor): tensor of embeddings for words represented in t</span></span><br><span class="line"><span class="string">                                (batch_size, n_features * embed_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~1-3 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1) Use `self.pretrained_embeddings` to lookup the embeddings for the input tokens in `t`.</span></span><br><span class="line">        <span class="comment">###     2) After you apply the embedding lookup, you will have a tensor shape (batch_size, n_features, embedding_size).</span></span><br><span class="line">        <span class="comment">###         Use the tensor `view` method to reshape the embeddings tensor to (batch_size, n_features * embedding_size)</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Note: In order to get batch_size, you may need use the tensor .size() function:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">###  Please see the following docs for support:</span></span><br><span class="line">        <span class="comment">###     Embedding Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding</span></span><br><span class="line">        <span class="comment">###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line">        tmp_features = self.pretrained_embeddings(t)</span><br><span class="line">        shape = tmp_features.size()</span><br><span class="line">        x = tmp_features.view(shape[<span class="number">0</span>],shape[<span class="number">1</span>]*shape[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, t)</span>:</span></span><br><span class="line">        <span class="string">""" Run the model forward.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            PyTorch Notes:</span></span><br><span class="line"><span class="string">                - Every nn.Module object (PyTorch model) has a `forward` function.</span></span><br><span class="line"><span class="string">                - When you apply your nn.Module to an input tensor `t` this function is applied to the tensor.</span></span><br><span class="line"><span class="string">                    For example, if you created an instance of your ParserModel and applied it to some `t` as follows,</span></span><br><span class="line"><span class="string">                    the `forward` function would called on `t` and the result would be stored in the `output` variable:</span></span><br><span class="line"><span class="string">                        model = ParserModel()</span></span><br><span class="line"><span class="string">                        output = model(t) # this calls the forward function</span></span><br><span class="line"><span class="string">                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param t (Tensor): input tensor of tokens (batch_size, n_features)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)</span></span><br><span class="line"><span class="string">                                 without applying softmax (batch_size, n_classes)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">###  YOUR CODE HERE (~3-5 lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1) Apply `self.embedding_lookup` to `t` to get the embeddings</span></span><br><span class="line">        <span class="comment">###     2) Apply `embed_to_hidden` linear layer to the embeddings</span></span><br><span class="line">        <span class="comment">###     3) Apply relu non-linearity to the output of step 2 to get the hidden units.</span></span><br><span class="line">        <span class="comment">###     4) Apply dropout layer to the output of step 3.</span></span><br><span class="line">        <span class="comment">###     5) Apply `hidden_to_logits` layer to the output of step 4 to get the logits.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Note: We do not apply the softmax to the logits here, because</span></span><br><span class="line">        <span class="comment">### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Please see the following docs for support:</span></span><br><span class="line">        <span class="comment">###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu</span></span><br><span class="line">        x = self.embedding_lookup(t)</span><br><span class="line">        x = self.embed_to_hidden(x)</span><br><span class="line">        x = nn.functional.relu(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        logits = self.hidden_to_logits(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><h2 id="Runing-the-model"><a href="#Runing-the-model" class="headerlink" title="Runing the model"></a>Runing the model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 3</span></span><br><span class="line"><span class="string">run.py: Run the dependency parser.</span></span><br><span class="line"><span class="string">Sahil Chopra &lt;schopra8@stanford.edu&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> parser_model <span class="keyword">import</span> ParserModel</span><br><span class="line"><span class="keyword">from</span> utils.parser_utils <span class="keyword">import</span> minibatches, load_and_preprocess_data, AverageMeter</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------</span></span><br><span class="line"><span class="comment"># Primary Functions</span></span><br><span class="line"><span class="comment"># -----------------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(parser, train_data, dev_data, output_path, batch_size=<span class="number">1024</span>, n_epochs=<span class="number">10</span>, lr=<span class="number">0.0005</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Train the neural dependency parser.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    @param parser (Parser): Neural Dependency Parser</span></span><br><span class="line"><span class="string">    @param train_data ():</span></span><br><span class="line"><span class="string">    @param dev_data ():</span></span><br><span class="line"><span class="string">    @param output_path (str): Path to which model weights and results are written.</span></span><br><span class="line"><span class="string">    @param batch_size (int): Number of examples in a single batch</span></span><br><span class="line"><span class="string">    @param n_epochs (int): Number of training epochs</span></span><br><span class="line"><span class="string">    @param lr (float): Learning rate</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    best_dev_UAS = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE (~2-7 lines)</span></span><br><span class="line">    <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">    <span class="comment">###      1) Construct Adam Optimizer in variable `optimizer`</span></span><br><span class="line">    <span class="comment">###      2) Construct the Cross Entropy Loss Function in variable `loss_func`</span></span><br><span class="line">    <span class="comment">###</span></span><br><span class="line">    <span class="comment">### Hint: Use `parser.model.parameters()` to pass optimizer</span></span><br><span class="line">    <span class="comment">###       necessary parameters to tune.</span></span><br><span class="line">    <span class="comment">### Please see the following docs for support:</span></span><br><span class="line">    <span class="comment">###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html</span></span><br><span class="line">    <span class="comment">###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss</span></span><br><span class="line">    optimizer = optim.Adam(parser.model.parameters(),lr=lr)</span><br><span class="line">    loss_func = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        print(<span class="string">"Epoch &#123;:&#125; out of &#123;:&#125;"</span>.format(epoch + <span class="number">1</span>, n_epochs))</span><br><span class="line">        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)</span><br><span class="line">        <span class="keyword">if</span> dev_UAS &gt; best_dev_UAS:</span><br><span class="line">            best_dev_UAS = dev_UAS</span><br><span class="line">            print(<span class="string">"New best dev UAS! Saving model."</span>)</span><br><span class="line">            torch.save(parser.model.state_dict(), output_path)</span><br><span class="line">        print(<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_for_epoch</span><span class="params">(parser, train_data, dev_data, optimizer, loss_func, batch_size)</span>:</span></span><br><span class="line">    <span class="string">""" Train the neural dependency parser for single epoch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note: In PyTorch we can signify train versus test and automatically have</span></span><br><span class="line"><span class="string">    the Dropout Layer applied and removed, accordingly, by specifying</span></span><br><span class="line"><span class="string">    whether we are training, `model.train()`, or evaluating, `model.eval()`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    @param parser (Parser): Neural Dependency Parser</span></span><br><span class="line"><span class="string">    @param train_data ():</span></span><br><span class="line"><span class="string">    @param dev_data ():</span></span><br><span class="line"><span class="string">    @param optimizer (nn.Optimizer): Adam Optimizer</span></span><br><span class="line"><span class="string">    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function</span></span><br><span class="line"><span class="string">    @param batch_size (int): batch size</span></span><br><span class="line"><span class="string">    @param lr (float): learning rate</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    parser.model.train() <span class="comment"># Places model in "train" mode, i.e. apply dropout layer</span></span><br><span class="line">    n_minibatches = math.ceil(len(train_data) / batch_size)</span><br><span class="line">    loss_meter = AverageMeter()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tqdm(total=(n_minibatches)) <span class="keyword">as</span> prog:</span><br><span class="line">        <span class="keyword">for</span> i, (train_x, train_y) <span class="keyword">in</span> enumerate(minibatches(train_data, batch_size)):</span><br><span class="line">            optimizer.zero_grad()   <span class="comment"># remove any baggage in the optimizer</span></span><br><span class="line">            loss = <span class="number">0.</span> <span class="comment"># store loss for this batch here</span></span><br><span class="line">            train_x = torch.from_numpy(train_x).long()</span><br><span class="line">            train_y = torch.from_numpy(train_y.nonzero()[<span class="number">1</span>]).long()</span><br><span class="line"></span><br><span class="line">            <span class="comment">### YOUR CODE HERE (~5-10 lines)</span></span><br><span class="line">            <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">            <span class="comment">###      1) Run train_x forward through model to produce `logits`</span></span><br><span class="line">            <span class="comment">###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.</span></span><br><span class="line">            <span class="comment">###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss</span></span><br><span class="line">            <span class="comment">###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)</span></span><br><span class="line">            <span class="comment">###         are the predictions (y^ from the PDF).</span></span><br><span class="line">            <span class="comment">###      3) Backprop losses</span></span><br><span class="line">            <span class="comment">###      4) Take step with the optimizer</span></span><br><span class="line">            <span class="comment">### Please see the following docs for support:</span></span><br><span class="line">            <span class="comment">###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step</span></span><br><span class="line">            logits = parser.model.forward(train_x)</span><br><span class="line">            loss = loss_func(logits,train_y)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            <span class="comment">### END YOUR CODE</span></span><br><span class="line">            prog.update(<span class="number">1</span>)</span><br><span class="line">            loss_meter.update(loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Average Train Loss: &#123;&#125;"</span>.format(loss_meter.avg))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Evaluating on dev set"</span>,)</span><br><span class="line">    parser.model.eval() <span class="comment"># Places model in "eval" mode, i.e. don't apply dropout layer</span></span><br><span class="line">    dev_UAS, _ = parser.parse(dev_data)</span><br><span class="line">    print(<span class="string">"- dev UAS: &#123;:.2f&#125;"</span>.format(dev_UAS * <span class="number">100.0</span>))</span><br><span class="line">    <span class="keyword">return</span> dev_UAS</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># Note: Set debug to False, when training on entire corpus</span></span><br><span class="line">    debug = <span class="keyword">True</span></span><br><span class="line">    <span class="comment"># debug = False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(torch.__version__ == <span class="string">"1.0.0"</span>),  <span class="string">"Please install torch version 1.0.0"</span></span><br><span class="line"></span><br><span class="line">    print(<span class="number">80</span> * <span class="string">"="</span>)</span><br><span class="line">    print(<span class="string">"INITIALIZING"</span>)</span><br><span class="line">    print(<span class="number">80</span> * <span class="string">"="</span>)</span><br><span class="line">    parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)</span><br><span class="line"></span><br><span class="line">    start = time.time()</span><br><span class="line">    model = ParserModel(embeddings)</span><br><span class="line">    parser.model = model</span><br><span class="line">    print(<span class="string">"took &#123;:.2f&#125; seconds\n"</span>.format(time.time() - start))</span><br><span class="line"></span><br><span class="line">    print(<span class="number">80</span> * <span class="string">"="</span>)</span><br><span class="line">    print(<span class="string">"TRAINING"</span>)</span><br><span class="line">    print(<span class="number">80</span> * <span class="string">"="</span>)</span><br><span class="line">    output_dir = <span class="string">"results/&#123;:%Y%m%d_%H%M%S&#125;/"</span>.format(datetime.now())</span><br><span class="line">    output_path = output_dir + <span class="string">"model.weights"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(output_dir):</span><br><span class="line">        os.makedirs(output_dir)</span><br><span class="line"></span><br><span class="line">    train(parser, train_data, dev_data, output_path, batch_size=<span class="number">1024</span>, n_epochs=<span class="number">10</span>, lr=<span class="number">0.0005</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> debug:</span><br><span class="line">        print(<span class="number">80</span> * <span class="string">"="</span>)</span><br><span class="line">        print(<span class="string">"TESTING"</span>)</span><br><span class="line">        print(<span class="number">80</span> * <span class="string">"="</span>)</span><br><span class="line">        print(<span class="string">"Restoring the best model weights found on the dev set"</span>)</span><br><span class="line">        parser.model.load_state_dict(torch.load(output_path))</span><br><span class="line">        print(<span class="string">"Final evaluation on test set"</span>,)</span><br><span class="line">        parser.model.eval()</span><br><span class="line">        UAS, dependencies = parser.parse(test_data)</span><br><span class="line">        print(<span class="string">"- test UAS: &#123;:.2f&#125;"</span>.format(UAS * <span class="number">100.0</span>))</span><br><span class="line">        print(<span class="string">"Done!"</span>)</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>slides and course notes of <a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">cs224n</a></li></ol>]]></content>
    
    <summary type="html">
    
      the course note of deependency parsing and the details of assignment 3
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>CS224n Assignment 2</title>
    <link href="https://zhangruochi.com/CS224n-Assignment-2/2019/12/07/"/>
    <id>https://zhangruochi.com/CS224n-Assignment-2/2019/12/07/</id>
    <published>2019-12-08T00:57:10.000Z</published>
    <updated>2019-12-19T18:10:14.128Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.gradcheck <span class="keyword">import</span> gradcheck_naive</span><br><span class="line"><span class="keyword">from</span> utils.utils <span class="keyword">import</span> normalizeRows, softmax</span><br></pre></td></tr></table></figure><h2 id="Helper-functions"><a href="#Helper-functions" class="headerlink" title="Helper functions"></a>Helper functions</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid function for the input here.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array.</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE</span></span><br><span class="line">    s = <span class="number">1.</span>/(<span class="number">1.</span> + np.exp(-x))</span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getNegativeSamples</span><span class="params">(outsideWordIdx, dataset, K)</span>:</span></span><br><span class="line">    <span class="string">""" Samples K indexes which are not the outsideWordIdx """</span></span><br><span class="line"></span><br><span class="line">    negSampleWordIndices = [<span class="keyword">None</span>] * K</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(K):</span><br><span class="line">        newidx = dataset.sampleTokenIdx()</span><br><span class="line">        <span class="keyword">while</span> newidx == outsideWordIdx:</span><br><span class="line">            newidx = dataset.sampleTokenIdx()</span><br><span class="line">        negSampleWordIndices[k] = newidx</span><br><span class="line">    <span class="keyword">return</span> negSampleWordIndices</span><br></pre></td></tr></table></figure><h2 id="Naive-Softmax-Loss-And-Its-Gradient"><a href="#Naive-Softmax-Loss-And-Its-Gradient" class="headerlink" title="Naive Softmax Loss And Its Gradient"></a>Naive Softmax Loss And Its Gradient</h2><p>In word2vec, the conditional probability distribution is given by taking vector dot-products and applying the softmax function:</p><script type="math/tex; mode=display">P(o\|c) = \frac{exp^{u_o^{T}v_c}}{\sum_{w\in v}exp^{u_w^{T}v_c}}</script><ul><li>$u_o$ is the ‘outside’ vector representing outside word o</li><li>$v_c$ is the ‘center’ vector representing center word c</li></ul><p>The Cross Entropy Loss between the true (discrete) probability distribution p and another distribution q is:</p><script type="math/tex; mode=display">-\sum_i p_i log(q_i)</script><p>So that the naive-softmax loss for word2vec given in following equation is the same as the cross-entropy loss between $y$ and $\hat{y}$:</p><script type="math/tex; mode=display">-\sum_{w \in Vocab} y_w log(\hat{y}_w) = -log(\hat{y}_o)</script><p>For the backpropagation, lets introduce the intermediate variable $p$, which is a vector of the (normalized) probabilities. The loss for one example is:</p><script type="math/tex; mode=display">p_k = \frac{e^{f_k}}{ \sum_j e^{f_j} } \hspace{1in} L_i =-\log\left(p_{y_i}\right)</script><p>We now wish to understand how the computed scores inside $f$ should change to decrease the loss $L_i$  that this example contributes to the full objective. In other words, we want to derive the gradient $\frac{\partial L_i}{\partial f_k}$. The loss $L_i$ is computed from $p$ which in turn depends on $f$.</p><script type="math/tex; mode=display">\frac{\partial L_i }{ \partial f_k } = p_k - \mathbb{1}(y_i = k)</script><p>Notice how elegant and simple this expression is. Suppose the probabilities we computed were <code>p = [0.2, 0.3, 0.5]</code>, and that the correct class was the middle one (with probability <code>0.3</code>). According to this derivation the gradient on the scores would be <code>df = [0.2, -0.7, 0.5]</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naiveSoftmaxLossAndGradient</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    centerWordVec,</span></span></span><br><span class="line"><span class="function"><span class="params">    outsideWordIdx,</span></span></span><br><span class="line"><span class="function"><span class="params">    outsideVectors,</span></span></span><br><span class="line"><span class="function"><span class="params">    dataset</span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    <span class="string">""" Naive Softmax loss &amp; gradient function for word2vec models</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Implement the naive softmax loss and gradients between a center word's </span></span><br><span class="line"><span class="string">    embedding and an outside word's embedding. This will be the building block</span></span><br><span class="line"><span class="string">    for our word2vec models.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    centerWordVec -- numpy ndarray, center word's embedding</span></span><br><span class="line"><span class="string">                    (v_c in the pdf handout)</span></span><br><span class="line"><span class="string">    outsideWordIdx -- integer, the index of the outside word</span></span><br><span class="line"><span class="string">                    (o of u_o in the pdf handout)</span></span><br><span class="line"><span class="string">    outsideVectors -- outside vectors (rows of matrix) for all words in vocab</span></span><br><span class="line"><span class="string">                      (U in the pdf handout)</span></span><br><span class="line"><span class="string">    dataset -- needed for negative sampling, unused here.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    loss -- naive softmax loss</span></span><br><span class="line"><span class="string">    gradCenterVec -- the gradient with respect to the center word vector</span></span><br><span class="line"><span class="string">                     (dJ / dv_c in the pdf handout)</span></span><br><span class="line"><span class="string">    gradOutsideVecs -- the gradient with respect to all the outside word vectors</span></span><br><span class="line"><span class="string">                    (dJ / dU)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### Please use the provided softmax function (imported earlier in this file)</span></span><br><span class="line">    <span class="comment">### This numerically stable implementation helps you avoid issues pertaining</span></span><br><span class="line">    <span class="comment">### to integer overflow. </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># centerWordVec:  (embedding_dim,1)</span></span><br><span class="line">    <span class="comment"># outsideVectors: (vocab_size,embedding_dim)</span></span><br><span class="line"></span><br><span class="line">    scores = np.matmul(outsideVectors, centerWordVec)   <span class="comment"># (vocab_size,1)</span></span><br><span class="line">    <span class="comment"># print(scores.shape)</span></span><br><span class="line">    probs = softmax(scores)                          <span class="comment"># (vocab_size,1)  y_hat</span></span><br><span class="line"></span><br><span class="line">    loss = -np.log(probs[outsideWordIdx])</span><br><span class="line"></span><br><span class="line">    dscores = probs.copy()   <span class="comment"># (vocab_size,1)</span></span><br><span class="line">    dscores[outsideWordIdx] = dscores[outsideWordIdx] - <span class="number">1</span>   <span class="comment">#  y_hat minus y</span></span><br><span class="line">    gradCenterVec = np.matmul(outsideVectors.T, dscores)  <span class="comment"># (embedding_dim,1)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(dscores.shape) # (5,)</span></span><br><span class="line">    <span class="comment"># print(centerWordVec.shape) # (3,)</span></span><br><span class="line">    <span class="comment"># exit()</span></span><br><span class="line"></span><br><span class="line">    gradOutsideVecs = np.outer(dscores, centerWordVec) <span class="comment"># (vocab_size,embedding_dim)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, gradCenterVec, gradOutsideVecs</span><br></pre></td></tr></table></figure><h2 id="Negative-Sampling-Loss-And-Its-Gradient"><a href="#Negative-Sampling-Loss-And-Its-Gradient" class="headerlink" title="Negative Sampling Loss And Its Gradient"></a>Negative Sampling Loss And Its Gradient</h2><p>Now we shall consider the Negative Sampling loss, which is an alternative to the Naive Softmax loss. Assume that K negative samples (words) are drawn from the vocabulary. For simplicity of notation we shall refer to them as $w_1,w_2,\cdots,w_k$ and their outside vectors as $u_1,\cdots,u_k$. Note that $o \in {w_1, \cdots, w_k}$. For a center word c and an outside word o, the negative sampling loss function is given by:</p><script type="math/tex; mode=display">J_{neg-sample}(v_c,o,U) = -log(\sigma(u_o^{T}v_c)) - \sum_{k=1}^{k}log(\sigma(-u_k^{T},v_c))</script><p>The sigmoid function and its gradient is as follows:</p><script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+e^{-x}} \\\\\rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right) = \left( 1 - \sigma(x) \right) \sigma(x)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">negSamplingLossAndGradient</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    centerWordVec,</span></span></span><br><span class="line"><span class="function"><span class="params">    outsideWordIdx,</span></span></span><br><span class="line"><span class="function"><span class="params">    outsideVectors,</span></span></span><br><span class="line"><span class="function"><span class="params">    dataset,</span></span></span><br><span class="line"><span class="function"><span class="params">    K=<span class="number">10</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    <span class="string">""" Negative sampling loss function for word2vec models</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Implement the negative sampling loss and gradients for a centerWordVec</span></span><br><span class="line"><span class="string">    and a outsideWordIdx word vector as a building block for word2vec</span></span><br><span class="line"><span class="string">    models. K is the number of negative samples to take.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note: The same word may be negatively sampled multiple times. For</span></span><br><span class="line"><span class="string">    example if an outside word is sampled twice, you shall have to</span></span><br><span class="line"><span class="string">    double count the gradient with respect to this word. Thrice if</span></span><br><span class="line"><span class="string">    it was sampled three times, and so forth.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Negative sampling of words is done for you. Do not modify this if you</span></span><br><span class="line">    <span class="comment"># wish to match the autograder and receive points!</span></span><br><span class="line">    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)</span><br><span class="line">    indices = [outsideWordIdx] + negSampleWordIndices</span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### Please use your implementation of sigmoid in here.</span></span><br><span class="line">    gradCenterVec   = np.zeros(centerWordVec.shape)</span><br><span class="line">    gradOutsideVecs = np.zeros(outsideVectors.shape)</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    u_o = outsideVectors[outsideWordIdx]</span><br><span class="line">    z = sigmoid(np.dot(u_o,centerWordVec))</span><br><span class="line">    loss -= np.log(z)</span><br><span class="line">    gradCenterVec += u_o*(z<span class="number">-1</span>)</span><br><span class="line">    gradOutsideVecs[outsideWordIdx] = centerWordVec*(z<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(K):</span><br><span class="line">        neg_id = indices[i+<span class="number">1</span>]</span><br><span class="line">        u_k = outsideVectors[neg_id]</span><br><span class="line">        z = sigmoid(-np.dot(u_k,centerWordVec))</span><br><span class="line">        loss -= np.log(z)</span><br><span class="line">        gradCenterVec += u_k*(<span class="number">1</span>-z)</span><br><span class="line">        gradOutsideVecs[neg_id] += centerWordVec*(<span class="number">1</span>-z)</span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, gradCenterVec, gradOutsideVecs</span><br></pre></td></tr></table></figure><h2 id="SkipGram"><a href="#SkipGram" class="headerlink" title="SkipGram"></a>SkipGram</h2><p>Suppose the center word is $c = w_t$ and the context window is $[w_{t-m},\cdots,w_{t-1},\cdots, w_{t}, \cdots, w_{t+1}, \cdots,w_{t+m} ]$, where m is the context window size. Recall that for the skip-gram version of word2vec, the total loss for the context window is:</p><script type="math/tex; mode=display">J_{skip-gram} (v_c,w_{t−m},\cdots,w_{t+m}, U) = \sum_{-m \leq j \leq m, j \neq 0} J(v_c,w_{t+j},U)</script><p>Here, $J(v_c,w_{t+j},U)$ represents an arbitrary loss term for the center word $c = w_t$ and outside word</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">skipgram</span><span class="params">(currentCenterWord, windowSize, outsideWords, word2Ind,</span></span></span><br><span class="line"><span class="function"><span class="params">             centerWordVectors, outsideVectors, dataset,</span></span></span><br><span class="line"><span class="function"><span class="params">             word2vecLossAndGradient=naiveSoftmaxLossAndGradient)</span>:</span></span><br><span class="line">    <span class="string">""" Skip-gram model in word2vec</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Implement the skip-gram model in this function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    currentCenterWord -- a string of the current center word</span></span><br><span class="line"><span class="string">    windowSize -- integer, context window size</span></span><br><span class="line"><span class="string">    outsideWords -- list of no more than 2*windowSize strings, the outside words</span></span><br><span class="line"><span class="string">    word2Ind -- a dictionary that maps words to their indices in</span></span><br><span class="line"><span class="string">              the word vector list</span></span><br><span class="line"><span class="string">    centerWordVectors -- center word vectors (as rows) for all words in vocab</span></span><br><span class="line"><span class="string">                        (V in pdf handout)</span></span><br><span class="line"><span class="string">    outsideVectors -- outside word vectors (as rows) for all words in vocab</span></span><br><span class="line"><span class="string">                    (U in pdf handout)</span></span><br><span class="line"><span class="string">    word2vecLossAndGradient -- the loss and gradient function for</span></span><br><span class="line"><span class="string">                               a prediction vector given the outsideWordIdx</span></span><br><span class="line"><span class="string">                               word vectors, could be one of the two</span></span><br><span class="line"><span class="string">                               loss functions you implemented above.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    loss -- the loss function value for the skip-gram model</span></span><br><span class="line"><span class="string">            (J in the pdf handout)</span></span><br><span class="line"><span class="string">    gradCenterVecs -- the gradient with respect to the center word vectors</span></span><br><span class="line"><span class="string">            (dJ / dV in the pdf handout)</span></span><br><span class="line"><span class="string">    gradOutsideVectors -- the gradient with respect to the outside word vectors</span></span><br><span class="line"><span class="string">                        (dJ / dU in the pdf handout)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    gradCenterVecs = np.zeros(centerWordVectors.shape)</span><br><span class="line">    gradOutsideVectors = np.zeros(outsideVectors.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE</span></span><br><span class="line">    center_id = word2Ind[currentCenterWord]</span><br><span class="line">    centerWordVec = centerWordVectors[center_id]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> outsideWords:</span><br><span class="line">        outside_id = word2Ind[word]</span><br><span class="line">        loss_mini, gradCenter_mini, gradOutside_mini= \</span><br><span class="line">        word2vecLossAndGradient(centerWordVec=centerWordVec,</span><br><span class="line">            outsideWordIdx=outside_id,outsideVectors=outsideVectors,dataset=dataset)</span><br><span class="line">        loss += loss_mini</span><br><span class="line">        <span class="comment"># print(gradCenterVecs[center_id].shape, gradCenter_mini.shape)</span></span><br><span class="line">        <span class="comment"># exit()</span></span><br><span class="line">        gradCenterVecs[center_id] += gradCenter_mini</span><br><span class="line">        gradOutsideVectors += gradOutside_mini</span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, gradCenterVecs, gradOutsideVectors</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      The details of the assignments 2
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Computational Graph</title>
    <link href="https://zhangruochi.com/Computational-Graph/2019/12/06/"/>
    <id>https://zhangruochi.com/Computational-Graph/2019/12/06/</id>
    <published>2019-12-07T01:21:51.000Z</published>
    <updated>2019-12-19T18:10:04.656Z</updated>
    
    <content type="html"><![CDATA[<h2 id="General-Computation-Graph"><a href="#General-Computation-Graph" class="headerlink" title="General Computation Graph"></a>General Computation Graph</h2><ol><li>Fprop: visit nodes in <strong>topological sort</strong> order<ul><li>Compute value of node given predecessors</li></ul></li><li>Bprop:<ul><li>initialize output gradient = 1 </li><li>visit nodes in reverse order:<ul><li>Compute gradient wrt each node using gradient wrt successors<script type="math/tex; mode=display">{y_1,y_2, \cdots, y_n} = successors of x</script><script type="math/tex; mode=display">\frac{\partial z}{\partial x} = \sum_{i=1}^{n}\frac{\partial z}{\partial y_i}\frac{\partial y_i}{\partial x}</script>Done correctly, big O() complexity of fprop and bprop is the same</li></ul></li></ul></li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture note of cs224n</div></center><h2 id="Automatic-Differentiation"><a href="#Automatic-Differentiation" class="headerlink" title="Automatic Differentiation"></a>Automatic Differentiation</h2><ul><li>The gradient computation canbe automatically inferred from the symbolic expression of the fprop</li><li>Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output</li><li>Modern DL frameworks(Tensorflow, PyTorch, etc.) do backpropagation for you but mainly leave layer/node writer to hand-calculate the local derivative</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture note of cs224n</div></center><h2 id="Example-Sigmoid"><a href="#Example-Sigmoid" class="headerlink" title="Example: Sigmoid"></a>Example: Sigmoid</h2><p>The gates we introduced above are relatively arbitrary. Any kind of <strong>differentiable function</strong> can act as a gate, and we can group multiple gates into a single gate, or decompose a function into multiple gates whenever it is convenient. Lets look at another expression that illustrates this point:</p><script type="math/tex; mode=display">f(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}</script><p>We have the knowlege of derivatives</p><script type="math/tex; mode=display">\begin{aligned}f(x) = \frac{1}{x} \hspace{1in} & \rightarrow \hspace{1in} \frac{df}{dx} = -1/x^2 \\f_c(x) = c + x\hspace{1in}  & \rightarrow  \hspace{1in} \frac{df}{dx} = 1 \\f(x) = e^x\hspace{1in}  & \rightarrow  \hspace{1in} \frac{df}{dx} = e^x \\f_a(x) = ax\hspace{1in} & \rightarrow  \hspace{1in} \frac{df}{dx} = a \\\end{aligned}</script><p>the picture blow shows the visual representation of the computation. The forward pass computes values from inputs to output (shown in green). The backward pass then performs backpropagation which starts at the end and recursively applies the chain rule to compute the gradients (shown in red) all the way to the inputs of the circuit. The gradients can be thought of as flowing backwards through the circuit.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="80%" height="80%">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">picture from lecture note of cs231n</div></center><p>It turns out that the derivative of the sigmoid function with respect to its input simplifies if you perform the derivation (after a fun tricky part where we add and subtract a 1 in the numerator):</p><script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+e^{-x}} \\\\\rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right) = \left( 1 - \sigma(x) \right) \sigma(x)</script><p>As we see, the gradient turns out to simplify and becomes surprisingly simple. For example, the sigmoid expression receives the input 1.0 and computes the output 0.73 during the forward pass. The derivation above shows that the local gradient would simply be (1 - 0.73) * 0.73 = 0.2, as the circuit computed before (see the image above), except this way it would be done with a single, simple and efficient expression (and with less numerical issues). Therefore, in any real practical application it would be very useful to group these operations into a single gate. Lets see the backprop for this neuron in code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">w = [<span class="number">2</span>,<span class="number">-3</span>,<span class="number">-3</span>] <span class="comment"># assume some random weights and data</span></span><br><span class="line">x = [<span class="number">-1</span>, <span class="number">-2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># forward pass</span></span><br><span class="line">dot = w[<span class="number">0</span>]*x[<span class="number">0</span>] + w[<span class="number">1</span>]*x[<span class="number">1</span>] + w[<span class="number">2</span>]</span><br><span class="line">f = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-dot)) <span class="comment"># sigmoid function</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># backward pass through the neuron (backpropagation)</span></span><br><span class="line">ddot = (<span class="number">1</span> - f) * f <span class="comment"># gradient on dot variable, using the sigmoid gradient derivation</span></span><br><span class="line">dx = [w[<span class="number">0</span>] * ddot, w[<span class="number">1</span>] * ddot] <span class="comment"># backprop into x</span></span><br><span class="line">dw = [x[<span class="number">0</span>] * ddot, x[<span class="number">1</span>] * ddot, <span class="number">1.0</span> * ddot] <span class="comment"># backprop into w</span></span><br><span class="line"><span class="comment"># we're done! we have the gradients on the inputs to the circuit</span></span><br></pre></td></tr></table></figure><h2 id="Staged-computation"><a href="#Staged-computation" class="headerlink" title="Staged computation"></a>Staged computation</h2><p>Lets see this with another example. Suppose that we have a function of the form:</p><script type="math/tex; mode=display">f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}</script><p>We don’t need to have an explicit function written down that evaluates the gradient. We only have to know how to compute it. Here is how we would structure the <strong>forward pass</strong> of such expression:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">3</span> <span class="comment"># example values</span></span><br><span class="line">y = <span class="number">-4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># forward pass</span></span><br><span class="line">sigy = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-y)) <span class="comment"># sigmoid in numerator   #(1)</span></span><br><span class="line">num = x + sigy <span class="comment"># numerator                               #(2)</span></span><br><span class="line">sigx = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-x)) <span class="comment"># sigmoid in denominator #(3)</span></span><br><span class="line">xpy = x + y                                              <span class="comment">#(4)</span></span><br><span class="line">xpysqr = xpy**<span class="number">2</span>                                          <span class="comment">#(5)</span></span><br><span class="line">den = sigx + xpysqr <span class="comment"># denominator                        #(6)</span></span><br><span class="line">invden = <span class="number">1.0</span> / den                                       <span class="comment">#(7)</span></span><br><span class="line">f = num * invden <span class="comment"># done!                                 #(8)</span></span><br></pre></td></tr></table></figure><p>Computing the backprop pass is easy: We’ll go backwards and for every variable along the way in the forward pass (sigy, num, sigx, xpy, xpysqr, den, invden) we will have the same variable, but one that begins with a <code>d</code>, which will hold the gradient of the output of the circuit with respect to that variable.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># backprop f = num * invden</span></span><br><span class="line">dnum = invden <span class="comment"># gradient on numerator                             #(8)</span></span><br><span class="line">dinvden = num                                                     <span class="comment">#(8)</span></span><br><span class="line"><span class="comment"># backprop invden = 1.0 / den </span></span><br><span class="line">dden = (<span class="number">-1.0</span> / (den**<span class="number">2</span>)) * dinvden                                <span class="comment">#(7)</span></span><br><span class="line"><span class="comment"># backprop den = sigx + xpysqr</span></span><br><span class="line">dsigx = (<span class="number">1</span>) * dden                                                <span class="comment">#(6)</span></span><br><span class="line">dxpysqr = (<span class="number">1</span>) * dden                                              <span class="comment">#(6)</span></span><br><span class="line"><span class="comment"># backprop xpysqr = xpy**2</span></span><br><span class="line">dxpy = (<span class="number">2</span> * xpy) * dxpysqr                                        <span class="comment">#(5)</span></span><br><span class="line"><span class="comment"># backprop xpy = x + y</span></span><br><span class="line">dx = (<span class="number">1</span>) * dxpy                                                   <span class="comment">#(4)</span></span><br><span class="line">dy = (<span class="number">1</span>) * dxpy                                                   <span class="comment">#(4)</span></span><br><span class="line"><span class="comment"># backprop sigx = 1.0 / (1 + math.exp(-x))</span></span><br><span class="line">dx += ((<span class="number">1</span> - sigx) * sigx) * dsigx <span class="comment"># Notice += !! See notes below  #(3)</span></span><br><span class="line"><span class="comment"># backprop num = x + sigy</span></span><br><span class="line">dx += (<span class="number">1</span>) * dnum                                                  <span class="comment">#(2)</span></span><br><span class="line">dsigy = (<span class="number">1</span>) * dnum                                                <span class="comment">#(2)</span></span><br><span class="line"><span class="comment"># backprop sigy = 1.0 / (1 + math.exp(-y))</span></span><br><span class="line">dy += ((<span class="number">1</span> - sigy) * sigy) * dsigy                                 <span class="comment">#(1)</span></span><br><span class="line"><span class="comment"># done! phew</span></span><br></pre></td></tr></table></figure><h2 id="Patterns"><a href="#Patterns" class="headerlink" title="Patterns"></a>Patterns</h2><ol><li><strong>add distributes the upstream gradient</strong>: The add gate always takes the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass. This follows from the fact that the local gradient for the add operation is simply +1.0, so the gradients on all inputs will exactly equal the gradients on the output because it will be multiplied by x1.0 (and remain unchanged).</li><li><strong>max “routes” the upstream gradient</strong>: The max gate routes the gradient. Unlike the add gate which distributed the gradient unchanged to all its inputs, the max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass). This is because the local gradient for a max gate is 1.0 for the highest value, and 0.0 for all other values.</li><li><strong>mul switches the upstream gradient</strong>: The multiply gate is a little less easy to interpret. Its local gradients are the input values (except switched), and this is multiplied by the gradient on its output during the chain rule. </li></ol><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="30%" height="30%">    <br><!--     <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;"></div> --></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiplyGate</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x,y)</span>:</span></span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">        z = x*y</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(dz)</span>:</span></span><br><span class="line">        dx = self.y * dz <span class="comment"># [dz/dz * dL/dz]</span></span><br><span class="line">        dy = self.x * dz <span class="comment"># [dz/dy * dL/dz]</span></span><br><span class="line">        <span class="keyword">return</span> [dx,dy]</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>lecture notes and slides from <a href="http://cs231n.github.io/optimization-2/" target="_blank" rel="noopener">http://cs231n.github.io/optimization-2/</a></li><li>lecture notes and slides from <a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/</a></li></ol>]]></content>
    
    <summary type="html">
    
      What is computational graph, how to use computationl graph ?
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Word Vectors</title>
    <link href="https://zhangruochi.com/Word-Vectors/2019/12/04/"/>
    <id>https://zhangruochi.com/Word-Vectors/2019/12/04/</id>
    <published>2019-12-04T21:14:11.000Z</published>
    <updated>2019-12-19T18:10:37.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Why-we-need-Word-Vectors"><a href="#Why-we-need-Word-Vectors" class="headerlink" title="Why we need Word Vectors ?"></a>Why we need Word Vectors ?</h2><p>We want to encode word tokens each into some vector that represents a point in some sort of “word” space. This is paramount for a number of reasons but the most intuitive reason is that perhaps there actually exists some N-dimensional space (such that N &lt;&lt; 13 million) that is sufficient to encode all semantics of our language. Each dimension would encode some meaning that we transfer using speech. For instance, semantic dimensions might indicate tense (past vs. present vs. future), count (singular vs. plural), and gender (masculine vs. feminine).</p><h2 id="One-hot-vector"><a href="#One-hot-vector" class="headerlink" title="One-hot vector"></a>One-hot vector</h2><p>Represent every word as an $\mathbb{R}^{|v|\cdot 1}$ vector with all 0s and one 1 at the index of that word in the sorted english language. $|V|$ is the size of our vocabulary. Word vectors in this type of encoding would appear as the following:</p><script type="math/tex; mode=display">W^{abandon} = \begin{bmatrix} 1 \\0 \\0 \\0 \\\vdots \\0 \\\end{bmatrix}</script><p>We represent each word as a completely independent entity. This word representation <strong>does not</strong> give us directly any notion of similarity. For instance,</p><script type="math/tex; mode=display">(W^{hotel})^{T}W^{motel} =(W^{hotel})^{T}W^{cat} = 0</script><h2 id="SVD-Based-Methods"><a href="#SVD-Based-Methods" class="headerlink" title="SVD Based Methods"></a>SVD Based Methods</h2><p>For this class of methods to find word embeddings (otherwise known as word vectors), we first loop over a massive dataset and accumulate word co-occurrence counts in some form of a matrix X, and then perform Singular Value Decomposition on X to get a<br>$USV^{T}$ decomposition. We then use the rows of U as the word embeddings for all words in our dictionary. Let us discuss a few choices of X.</p><h3 id="Word-Document-Matrix"><a href="#Word-Document-Matrix" class="headerlink" title="Word-Document Matrix"></a>Word-Document Matrix</h3><p>As our first attempt, we make the bold conjecture that words thatare related will often appear in the <strong>same documents</strong>. We use this fact to build a word-document matrix, $X$ in the following manner: Loop over billions of documents and for each time word $i$ appears in document $j$, we add one to entry $X_{ij}$. This is obviously a very large matrix $\mathbb{R}^{|v|\cdot M}$ and it scales with the number of documents (M). So perhaps we can try something better.</p><h3 id="Window-based-Co-occurrence-Matrix"><a href="#Window-based-Co-occurrence-Matrix" class="headerlink" title="Window based Co-occurrence Matrix"></a>Window based Co-occurrence Matrix</h3><p>A co-occurrence matrix counts how often things co-occur in some environment. Given some word $w_i$ occurring in the document, we consider the <em>context window</em> surrounding $w_i$. Supposing our fixed window size is $n$, then this is the $n$ preceding and $n$ subsequent words in that document, i.e. words $w_{i-n} \dots w_{i-1}$ and $w_{i+1} \dots w_{i+n}$. We build a <em>co-occurrence matrix</em> $M$, which is a symmetric word-by-word matrix in which $M_{ij}$ is the number of times $w_j$ appears inside $w_i$’s window.</p><p><strong>Example: Co-Occurrence with Fixed Window of n=1</strong>:</p><ul><li>Document 1: “all that glitters is not gold”</li><li>Document 2: “all is well that ends well”</li></ul><div class="table-container"><table><thead><tr><th>*</th><th>START</th><th>all</th><th>that</th><th>glitters</th><th>is</th><th>not</th><th>gold</th><th>well</th><th>ends</th><th>END</th></tr></thead><tbody><tr><td>START</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>all</td><td>2</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>that</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>glitters</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>is</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>not</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>gold</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>well</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td>ends</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>END</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td></tr></tbody></table></div><p><strong>Note:</strong> In NLP, we often add START and END tokens to represent the beginning and end of sentences, paragraphs or documents. In thise case we imagine START and END tokens encapsulating each document, e.g., “START All that glitters is not gold END”, and include these tokens in our co-occurrence counts.</p><p>The rows (or columns) of this matrix provide one type of word vectors (those based on word-word co-occurrence), but the vectors will be large in general (linear in the number of distinct words in a corpus). Thus, our next step is to run <em>dimensionality reduction</em>. In particular, we will run <em>SVD (Singular Value Decomposition)</em>, which is a kind of generalized <em>PCA (Principal Components Analysis)</em> to select the top $k$ principal components. Here’s a visualization of dimensionality reduction with SVD. In this picture our co-occurrence matrix is $A$ with $n$ rows corresponding to $n$ words. We obtain a full matrix decomposition, with the singular values ordered in the diagonal $S$ matrix, and our new, shorter length-$k$ word vectors in $U_k$.</p><h3 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h3><p><strong>Eigenvalues</strong> quantify the importance of information along the line of <strong>eigenvectors</strong>. Equipped with this information, we know what part of the information can be ignored and how to compress information (SVD, Dimension reduction &amp; PCA). It also helps us to extract features in developing machine learning models. Sometimes, it makes the model easier to train because of the reduction of tangled information. It also serves the purpose to visualize tangled raw data.</p><p>for Eigenvalues $\lambda$ and Eigenvector $V$, we have:</p><script type="math/tex; mode=display">AV = \lambda V</script><p>the dimension of A is $\mathbb{R}^{n\cdot n}$ and $V$ is a $\mathbb{R}^{n\cdot 1}$ vector.</p><h4 id="Diagonalizable"><a href="#Diagonalizable" class="headerlink" title="Diagonalizable"></a>Diagonalizable</h4><p>Let’s assume a matrix A has two eigenvalues and eigenvectors.</p><script type="math/tex; mode=display">Av_1 = \lambda_1 v_1</script><script type="math/tex; mode=display">Av_2 = \lambda_2 v_2</script><p>We can concatenate them together and rewrite the equations in the matrix form.</p><script type="math/tex; mode=display">A \begin{bmatrix} v1 & v2 \end{bmatrix} = \begin{bmatrix} \lambda_1 v_1 & \lambda_2 v_2 \end{bmatrix} = \begin{bmatrix} v1 & v2 \end{bmatrix} \begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix}</script><p>We can generalize it into any number of eigenvectors as</p><script type="math/tex; mode=display">AV = V\land</script><p>A square matrix A is diagonalizable if we can convert it into a diagonal matrix, like</p><script type="math/tex; mode=display">V^{-1} A V = \land</script><p>An n × n square matrix is diagonalizable if it has n linearly independent eigenvectors. If a matrix is symmetric, it is diagonalizable. If a matrix does not have repeated eigenvalue, it always generates enough linearly independent eigenvectors to diagonalize a vector. If it has repeated eigenvalues, there is no guarantee we have enough eigenvectors. Some will not be diagonalizable.</p><p>If $A$ is a square matrix with $N$ linearly independent eigenvectors ($v_1$, $v_2$, $\cdots$, $v_n$) and corresponding eigenvalues ($\lambda_1$, $\lambda_2$, $\cdots$, $\lambda_n$), we can rearrange</p><script type="math/tex; mode=display">V^{-1} A V = \land</script><p>into </p><script type="math/tex; mode=display">A = V \land V^{-1}</script><p>For example,</p><p><img src="diagonalizable.png" alt></p><h4 id="Singular-vectors-amp-singular-values"><a href="#Singular-vectors-amp-singular-values" class="headerlink" title="Singular vectors &amp; singular values"></a>Singular vectors &amp; singular values</h4><p>However, the above method is possible only if $A$ is a square matrix and $A$ has n linearly independent eigenvectors. Now, it is time to develop a solution for all matrices using SVD.</p><p>The matrix $AA^{T}$ and $A^{T}A$ are very special in linear algebra. Consider any m × n matrix A, we can multiply it with $A^{T}$ to form $AA^{T}$ and $A^{T}A$ separately. These matrices are</p><ul><li>symmetrical,</li><li>square,</li><li>at least positive semidefinite (eigenvalues are zero or positive),</li><li>both matrices have the same positive eigenvalues, and</li><li>both have the same rank r as A.</li></ul><p>We name the eigenvectors for $AA^{T}$ as $u_i$ and $A^{T}A$ as $v_i$ here and call these sets of eigenvectors $u$ and $v$ the <strong>singular vectors</strong> of A. Both matrices have the same positive eigenvalues. The square roots of these eigenvalues are called <strong>singular values</strong>. We concatenate vectors $u_i$ into $U$ and $v_i$ into $V$ to form orthogonal matrices.</p><p><strong>SVD states that any matrix A can be factorized as</strong>:</p><script type="math/tex; mode=display">A_{m\cdot n} = U_{m\cdot m} S_{m\cdot n} V_{n\cdot n}^{T}</script><p>S is a diagonal matrix with r elements equal to the root of the positive eigenvalues of $AA^{T}$ or $A^{T}A$ (both matrics have the same positive eigenvalues anyway).</p><p><img src="usv.png" alt></p><h4 id="Applying-SVD-to-the-cooccurrence-matrix"><a href="#Applying-SVD-to-the-cooccurrence-matrix" class="headerlink" title="Applying SVD to the cooccurrence matrix"></a>Applying SVD to the cooccurrence matrix</h4><p><img src="svd.png" alt="Picture of an SVD"></p><p>This reduced-dimensionality co-occurrence representation preserves semantic relationships between words, e.g. <em>doctor</em> and <em>hospital</em> will be closer than <em>doctor</em> and <em>dog</em>. </p><p>Although these methods give us word vectors that are more than sufficient to encode semantic and syntactic (part of speech) information but are associated with many other problems:</p><ul><li>The dimensions of the matrix change very often (new words are added very frequently and corpus changes in size).</li><li>The matrix is extremely sparse since most words do not co-occur.</li><li>The matrix is very high dimensional in general (≈ 10e6 × 10e6)</li><li>Quadratic cost to train (i.e. to perform SVD)</li><li>Requires the incorporation of some hacks on X to account for the drastic imbalance in word frequency</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_co_occurrence_matrix</span><span class="params">(corpus, window_size=<span class="number">4</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Compute co-occurrence matrix for the given corpus and window_size (default of 4).</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller</span></span><br><span class="line"><span class="string">              number of co-occurring words.</span></span><br><span class="line"><span class="string">              </span></span><br><span class="line"><span class="string">              For example, if we take the document "START All that glitters is not gold END" with window size of 4,</span></span><br><span class="line"><span class="string">              "All" will co-occur with "START", "that", "glitters", "is", and "not".</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            corpus (list of list of strings): corpus of documents</span></span><br><span class="line"><span class="string">            window_size (int): size of context window</span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            M (numpy matrix of shape (number of corpus words, number of corpus words)): </span></span><br><span class="line"><span class="string">                Co-occurence matrix of word counts. </span></span><br><span class="line"><span class="string">                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.</span></span><br><span class="line"><span class="string">            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    words, num_words = distinct_words(corpus)</span><br><span class="line">    M = <span class="keyword">None</span></span><br><span class="line">    word2Ind = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    word2Ind = &#123;word:idx <span class="keyword">for</span> word,idx <span class="keyword">in</span> zip(words, range(num_words))&#125;</span><br><span class="line">    M = np.zeros((num_words,num_words))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> corpus:</span><br><span class="line">        <span class="keyword">for</span> index, central_word <span class="keyword">in</span> enumerate(doc):</span><br><span class="line">            left = max(<span class="number">0</span>,index - window_size)</span><br><span class="line">            right = min(num_words, index + window_size+<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">for</span> context_word <span class="keyword">in</span> doc[left:right]:</span><br><span class="line">                <span class="keyword">if</span> context_word == central_word:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                M[word2Ind[central_word]][word2Ind[context_word]] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> M, word2Ind</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_to_k_dim</span><span class="params">(M, k=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)</span></span><br><span class="line"><span class="string">        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:</span></span><br><span class="line"><span class="string">            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            M (numpy matrix of shape (number of corpus words, number of corpus words)): co-occurence matrix of word counts</span></span><br><span class="line"><span class="string">            k (int): embedding size of each word after dimension reduction</span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.</span></span><br><span class="line"><span class="string">                    In terms of the SVD from math class, this actually returns U * S</span></span><br><span class="line"><span class="string">    """</span>    </span><br><span class="line">    n_iters = <span class="number">10</span>     <span class="comment"># Use this parameter in your call to `TruncatedSVD`</span></span><br><span class="line">    M_reduced = <span class="keyword">None</span></span><br><span class="line">    print(<span class="string">"Running Truncated SVD over %i words..."</span> % (M.shape[<span class="number">0</span>]))</span><br><span class="line">    </span><br><span class="line">    t_svd = TruncatedSVD(n_components=k, n_iter = n_iters)</span><br><span class="line">    M_reduced = t_svd.fit_transform(M)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ------------------</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Done."</span>)</span><br><span class="line">    <span class="keyword">return</span> M_reduced</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_embeddings</span><span class="params">(M_reduced, word2Ind, words)</span>:</span></span><br><span class="line">    <span class="string">""" Plot in a scatterplot the embeddings of the words specified in the list "words".</span></span><br><span class="line"><span class="string">        NOTE: do not plot all the words listed in M_reduced / word2Ind.</span></span><br><span class="line"><span class="string">        Include a label next to each point.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            M_reduced (numpy matrix of shape (number of unique words in the corpus , k)): matrix of k-dimensioal word embeddings</span></span><br><span class="line"><span class="string">            word2Ind (dict): dictionary that maps word to indices for matrix M</span></span><br><span class="line"><span class="string">            words (list of strings): words whose embeddings we want to visualize</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    print(M_reduced.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i,word <span class="keyword">in</span> enumerate(words):</span><br><span class="line">        x = M_reduced[i][<span class="number">0</span>]</span><br><span class="line">        y = M_reduced[i][<span class="number">1</span>]</span><br><span class="line">        plt.scatter(x, y, marker=<span class="string">'x'</span>, color=<span class="string">'red'</span>)</span><br><span class="line">        plt.text(x, y, word, fontsize=<span class="number">9</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h2 id="Iteration-Based-Methods-Word2vec"><a href="#Iteration-Based-Methods-Word2vec" class="headerlink" title="Iteration Based Methods - Word2vec"></a>Iteration Based Methods - Word2vec</h2><p>Instead of computing and storing global information about some huge dataset (which might be billions of sentences), we can try to create a model that will be able to learn one iteration at a time and eventually be able to encode the probability of a word given its context. The idea is to design a model whose parameters are the word vec- tors. Then, train the model on a certain objective. At every iteration we run our model, evaluate the errors, and follow an update rule that has some notion of penalizing the model parameters that caused the error. Thus, we learn our word vectors.</p><p>Word2vec is a software package that actually includes :</p><ul><li><strong>2 algorithms</strong>: continuous bag-of-words (CBOW) and skip-gram. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word.</li><li><strong>2 training methods</strong>: negative sampling and hierarchical softmax. Negative sampling defines an objective by sampling negative exam- ples, while hierarchical softmax defines an objective using an efficient tree structure to compute probabilities for all the vocabulary.</li></ul><h3 id="Language-Models"><a href="#Language-Models" class="headerlink" title="Language Models"></a>Language Models</h3><p>First, we need to create such a model that will assign a probability to a sequence of tokens. Let us start with an example:</p><blockquote><p>“The cat jumped over the puddle.”</p></blockquote><p>A good language model will give this sentence a high probability because this is a completely valid sentence, syntactically and semantically. Mathematically, we can call this probability on any given sequence of n words:</p><script type="math/tex; mode=display">P(w_1,w_2,\cdots,w_n)</script><p>We can take the unary language model approach and break apart this probability by assuming the word occurrences are completely independent:</p><script type="math/tex; mode=display">P(w_1,w_2,\cdots,w_n) = \prod_{i=1}^{n} P(w_i)</script><p>However, we know this is a bit ludicrous because we know the next word is highly contingent upon the previous sequence of words. And the silly sentence example might actually score highly. So perhaps we let the probability of the sequence depend on the pairwise probability of a word in the sequence and the word next to it. We call this the bigram model and represent it as: </p><script type="math/tex; mode=display">P(w_1,w_2,\cdots,w_n) = \prod_{i=1}^{n} P(w_i|w_{i-1})</script><p>Again this is certainly a bit naive since we are only concerning ourselves with pairs of neighboring words rather than evaluating a whole sentence, but as we will see, this representation gets us pretty far along.</p><h3 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h3><p>One approach is to create a model such that given the center word “jumped”, the model will be able to predict or generate the surrounding words “The”, “cat”, “over”, “the”, “puddle”. Here we call the word “jumped” the context. We call this type of model a Skip-Gram model.<br><img src="Skip.png" alt><br><img src="Skip_2.png" alt></p><p>We breakdown the way this model works in these 6 steps:</p><ol><li>We generate our one hot input vector $x \in \mathbb{R}^{|v|}$ of the center word.</li><li>We get our embedded word vector for the center word<script type="math/tex; mode=display">v_c = Vx  \qquad \in \mathbb{R}^{|v|}</script></li><li>Generate a score vector<script type="math/tex; mode=display">z = Uv_c  \qquad \in \mathbb{R}^{|v|}</script></li><li>Turn the score vector into probabilities,$\hat{y} = softmax(z)$<script type="math/tex; mode=display">\hat y_{c-m}, \cdots, \hat y_{c-1}, \cdots, \hat y_{c+m}</script></li><li>We desire our probability vector generated to match the true prob- abilities which is the one hot vectors of the actual output.<script type="math/tex; mode=display">y_{c-m}, \cdots, y_{c-1}, \cdots, y_{c+m}</script></li></ol><h4 id="Objective-function"><a href="#Objective-function" class="headerlink" title="Objective function"></a>Objective function</h4><p><img src="objective.png" alt></p><p><strong> How to calculate $P(o|c)$? We will use two vectors per word w</strong>:</p><ul><li>$V_w$ when w is a center word</li><li>$U_w$ when w is a context word</li></ul><p>Then for a center word c and a context word o:</p><script type="math/tex; mode=display">P(o|c) = \frac{exp^{u_o^{T}v_c}}{\sum_{w\in v}exp^{u_w^{T}v_c}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SkipGram</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""x --&gt; batch_size x word_index</span></span><br><span class="line"><span class="string">       output --&gt; batch_size x context_predicted x vocabulary"""</span>    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocabulary_size, embedding_features, context_len, padding_idx=<span class="number">0</span> )</span>:</span></span><br><span class="line">        super(SkipGram, self).__init__()</span><br><span class="line">        self.context_len = context_len</span><br><span class="line">        self.embedding = nn.Embedding(num_embeddings = vocabulary_size, embedding_dim=embedding_features, padding_idx=padding_idx)</span><br><span class="line">        self.fc = nn.Linear(in_features = embedding_features, out_features = vocabulary_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        context_out = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.context_len):</span><br><span class="line">            wordvec_x = self.embedding(x)</span><br><span class="line">            context_word_i = self.fc(wordvec_x)</span><br><span class="line">            context_out.append(context_word_i)</span><br><span class="line">        log_prob = F.log_softmax(torch.stack(context_out, dim=<span class="number">1</span>).squeeze(), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> log_prob</span><br><span class="line"></span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">model = SkipGram()</span><br><span class="line">log_prob = model(centre_word)</span><br><span class="line">loss=<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(log_prob.shape[<span class="number">1</span>]):</span><br><span class="line">    loss_i = loss_function(log_prob[:,i,], context_word_i[:,i])</span><br><span class="line">    loss *= loss_i</span><br><span class="line">loss = loss/(i+<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Continuous-Bag-of-Words-Model-CBOW"><a href="#Continuous-Bag-of-Words-Model-CBOW" class="headerlink" title="Continuous Bag of Words Model (CBOW)"></a>Continuous Bag of Words Model (CBOW)</h3><p>Another approach is to treat {“The”, “cat”, ’over”, “the’, “puddle”} as a <strong>context</strong> and from these words, be able to predict or generate the <strong>center word</strong> “jumped”. This type of model we call a Continuous Bag of Words (CBOW) Model.</p><p><img src="CBOW.png" alt></p><p>We breakdown the way this model works in these steps:</p><ol><li>We generate our one hot word vectors for the input context of size m:<script type="math/tex; mode=display">x^{(c−m)},\cdots,x^{(c−1)},x^{(c+1)},\cdots,x^{(c+m)}\in\mathbb{R}^{|v|}</script></li><li>We get our embedded word vectors for the context:<script type="math/tex; mode=display">V_{c-m} = Vx^{(c−m)},V_{c-m+1} = Vx^{(c−m+1)},\cdots,V_{c+m} = Vx^{(c+m)}</script></li><li>Average these vectors to get <script type="math/tex; mode=display">\hat{v} = \frac{v_{c-m} + v_{c-m+1} + \cdots + v_{c+m}}{2m}</script></li><li>Generate a score vector <script type="math/tex; mode=display">z = U\hat{v}  \qquad \in \mathbb{R}^{|v|}</script>As dot product of similar vectors is higher, it will push similar words close to each other in order to achieve a high score.</li><li>Turnthescoresintoprobabilities <script type="math/tex; mode=display">\hat{y} = softmax(z)  \qquad \in \mathbb{R}^{|v|}</script></li><li>We desire our probabilities generated, $\hat{y} \in \mathbb{R}^{|v|}$, to match the true probabilities, $y \in \mathbb{R}^{|v|}$ which also happens to be the one hot vector of the actual word.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""input  -- &gt; batch_size x context_size</span></span><br><span class="line"><span class="string">       output --&gt; batch_size x vocabulary"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocabulary_size, embedding_features, padding_idx=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""nn.Embedding holds a tensor of dimmension (vocabulary_size, feature_size)--&gt;N(0,1)"""</span></span><br><span class="line">        super(CBOW, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(num_embeddings = vocabulary_size, embedding_dim = embedding_features, padding_idx=padding_idx)</span><br><span class="line">        self.fc = nn.Linear(in_features = embedding_features, out_features = vocabulary_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, context_words)</span>:</span></span><br><span class="line">        wordvecs = self.embedding(context_words)</span><br><span class="line">        mean_wordvecs = wordvecs.sum(dim=<span class="number">1</span>)/x.shape[<span class="number">1</span>] </span><br><span class="line">        log_prob = F.log_softmax(self.fc(mean_wordvecs), dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> log_prob</span><br><span class="line"></span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">model  = CBOW()</span><br><span class="line">log_prob = model(context_words)</span><br><span class="line">loss = loss_function(log_prob.squeeze(), centre_word.squeeze())</span><br></pre></td></tr></table></figure><h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>Lets take a second to look at the objective function. Note that the summation over |V| is computationally huge! Any update we do or evaluation of the objective function would take O(|V|) time which if we recall is in the millions. A simple idea is we could instead just approximate it.</p><p><strong>For every training step, instead of looping over the entire vocabulary, we can just sample several negative examples!</strong> We “sample” from a noise distribution $P_n(w)$ whose probabilities match the ordering of the frequency of the vocabulary. Unlike the probabilistic model of Word2Vec where for each input word probability is computed from all the target words in the vocabulary, here for each input word has only few target words (few true and rest randomly selected false targets). <strong>The key difference compared to the probabilistic model is the use of sigmoid activation as final discriminator replacing softmax function in the probabilistic model.</strong></p><p>Given this example(We get positive example by using the same skip-grams technique, a fixed window that goes around):</p><blockquote><p>“I want a glass of orange juice to go along with my cereal”</p></blockquote><p>The sampling will look like this:</p><div class="table-container"><table><thead><tr><th style="text-align:left">Context</th><th style="text-align:left">Word</th><th style="text-align:left">target</th></tr></thead><tbody><tr><td style="text-align:left">orange</td><td style="text-align:left">juice</td><td style="text-align:left">1</td></tr><tr><td style="text-align:left">orange</td><td style="text-align:left">king</td><td style="text-align:left">0</td></tr><tr><td style="text-align:left">orange</td><td style="text-align:left">book</td><td style="text-align:left">0</td></tr><tr><td style="text-align:left">orange</td><td style="text-align:left">the</td><td style="text-align:left">0</td></tr><tr><td style="text-align:left">orange</td><td style="text-align:left">of</td><td style="text-align:left">0</td></tr></tbody></table></div><p>So the steps to generate the samples are:</p><ol><li>Pick a positive context</li><li>Pick a k negative contexts from the dictionary.<br>We will have a k negative examples to 1 positive ones in the data we are collecting.</li></ol><p><img src="negative_sampling.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Merge</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Reshape</span><br><span class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"></span><br><span class="line"><span class="comment"># build skip-gram architecture</span></span><br><span class="line">word_model = Sequential()</span><br><span class="line">word_model.add(Embedding(vocab_size, embed_size,</span><br><span class="line">                         embeddings_initializer=<span class="string">"glorot_uniform"</span>,</span><br><span class="line">                         input_length=<span class="number">1</span>))</span><br><span class="line">word_model.add(Reshape((embed_size, )))</span><br><span class="line"></span><br><span class="line">context_model = Sequential()</span><br><span class="line">context_model.add(Embedding(vocab_size, embed_size,</span><br><span class="line">                  embeddings_initializer=<span class="string">"glorot_uniform"</span>,</span><br><span class="line">                  input_length=<span class="number">1</span>))</span><br><span class="line">context_model.add(Reshape((embed_size,)))</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Merge([word_model, context_model], mode=<span class="string">"dot"</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, kernel_initializer=<span class="string">"glorot_uniform"</span>, activation=<span class="string">"sigmoid"</span>))</span><br><span class="line">model.compile(loss=<span class="string">"mean_squared_error"</span>, optimizer=<span class="string">"rmsprop"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, elem <span class="keyword">in</span> enumerate(skip_grams):</span><br><span class="line">        pair_first_elem = np.array(list(zip(*elem[<span class="number">0</span>]))[<span class="number">0</span>], dtype=<span class="string">'int32'</span>)</span><br><span class="line">        pair_second_elem = np.array(list(zip(*elem[<span class="number">0</span>]))[<span class="number">1</span>], dtype=<span class="string">'int32'</span>)</span><br><span class="line">        labels = np.array(elem[<span class="number">1</span>], dtype=<span class="string">'int32'</span>)</span><br><span class="line">        X = [pair_first_elem, pair_second_elem]</span><br><span class="line">        Y = labels</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Processed &#123;&#125; (skip_first, skip_second, relevance) pairs'</span>.format(i))</span><br><span class="line">        loss += model.train_on_batch(X,Y)  </span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Epoch:'</span>, epoch, <span class="string">'Loss:'</span>, loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## get word embedding</span></span><br><span class="line">merge_layer = model.layers[<span class="number">0</span>]</span><br><span class="line">word_model = merge_layer.layers[<span class="number">0</span>]</span><br><span class="line">word_embed_layer = word_model.layers[<span class="number">0</span>]</span><br><span class="line">weights = word_embed_layer.get_weights()[<span class="number">0</span>][<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">print(weights.shape)</span><br><span class="line">pd.DataFrame(weights, index=id2word.values()).head()</span><br></pre></td></tr></table></figure><h3 id="hierarchical-softmax"><a href="#hierarchical-softmax" class="headerlink" title="hierarchical softmax"></a>hierarchical softmax</h3><p>hierarchical softmax is a much more efficient alternative to the normal softmax. In practice, hierarchical softmax tends to be better for infrequent words, while negative sampling works better for frequent words and lower dimensional vectors.</p><p>Hierarchical softmax uses a binary tree to represent all words in the vocabulary. Each leaf of the tree is a word, and there is a unique path from root to leaf. In this model, there is no output representation for words. Instead, each node of the graph (except the root and the leaves) is associated to a vector that the model is going to learn.</p><p>In this model, the probability of a word w given a vector $w_i$, p(w|w_i),is equal to the probability of a random walk starting in the root and ending in the leaf node corresponding to w. The main advantage in computing the probability this way is that the cost is only O(log(|V|)), corresponding to the length of the path.<br><img src="hafftree.png" alt></p><p>Taking $w_2$ in above figure, we must take two left edges and<br>then a right edge to reach w2 from the root, so</p><script type="math/tex; mode=display">p(w_2) = p(n(w_2,1),left) \cdot p(n(w_2,2),left) \cdot p(n(w_2,3),right) \\ = \sigma({\theta_{n(w_2,1)}}^T \cdot h) \cdot \sigma({\theta_{n(w_2,2)}}^T \cdot h) \cdot \sigma({-\theta_{n(w_2,3)}}^T \cdot h)</script><p>Therefore,</p><script type="math/tex; mode=display">p(w)=\prod_{j=1}^{L(w)-1}\sigma( sign(w,j)\cdot {\theta_{n(w,j)}}^Th )</script><script type="math/tex; mode=display">sign(w,j)= \begin{cases} 1, & \text{if n(w,j+1) is the left child of n(w,j)} \\ -1,& \text{if n(w,j+1) is the right child of n(w,j)}\end{cases}</script><ul><li>$\theta_{n(w,j)}$ is the vector representation of $n(w,j)$</li><li>$h$ is the output of hidden layer</li></ul><h3 id="Global-Vectors-for-Word-Representation-GloVe"><a href="#Global-Vectors-for-Word-Representation-GloVe" class="headerlink" title="Global Vectors for Word Representation (GloVe)"></a>Global Vectors for Word Representation (GloVe)</h3><p>So far, we have looked at two main classes of methods to find word embeddings. </p><ul><li>The first set are count-based and rely on matrix factor- ization (e.g. LSA, HAL). While these methods effectively leverage global statistical information, they are primarily used to capture word similarities and do poorly on tasks such as word analogy, indi- cating a sub-optimal vector space structure. </li><li>The other set of methods are shallow window-based (e.g. the skip-gram and the CBOW mod- els), which learn word embeddings by making predictions in local context windows. These models demonstrate the capacity to capture complex linguistic patterns beyond word similarity, but fail to make use of the global co-occurrence statistics.</li></ul><p>In comparison, GloVe consists of a weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics. The model produces a word vector space with meaningful sub-structure. It shows state-of-the-art per- formance on the word analogy task, and outperforms other current methods on several word similarity tasks.</p><ol><li>Construct co-occurrence Matrix</li><li>Construct relationships between word vectors and co-occurrence Matrix<ul><li>Let X denote the word-word co-occurrence matrix, where $X_{ij}$ indicates the number of times word j occur in the context of word i</li><li>$w_{i}$,$\tilde{w_{j}}$ is the word vector</li><li>$b_i,b_j$ is the bias term<script type="math/tex; mode=display">w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} = \log(X_{ij}) \tag{1}</script></li></ul></li><li>Construct loss function: Mean Square Loss<script type="math/tex; mode=display">J = \sum_{i,j=1}^{V} f(X_{ij})(w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} – \log(X_{ij}) )^2</script><script type="math/tex; mode=display">f(x)=\begin{equation} \begin{cases} (x/x_{max})^{\alpha}  & \text{if} \ x < x_{max} \\ 1 & \text{otherwise} \end{cases} \end{equation}</script></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_func</span><span class="params">(x, x_max, alpha)</span>:</span></span><br><span class="line">    wx = (x/x_max)**alpha</span><br><span class="line">    wx = torch.min(wx, torch.ones_like(wx))</span><br><span class="line">    <span class="keyword">return</span> wx</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wmse_loss</span><span class="params">(weights, inputs, targets)</span>:</span></span><br><span class="line">    loss = weights * F.mse_loss(inputs, targets, reduction=<span class="string">'none'</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.mean(loss)</span><br><span class="line"></span><br><span class="line">EMBED_DIM = <span class="number">300</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GloveModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_embeddings, embedding_dim)</span>:</span></span><br><span class="line">        super(GloveModel, self).__init__()</span><br><span class="line">        self.wi = nn.Embedding(num_embeddings, embedding_dim)</span><br><span class="line">        self.wj = nn.Embedding(num_embeddings, embedding_dim)</span><br><span class="line">        self.bi = nn.Embedding(num_embeddings, <span class="number">1</span>)</span><br><span class="line">        self.bj = nn.Embedding(num_embeddings, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.wi.weight.data.uniform_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        self.wj.weight.data.uniform_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        self.bi.weight.data.zero_()</span><br><span class="line">        self.bj.weight.data.zero_()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, i_indices, j_indices)</span>:</span></span><br><span class="line">        w_i = self.wi(i_indices)</span><br><span class="line">        w_j = self.wj(j_indices)</span><br><span class="line">        b_i = self.bi(i_indices).squeeze()</span><br><span class="line">        b_j = self.bj(j_indices).squeeze()</span><br><span class="line">        </span><br><span class="line">        x = torch.sum(w_i * w_j, dim=<span class="number">1</span>) + b_i + b_j</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">glove = GloveModel(dataset._vocab_len, EMBED_DIM)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">outputs = glove(i_idx, j_idx)</span><br><span class="line">weights_x = weight_func(x_ij, X_MAX, ALPHA)</span><br><span class="line">loss = wmse_loss(weights_x, outputs, torch.log(x_ij))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure><p>In conclusion, the GloVe model efficiently leverages global statistical information by training only on the nonzero elements in a word- word co-occurrence matrix, and produces a vector space with mean- ingful sub-structure. It consistently outperforms word2vec on the word analogy task, given the same corpus, vocabulary, window size, and training time. It achieves better results faster, and also obtains the best results irrespective of speed.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Course note and slides of <a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">cs224n</a></li><li><a href="https://medium.com/@jonathan_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491" target="_blank" rel="noopener">Machine Learning — Singular Value Decomposition (SVD) &amp; Principal Component Analysis (PCA)</a></li><li><a href="https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html" target="_blank" rel="noopener">https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html</a></li><li><a href="https://zhuanlan.zhihu.com/p/42651829" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/42651829</a></li><li><a href="https://nlpython.com/implementing-glove-model-with-pytorch/" target="_blank" rel="noopener">https://nlpython.com/implementing-glove-model-with-pytorch/</a></li></ul>]]></content>
    
    <summary type="html">
    
      Word Vectors Summary
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="cs224n" scheme="https://zhangruochi.com/tags/cs224n/"/>
    
      <category term="NLP" scheme="https://zhangruochi.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Composition</title>
    <link href="https://zhangruochi.com/Composition/2019/12/01/"/>
    <id>https://zhangruochi.com/Composition/2019/12/01/</id>
    <published>2019-12-02T01:25:39.000Z</published>
    <updated>2019-12-02T02:36:34.481Z</updated>
    
    <content type="html"><![CDATA[<h2 id="What-is-Composition"><a href="#What-is-Composition" class="headerlink" title="What is Composition?"></a>What is Composition?</h2><ol><li>组合是指将不同的部分结合成一个整体的行为。使用面向对象的组合技术，可以将简单的、独立的对象组合成更大更复杂的整体。</li><li>从本质上讲，参与组合的那些对象都很小，他们在结构上都是独立的。这使得他们能够无缝低转换为可插入、可互换的组件。</li></ol><h2 id="重构"><a href="#重构" class="headerlink" title="重构"></a>重构</h2><h3 id="创建零件"><a href="#创建零件" class="headerlink" title="创建零件"></a>创建零件</h3><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bicycle</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:size</span>, <span class="symbol">:parts</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(args = &#123;&#125;)</span></span></span><br><span class="line">        @size = args[<span class="symbol">:size</span>]</span><br><span class="line">        @parts = args[<span class="symbol">:parts</span>]</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spares</span></span></span><br><span class="line">        parts.spares</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Parts</span> &lt; Array</span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:parts</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(parts)</span></span></span><br><span class="line">        @parts = parts</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spares</span></span></span><br><span class="line">        parts.select &#123;<span class="params">|part|</span> part.needs_spare&#125;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span></span></span><br><span class="line">        parts.size</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Part</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:name</span>, <span class="symbol">:description</span>, <span class="symbol">:needs_spare</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(args)</span></span></span><br><span class="line">        @name = args[<span class="symbol">:name</span>]</span><br><span class="line">        @description = args[<span class="symbol">:description</span>]</span><br><span class="line">        @needs_spare = args.fetch(<span class="symbol">:needs_spare</span>,<span class="literal">true</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">chain = Part.new(<span class="symbol">name:</span> <span class="string">"chain"</span>, <span class="symbol">description:</span> <span class="string">'10-speed'</span>)</span><br><span class="line">road_tire = Part.new(<span class="symbol">name:</span> <span class="string">"tire_size"</span>, <span class="symbol">description:</span> <span class="string">'23'</span>)</span><br><span class="line">tape = Part.new(<span class="symbol">name:</span> <span class="string">"tape_color"</span>, <span class="symbol">description:</span> <span class="string">'red'</span>)</span><br><span class="line">mountain_tire = Part.new(<span class="symbol">name:</span> <span class="string">"tire_size"</span>, <span class="symbol">description:</span> <span class="string">'2.1'</span>)</span><br><span class="line">rear_shock = Part.new(<span class="symbol">name:</span> <span class="string">"rear_shock"</span>, <span class="symbol">description:</span> <span class="string">'Fox'</span>)</span><br><span class="line">front_shock = Part.new(<span class="symbol">name:</span> <span class="string">'front_shock'</span>, <span class="symbol">description:</span> <span class="string">'Manitou'</span>, <span class="symbol">needs_spare:</span> <span class="literal">false</span>)</span><br></pre></td></tr></table></figure><h3 id="组装"><a href="#组装" class="headerlink" title="组装"></a>组装</h3><p>此时<code>size</code>能正常响应，但是不能执行数组之间的加法会导致问题. 尽管<code>+</code>连接的是 <code>Parts</code> 对象，但是<code>+</code> 返回的对象是 <code>Array</code>实例。<code>Array</code>并不知道如何响应<code>spares</code></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">road_bike_parts = Parts.new([chain, road_tire, tape])</span><br><span class="line">p road_bike_parts.spares</span><br><span class="line">p road_bike_parts.size</span><br></pre></td></tr></table></figure><h3 id="让Parts对象更像一个数组"><a href="#让Parts对象更像一个数组" class="headerlink" title="让Parts对象更像一个数组"></a>让Parts对象更像一个数组</h3><ol><li>The Forwardable module<br>Forwardable is a module that can be used to add behavior to all the <strong>instances</strong> of a given class. This module is included to the singleton class using the extend keyword in order to add methods at class-level (to keep it simple).</li><li>The Forwardable#def_delegator method allows an object to forward a message to a defined receiver.<ul><li>The first argument correspond to the receiver of the message forwarding.</li><li>The second argument is the message to forward.</li><li>And finally the third argument is an alias of the message.</li></ul></li><li>The def_delegators method</li><li>The delegate method<br> The delegate method accepts a hash as argument where:<ul><li>the key is one or more messages</li><li>the value is the receiver of the messages defined as key<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in forwardable.rb</span></span><br><span class="line"><span class="keyword">require</span> <span class="string">'forwardable'</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Computer</span></span></span><br><span class="line">  attr <span class="symbol">:cores</span>, <span class="symbol">:screens</span></span><br><span class="line">  extend Forwardable</span><br><span class="line">  delegate %I[size]   =&gt; <span class="symbol">:</span>@cores,</span><br><span class="line">           %I[length] =&gt; <span class="symbol">:</span>@screens</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span></span><br><span class="line">    @cores  = (<span class="number">1</span>..<span class="number">8</span>).to_a</span><br><span class="line">    @screens = [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">macrosoft = Computer.new</span><br><span class="line">puts <span class="string">"Cores:   <span class="subst">#&#123;macrosoft.size&#125;</span>"</span></span><br><span class="line">puts <span class="string">"Screens: <span class="subst">#&#123;macrosoft.length&#125;</span>"</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><p>The 2 main differences with the def_delegator method is that it takes a set of methods to forward and the methods cannot be aliased<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">require</span> <span class="string">'forwardable'</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bicycle</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:size</span>, <span class="symbol">:parts</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(args = &#123;&#125;)</span></span></span><br><span class="line">        @size = args[<span class="symbol">:size</span>]</span><br><span class="line">        @parts = args[<span class="symbol">:parts</span>]</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spares</span></span></span><br><span class="line">        parts.spares</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Parts</span></span></span><br><span class="line">    extend Forwardable</span><br><span class="line">    def_delegators <span class="symbol">:</span>@parts, <span class="symbol">:size</span>, <span class="symbol">:each</span></span><br><span class="line">    <span class="keyword">include</span> Enumerable</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(parts)</span></span></span><br><span class="line">        @parts = parts</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spares</span></span></span><br><span class="line">        select &#123;<span class="params">|part|</span> part.needs_spare&#125;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Part</span></span></span><br><span class="line">    <span class="keyword">attr_reader</span> <span class="symbol">:name</span>, <span class="symbol">:description</span>, <span class="symbol">:needs_spare</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(args)</span></span></span><br><span class="line">        @name = args[<span class="symbol">:name</span>]</span><br><span class="line">        @description = args[<span class="symbol">:description</span>]</span><br><span class="line">        @needs_spare = args.fetch(<span class="symbol">:needs_spare</span>,<span class="literal">true</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">chain = Part.new(<span class="symbol">name:</span> <span class="string">"chain"</span>, <span class="symbol">description:</span> <span class="string">'10-speed'</span>)</span><br><span class="line">road_tire = Part.new(<span class="symbol">name:</span> <span class="string">"tire_size"</span>, <span class="symbol">description:</span> <span class="string">'23'</span>)</span><br><span class="line">tape = Part.new(<span class="symbol">name:</span> <span class="string">"tape_color"</span>, <span class="symbol">description:</span> <span class="string">'red'</span>)</span><br><span class="line">mountain_tire = Part.new(<span class="symbol">name:</span> <span class="string">"tire_size"</span>, <span class="symbol">description:</span> <span class="string">'2.1'</span>)</span><br><span class="line">rear_shock = Part.new(<span class="symbol">name:</span> <span class="string">"rear_shock"</span>, <span class="symbol">description:</span> <span class="string">'Fox'</span>)</span><br><span class="line">front_shock = Part.new(<span class="symbol">name:</span> <span class="string">'front_shock'</span>, <span class="symbol">description:</span> <span class="string">'Manitou'</span>, <span class="symbol">needs_spare:</span> <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">mountain_bike = Bicycle.new(</span><br><span class="line">    <span class="symbol">size:</span> <span class="string">'L'</span>,</span><br><span class="line">    <span class="symbol">parts:</span> Parts.new([chain, mountain_tire, front_shock, rear_shock])</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">road_bike = Bicycle.new(</span><br><span class="line">    <span class="symbol">size:</span> <span class="string">'L'</span>,</span><br><span class="line">    <span class="symbol">parts:</span> Parts.new([chain, road_tire, tape])</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">p mountain_bike.size</span><br><span class="line">p road_bike.size</span><br></pre></td></tr></table></figure></p><h3 id="创建零件工厂"><a href="#创建零件工厂" class="headerlink" title="创建零件工厂"></a>创建零件工厂</h3><p>对象如何创建的知识，最好放在<code>工厂里面</code>。这样你就只需要一个说明书，就能创建对象。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">road_config = [[<span class="string">'chain'</span>,<span class="string">'10-speed'</span>],</span><br><span class="line">               [<span class="string">'tire_size'</span>,<span class="string">'23'</span>],</span><br><span class="line">               [<span class="string">'tape_color'</span>,<span class="string">'red'</span>]</span><br><span class="line">           ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">module</span> <span class="title">PartsFactory</span></span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">self</span>.<span class="title">build</span><span class="params">(config, part_class = Part, parts_class = Parts)</span></span></span><br><span class="line">        parts_class.new(</span><br><span class="line">            config.collect &#123;<span class="params">|part_config|</span></span><br><span class="line">                part_class.new (&#123;</span><br><span class="line">                    <span class="symbol">name:</span> part_config[<span class="number">0</span>],</span><br><span class="line">                    <span class="symbol">description:</span> part_config[<span class="number">1</span>],</span><br><span class="line">                    <span class="symbol">needs_spare:</span> part_config.fetch(<span class="number">2</span>,<span class="literal">true</span>)&#125;)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">road_parts = PartsFactory.build(road_config)</span><br><span class="line">p road_parts.spares</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;What-is-Composition&quot;&gt;&lt;a href=&quot;#What-is-Composition&quot; class=&quot;headerlink&quot; title=&quot;What is Composition?&quot;&gt;&lt;/a&gt;What is Composition?&lt;/h2&gt;&lt;ol
      
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
      <category term="Object Oriented Design" scheme="https://zhangruochi.com/categories/Programming-Language/Object-Oriented-Design/"/>
    
    
  </entry>
  
</feed>
