<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RUOCHI.AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhangruochi.com/"/>
  <updated>2020-04-19T03:51:38.577Z</updated>
  <id>https://zhangruochi.com/</id>
  
  <author>
    <name>Ruochi Zhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Build and Evaluate a Linear Risk model</title>
    <link href="https://zhangruochi.com/Build-and-Evaluate-a-Linear-Risk-model/2020/04/18/"/>
    <id>https://zhangruochi.com/Build-and-Evaluate-a-Linear-Risk-model/2020/04/18/</id>
    <published>2020-04-19T03:50:08.000Z</published>
    <updated>2020-04-19T03:51:38.577Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Build-and-Evaluate-a-Linear-Risk-model"><a href="#Build-and-Evaluate-a-Linear-Risk-model" class="headerlink" title="Build and Evaluate a Linear Risk model"></a>Build and Evaluate a Linear Risk model</h1><p>Welcome to the first assignment in Course 2!</p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#1">1. Import Packages</a></li><li><a href="#2">2. Load Data</a></li><li><a href="#3">3. Explore the Dataset</a></li><li><a href="#4">4. Mean-Normalize the Data</a><ul><li><a href="#Ex-1">Exercise 1</a></li></ul></li><li><a href="#Ex-2">5. Build the Model</a><ul><li><a href="#Ex-2">Exercise 2</a></li></ul></li><li><a href="#6">6. Evaluate the Model Using the C-Index</a><ul><li><a href="#Ex-3">Exercise 3</a></li></ul></li><li><a href="#7">7. Evaluate the Model on the Test Set</a></li><li><a href="#8">8. Improve the Model</a><ul><li><a href="#Ex-4">Exercise 4</a></li></ul></li><li><a href="#9">9. Evalute the Improved Model</a></li></ul><h2 id="Overview-of-the-Assignment"><a href="#Overview-of-the-Assignment" class="headerlink" title="Overview of the Assignment"></a>Overview of the Assignment</h2><p>In this assignment, you’ll build a risk score model for retinopathy in diabetes patients using logistic regression.</p><p>As we develop the model, we will learn about the following topics:</p><ul><li>Data preprocessing<ul><li>Log transformations</li><li>Standardization</li></ul></li><li>Basic Risk Models<ul><li>Logistic Regression</li><li>C-index</li><li>Interactions Terms</li></ul></li></ul><h3 id="Diabetic-Retinopathy"><a href="#Diabetic-Retinopathy" class="headerlink" title="Diabetic Retinopathy"></a>Diabetic Retinopathy</h3><p>Retinopathy is an eye condition that causes changes to the blood vessels in the part of the eye called the retina.<br>This often leads to vision changes or blindness.<br>Diabetic patients are known to be at high risk for retinopathy. </p><h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>Logistic regression is an appropriate analysis to use for predicting the probability of a binary outcome. In our case, this would be the probability of having or not having diabetic retinopathy.<br>Logistic Regression is one of the most commonly used algorithms for binary classification. It is used to find the best fitting model to describe the relationship between a set of features (also referred to as input, independent, predictor, or explanatory variables) and a binary outcome label (also referred to as an output, dependent, or response variable). Logistic regression has the property that the output prediction is always in the range $[0,1]$. Sometimes this output is used to represent a probability from 0%-100%, but for straight binary classification, the output is converted to either $0$ or $1$ depending on whether it is below or above a certain threshold, usually $0.5$.</p><p>It may be  confusing that the term regression appears in the name even though logistic regression is actually a classification algorithm, but that’s just a name it was given for historical reasons.</p><p><a name="1"></a></p><h2 id="1-Import-Packages"><a href="#1-Import-Packages" class="headerlink" title="1.  Import Packages"></a>1.  Import Packages</h2><p>We’ll first import all the packages that we need for this assignment. </p><ul><li><code>numpy</code> is the fundamental package for scientific computing in python.</li><li><code>pandas</code> is what we’ll use to manipulate our data.</li><li><code>matplotlib</code> is a plotting library.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><p><a name="2"></a></p><h2 id="2-Load-Data"><a href="#2-Load-Data" class="headerlink" title="2. Load Data"></a>2. Load Data</h2><p>First we will load in the dataset that we will use for training and testing our model.</p><ul><li>Run the next cell to load the data using a function imported from our local utils module.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># This function creates randomly generated data</span></span><br><span class="line"><span class="comment"># X, y = load_data(6000)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># For stability, load data from files that were generated using the load_data</span></span><br><span class="line">X = pd.read_csv(<span class="string">'X_data.csv'</span>,index_col=<span class="number">0</span>)</span><br><span class="line">y_df = pd.read_csv(<span class="string">'y_data.csv'</span>,index_col=<span class="number">0</span>)</span><br><span class="line">y = y_df[<span class="string">'y'</span>]</span><br></pre></td></tr></table></figure><p><code>X</code> and <code>y</code> are Pandas DataFrames that hold the data for 6,000 diabetic patients. </p><p><a name="3"></a></p><h2 id="3-Explore-the-Dataset"><a href="#3-Explore-the-Dataset" class="headerlink" title="3. Explore the Dataset"></a>3. Explore the Dataset</h2><p>The features (<code>X</code>) include the following fields:</p><ul><li>Age: (years)</li><li>Systolic_BP: Systolic blood pressure (mmHg)</li><li>Diastolic_BP: Diastolic blood pressure (mmHg)</li><li>Cholesterol: (mg/DL)</li></ul><p>We can use the <code>head()</code> method to display the first few records of each.    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Age</th>      <th>Systolic_BP</th>      <th>Diastolic_BP</th>      <th>Cholesterol</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>77.196340</td>      <td>85.288742</td>      <td>80.021878</td>      <td>79.957109</td>    </tr>    <tr>      <th>1</th>      <td>63.529850</td>      <td>99.379736</td>      <td>84.852361</td>      <td>110.382411</td>    </tr>    <tr>      <th>2</th>      <td>69.003986</td>      <td>111.349455</td>      <td>109.850616</td>      <td>100.828246</td>    </tr>    <tr>      <th>3</th>      <td>82.638210</td>      <td>95.056128</td>      <td>79.666851</td>      <td>87.066303</td>    </tr>    <tr>      <th>4</th>      <td>78.346286</td>      <td>109.154591</td>      <td>90.713220</td>      <td>92.511770</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.shape</span><br></pre></td></tr></table></figure><pre><code>(6000, 4)</code></pre><p>The target (<code>y</code>) is an indicator of whether or not the patient developed retinopathy.</p><ul><li>y = 1 : patient has retinopathy.</li><li>y = 0 : patient does not have retinopathy.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.head()</span><br></pre></td></tr></table></figure><pre><code>0    1.01    1.02    1.03    1.04    1.0Name: y, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.shape</span><br></pre></td></tr></table></figure><pre><code>(6000,)</code></pre><p>Before we build a model, let’s take a closer look at the distribution of our training data. To do this, we will split the data into train and test sets using a 75/25 split.</p><p>For this, we can use the built in function provided by sklearn library.  See the documentation for <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener">sklearn.model_selection.train_test_split</a>. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=<span class="number">0.75</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>Plot the histograms of each column of <code>X_train</code> below: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> X.columns:</span><br><span class="line">    X_train_raw.loc[:, col].hist()</span><br><span class="line">    plt.title(col)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_18_0.png" alt="png"></p><p><img src="output_18_1.png" alt="png"></p><p><img src="output_18_2.png" alt="png"></p><p><img src="output_18_3.png" alt="png"></p><p>As we can see, the distributions have a generally bell shaped distribution, but with slight rightward skew.</p><p>Many statistical models assume that the data is normally distributed, forming a symmetric Gaussian bell shape (with no skew) more like the example below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm</span><br><span class="line">data = np.random.normal(<span class="number">50</span>,<span class="number">12</span>, <span class="number">5000</span>)</span><br><span class="line">fitting_params = norm.fit(data)</span><br><span class="line">norm_dist_fitted = norm(*fitting_params)</span><br><span class="line">t = np.linspace(<span class="number">0</span>,<span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line">plt.hist(data, bins=<span class="number">60</span>, density=<span class="keyword">True</span>)</span><br><span class="line">plt.plot(t, norm_dist_fitted.pdf(t))</span><br><span class="line">plt.title(<span class="string">'Example of Normally Distributed Data'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_20_0.png" alt="png"></p><p>We can transform our data to be closer to a normal distribution by removing the skew. One way to remove the skew is by applying the log function to the data.</p><p>Let’s plot the log of the feature variables to see that it produces the desired effect.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> X_train_raw.columns:</span><br><span class="line">    np.log(X_train_raw.loc[:, col]).hist()</span><br><span class="line">    plt.title(col)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_22_0.png" alt="png"></p><p><img src="output_22_1.png" alt="png"></p><p><img src="output_22_2.png" alt="png"></p><p><img src="output_22_3.png" alt="png"></p><p>We can see that the data is more symmetric after taking the log.</p><p><a name="4"></a></p><h2 id="4-Mean-Normalize-the-Data"><a href="#4-Mean-Normalize-the-Data" class="headerlink" title="4. Mean-Normalize the Data"></a>4. Mean-Normalize the Data</h2><p>Let’s now transform our data so that the distributions are closer to standard normal distributions.</p><p>First we will remove some of the skew from the distribution by using the log transformation.<br>Then we will “standardize” the distribution so that it has a mean of zero and standard deviation of 1. Recall that a standard normal distribution has mean of zero and standard deviation of 1. </p><p><a name="Ex-1"></a></p><h3 id="Exercise-1"><a href="#Exercise-1" class="headerlink" title="Exercise 1"></a>Exercise 1</h3><ul><li>Write a function that first removes some of the skew in the data, and then standardizes the distribution so that for each data point $x$,<script type="math/tex; mode=display">\overline{x} = \frac{x - mean(x)}{std(x)}</script></li><li>Keep in mind that we want to pretend that the test data is “unseen” data. <ul><li>This implies that it is unavailable to us for the purpose of preparing our data, and so we do not want to consider it when evaluating the mean and standard deviation that we use in the above equation. Instead we want to calculate these values using the training data alone, but then use them for standardizing both the training and the test data.</li><li>For a further discussion on the topic, see this article <a href="https://sebastianraschka.com/faq/docs/scale-training-test.html" target="_blank" rel="noopener">“Why do we need to re-use training parameters to transform test data”</a>. </li></ul></li></ul><h4 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h4><ul><li>For the sample standard deviation, please calculate the unbiased estimator:<script type="math/tex; mode=display">s = \sqrt{\frac{\sum_{i=1}^n(x_{i} - \bar{x})^2}{n-1}}</script></li><li>In other words, if you numpy, set the degrees of freedom <code>ddof</code> to 1.</li><li>For pandas, the default <code>ddof</code> is already set to 1.</li></ul><details>    <summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary><p>    <ul>        <li> When working with Pandas DataFrames, you can use the aggregation functions <code>mean</code> and <code>std</code> functions. Note that in order to apply an aggregation function separately for each row or each column, you'll set the axis parameter to either <code>0</code> or <code>1</code>. One produces the aggregation along columns and the other along rows, but it is easy to get them confused. So experiment with each option below to see which one you should use to get an average for each column in the dataframe.<code>avg = df.mean(axis=0)avg = df.mean(axis=1) </code>        </li>        <br><br>    <li>Remember to use <b>training</b> data statistics when standardizing both the training and the test data.</li>    </ul></p></details> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_standard_normal</span><span class="params">(df_train, df_test)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    In order to make the data closer to a normal distribution, take log</span></span><br><span class="line"><span class="string">    transforms to reduce the skew.</span></span><br><span class="line"><span class="string">    Then standardize the distribution with a mean of zero and standard deviation of 1. </span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      df_train (dataframe): unnormalized training data.</span></span><br><span class="line"><span class="string">      df_test (dataframe): unnormalized test data.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      df_train_normalized (dateframe): normalized training data.</span></span><br><span class="line"><span class="string">      df_test_normalized (dataframe): normalized test data.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###  </span></span><br><span class="line">    <span class="comment"># Remove skew by applying the log function to the train set, and to the test set</span></span><br><span class="line">    df_train_unskewed = np.log(df_train)</span><br><span class="line">    df_test_unskewed = np.log(df_test)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#calculate the mean and standard deviation of the training set</span></span><br><span class="line">    mean = df_train_unskewed.mean(axis = <span class="number">0</span>)</span><br><span class="line">    stdev = df_train_unskewed.std(axis = <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># standardize the training set</span></span><br><span class="line">    df_train_standardized = (df_train_unskewed - mean)/ stdev</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># standardize the test set (see instructions and hints above)</span></span><br><span class="line">    df_test_standardized = (df_test_unskewed - mean) / stdev</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> df_train_standardized, df_test_standardized</span><br></pre></td></tr></table></figure><h4 id="Test-Your-Work"><a href="#Test-Your-Work" class="headerlink" title="Test Your Work"></a>Test Your Work</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test</span></span><br><span class="line">tmp_train = pd.DataFrame(&#123;<span class="string">'field1'</span>: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">10</span>], <span class="string">'field2'</span>: [<span class="number">4</span>,<span class="number">5</span>,<span class="number">11</span>]&#125;)</span><br><span class="line">tmp_test = pd.DataFrame(&#123;<span class="string">'field1'</span>: [<span class="number">1</span>,<span class="number">3</span>,<span class="number">10</span>], <span class="string">'field2'</span>: [<span class="number">4</span>,<span class="number">6</span>,<span class="number">11</span>]&#125;)</span><br><span class="line">tmp_train_transformed, tmp_test_transformed = make_standard_normal(tmp_train,tmp_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"Training set transformed field1 has mean <span class="subst">&#123;tmp_train_transformed[<span class="string">'field1'</span>].mean(axis=<span class="number">0</span>):<span class="number">.4</span>f&#125;</span> and standard deviation <span class="subst">&#123;tmp_train_transformed[<span class="string">'field1'</span>].std(axis=<span class="number">0</span>):<span class="number">.4</span>f&#125;</span> "</span>)</span><br><span class="line">print(<span class="string">f"Test set transformed, field1 has mean <span class="subst">&#123;tmp_test_transformed[<span class="string">'field1'</span>].mean(axis=<span class="number">0</span>):<span class="number">.4</span>f&#125;</span> and standard deviation <span class="subst">&#123;tmp_test_transformed[<span class="string">'field1'</span>].std(axis=<span class="number">0</span>):<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Skew of training set field1 before transformation: <span class="subst">&#123;tmp_train[<span class="string">'field1'</span>].skew(axis=<span class="number">0</span>):<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Skew of training set field1 after transformation: <span class="subst">&#123;tmp_train_transformed[<span class="string">'field1'</span>].skew(axis=<span class="number">0</span>):<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Skew of test set field1 before transformation: <span class="subst">&#123;tmp_test[<span class="string">'field1'</span>].skew(axis=<span class="number">0</span>):<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Skew of test set field1 after transformation: <span class="subst">&#123;tmp_test_transformed[<span class="string">'field1'</span>].skew(axis=<span class="number">0</span>):<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Training set transformed field1 has mean -0.0000 and standard deviation 1.0000 Test set transformed, field1 has mean 0.1144 and standard deviation 0.9749Skew of training set field1 before transformation: 1.6523Skew of training set field1 after transformation: 1.0857Skew of test set field1 before transformation: 1.3896Skew of test set field1 after transformation: 0.1371</code></pre><h4 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output:"></a>Expected Output:</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Training <span class="built_in">set</span> transformed field1 has mean <span class="number">-0.0000</span> <span class="keyword">and</span> standard deviation <span class="number">1.0000</span> </span><br><span class="line">Test <span class="built_in">set</span> transformed, field1 has mean <span class="number">0.1144</span> <span class="keyword">and</span> standard deviation <span class="number">0.9749</span></span><br><span class="line">Skew of training <span class="built_in">set</span> field1 before transformation: <span class="number">1.6523</span></span><br><span class="line">Skew of training <span class="built_in">set</span> field1 after transformation: <span class="number">1.0857</span></span><br><span class="line">Skew of test <span class="built_in">set</span> field1 before transformation: <span class="number">1.3896</span></span><br><span class="line">Skew of test <span class="built_in">set</span> field1 after transformation: <span class="number">0.1371</span></span><br></pre></td></tr></table></figure><h4 id="Transform-training-and-test-data"><a href="#Transform-training-and-test-data" class="headerlink" title="Transform training and test data"></a>Transform training and test data</h4><p>Use the function that you just implemented to make the data distribution closer to a standard normal distribution.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test = make_standard_normal(X_train_raw, X_test_raw)</span><br></pre></td></tr></table></figure><p>After transforming the training and test sets, we’ll expect the training set to be centered at zero with a standard deviation of $1$.</p><p>We will avoid observing the test set during model training in order to avoid biasing the model training process, but let’s have a look at the distributions of the transformed training data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> X_train.columns:</span><br><span class="line">    X_train[col].hist()</span><br><span class="line">    plt.title(col)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_35_0.png" alt="png"></p><p><img src="output_35_1.png" alt="png"></p><p><img src="output_35_2.png" alt="png"></p><p><img src="output_35_3.png" alt="png"></p><p><a name="5"></a></p><h2 id="5-Build-the-Model"><a href="#5-Build-the-Model" class="headerlink" title="5. Build the Model"></a>5. Build the Model</h2><p>Now we are ready to build the risk model by training logistic regression with our data.</p><p><a name="Ex-2"></a></p><h3 id="Exercise-2"><a href="#Exercise-2" class="headerlink" title="Exercise 2"></a>Exercise 2</h3><ul><li>Implement the <code>lr_model</code> function to build a model using logistic regression with the <code>LogisticRegression</code> class from <code>sklearn</code>. </li><li>See the documentation for <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit" target="_blank" rel="noopener">sklearn.linear_model.LogisticRegression</a>.</li></ul><details>    <summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary><p>    <ul>        <li>You can leave all the parameters to their default values when constructing an instance of the <code>sklearn.linear_model.LogisticRegression</code> class. If you get a warning message regarding the <code>solver</code> parameter, however, you may want to specify that particular one explicitly with <code>solver='lbfgs'</code>.         </li>        <br><br>    </ul></p></details> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lr_model</span><span class="params">(X_train, y_train)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    <span class="comment"># import the LogisticRegression class</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># create the model object</span></span><br><span class="line">    model = LogisticRegression(solver=<span class="string">'lbfgs'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># fit the model to the training data</span></span><br><span class="line">    model.fit(X_train, y_train)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="comment">#return the fitted model</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h4 id="Test-Your-Work-1"><a href="#Test-Your-Work-1" class="headerlink" title="Test Your Work"></a>Test Your Work</h4><p>Note: the <code>predict</code> method returns the model prediction <em>after</em> converting it from a value in the $[0,1]$ range to a $0$ or $1$ depending on whether it is below or above $0.5$.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test</span></span><br><span class="line">tmp_model = lr_model(X_train[<span class="number">0</span>:<span class="number">3</span>], y_train[<span class="number">0</span>:<span class="number">3</span>] )</span><br><span class="line">print(tmp_model.predict(X_train[<span class="number">4</span>:<span class="number">5</span>]))</span><br><span class="line">print(tmp_model.predict(X_train[<span class="number">5</span>:<span class="number">6</span>]))</span><br></pre></td></tr></table></figure><pre><code>[1.][1.]</code></pre><h4 id="Expected-Output-1"><a href="#Expected-Output-1" class="headerlink" title="Expected Output:"></a>Expected Output:</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.</span>]</span><br><span class="line">[<span class="number">0.</span>]</span><br></pre></td></tr></table></figure><p>Now that we’ve tested our model, we can go ahead and build it. Note that the <code>lr_model</code> function also fits  the model to the training data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_X = lr_model(X_train, y_train)</span><br></pre></td></tr></table></figure><p><a name="6"></a></p><h2 id="6-Evaluate-the-Model-Using-the-C-index"><a href="#6-Evaluate-the-Model-Using-the-C-index" class="headerlink" title="6. Evaluate the Model Using the C-index"></a>6. Evaluate the Model Using the C-index</h2><p>Now that we have a model, we need to evaluate it. We’ll do this using the c-index. </p><ul><li>The c-index measures the discriminatory power of a risk score. </li><li>Intuitively, a higher c-index indicates that the model’s prediction is in agreement with the actual outcomes of a pair of patients.</li><li>The formula for the c-index is</li></ul><script type="math/tex; mode=display">\mbox{cindex} = \frac{\mbox{concordant} + 0.5 \times \mbox{ties}}{\mbox{permissible}}</script><ul><li>A permissible pair is a pair of patients who have different outcomes.</li><li>A concordant pair is a permissible pair in which the patient with the higher risk score also has the worse outcome.</li><li>A tie is a permissible pair where the patients have the same risk score.</li></ul><p><a name="Ex-3"></a></p><h3 id="Exercise-3"><a href="#Exercise-3" class="headerlink" title="Exercise 3"></a>Exercise 3</h3><ul><li>Implement the <code>cindex</code> function to compute c-index.</li><li><code>y_true</code> is the array of actual patient outcomes, 0 if the patient does not eventually get the disease, and 1 if the patient eventually gets the disease.</li><li><code>scores</code> is the risk score of each patient.  These provide relative measures of risk, so they can be any real numbers. By convention, they are always non-negative.</li><li><p>Here is an example of input data and how to interpret it:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_true = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">scores = [<span class="number">0.45</span>, <span class="number">1.25</span>]</span><br></pre></td></tr></table></figure><ul><li>There are two patients. Index 0 of each array is associated with patient 0.  Index 1 is associated with patient 1.</li><li>Patient 0 does not have the disease in the future (<code>y_true</code> is 0), and based on past information, has a risk score of 0.45.</li><li>Patient 1 has the disease at some point in the future (<code>y_true</code> is 1), and based on past information, has a risk score of 1.25.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cindex</span><span class="params">(y_true, scores)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    y_true (np.array): a 1-D array of true binary outcomes (values of zero or one)</span></span><br><span class="line"><span class="string">        0: patient does not get the disease</span></span><br><span class="line"><span class="string">        1: patient does get the disease</span></span><br><span class="line"><span class="string">    scores (np.array): a 1-D array of corresponding risk scores output by the model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">    c_index (float): (concordant pairs + 0.5*ties) / number of permissible pairs</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    n = len(y_true)</span><br><span class="line">    <span class="keyword">assert</span> len(scores) == n</span><br><span class="line"></span><br><span class="line">    concordant = <span class="number">0</span></span><br><span class="line">    permissible = <span class="number">0</span></span><br><span class="line">    ties = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    <span class="comment"># use two nested for loops to go through all unique pairs of patients</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, n): <span class="comment">#choose the range of j so that j&gt;i</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Check if the pair is permissible (the patient outcomes are different)</span></span><br><span class="line">            <span class="keyword">if</span> y_true[i] != y_true[j]:</span><br><span class="line">                <span class="comment"># Count the pair if it's permissible</span></span><br><span class="line">                permissible += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># For permissible pairs, check if they are concordant or are ties</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># check for ties in the score</span></span><br><span class="line">                <span class="keyword">if</span> scores[i] == scores[j]:</span><br><span class="line">                    <span class="comment"># count the tie</span></span><br><span class="line">                    ties += <span class="number">1</span></span><br><span class="line">                    <span class="comment"># if it's a tie, we don't need to check patient outcomes, continue to the top of the for loop.</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># case 1: patient i doesn't get the disease, patient j does</span></span><br><span class="line">                <span class="keyword">if</span> y_true[i] == <span class="number">0</span> <span class="keyword">and</span> y_true[j] == <span class="number">1</span>:</span><br><span class="line">                    <span class="comment"># Check if patient i has a lower risk score than patient j</span></span><br><span class="line">                    <span class="keyword">if</span> scores[i] &lt; scores[j]:</span><br><span class="line">                        <span class="comment"># count the concordant pair</span></span><br><span class="line">                        concordant += <span class="number">1</span></span><br><span class="line">                    <span class="comment"># Otherwise if patient i has a higher risk score, it's not a concordant pair.</span></span><br><span class="line">                    <span class="comment"># Already checked for ties earlier</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># case 2: patient i gets the disease, patient j does not</span></span><br><span class="line">                <span class="keyword">if</span> y_true[i] == <span class="number">1</span> <span class="keyword">and</span> y_true[j] == <span class="number">0</span>:</span><br><span class="line">                    <span class="comment"># Check if patient i has a higher risk score than patient j</span></span><br><span class="line">                    <span class="keyword">if</span> scores[i] &gt; scores[j]:</span><br><span class="line">                        <span class="comment">#count the concordant pair</span></span><br><span class="line">                        concordant += <span class="number">1</span></span><br><span class="line">                    <span class="comment"># Otherwise if patient i has a lower risk score, it's not a concordant pair.</span></span><br><span class="line">                    <span class="comment"># We already checked for ties earlier</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate the c-index using the count of permissible pairs, concordant pairs, and tied pairs.</span></span><br><span class="line">    c_index = (concordant + <span class="number">0.5</span> * ties) / permissible</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> c_index</span><br></pre></td></tr></table></figure><h4 id="Test-Your-Work-2"><a href="#Test-Your-Work-2" class="headerlink" title="Test Your Work"></a>Test Your Work</h4><p>You can use the following test cases to make sure your implementation is correct.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test</span></span><br><span class="line">y_true = np.array([<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 1</span></span><br><span class="line">scores = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">print(<span class="string">'Case 1 Output: &#123;&#125;'</span>.format(cindex(y_true, scores)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 2</span></span><br><span class="line">scores = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">'Case 2 Output: &#123;&#125;'</span>.format(cindex(y_true, scores)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 3</span></span><br><span class="line">scores = np.array([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">print(<span class="string">'Case 3 Output: &#123;&#125;'</span>.format(cindex(y_true, scores)))</span><br><span class="line">cindex(y_true, scores)</span><br></pre></td></tr></table></figure><pre><code>Case 1 Output: 0.0Case 2 Output: 1.0Case 3 Output: 0.8750.875</code></pre><h4 id="Expected-Output-2"><a href="#Expected-Output-2" class="headerlink" title="Expected Output:"></a>Expected Output:</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Case <span class="number">1</span> Output: <span class="number">0.0</span></span><br><span class="line">Case <span class="number">2</span> Output: <span class="number">1.0</span></span><br><span class="line">Case <span class="number">3</span> Output: <span class="number">0.875</span></span><br></pre></td></tr></table></figure><h4 id="Note-1"><a href="#Note-1" class="headerlink" title="Note"></a>Note</h4><p>Please check your implementation of the for loops. </p><ul><li>There is way to make a mistake on the for loops that cannot be caught with unit tests.</li><li>Bonus: Can you think of what this error could be, and why it can’t be caught by unit tests?</li></ul><p><a name="7"></a></p><h2 id="7-Evaluate-the-Model-on-the-Test-Set"><a href="#7-Evaluate-the-Model-on-the-Test-Set" class="headerlink" title="7. Evaluate the Model on the Test Set"></a>7. Evaluate the Model on the Test Set</h2><p>Now, you can evaluate your trained model on the test set.  </p><p>To get the predicted probabilities, we use the <code>predict_proba</code> method. This method will return the result from the model <em>before</em> it is converted to a binary 0 or 1. For each input case, it returns an array of two values which represent the probabilities for both the negative case (patient does not get the disease) and positive case (patient the gets the disease). </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scores = model_X.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line">c_index_X_test = cindex(y_test.values, scores)</span><br><span class="line">print(<span class="string">f"c-index on test set is <span class="subst">&#123;c_index_X_test:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>c-index on test set is 0.8182</code></pre><h4 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c-index on test <span class="built_in">set</span> is <span class="number">0.8336</span></span><br></pre></td></tr></table></figure><p>Let’s plot the coefficients to see which variables (patient features) are having the most effect. You can access the model coefficients by using <code>model.coef_</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">coeffs = pd.DataFrame(data = model_X.coef_, columns = X_train.columns)</span><br><span class="line">coeffs.T.plot.bar(legend=<span class="keyword">None</span>);</span><br></pre></td></tr></table></figure><p><img src="output_56_0.png" alt="png"></p><h3 id="Question"><a href="#Question" class="headerlink" title="Question:"></a>Question:</h3><blockquote><p><strong>Which three variables have the largest impact on the model’s predictions?</strong></p></blockquote><p><a name="8"></a></p><h2 id="8-Improve-the-Model"><a href="#8-Improve-the-Model" class="headerlink" title="8. Improve the Model"></a>8. Improve the Model</h2><p>You can try to improve your model by including interaction terms. </p><ul><li>An interaction term is the product of two variables. <ul><li>For example, if we have data <script type="math/tex; mode=display">x = [x_1, x_2]</script></li><li>We could add the product so that:<script type="math/tex; mode=display">\hat{x} = [x_1, x_2, x_1*x_2]</script></li></ul></li></ul><p><a name="Ex-4"></a></p><h3 id="Exercise-4"><a href="#Exercise-4" class="headerlink" title="Exercise 4"></a>Exercise 4</h3><p>Write code below to add all interactions between every pair of variables to the training and test datasets. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_interactions</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Add interaction terms between columns to dataframe.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">    X (dataframe): Original data</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_int (dataframe): Original data with interaction terms appended. </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    features = X.columns</span><br><span class="line">    m = len(features)</span><br><span class="line">    X_int = X.copy(deep=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    <span class="comment"># 'i' loops through all features in the original dataframe X</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m<span class="number">-1</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get the name of feature 'i'</span></span><br><span class="line">        feature_i_name = features[i]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get the data for feature 'i'</span></span><br><span class="line">        feature_i_data = X[feature_i_name]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># choose the index of column 'j' to be greater than column i</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, m):</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># get the name of feature 'j'</span></span><br><span class="line">            feature_j_name = features[j]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># get the data for feature j'</span></span><br><span class="line">            feature_j_data = X[feature_j_name]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># create the name of the interaction feature by combining both names</span></span><br><span class="line">            <span class="comment"># example: "apple" and "orange" are combined to be "apple_x_orange"</span></span><br><span class="line">            feature_i_j_name = feature_i_name + <span class="string">"_x_"</span> + feature_j_name</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Multiply the data for feature 'i' and feature 'j'</span></span><br><span class="line">            <span class="comment"># store the result as a column in dataframe X_int</span></span><br><span class="line">            X_int[feature_i_j_name] = feature_i_data * feature_j_data</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_int</span><br></pre></td></tr></table></figure><h4 id="Test-Your-Work-3"><a href="#Test-Your-Work-3" class="headerlink" title="Test Your Work"></a>Test Your Work</h4><p>Run the cell below to check your implementation. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Original Data"</span>)</span><br><span class="line">print(X_train.loc[:, [<span class="string">'Age'</span>, <span class="string">'Systolic_BP'</span>]].head())</span><br><span class="line">print(<span class="string">"Data w/ Interactions"</span>)</span><br><span class="line">print(add_interactions(X_train.loc[:, [<span class="string">'Age'</span>, <span class="string">'Systolic_BP'</span>]].head()))</span><br></pre></td></tr></table></figure><pre><code>Original Data           Age  Systolic_BP1824 -0.912451    -0.068019253  -0.302039     1.7195381114  2.576274     0.1559623220  1.163621    -2.0339312108 -0.446238    -0.054554Data w/ Interactions           Age  Systolic_BP  Age_x_Systolic_BP1824 -0.912451    -0.068019           0.062064253  -0.302039     1.719538          -0.5193671114  2.576274     0.155962           0.4018003220  1.163621    -2.033931          -2.3667252108 -0.446238    -0.054554           0.024344</code></pre><h4 id="Expected-Output-3"><a href="#Expected-Output-3" class="headerlink" title="Expected Output:"></a>Expected Output:</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Original Data</span><br><span class="line">           Age  Systolic_BP</span><br><span class="line"><span class="number">2472</span> <span class="number">-2.340711</span>     <span class="number">0.077089</span></span><br><span class="line"><span class="number">4496</span>  <span class="number">0.009916</span>    <span class="number">-1.324053</span></span><br><span class="line"><span class="number">2243</span>  <span class="number">0.927302</span>    <span class="number">-0.424337</span></span><br><span class="line"><span class="number">4311</span> <span class="number">-0.087282</span>     <span class="number">0.399865</span></span><br><span class="line"><span class="number">843</span>   <span class="number">2.204586</span>     <span class="number">0.025521</span></span><br><span class="line">Data w/ Interactions</span><br><span class="line">           Age  Systolic_BP  Age_x_Systolic_BP</span><br><span class="line"><span class="number">2472</span> <span class="number">-2.340711</span>     <span class="number">0.077089</span>          <span class="number">-0.180444</span></span><br><span class="line"><span class="number">4496</span>  <span class="number">0.009916</span>    <span class="number">-1.324053</span>          <span class="number">-0.013129</span></span><br><span class="line"><span class="number">2243</span>  <span class="number">0.927302</span>    <span class="number">-0.424337</span>          <span class="number">-0.393489</span></span><br><span class="line"><span class="number">4311</span> <span class="number">-0.087282</span>     <span class="number">0.399865</span>          <span class="number">-0.034901</span></span><br><span class="line"><span class="number">843</span>   <span class="number">2.204586</span>     <span class="number">0.025521</span>           <span class="number">0.056264</span></span><br></pre></td></tr></table></figure><p>Once you have correctly implemented <code>add_interactions</code>, use it to make transformed version of <code>X_train</code> and <code>X_test</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train_int = add_interactions(X_train)</span><br><span class="line">X_test_int = add_interactions(X_test)</span><br></pre></td></tr></table></figure><p><a name="9"></a></p><h2 id="9-Evaluate-the-Improved-Model"><a href="#9-Evaluate-the-Improved-Model" class="headerlink" title="9. Evaluate the Improved Model"></a>9. Evaluate the Improved Model</h2><p>Now we can train the new and improved version of the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_X_int = lr_model(X_train_int, y_train)</span><br></pre></td></tr></table></figure><p>Let’s evaluate our new model on the test set.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scores_X = model_X.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line">c_index_X_int_test = cindex(y_test.values, scores_X)</span><br><span class="line"></span><br><span class="line">scores_X_int = model_X_int.predict_proba(X_test_int)[:, <span class="number">1</span>]</span><br><span class="line">c_index_X_int_test = cindex(y_test.values, scores_X_int)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"c-index on test set without interactions is <span class="subst">&#123;c_index_X_test:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"c-index on test set with interactions is <span class="subst">&#123;c_index_X_int_test:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>c-index on test set without interactions is 0.8182c-index on test set with interactions is 0.8281</code></pre><p>You should see that the model with interaction terms performs a bit better than the model without interactions.</p><p>Now let’s take another look at the model coefficients to try and see which variables made a difference. Plot the coefficients and report which features seem to be the most important.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int_coeffs = pd.DataFrame(data = model_X_int.coef_, columns = X_train_int.columns)</span><br><span class="line">int_coeffs.T.plot.bar();</span><br></pre></td></tr></table></figure><p><img src="output_71_0.png" alt="png"></p><h3 id="Questions"><a href="#Questions" class="headerlink" title="Questions:"></a>Questions:</h3><blockquote><p><strong>Which variables are most important to the model?</strong><br><br><strong>Have the relevant variables changed?</strong><br><br><strong>What does it mean when the coefficients are positive or negative?</strong><br></p></blockquote><p>You may notice that Age, Systolic_BP, and Cholesterol have a positive coefficient. This means that a higher value in these three features leads to a higher prediction probability for the disease. You also may notice that the interaction of Age x Cholesterol has a negative coefficient. This means that a higher value for the Age x Cholesterol product reduces the prediction probability for the disease.</p><p>To understand the effect of interaction terms, let’s compare the output of the model we’ve trained on sample cases with and without the interaction. Run the cell below to choose an index and look at the features corresponding to that case in the training set. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index = index = <span class="number">3432</span></span><br><span class="line">case = X_train_int.iloc[index, :]</span><br><span class="line">print(case)</span><br></pre></td></tr></table></figure><pre><code>Age                           2.502061Systolic_BP                   1.713547Diastolic_BP                  0.268265Cholesterol                   2.146349Age_x_Systolic_BP             4.287400Age_x_Diastolic_BP            0.671216Age_x_Cholesterol             5.370296Systolic_BP_x_Diastolic_BP    0.459685Systolic_BP_x_Cholesterol     3.677871Diastolic_BP_x_Cholesterol    0.575791Name: 5970, dtype: float64</code></pre><p>We can see that they have above average Age and Cholesterol. We can now see what our original model would have output by zero-ing out the value for Cholesterol and Age.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_case = case.copy(deep=<span class="keyword">True</span>)</span><br><span class="line">new_case.loc[<span class="string">"Age_x_Cholesterol"</span>] = <span class="number">0</span></span><br><span class="line">new_case</span><br></pre></td></tr></table></figure><pre><code>Age                           2.502061Systolic_BP                   1.713547Diastolic_BP                  0.268265Cholesterol                   2.146349Age_x_Systolic_BP             4.287400Age_x_Diastolic_BP            0.671216Age_x_Cholesterol             0.000000Systolic_BP_x_Diastolic_BP    0.459685Systolic_BP_x_Cholesterol     3.677871Diastolic_BP_x_Cholesterol    0.575791Name: 5970, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f"Output with interaction: \t<span class="subst">&#123;model_X_int.predict_proba([case.values])[:, <span class="number">1</span>][<span class="number">0</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Output without interaction: \t<span class="subst">&#123;model_X_int.predict_proba([new_case.values])[:, <span class="number">1</span>][<span class="number">0</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Output with interaction:    0.9448Output without interaction:     0.9965</code></pre><h4 id="Expected-output-1"><a href="#Expected-output-1" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Output with interaction: <span class="number">0.9448</span></span><br><span class="line">Output without interaction: <span class="number">0.9965</span></span><br></pre></td></tr></table></figure><p>We see that the model is less confident in its prediction with the interaction term than without (the prediction value is lower when including the interaction term). With the interaction term, the model has adjusted for the fact that the effect of high cholesterol becomes less important for older patients compared to younger patients.</p><h1 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations!"></a>Congratulations!</h1><p>You have finished the first assignment of Course 2. </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Build-and-Evaluate-a-Linear-Risk-model&quot;&gt;&lt;a href=&quot;#Build-and-Evaluate-a-Linear-Risk-model&quot; class=&quot;headerlink&quot; title=&quot;Build and Evalua
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Machine-Learning/"/>
    
    
      <category term="Medicine" scheme="https://zhangruochi.com/tags/Medicine/"/>
    
  </entry>
  
  <entry>
    <title>Brain Tumor Auto-Segmentation for Magnetic Resonance Imaging (MRI)</title>
    <link href="https://zhangruochi.com/Brain-Tumor-Auto-Segmentation-for-Magnetic-Resonance-Imaging-MRI/2020/04/17/"/>
    <id>https://zhangruochi.com/Brain-Tumor-Auto-Segmentation-for-Magnetic-Resonance-Imaging-MRI/2020/04/17/</id>
    <published>2020-04-18T03:10:42.000Z</published>
    <updated>2020-04-18T03:11:43.295Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://miro.medium.com/max/2652/1*eTkBMyqdg9JodNcG_O4-Kw.jpeg" width="100%"></p><p><a href="https://medium.com/stanford-ai-for-healthcare/its-a-no-brainer-deep-learning-for-brain-mr-images-f60116397472" target="_blank" rel="noopener">Image Source</a></p><h1 id="Brain-Tumor-Auto-Segmentation-for-Magnetic-Resonance-Imaging-MRI"><a href="#Brain-Tumor-Auto-Segmentation-for-Magnetic-Resonance-Imaging-MRI" class="headerlink" title="Brain Tumor Auto-Segmentation for Magnetic Resonance Imaging (MRI)"></a>Brain Tumor Auto-Segmentation for Magnetic Resonance Imaging (MRI)</h1><p>Welcome to the final part of the “Artificial Intelligence for Medicine” course 1!</p><p>You will learn how to build a neural network to automatically segment tumor regions in brain, using <a href="https://en.wikipedia.org/wiki/Magnetic_resonance_imaging" target="_blank" rel="noopener">MRI (Magnetic Resonance Imaging</a>) scans.</p><p>The MRI scan is one of the most common image modalities that we encounter in the radiology field.<br>Other data modalities include: </p><ul><li><a href="https://en.wikipedia.org/wiki/CT_scan" target="_blank" rel="noopener">Computer Tomography (CT)</a>, </li><li><a href="https://en.wikipedia.org/wiki/Ultrasound" target="_blank" rel="noopener">Ultrasound</a></li><li><a href="https://en.wikipedia.org/wiki/X-ray" target="_blank" rel="noopener">X-Rays</a>. </li></ul><p>In this assignment we will be focusing on MRIs but many of our learnings applies to other mentioned modalities as well.  We’ll walk you through some of the steps of training a deep learning model for segmentation.</p><p><strong>You will learn:</strong></p><ul><li>What is in an MR image</li><li>Standard data preparation techniques for MRI datasets</li><li>Metrics and loss functions for segmentation</li><li>Visualizing and evaluating segmentation models</li></ul><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><p>Use these links to jump to particular sections of this assignment!</p><ul><li><a href="#1">1. Dataset</a><ul><li><a href="#1-1">1.1 What is an MRI?</a></li><li><a href="#1-2">1.2 MRI Data Processing</a></li><li><a href="#1-3">1.3 Exploring the Dataset</a></li><li><a href="#1-4">1.4 Data Preprocessing</a><ul><li><a href="#1-4-1">1.4.1 Sub-volume Sampling</a></li><li><a href="#1-4-2">1.4.2 Standardization</a></li></ul></li></ul></li><li><a href="#2">2. Model: 3D U-Net</a></li><li><a href="#3">3. Metrics</a><ul><li><a href="#3-1">3.1 Dice Coefficient</a></li><li><a href="#3-2">3.2 Soft Dice Loss</a></li></ul></li><li><a href="#4">4. Training</a></li><li><a href="#5">5. Evaluation</a><ul><li><a href="#5-1">5.1 Overall Performance</a></li><li><a href="#5-2">5.2 Patch-level Predictions</a></li><li><a href="#5-3">5.3 Running on Entire Scans</a></li></ul></li></ul><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>In this assignment, we’ll make use of the following packages:</p><ul><li><code>keras</code> is a framework for building deep learning models.</li><li><code>keras.backend</code> allows us to perform math operations on tensors.</li><li><code>nibabel</code> will let us extract the images and labels from the files in our dataset.</li><li><code>numpy</code> is a library for mathematical and scientific operations.</li><li><code>pandas</code> is what we’ll use to manipulate our data.</li></ul><h2 id="Import-Packages"><a href="#Import-Packages" class="headerlink" title="Import Packages"></a>Import Packages</h2><p>Run the next cell to import all the necessary packages, dependencies and custom util functions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> nibabel <span class="keyword">as</span> nib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> backend <span class="keyword">as</span> K </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> util</span><br></pre></td></tr></table></figure><pre><code>Using TensorFlow backend.</code></pre><p><a name="1"></a></p><h1 id="1-Dataset"><a href="#1-Dataset" class="headerlink" title="1 Dataset"></a>1 Dataset</h1><p><a name="1-1"></a></p><h2 id="1-1-What-is-an-MRI"><a href="#1-1-What-is-an-MRI" class="headerlink" title="1.1 What is an MRI?"></a>1.1 What is an MRI?</h2><p>Magnetic resonance imaging (MRI) is an advanced imaging technique that is used to observe a variety of diseases and parts of the body. </p><p>As we will see later, neural networks can analyze these images individually (as a radiologist would) or combine them into a single 3D volume to make predictions.</p><p>At a high level, MRI works by measuring the radio waves emitting by atoms subjected to a magnetic field. </p><p><img src="https://miro.medium.com/max/1740/1*yC1Bt3IOzNv8Pp7t1v7F1Q.png"></p><p>In this assignment, we’ll build a multi-class segmentation model. We’ll  identify 3 different abnormalities in each image: edemas, non-enhancing tumors, and enhancing tumors.</p><p><a name="1-2"></a></p><h2 id="1-2-MRI-Data-Processing"><a href="#1-2-MRI-Data-Processing" class="headerlink" title="1.2 MRI Data Processing"></a>1.2 MRI Data Processing</h2><p>We often encounter MR images in the <a href="https://en.wikipedia.org/wiki/DICOM" target="_blank" rel="noopener">DICOM format</a>. </p><ul><li>The DICOM format is the output format for most commercial MRI scanners. This type of data can be processed using the <a href="https://pydicom.github.io/pydicom/stable/getting_started.html" target="_blank" rel="noopener">pydicom</a> Python library. </li></ul><p>In this assignment, we will be using the data from the <a href="https://decathlon-10.grand-challenge.org" target="_blank" rel="noopener">Decathlon 10 Challenge</a>. This data has been mostly pre-processed for the competition participants, however in real practice, MRI data needs to be significantly pre-preprocessed before we can use it to train our models.</p><p><a name="1-3"></a></p><h2 id="1-3-Exploring-the-Dataset"><a href="#1-3-Exploring-the-Dataset" class="headerlink" title="1.3 Exploring the Dataset"></a>1.3 Exploring the Dataset</h2><p>Our dataset is stored in the <a href="https://nifti.nimh.nih.gov/nifti-1/" target="_blank" rel="noopener">NifTI-1 format</a> and we will be using the <a href="https://github.com/nipy/nibabel" target="_blank" rel="noopener">NiBabel library</a> to interact with the files. Each training sample is composed of two separate files:</p><p>The first file is an image file containing a 4D array of MR image in the shape of (240, 240, 155, 4). </p><ul><li>The first 3 dimensions are the X, Y, and Z values for each point in the 3D volume, which is commonly called a voxel. </li><li>The 4th dimension is the values for 4 different sequences<ul><li>0: FLAIR: “Fluid Attenuated Inversion Recovery” (FLAIR)</li><li>1: T1w: “T1-weighted”</li><li>2: t1gd: “T1-weighted with gadolinium contrast enhancement” (T1-Gd)</li><li>3: T2w: “T2-weighted”</li></ul></li></ul><p>The second file in each training example is a label file containing a 3D array with the shape of (240, 240, 155).  </p><ul><li>The integer values in this array indicate the “label” for each voxel in the corresponding image files:<ul><li>0: background</li><li>1: edema</li><li>2: non-enhancing tumor</li><li>3: enhancing tumor</li></ul></li></ul><p>We have access to a total of 484 training images which we will be splitting into a training (80%) and validation (20%) dataset.</p><p>Let’s begin by looking at one single case and visualizing the data! You have access to 10 different cases via this notebook and we strongly encourage you to explore the data further on your own.</p><p>We’ll use the <a href="https://nipy.org/nibabel/nibabel_images.html" target="_blank" rel="noopener">NiBabel library</a> to load the image and label for a case. The function is shown below to give you a sense of how it works. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set home directory and data directory</span></span><br><span class="line">HOME_DIR = <span class="string">"./BraTS-Data/"</span></span><br><span class="line">DATA_DIR = HOME_DIR</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_case</span><span class="params">(image_nifty_file, label_nifty_file)</span>:</span></span><br><span class="line">    <span class="comment"># load the image and label file, get the image content and return a numpy array for each</span></span><br><span class="line">    image = np.array(nib.load(image_nifty_file).get_fdata())</span><br><span class="line">    label = np.array(nib.load(label_nifty_file).get_fdata())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure><p>We’ll now visualize an example.  For this, we use a pre-defined function we have written in the <code>util.py</code> file that uses <code>matplotlib</code> to generate a summary of the image. </p><p>The colors correspond to each class.</p><ul><li>Red is edema</li><li>Green is a non-enhancing tumor</li><li>Blue is an enhancing tumor. </li></ul><p>Do feel free to look at this function at your own time to understand how this is achieved.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">image, label = load_case(DATA_DIR + <span class="string">"imagesTr/BRATS_003.nii.gz"</span>, DATA_DIR + <span class="string">"labelsTr/BRATS_003.nii.gz"</span>)</span><br><span class="line">image = util.get_labeled_image(image, label)</span><br><span class="line"></span><br><span class="line">util.plot_image_grid(image)</span><br></pre></td></tr></table></figure><p><img src="output_10_0.png" alt="png"></p><p>We’ve also written a utility function which generates a GIF that shows what it looks like to iterate over each axis.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">image, label = load_case(DATA_DIR + <span class="string">"imagesTr/BRATS_003.nii.gz"</span>, DATA_DIR + <span class="string">"labelsTr/BRATS_003.nii.gz"</span>)</span><br><span class="line">util.visualize_data_gif(util.get_labeled_image(image, label))</span><br></pre></td></tr></table></figure><p><img src="output_12_0.png" alt="png"></p><p><strong>Reminder:</strong> You can explore more images in the <code>imagesTr</code> directory by changing the image name file.</p><p><a name="1-4"></a></p><h2 id="1-4-Data-Preprocessing-using-patches"><a href="#1-4-Data-Preprocessing-using-patches" class="headerlink" title="1.4 Data Preprocessing using patches"></a>1.4 Data Preprocessing using patches</h2><p>While our dataset is provided to us post-registration and in the NIfTI format, we still have to do some minor pre-processing before feeding the data to our model. </p><h5 id="Generate-sub-volumes"><a href="#Generate-sub-volumes" class="headerlink" title="Generate sub-volumes"></a>Generate sub-volumes</h5><p>We are going to first generate “patches” of our data which you can think of as sub-volumes of the whole MR images. </p><ul><li>The reason that we are generating patches is because a network that can process the entire volume at once will simply not fit inside our current environment’s memory/GPU.</li><li>Therefore we will be using this common technique to generate spatially consistent sub-volumes of our data, which can be fed into our network.</li><li>Specifically, we will be generating randomly sampled sub-volumes of shape [160, 160, 16] from our images. </li><li>Furthermore, given that a large portion of the MRI volumes are just brain tissue or black background without any tumors, we want to make sure that we pick patches that at least include some amount of tumor data. </li><li>Therefore, we are only going to pick patches that have at most 95% non-tumor regions (so at least 5% tumor). </li><li>We do this by filtering the volumes based on the values present in the background labels.</li></ul><h5 id="Standardization-mean-0-stdev-1"><a href="#Standardization-mean-0-stdev-1" class="headerlink" title="Standardization (mean 0, stdev 1)"></a>Standardization (mean 0, stdev 1)</h5><p>Lastly, given that the values in MR images cover a very wide range, we will standardize the values to have a mean of zero and standard deviation of 1. </p><ul><li>This is a common technique in deep image processing since standardization makes it much easier for the network to learn.</li></ul><p>Let’s walk through these steps in the following exercises.</p><p><a name="1-4-1"></a></p><h3 id="1-4-1-Sub-volume-Sampling"><a href="#1-4-1-Sub-volume-Sampling" class="headerlink" title="1.4.1 Sub-volume Sampling"></a>1.4.1 Sub-volume Sampling</h3><p>Fill in the function below takes in:</p><ul><li>a 4D image (shape: [240, 240, 155, 4])</li><li>its 3D label (shape: [240, 240, 155]) arrays, </li></ul><p>The function returns:</p><ul><li>A randomly generated sub-volume of size [160, 160, 16]</li><li>Its corresponding label in a 1-hot format which has the shape [3, 160, 160, 160]</li></ul><p>Additionally: </p><ol><li>Make sure that at most 95% of the returned patch is non-tumor regions. </li><li>Given that our network expects the channels for our images to appear as the first dimension (instead of the last one in our current setting) reorder the dimensions of the image to have the channels appear as the first dimension.</li><li>Reorder the dimensions of the label array to have the first dimension as the classes (instead of the last one in our current setting)</li><li>Reduce the labels array dimension to only include the non-background classes (total of 3 instead of 4)</li></ol><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Check the lecture notebook for a similar example in 1 dimension</li>    <li>To check the ratio of background to the whole sub-volume, the numerator is the number of background labels in the sub-volume.  The last dimension of the label array at index 0 contains the labels to identify whether the voxel is a background (value of 1) or not a a background (value of 0).        </li>    <li>For the denominator of the background ratio, this is the volume of the output (see <code>output_x</code>, <code>output_y</code>, <code>output_z</code> in the function parameters).</li>    <li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical" target="_blank" rel="noopener">keras.utils.to_categorical(y, num_classes=)</a></li>    <li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.moveaxis.html" target="_blank" rel="noopener"> np.moveaxis </a> can help you re-arrange the dimensions of the arrays </li>    <li> <a href="https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randint.html" target="_blank" rel="noopener">np.random.randint</a> for random sampling</li>    <li> When taking a subset of the label <code>'y'</code> that excludes the background class, remember which dimension contains the <code>'num_classes'</code> channel after re-ordering the axes. </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sub_volume</span><span class="params">(image, label, </span></span></span><br><span class="line"><span class="function"><span class="params">                   orig_x = <span class="number">240</span>, orig_y = <span class="number">240</span>, orig_z = <span class="number">155</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                   output_x = <span class="number">160</span>, output_y = <span class="number">160</span>, output_z = <span class="number">16</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                   num_classes = <span class="number">4</span>, max_tries = <span class="number">1000</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                   background_threshold=<span class="number">0.95</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Extract random sub-volume from original images.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        image (np.array): original image, </span></span><br><span class="line"><span class="string">            of shape (orig_x, orig_y, orig_z, num_channels)</span></span><br><span class="line"><span class="string">        label (np.array): original label. </span></span><br><span class="line"><span class="string">            labels coded using discrete values rather than</span></span><br><span class="line"><span class="string">            a separate dimension, </span></span><br><span class="line"><span class="string">            so this is of shape (orig_x, orig_y, orig_z)</span></span><br><span class="line"><span class="string">        orig_x (int): x_dim of input image</span></span><br><span class="line"><span class="string">        orig_y (int): y_dim of input image</span></span><br><span class="line"><span class="string">        orig_z (int): z_dim of input image</span></span><br><span class="line"><span class="string">        output_x (int): desired x_dim of output</span></span><br><span class="line"><span class="string">        output_y (int): desired y_dim of output</span></span><br><span class="line"><span class="string">        output_z (int): desired z_dim of output</span></span><br><span class="line"><span class="string">        num_classes (int): number of class labels</span></span><br><span class="line"><span class="string">        max_tries (int): maximum trials to do when sampling</span></span><br><span class="line"><span class="string">        background_threshold (float): limit on the fraction </span></span><br><span class="line"><span class="string">            of the sample which can be the background</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    returns:</span></span><br><span class="line"><span class="string">        X (np.array): sample of original image of dimension </span></span><br><span class="line"><span class="string">            (num_channels, output_x, output_y, output_z)</span></span><br><span class="line"><span class="string">        y (np.array): labels which correspond to X, of dimension </span></span><br><span class="line"><span class="string">            (num_classes, output_x, output_y, output_z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Initialize features and labels with `None`</span></span><br><span class="line">    X = <span class="keyword">None</span></span><br><span class="line">    y = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    tries = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> tries &lt; max_tries:</span><br><span class="line">        <span class="comment"># randomly sample sub-volume by sampling the corner voxel</span></span><br><span class="line">        <span class="comment"># hint: make sure to leave enough room for the output dimensions!</span></span><br><span class="line">        start_x = np.random.randint(orig_x - output_x + <span class="number">1</span> )</span><br><span class="line">        start_y = np.random.randint(orig_y - output_y + <span class="number">1</span> )</span><br><span class="line">        start_z = np.random.randint(orig_z - output_z + <span class="number">1</span> )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># extract relevant area of label</span></span><br><span class="line">        y = label[start_x: start_x + output_x,</span><br><span class="line">                  start_y: start_y + output_y,</span><br><span class="line">                  start_z: start_z + output_z]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># One-hot encode the categories.</span></span><br><span class="line">        <span class="comment"># This adds a 4th dimension, 'num_classes'</span></span><br><span class="line">        <span class="comment"># (output_x, output_y, output_z, num_classes)</span></span><br><span class="line">        y = keras.utils.to_categorical(y, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute the background ratio</span></span><br><span class="line">        bgrd_ratio = y[:,:,:, <span class="number">0</span>].sum() / (output_x * output_y * output_z)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># increment tries counter</span></span><br><span class="line">        tries += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># if background ratio is below the desired threshold,</span></span><br><span class="line">        <span class="comment"># use that sub-volume.</span></span><br><span class="line">        <span class="comment"># otherwise continue the loop and try another random sub-volume</span></span><br><span class="line">        <span class="keyword">if</span> bgrd_ratio &lt; background_threshold:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># make copy of the sub-volume</span></span><br><span class="line">            X = np.copy(image[start_x: start_x + output_x,</span><br><span class="line">                              start_y: start_y + output_y,</span><br><span class="line">                              start_z: start_z + output_z, :])</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># change dimension of X</span></span><br><span class="line">            <span class="comment"># from (x_dim, y_dim, z_dim, num_channels)</span></span><br><span class="line">            <span class="comment"># to (num_channels, x_dim, y_dim, z_dim)</span></span><br><span class="line">            X = np.moveaxis(X, <span class="number">-1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># change dimension of y</span></span><br><span class="line">            <span class="comment"># from (x_dim, y_dim, z_dim, num_classes)</span></span><br><span class="line">            <span class="comment"># to (num_classes, x_dim, y_dim, z_dim)</span></span><br><span class="line">            y = np.moveaxis(y, <span class="number">-1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment">### END CODE HERE ###</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># take a subset of y that excludes the background class</span></span><br><span class="line">            <span class="comment"># in the 'num_classes' dimension</span></span><br><span class="line">            y = y[<span class="number">1</span>:, :, :, :]</span><br><span class="line">    </span><br><span class="line">            <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line">    <span class="comment"># if we've tried max_tries number of samples</span></span><br><span class="line">    <span class="comment"># Give up in order to avoid looping forever.</span></span><br><span class="line">    print(<span class="string">f"Tried <span class="subst">&#123;tries&#125;</span> times to find a sub-volume. Giving up..."</span>)</span><br></pre></td></tr></table></figure><h3 id="Test-Case"><a href="#Test-Case" class="headerlink" title="Test Case:"></a>Test Case:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">image = np.zeros((<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">label = np.zeros((<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">            image[i, j, k, <span class="number">0</span>] = i*j*k</span><br><span class="line">            label[i, j, k] = k</span><br><span class="line"></span><br><span class="line">print(<span class="string">"image:"</span>)</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    print(<span class="string">f"z = <span class="subst">&#123;k&#125;</span>"</span>)</span><br><span class="line">    print(image[:, :, k, <span class="number">0</span>])</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">"label:"</span>)</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    print(<span class="string">f"z = <span class="subst">&#123;k&#125;</span>"</span>)</span><br><span class="line">    print(label[:, :, k])</span><br></pre></td></tr></table></figure><pre><code>image:z = 0[[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]]z = 1[[0. 0. 0. 0.] [0. 1. 2. 3.] [0. 2. 4. 6.] [0. 3. 6. 9.]]z = 2[[ 0.  0.  0.  0.] [ 0.  2.  4.  6.] [ 0.  4.  8. 12.] [ 0.  6. 12. 18.]]label:z = 0[[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]]z = 1[[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]]z = 2[[2. 2. 2. 2.] [2. 2. 2. 2.] [2. 2. 2. 2.] [2. 2. 2. 2.]]</code></pre><h4 id="Test-Extracting-2-2-2-sub-volume"><a href="#Test-Extracting-2-2-2-sub-volume" class="headerlink" title="Test: Extracting (2, 2, 2) sub-volume"></a>Test: Extracting (2, 2, 2) sub-volume</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sample_image, sample_label = get_sub_volume(image, </span><br><span class="line">                                            label,</span><br><span class="line">                                            orig_x=<span class="number">4</span>, </span><br><span class="line">                                            orig_y=<span class="number">4</span>, </span><br><span class="line">                                            orig_z=<span class="number">3</span>,</span><br><span class="line">                                            output_x=<span class="number">2</span>, </span><br><span class="line">                                            output_y=<span class="number">2</span>, </span><br><span class="line">                                            output_z=<span class="number">2</span>,</span><br><span class="line">                                            num_classes = <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Sampled Image:"</span>)</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    print(<span class="string">"z = "</span> + str(k))</span><br><span class="line">    print(sample_image[<span class="number">0</span>, :, :, k])</span><br></pre></td></tr></table></figure><pre><code>Sampled Image:z = 0[[0. 2.] [0. 3.]]z = 1[[0. 4.] [0. 6.]]</code></pre><h4 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Sampled Image:</span><br><span class="line">z = <span class="number">0</span></span><br><span class="line">[[<span class="number">0.</span> <span class="number">2.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">3.</span>]]</span><br><span class="line">z = <span class="number">1</span></span><br><span class="line">[[<span class="number">0.</span> <span class="number">4.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">6.</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Sampled Label:"</span>)</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    print(<span class="string">"class = "</span> + str(c))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        print(<span class="string">"z = "</span> + str(k))</span><br><span class="line">        print(sample_label[c, :, :, k])</span><br></pre></td></tr></table></figure><pre><code>Sampled Label:class = 0z = 0[[1. 1.] [1. 1.]]z = 1[[0. 0.] [0. 0.]]class = 1z = 0[[0. 0.] [0. 0.]]z = 1[[1. 1.] [1. 1.]]</code></pre><h4 id="Expected-output-1"><a href="#Expected-output-1" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Sampled Label:</span><br><span class="line">class = 0</span><br><span class="line">z = <span class="number">0</span></span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span>]]</span><br><span class="line">z = <span class="number">1</span></span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">class = 1</span><br><span class="line">z = <span class="number">0</span></span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">z = <span class="number">1</span></span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span>]]</span><br></pre></td></tr></table></figure><p>You can run the following cell to look at a candidate patch and ensure that the function works correctly. We’ll look at the enhancing tumor part of the label.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">image, label = load_case(DATA_DIR + <span class="string">"imagesTr/BRATS_001.nii.gz"</span>, DATA_DIR + <span class="string">"labelsTr/BRATS_001.nii.gz"</span>)</span><br><span class="line">X, y = get_sub_volume(image, label)</span><br><span class="line"><span class="comment"># enhancing tumor is channel 2 in the class label</span></span><br><span class="line"><span class="comment"># you can change indexer for y to look at different classes</span></span><br><span class="line">util.visualize_patch(X[<span class="number">0</span>, :, :, :], y[<span class="number">2</span>])</span><br></pre></td></tr></table></figure><p><img src="output_26_0.png" alt="png"></p><p><a name="1-4-2"></a></p><h3 id="1-4-2-Standardization"><a href="#1-4-2-Standardization" class="headerlink" title="1.4.2 Standardization"></a>1.4.2 Standardization</h3><p>Next, fill in the following function that given a patch (sub-volume), standardizes the values across each channel and each Z plane to have a mean of zero and standard deviation of 1. </p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li> Check that the standard deviation is not zero before dividing by it.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standardize</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Standardize mean and standard deviation </span></span><br><span class="line"><span class="string">        of each channel and z_dimension.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        image (np.array): input image, </span></span><br><span class="line"><span class="string">            shape (num_channels, dim_x, dim_y, dim_z)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        standardized_image (np.array): standardized version of input image</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize to array of zeros, with same shape as the image</span></span><br><span class="line">    standardized_image = np.empty(image.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># iterate over channels</span></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(image.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="comment"># iterate over the `z` dimension</span></span><br><span class="line">        <span class="keyword">for</span> z <span class="keyword">in</span> range(image.shape[<span class="number">3</span>]):</span><br><span class="line">            <span class="comment"># get a slice of the image </span></span><br><span class="line">            <span class="comment"># at channel c and z-th dimension `z`</span></span><br><span class="line">            image_slice = image[c,:,:,z]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># subtract the mean from image_slice</span></span><br><span class="line">            centered = image_slice - image_slice.mean()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> image_slice.std() != <span class="number">0</span>:</span><br><span class="line">                centered_scaled = image_slice / image_slice.std()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                centered_scaled = centered</span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">            <span class="comment"># update  the slice of standardized image</span></span><br><span class="line">            <span class="comment"># with the scaled centered and scaled image</span></span><br><span class="line">            standardized_image[c, :, :, z] = centered_scaled</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> standardized_image</span><br></pre></td></tr></table></figure><p>And to sanity check, let’s look at the output of our function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_norm = standardize(X)</span><br><span class="line">print(<span class="string">"standard deviation for a slice should be 1.0"</span>)</span><br><span class="line">print(<span class="string">f"stddv for X_norm[0, :, :, 0]: <span class="subst">&#123;X_norm[<span class="number">0</span>,:,:,<span class="number">0</span>].std():<span class="number">.2</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>standard deviation for a slice should be 1.0stddv for X_norm[0, :, :, 0]: 1.00</code></pre><p>Let’s visualize our patch again just to make sure (it won’t look different since the <code>imshow</code> function we use to visualize automatically normalizes the pixels when displaying in black and white).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.visualize_patch(X_norm[<span class="number">0</span>, :, :, :], y[<span class="number">2</span>])</span><br></pre></td></tr></table></figure><p><img src="output_33_0.png" alt="png"></p><p><a name="2"></a></p><h1 id="2-Model-3D-U-Net"><a href="#2-Model-3D-U-Net" class="headerlink" title="2 Model: 3D U-Net"></a>2 Model: 3D U-Net</h1><p>Now let’s build our model. In this assignment we will be building a <a href="https://arxiv.org/abs/1606.06650" target="_blank" rel="noopener">3D U-net</a>. </p><ul><li>This architecture will take advantage of the volumetric shape of MR images and is one of the best performing models for this task. </li><li>Feel free to familiarize yourself with the architecture by reading <a href="https://arxiv.org/abs/1606.06650" target="_blank" rel="noopener">this paper</a>.</li></ul><p><img src="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png" width="50%"></p><p><a name="3"></a></p><h1 id="3-Metrics"><a href="#3-Metrics" class="headerlink" title="3 Metrics"></a>3 Metrics</h1><p><a name="3-1"></a></p><h2 id="3-1-Dice-Similarity-Coefficient"><a href="#3-1-Dice-Similarity-Coefficient" class="headerlink" title="3.1 Dice Similarity Coefficient"></a>3.1 Dice Similarity Coefficient</h2><p>Aside from the architecture, one of the most important elements of any deep learning method is the choice of our loss function. </p><p>A natural choice that you may be familiar with is the cross-entropy loss function. </p><ul><li>However, this loss function is not ideal for segmentation tasks due to heavy class imbalance (there are typically not many positive regions). </li></ul><p>A much more common loss for segmentation tasks is the Dice similarity coefficient, which is a measure of how well two contours overlap. </p><ul><li>The Dice index ranges from 0 (complete mismatch) </li><li>To 1 (perfect match).</li></ul><p>In general, for two sets $A$ and $B$, the Dice similarity coefficient is defined as: </p><script type="math/tex; mode=display">\text{DSC}(A, B) = \frac{2 \times |A \cap B|}{|A| + |B|}.</script><p>Here we can interpret $A$ and $B$ as sets of voxels, $A$ being the predicted tumor region and $B$ being the ground truth. </p><p>Our model will map each voxel to 0 or 1</p><ul><li>0 means it is a background voxel</li><li>1 means it is part of the segmented region.</li></ul><p>In the dice coefficient, the variables in the formula are:</p><ul><li>$x$ : the input image</li><li>$f(x)$ : the model output (prediction)</li><li>$y$ : the label (actual ground truth)</li></ul><p>The dice coefficient “DSC” is:</p><script type="math/tex; mode=display">\text{DSC}(f, x, y) = \frac{2 \times \sum_{i, j} f(x)_{ij} \times y_{ij} + \epsilon}{\sum_{i,j} f(x)_{ij} + \sum_{i, j} y_{ij} + \epsilon}</script><ul><li>$\epsilon$ is a small number that is added to avoid division by zero</li></ul><p><img src="https://www.researchgate.net/publication/328671987/figure/fig4/AS:688210103529478@1541093483784/Calculation-of-the-Dice-similarity-coefficient-The-deformed-contour-of-the-liver-from.ppm" width="30%"></p><p><a href="https://www.researchgate.net/figure/Calculation-of-the-Dice-similarity-coefficient-The-deformed-contour-of-the-liver-from_fig4_328671987" target="_blank" rel="noopener">Image Source</a></p><p>Implement the dice coefficient for a single output class below.</p><ul><li>Please use the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/backend/sum" target="_blank" rel="noopener">Keras.sum(x,axis=)</a> function to compute the numerator and denominator of the dice coefficient.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">single_class_dice_coefficient</span><span class="params">(y_true, y_pred, axis=<span class="params">(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>)</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                                  epsilon=<span class="number">0.00001</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute dice coefficient for single class.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y_true (Tensorflow tensor): tensor of ground truth values for single class.</span></span><br><span class="line"><span class="string">                                    shape: (x_dim, y_dim, z_dim)</span></span><br><span class="line"><span class="string">        y_pred (Tensorflow tensor): tensor of predictions for single class.</span></span><br><span class="line"><span class="string">                                    shape: (x_dim, y_dim, z_dim)</span></span><br><span class="line"><span class="string">        axis (tuple): spatial axes to sum over when computing numerator and</span></span><br><span class="line"><span class="string">                      denominator of dice coefficient.</span></span><br><span class="line"><span class="string">                      Hint: pass this as the 'axis' argument to the K.sum function.</span></span><br><span class="line"><span class="string">        epsilon (float): small constant added to numerator and denominator to</span></span><br><span class="line"><span class="string">                        avoid divide by 0 errors.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dice_coefficient (float): computed value of dice coefficient.     </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    dice_numerator = K.sum(<span class="number">2</span> * y_true * y_pred, axis= axis) + epsilon</span><br><span class="line">    dice_denominator = K.sum(y_true,axis= axis) + K.sum(y_pred, axis = axis) + epsilon</span><br><span class="line">    dice_coefficient = dice_numerator / dice_denominator </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dice_coefficient</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEST CASES</span></span><br><span class="line">sess = K.get_session()</span><br><span class="line"><span class="comment">#sess = tf.compat.v1.Session()</span></span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    pred = np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Test Case #1"</span>)</span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(pred[:, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(label[:, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># choosing a large epsilon to help check for implementation errors</span></span><br><span class="line">    dc = single_class_dice_coefficient(pred, label,epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"dice coefficient: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Test Case #2"</span>)</span><br><span class="line">    pred = np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]]), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(pred[:, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(label[:, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># choosing a large epsilon to help check for implementation errors</span></span><br><span class="line">    dc = single_class_dice_coefficient(pred, label,epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"dice_coefficient: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #1pred:[[1. 0.] [0. 1.]]label:[[1. 1.] [0. 0.]]dice coefficient: 0.6000Test Case #2pred:[[1. 0.] [0. 1.]]label:[[1. 1.] [0. 1.]]dice_coefficient: 0.8333</code></pre><h5 id="Expected-output-2"><a href="#Expected-output-2" class="headerlink" title="Expected output"></a>Expected output</h5><p>If you get a different result, please check that you implemented the equation completely.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#1</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">dice coefficient: <span class="number">0.6000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Test Case <span class="comment">#2</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">dice_coefficient: <span class="number">0.8333</span></span><br></pre></td></tr></table></figure></p><h3 id="Dice-Coefficient-for-Multiple-classes"><a href="#Dice-Coefficient-for-Multiple-classes" class="headerlink" title="Dice Coefficient for Multiple classes"></a>Dice Coefficient for Multiple classes</h3><p>Now that we have the single class case, we can think about how to approach the multi class context. </p><ul><li>Remember that for this task, we want segmentations for each of the 3 classes of abnormality (edema, enhancing tumor, non-enhancing tumor). </li><li>This will give us 3 different dice coefficients (one for each abnormality class). </li><li>To combine these, we can just take the average. We can write that the overall dice coefficient is: </li></ul><script type="math/tex; mode=display">DC(f, x, y) = \frac{1}{3} \left ( DC_{1}(f, x, y) + DC_{2}(f, x, y) + DC_{3}(f, x, y) \right )</script><ul><li>$DC_{1}$, $DC_{2}$ and $DC_{3}$ are edema, enhancing tumor, and non-enhancing tumor dice coefficients.</li></ul><p>For any number of classes, the equation becomes:  </p><script type="math/tex; mode=display">DC(f, x, y) = \frac{1}{N} \sum_{c=1}^{C} \left ( DC_{c}(f, x, y) \right )</script><p>In this case, with three categories, $C = 3$</p><p>Implement the mean dice coefficient below. This should not be very different from your singe-class implementation.</p><p>Please use the <a href="https://keras.io/backend/#mean" target="_blank" rel="noopener">K.mean</a> function to take the average of the three classes.  </p><ul><li>Apply the mean to the ratio that you calculate in the last line of code that you’ll implement.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dice_coefficient</span><span class="params">(y_true, y_pred, axis=<span class="params">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                     epsilon=<span class="number">0.00001</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute mean dice coefficient over all abnormality classes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y_true (Tensorflow tensor): tensor of ground truth values for all classes.</span></span><br><span class="line"><span class="string">                                    shape: (num_classes, x_dim, y_dim, z_dim)</span></span><br><span class="line"><span class="string">        y_pred (Tensorflow tensor): tensor of predictions for all classes.</span></span><br><span class="line"><span class="string">                                    shape: (num_classes, x_dim, y_dim, z_dim)</span></span><br><span class="line"><span class="string">        axis (tuple): spatial axes to sum over when computing numerator and</span></span><br><span class="line"><span class="string">                      denominator of dice coefficient.</span></span><br><span class="line"><span class="string">                      Hint: pass this as the 'axis' argument to the K.sum</span></span><br><span class="line"><span class="string">                            and K.mean functions.</span></span><br><span class="line"><span class="string">        epsilon (float): small constant add to numerator and denominator to</span></span><br><span class="line"><span class="string">                        avoid divide by 0 errors.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dice_coefficient (float): computed value of dice coefficient.     </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    dice_numerator = K.sum(<span class="number">2</span> * y_true * y_pred, axis= axis) + epsilon</span><br><span class="line">    dice_denominator = K.sum(y_true,axis= axis) + K.sum(y_pred, axis = axis) + epsilon</span><br><span class="line">    dice_coefficient = K.mean(dice_numerator / dice_denominator,axis = <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dice_coefficient</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEST CASES</span></span><br><span class="line">sess = K.get_session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Test Case #1"</span>)</span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    dc = dice_coefficient(pred, label,epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"dice coefficient: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Test Case #2"</span>)</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    dc = dice_coefficient(pred, label,epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"dice coefficient: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">    print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Test Case #3"</span>)</span><br><span class="line">    pred = np.zeros((<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">    pred[<span class="number">0</span>, :, :, :] = np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">-1</span>)</span><br><span class="line">    pred[<span class="number">1</span>, :, :, :] = np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    label = np.zeros((<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">    label[<span class="number">0</span>, :, :, :] = np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">-1</span>)</span><br><span class="line">    label[<span class="number">1</span>, :, :, :] = np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]]), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(<span class="string">"class = 0"</span>)</span><br><span class="line">    print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"class = 1"</span>)</span><br><span class="line">    print(pred[<span class="number">1</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(<span class="string">"class = 0"</span>)</span><br><span class="line">    print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"class = 1"</span>)</span><br><span class="line">    print(label[<span class="number">1</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    dc = dice_coefficient(pred, label,epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"dice coefficient: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #1pred:[[1. 0.] [0. 1.]]label:[[1. 1.] [0. 0.]]dice coefficient: 0.6000Test Case #2pred:[[1. 0.] [0. 1.]]label:[[1. 1.] [0. 1.]]dice coefficient: 0.8333Test Case #3pred:class = 0[[1. 0.] [0. 1.]]class = 1[[1. 0.] [0. 1.]]label:class = 0[[1. 1.] [0. 0.]]class = 1[[1. 1.] [0. 1.]]dice coefficient: 0.7167</code></pre><h4 id="Expected-output-3"><a href="#Expected-output-3" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#1</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">dice coefficient: <span class="number">0.6000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Test Case <span class="comment">#2</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">dice coefficient: <span class="number">0.8333</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Test Case <span class="comment">#3</span></span><br><span class="line">pred:</span><br><span class="line">class = 0</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">class = 1</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">class = 0</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">class = 1</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">dice coefficient: <span class="number">0.7167</span></span><br></pre></td></tr></table></figure><p><a name="3-2"></a></p><h2 id="3-2-Soft-Dice-Loss"><a href="#3-2-Soft-Dice-Loss" class="headerlink" title="3.2 Soft Dice Loss"></a>3.2 Soft Dice Loss</h2><p>While the Dice Coefficient makes intuitive sense, it is not the best for training. </p><ul><li>This is because it takes in discrete values (zeros and ones). </li><li>The model outputs <em>probabilities</em> that each pixel is, say, a tumor or not, and we want to be able to backpropagate through those outputs. </li></ul><p>Therefore, we need an analogue of the Dice loss which takes real valued input. This is where the <strong>Soft Dice loss</strong> comes in. The formula is: </p><script type="math/tex; mode=display">\mathcal{L}_{Dice}(p, q) = 1 - \frac{2\times\sum_{i, j} p_{ij}q_{ij} + \epsilon}{\left(\sum_{i, j} p_{ij}^2 \right) + \left(\sum_{i, j} q_{ij}^2 \right) + \epsilon}</script><ul><li>$p$ is our predictions</li><li>$q$ is the ground truth </li><li>In practice each $q_i$ will either be 0 or 1. </li><li>$\epsilon$ is a small number that is added to avoid division by zero</li></ul><p>The soft Dice loss ranges between </p><ul><li>0: perfectly matching the ground truth distribution $q$</li><li>1: complete mismatch with the ground truth.</li></ul><p>You can also check that if $p_i$ and $q_i$ are each 0 or 1, then the soft Dice loss is just one minus the dice coefficient.</p><h3 id="Multi-Class-Soft-Dice-Loss"><a href="#Multi-Class-Soft-Dice-Loss" class="headerlink" title="Multi-Class Soft Dice Loss"></a>Multi-Class Soft Dice Loss</h3><p>We’ve explained the single class case for simplicity, but the multi-class generalization is exactly the same as that of the dice coefficient. </p><ul><li>Since you’ve already implemented the multi-class dice coefficient, we’ll have you jump directly to the multi-class soft dice loss.</li></ul><p>For any number of categories of diseases, the expression becomes:</p><script type="math/tex; mode=display">\mathcal{L}_{Dice}(p, q) = 1 - \frac{1}{N} \sum_{c=1}^{C} \frac{2\times\sum_{i, j} p_{cij}q_{cij} + \epsilon}{\left(\sum_{i, j} p_{cij}^2 \right) + \left(\sum_{i, j} q_{cij}^2 \right) + \epsilon}</script><p>Please implement the soft dice loss below!</p><p>As before, you will use K.mean()</p><ul><li>Apply the average the mean to ratio that you’ll calculate in the last line of code that you’ll implement.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">soft_dice_loss</span><span class="params">(y_true, y_pred, axis=<span class="params">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                   epsilon=<span class="number">0.00001</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute mean soft dice loss over all abnormality classes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y_true (Tensorflow tensor): tensor of ground truth values for all classes.</span></span><br><span class="line"><span class="string">                                    shape: (num_classes, x_dim, y_dim, z_dim)</span></span><br><span class="line"><span class="string">        y_pred (Tensorflow tensor): tensor of soft predictions for all classes.</span></span><br><span class="line"><span class="string">                                    shape: (num_classes, x_dim, y_dim, z_dim)</span></span><br><span class="line"><span class="string">        axis (tuple): spatial axes to sum over when computing numerator and</span></span><br><span class="line"><span class="string">                      denominator in formula for dice loss.</span></span><br><span class="line"><span class="string">                      Hint: pass this as the 'axis' argument to the K.sum</span></span><br><span class="line"><span class="string">                            and K.mean functions.</span></span><br><span class="line"><span class="string">        epsilon (float): small constant added to numerator and denominator to</span></span><br><span class="line"><span class="string">                        avoid divide by 0 errors.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dice_loss (float): computed value of dice loss.     </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line"></span><br><span class="line">    dice_numerator = <span class="number">2</span> * K.sum(y_true * y_pred, axis=axis) + epsilon</span><br><span class="line">    dice_denominator = K.sum(K.square(y_true), axis = axis) + K.sum(K.square(y_pred), axis = axis) + epsilon</span><br><span class="line">    dice_loss = <span class="number">1</span> - K.mean(dice_numerator / dice_denominator, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dice_loss</span><br></pre></td></tr></table></figure><h4 id="Test-Case-1"><a href="#Test-Case-1" class="headerlink" title="Test Case 1"></a>Test Case 1</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEST CASES</span></span><br><span class="line">sess = K.get_session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Test Case #1"</span>)</span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    dc = soft_dice_loss(pred, label, epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"soft dice loss:<span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #1pred:[[1. 0.] [0. 1.]]label:[[1. 1.] [0. 0.]]soft dice loss:0.4000</code></pre><h4 id="Expected-output-4"><a href="#Expected-output-4" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#1</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">soft dice loss:<span class="number">0.4000</span></span><br></pre></td></tr></table></figure><h4 id="Test-Case-2"><a href="#Test-Case-2" class="headerlink" title="Test Case 2"></a>Test Case 2</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sess = K.get_session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"Test Case #2"</span>)</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(<span class="number">0.5</span>*np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    dc = soft_dice_loss(pred, label, epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"soft dice loss: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #2pred:[[0.5 0. ] [0.  0.5]]label:[[1. 1.] [0. 0.]]soft dice loss: 0.4286</code></pre><h4 id="Expected-output-5"><a href="#Expected-output-5" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#2</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">0.5</span> <span class="number">0.</span> ]</span><br><span class="line"> [<span class="number">0.</span>  <span class="number">0.5</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">soft dice loss: <span class="number">0.4286</span></span><br></pre></td></tr></table></figure><h4 id="Test-Case-3"><a href="#Test-Case-3" class="headerlink" title="Test Case 3"></a>Test Case 3</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sess = K.get_session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"Test Case #3"</span>)</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    dc = soft_dice_loss(pred, label, epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"soft dice loss: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #3pred:[[1. 0.] [0. 1.]]label:[[1. 1.] [0. 1.]]soft dice loss: 0.1667</code></pre><h4 id="Expected-output-6"><a href="#Expected-output-6" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#3</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">soft dice loss: <span class="number">0.1667</span></span><br></pre></td></tr></table></figure><h4 id="Test-Case-4"><a href="#Test-Case-4" class="headerlink" title="Test Case 4"></a>Test Case 4</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">sess = K.get_session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Test Case #4"</span>)</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    pred[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>] = <span class="number">0.8</span></span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    dc = soft_dice_loss(pred, label, epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"soft dice loss: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #4pred:[[1.  0.8] [0.  1. ]]label:[[1. 1.] [0. 1.]]soft dice loss: 0.0060</code></pre><h4 id="Expected-output-7"><a href="#Expected-output-7" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#4</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span>  <span class="number">0.8</span>]</span><br><span class="line"> [<span class="number">0.</span>  <span class="number">1.</span> ]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">soft dice loss: <span class="number">0.0060</span></span><br></pre></td></tr></table></figure><h4 id="Test-Case-5"><a href="#Test-Case-5" class="headerlink" title="Test Case 5"></a>Test Case 5</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">sess = K.get_session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"Test Case #5"</span>)</span><br><span class="line">    pred = np.zeros((<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">    pred[<span class="number">0</span>, :, :, :] = np.expand_dims(<span class="number">0.5</span>*np.eye(<span class="number">2</span>), <span class="number">-1</span>)</span><br><span class="line">    pred[<span class="number">1</span>, :, :, :] = np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">-1</span>)</span><br><span class="line">    pred[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>] = <span class="number">0.8</span></span><br><span class="line"></span><br><span class="line">    label = np.zeros((<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">    label[<span class="number">0</span>, :, :, :] = np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">-1</span>)</span><br><span class="line">    label[<span class="number">1</span>, :, :, :] = np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]]), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(<span class="string">"class = 0"</span>)</span><br><span class="line">    print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"class = 1"</span>)</span><br><span class="line">    print(pred[<span class="number">1</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(<span class="string">"class = 0"</span>)</span><br><span class="line">    print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"class = 1"</span>)</span><br><span class="line">    print(label[<span class="number">1</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    dc = soft_dice_loss(pred, label, epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"soft dice loss: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #5pred:class = 0[[0.5 0. ] [0.  0.5]]class = 1[[1.  0.8] [0.  1. ]]label:class = 0[[1. 1.] [0. 0.]]class = 1[[1. 1.] [0. 1.]]soft dice loss: 0.2173</code></pre><h4 id="Expected-output-8"><a href="#Expected-output-8" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#5</span></span><br><span class="line">pred:</span><br><span class="line">class = 0</span><br><span class="line">[[<span class="number">0.5</span> <span class="number">0.</span> ]</span><br><span class="line"> [<span class="number">0.</span>  <span class="number">0.5</span>]]</span><br><span class="line">class = 1</span><br><span class="line">[[<span class="number">1.</span>  <span class="number">0.8</span>]</span><br><span class="line"> [<span class="number">0.</span>  <span class="number">1.</span> ]]</span><br><span class="line">label:</span><br><span class="line">class = 0</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">class = 1</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">soft dice loss: <span class="number">0.2173</span></span><br></pre></td></tr></table></figure><h4 id="Test-Case-6"><a href="#Test-Case-6" class="headerlink" title="Test Case 6"></a>Test Case 6</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test case 6</span></span><br><span class="line">pred = np.array([</span><br><span class="line">                    [</span><br><span class="line">                        [ </span><br><span class="line">                            [<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">                        ],</span><br><span class="line">                        [</span><br><span class="line">                            [<span class="number">1.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]</span><br><span class="line">                        ]</span><br><span class="line">                    ],</span><br><span class="line">                    [</span><br><span class="line">                        [ </span><br><span class="line">                            [<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">                        ],</span><br><span class="line">                        [</span><br><span class="line">                            [<span class="number">1.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]</span><br><span class="line">                        ]</span><br><span class="line">                    ],</span><br><span class="line">                  ])</span><br><span class="line">label = np.array([</span><br><span class="line">                    [</span><br><span class="line">                        [ </span><br><span class="line">                            [<span class="number">1.0</span>, <span class="number">0.0</span>], [<span class="number">1.0</span>, <span class="number">0.0</span>]</span><br><span class="line">                        ],</span><br><span class="line">                        [</span><br><span class="line">                            [<span class="number">1.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">                        ]</span><br><span class="line">                    ],</span><br><span class="line">                    [</span><br><span class="line">                        [ </span><br><span class="line">                            [<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">                        ],</span><br><span class="line">                        [</span><br><span class="line">                            [<span class="number">1.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">                        ]</span><br><span class="line">                    ]</span><br><span class="line">                  ])</span><br><span class="line"></span><br><span class="line">sess = K.get_session()</span><br><span class="line">print(<span class="string">"Test case #6"</span>)</span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    dc = soft_dice_loss(pred, label, epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"soft dice loss"</span>,dc.eval())</span><br></pre></td></tr></table></figure><pre><code>Test case #6soft dice loss 0.4375</code></pre><h4 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Test case <span class="comment">#6</span></span><br><span class="line">soft dice loss: <span class="number">0.4375</span></span><br></pre></td></tr></table></figure><p>Note, if you don’t have a scalar, and have an array with more than one value, please check your implementation!</p><p><a name="4"></a></p><h1 id="4-Create-and-Train-the-model"><a href="#4-Create-and-Train-the-model" class="headerlink" title="4 Create and Train the model"></a>4 Create and Train the model</h1><p>Once you’ve finished implementing the soft dice loss, we can create the model! </p><p>We’ll use the <code>unet_model_3d</code> function in <code>utils</code> which we implemented for you.</p><ul><li>This creates the model architecture and compiles the model with the specified loss functions and metrics. </li><li>Check out function <code>util.unet_model_3d(loss_function)</code> in the <code>util.py</code> file.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = util.unet_model_3d(loss_function=soft_dice_loss, metrics=[dice_coefficient])</span><br></pre></td></tr></table></figure><pre><code>WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.Instructions for updating:If using Keras pass *_constraint arguments to layers.</code></pre><p><a name="4-1"></a></p><h2 id="4-1-Training-on-a-Large-Dataset"><a href="#4-1-Training-on-a-Large-Dataset" class="headerlink" title="4.1 Training on a Large Dataset"></a>4.1 Training on a Large Dataset</h2><p>In order to facilitate the training on the large dataset:</p><ul><li>We have pre-processed the entire dataset into patches and stored the patches in the <a href="http://docs.h5py.org/en/stable/" target="_blank" rel="noopener"><code>h5py</code></a> format. </li><li>We also wrote a custom Keras <a href="https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence" target="_blank" rel="noopener"><code>Sequence</code></a> class which can be used as a <code>Generator</code> for the keras model to train on large datasets. </li><li>Feel free to look at the <code>VolumeDataGenerator</code> class in <code>util.py</code> to learn about how such a generator can be coded.</li></ul><p>Note: <a href="https://www.geeksforgeeks.org/keras-fit-and-keras-fit_generator/" target="_blank" rel="noopener">Here</a> you can check the difference between <code>fit</code> and <code>fit_generator</code> functions.</p><p>To get a flavor of the training on the larger dataset, you can run the following cell to train the model on a small subset of the dataset (85 patches). You should see the loss going down and the dice coefficient going up. </p><p>Running <code>model.fit()</code> on the Coursera workspace may cause the kernel to die.</p><ul><li>Soon, we will load a pre-trained version of this model, so that you don’t need to train the model on this workspace.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run this on your local machine only</span></span><br><span class="line"><span class="comment"># May cause the kernel to die if running in the Coursera platform</span></span><br><span class="line"></span><br><span class="line">base_dir = HOME_DIR + <span class="string">"processed/"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(base_dir + <span class="string">"config.json"</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    config = json.load(json_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get generators for training and validation sets</span></span><br><span class="line">train_generator = util.VolumeDataGenerator(config[<span class="string">"train"</span>], base_dir + <span class="string">"train/"</span>, batch_size=<span class="number">3</span>, dim=(<span class="number">160</span>, <span class="number">160</span>, <span class="number">16</span>), verbose=<span class="number">0</span>)</span><br><span class="line">valid_generator = util.VolumeDataGenerator(config[<span class="string">"valid"</span>], base_dir + <span class="string">"valid/"</span>, batch_size=<span class="number">3</span>, dim=(<span class="number">160</span>, <span class="number">160</span>, <span class="number">16</span>), verbose=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">steps_per_epoch = <span class="number">20</span></span><br><span class="line">n_epochs=<span class="number">10</span></span><br><span class="line">validation_steps = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">model.fit_generator(generator=train_generator,</span><br><span class="line">        steps_per_epoch=steps_per_epoch,</span><br><span class="line">        epochs=n_epochs,</span><br><span class="line">        use_multiprocessing=<span class="keyword">True</span>,</span><br><span class="line">        validation_data=valid_generator,</span><br><span class="line">        validation_steps=validation_steps)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run this cell if you to save the weights of your trained model in cell section 4.1</span></span><br><span class="line"><span class="comment">#model.save_weights(base_dir + 'my_model_pretrained.hdf5')</span></span><br></pre></td></tr></table></figure><p><a name="4-2"></a></p><h2 id="4-2-Loading-a-Pre-Trained-Model"><a href="#4-2-Loading-a-Pre-Trained-Model" class="headerlink" title="4.2 Loading a Pre-Trained Model"></a>4.2 Loading a Pre-Trained Model</h2><p>As in assignment 1, instead of having the model train for longer, we’ll give you access to a pretrained version. We’ll use this to extract predictions and measure performance.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># run this cell if you didn't run the training cell in section 4.1</span></span><br><span class="line">base_dir = HOME_DIR + <span class="string">"processed/"</span></span><br><span class="line"><span class="keyword">with</span> open(base_dir + <span class="string">"config.json"</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    config = json.load(json_file)</span><br><span class="line"><span class="comment"># Get generators for training and validation sets</span></span><br><span class="line">train_generator = util.VolumeDataGenerator(config[<span class="string">"train"</span>], base_dir + <span class="string">"train/"</span>, batch_size=<span class="number">3</span>, dim=(<span class="number">160</span>, <span class="number">160</span>, <span class="number">16</span>), verbose=<span class="number">0</span>)</span><br><span class="line">valid_generator = util.VolumeDataGenerator(config[<span class="string">"valid"</span>], base_dir + <span class="string">"valid/"</span>, batch_size=<span class="number">3</span>, dim=(<span class="number">160</span>, <span class="number">160</span>, <span class="number">16</span>), verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_weights(HOME_DIR + <span class="string">"model_pretrained.hdf5"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;model_1&quot;__________________________________________________________________________________________________Layer (type)                    Output Shape         Param #     Connected to                     ==================================================================================================input_1 (InputLayer)            (None, 4, 160, 160,  0                                            __________________________________________________________________________________________________conv3d_1 (Conv3D)               (None, 32, 160, 160, 3488        input_1[0][0]                    __________________________________________________________________________________________________activation_1 (Activation)       (None, 32, 160, 160, 0           conv3d_1[0][0]                   __________________________________________________________________________________________________conv3d_2 (Conv3D)               (None, 64, 160, 160, 55360       activation_1[0][0]               __________________________________________________________________________________________________activation_2 (Activation)       (None, 64, 160, 160, 0           conv3d_2[0][0]                   __________________________________________________________________________________________________max_pooling3d_1 (MaxPooling3D)  (None, 64, 80, 80, 8 0           activation_2[0][0]               __________________________________________________________________________________________________conv3d_3 (Conv3D)               (None, 64, 80, 80, 8 110656      max_pooling3d_1[0][0]            __________________________________________________________________________________________________activation_3 (Activation)       (None, 64, 80, 80, 8 0           conv3d_3[0][0]                   __________________________________________________________________________________________________conv3d_4 (Conv3D)               (None, 128, 80, 80,  221312      activation_3[0][0]               __________________________________________________________________________________________________activation_4 (Activation)       (None, 128, 80, 80,  0           conv3d_4[0][0]                   __________________________________________________________________________________________________max_pooling3d_2 (MaxPooling3D)  (None, 128, 40, 40,  0           activation_4[0][0]               __________________________________________________________________________________________________conv3d_5 (Conv3D)               (None, 128, 40, 40,  442496      max_pooling3d_2[0][0]            __________________________________________________________________________________________________activation_5 (Activation)       (None, 128, 40, 40,  0           conv3d_5[0][0]                   __________________________________________________________________________________________________conv3d_6 (Conv3D)               (None, 256, 40, 40,  884992      activation_5[0][0]               __________________________________________________________________________________________________activation_6 (Activation)       (None, 256, 40, 40,  0           conv3d_6[0][0]                   __________________________________________________________________________________________________max_pooling3d_3 (MaxPooling3D)  (None, 256, 20, 20,  0           activation_6[0][0]               __________________________________________________________________________________________________conv3d_7 (Conv3D)               (None, 256, 20, 20,  1769728     max_pooling3d_3[0][0]            __________________________________________________________________________________________________activation_7 (Activation)       (None, 256, 20, 20,  0           conv3d_7[0][0]                   __________________________________________________________________________________________________conv3d_8 (Conv3D)               (None, 512, 20, 20,  3539456     activation_7[0][0]               __________________________________________________________________________________________________activation_8 (Activation)       (None, 512, 20, 20,  0           conv3d_8[0][0]                   __________________________________________________________________________________________________up_sampling3d_1 (UpSampling3D)  (None, 512, 40, 40,  0           activation_8[0][0]               __________________________________________________________________________________________________concatenate_1 (Concatenate)     (None, 768, 40, 40,  0           up_sampling3d_1[0][0]                                                                             activation_6[0][0]               __________________________________________________________________________________________________conv3d_9 (Conv3D)               (None, 256, 40, 40,  5308672     concatenate_1[0][0]              __________________________________________________________________________________________________activation_9 (Activation)       (None, 256, 40, 40,  0           conv3d_9[0][0]                   __________________________________________________________________________________________________conv3d_10 (Conv3D)              (None, 256, 40, 40,  1769728     activation_9[0][0]               __________________________________________________________________________________________________activation_10 (Activation)      (None, 256, 40, 40,  0           conv3d_10[0][0]                  __________________________________________________________________________________________________up_sampling3d_2 (UpSampling3D)  (None, 256, 80, 80,  0           activation_10[0][0]              __________________________________________________________________________________________________concatenate_2 (Concatenate)     (None, 384, 80, 80,  0           up_sampling3d_2[0][0]                                                                             activation_4[0][0]               __________________________________________________________________________________________________conv3d_11 (Conv3D)              (None, 128, 80, 80,  1327232     concatenate_2[0][0]              __________________________________________________________________________________________________activation_11 (Activation)      (None, 128, 80, 80,  0           conv3d_11[0][0]                  __________________________________________________________________________________________________conv3d_12 (Conv3D)              (None, 128, 80, 80,  442496      activation_11[0][0]              __________________________________________________________________________________________________activation_12 (Activation)      (None, 128, 80, 80,  0           conv3d_12[0][0]                  __________________________________________________________________________________________________up_sampling3d_3 (UpSampling3D)  (None, 128, 160, 160 0           activation_12[0][0]              __________________________________________________________________________________________________concatenate_3 (Concatenate)     (None, 192, 160, 160 0           up_sampling3d_3[0][0]                                                                             activation_2[0][0]               __________________________________________________________________________________________________conv3d_13 (Conv3D)              (None, 64, 160, 160, 331840      concatenate_3[0][0]              __________________________________________________________________________________________________activation_13 (Activation)      (None, 64, 160, 160, 0           conv3d_13[0][0]                  __________________________________________________________________________________________________conv3d_14 (Conv3D)              (None, 64, 160, 160, 110656      activation_13[0][0]              __________________________________________________________________________________________________activation_14 (Activation)      (None, 64, 160, 160, 0           conv3d_14[0][0]                  __________________________________________________________________________________________________conv3d_15 (Conv3D)              (None, 3, 160, 160,  195         activation_14[0][0]              __________________________________________________________________________________________________activation_15 (Activation)      (None, 3, 160, 160,  0           conv3d_15[0][0]                  ==================================================================================================Total params: 16,318,307Trainable params: 16,318,307Non-trainable params: 0__________________________________________________________________________________________________</code></pre><p><a name="5"></a></p><h1 id="5-Evaluation"><a href="#5-Evaluation" class="headerlink" title="5 Evaluation"></a>5 Evaluation</h1><p>Now that we have a trained model, we’ll learn to extract its predictions and evaluate its performance on scans from our validation set.</p><p><a name="5-1"></a></p><h2 id="5-1-Overall-Performance"><a href="#5-1-Overall-Performance" class="headerlink" title="5.1 Overall Performance"></a>5.1 Overall Performance</h2><p>First let’s measure the overall performance on the validation set. </p><ul><li>We can do this by calling the keras <a href="https://keras.io/models/model/#evaluate_generator" target="_blank" rel="noopener">evaluate_generator</a> function and passing in the validation generator, created in section 4.1. </li></ul><h4 id="Using-the-validation-set-for-testing"><a href="#Using-the-validation-set-for-testing" class="headerlink" title="Using the validation set for testing"></a>Using the validation set for testing</h4><ul><li>Note: since we didn’t do cross validation tuning on the final model, it’s okay to use the validation set.</li><li>For real life implementations, however, you would want to do cross validation as usual to choose hyperparamters and then use a hold out test set to assess performance</li></ul><p>Python Code for measuring the overall performance on the validation set:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val_loss, val_dice = model.evaluate_generator(valid_generator)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"validation soft dice loss: <span class="subst">&#123;val_loss:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"validation dice coefficient: <span class="subst">&#123;val_dice:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><h4 id="Expected-output-9"><a href="#Expected-output-9" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">validation soft dice loss: <span class="number">0.4742</span></span><br><span class="line">validation dice coefficient: <span class="number">0.5152</span></span><br></pre></td></tr></table></figure><p><strong>NOTE:</strong> Do not run the code shown above on the Coursera platform as it will exceed the platform’s memory limitations. However, you can run the code shown above locally on your machine or in Colab to practice measuring the overall performance on the validation set.</p><p>Like we mentioned above, due to memory limitiations on the Coursera platform we won’t be runing the above code, however, you should take note of the <strong>expected output</strong> below it. We should note that due to the randomness in choosing sub-volumes, the values for soft dice loss and dice coefficient will be different each time that you run it.</p><p><a name="5-2"></a></p><h2 id="5-2-Patch-level-predictions"><a href="#5-2-Patch-level-predictions" class="headerlink" title="5.2 Patch-level predictions"></a>5.2 Patch-level predictions</h2><p>When applying the model, we’ll want to look at segmentations for individual scans (entire scans, not just the sub-volumes)</p><ul><li>This will be a bit complicated because of our sub-volume approach. </li><li>First let’s keep things simple and extract model predictions for sub-volumes.</li><li>We can use the sub-volume which we extracted at the beginning of the assignment.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.visualize_patch(X_norm[<span class="number">0</span>, :, :, :], y[<span class="number">2</span>])</span><br></pre></td></tr></table></figure><p><img src="output_76_0.png" alt="png"></p><h4 id="Add-a-‘batch’-dimension"><a href="#Add-a-‘batch’-dimension" class="headerlink" title="Add a ‘batch’ dimension"></a>Add a ‘batch’ dimension</h4><p>We can extract predictions by calling <code>model.predict</code> on the patch. </p><ul><li>We’ll add an <code>images_per_batch</code> dimension, since the <code>predict</code> method is written to take in batches. </li><li>The dimensions of the input should be <code>(images_per_batch, num_channels, x_dim, y_dim, z_dim)</code>.</li><li>Use <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.expand_dims.html" target="_blank" rel="noopener">numpy.expand_dims</a> to add a new dimension as the zero-th dimension by setting axis=0</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_norm_with_batch_dimension = np.expand_dims(X_norm, axis=<span class="number">0</span>)</span><br><span class="line">patch_pred = model.predict(X_norm_with_batch_dimension)</span><br></pre></td></tr></table></figure><h4 id="Convert-prediction-from-probability-into-a-category"><a href="#Convert-prediction-from-probability-into-a-category" class="headerlink" title="Convert prediction from probability into a category"></a>Convert prediction from probability into a category</h4><p>Currently, each element of <code>patch_pred</code> is a number between 0.0 and 1.0.</p><ul><li>Each number is the model’s confidence that a voxel is part of a given class. </li><li>You will convert these to discrete 0 and 1 integers by using a threshold. </li><li>We’ll use a threshold of 0.5. </li><li>In real applications, you would tune this to achieve your required level  of sensitivity or specificity.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set threshold.</span></span><br><span class="line">threshold = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># use threshold to get hard predictions</span></span><br><span class="line">patch_pred[patch_pred &gt; threshold] = <span class="number">1.0</span></span><br><span class="line">patch_pred[patch_pred &lt;= threshold] = <span class="number">0.0</span></span><br></pre></td></tr></table></figure><p>Now let’s visualize the original patch and ground truth alongside our thresholded predictions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Patch and ground truth"</span>)</span><br><span class="line">util.visualize_patch(X_norm[<span class="number">0</span>, :, :, :], y[<span class="number">2</span>])</span><br><span class="line">plt.show()</span><br><span class="line">print(<span class="string">"Patch and prediction"</span>)</span><br><span class="line">util.visualize_patch(X_norm[<span class="number">0</span>, :, :, :], patch_pred[<span class="number">0</span>, <span class="number">2</span>, :, :, :])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>Patch and ground truth</code></pre><p><img src="output_82_1.png" alt="png"></p><pre><code>Patch and prediction</code></pre><p><img src="output_82_3.png" alt="png"></p><h4 id="Sensitivity-and-Specificity"><a href="#Sensitivity-and-Specificity" class="headerlink" title="Sensitivity and Specificity"></a>Sensitivity and Specificity</h4><p>The model is covering some of the relevant areas, but it’s definitely not perfect. </p><ul><li>To quantify its performance, we can use per-pixel sensitivity and specificity. </li></ul><p>Recall that in terms of the true positives, true negatives, false positives, and false negatives, </p><script type="math/tex; mode=display">\text{sensitivity} = \frac{\text{true positives}}{\text{true positives} + \text{false negatives}}</script><script type="math/tex; mode=display">\text{specificity} = \frac{\text{true negatives}}{\text{true negatives} + \text{false positives}}</script><p>Below let’s write a function to compute the sensitivity and specificity per output class.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Recall that a true positive occurs when the class prediction is equal to 1, and the class label is also equal to 1</li>    <li>Use <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html" target="_blank" rel="noopener"> numpy.sum() </a> </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_class_sens_spec</span><span class="params">(pred, label, class_num)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute sensitivity and specificity for a particular example</span></span><br><span class="line"><span class="string">    for a given class.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pred (np.array): binary arrary of predictions, shape is</span></span><br><span class="line"><span class="string">                         (num classes, height, width, depth).</span></span><br><span class="line"><span class="string">        label (np.array): binary array of labels, shape is</span></span><br><span class="line"><span class="string">                          (num classes, height, width, depth).</span></span><br><span class="line"><span class="string">        class_num (int): number between 0 - (num_classes -1) which says</span></span><br><span class="line"><span class="string">                         which prediction class to compute statistics</span></span><br><span class="line"><span class="string">                         for.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        sensitivity (float): precision for given class_num.</span></span><br><span class="line"><span class="string">        specificity (float): recall for given class_num</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># extract sub-array for specified class</span></span><br><span class="line">    class_pred = pred[class_num]</span><br><span class="line">    class_label = label[class_num]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># true positives</span></span><br><span class="line">    tp = np.sum((class_label == <span class="number">1</span>) &amp; (class_pred == <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># true negatives</span></span><br><span class="line">    tn = np.sum((class_label == <span class="number">0</span>) &amp; (class_pred == <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#false positives</span></span><br><span class="line">    fp = np.sum((class_label == <span class="number">0</span>) &amp; (class_pred == <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># false negatives</span></span><br><span class="line">    fn = np.sum((class_label == <span class="number">1</span>) &amp; (class_pred == <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute sensitivity and specificity</span></span><br><span class="line">    sensitivity = tp / (tp + fn)</span><br><span class="line">    specificity = tn / (tn + fp)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sensitivity, specificity</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEST CASES</span></span><br><span class="line">pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test Case #1"</span>)</span><br><span class="line">print(<span class="string">"pred:"</span>)</span><br><span class="line">print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">print(<span class="string">"label:"</span>)</span><br><span class="line">print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">sensitivity, specificity = compute_class_sens_spec(pred, label, <span class="number">0</span>)</span><br><span class="line">print(<span class="string">f"sensitivity: <span class="subst">&#123;sensitivity:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"specificity: <span class="subst">&#123;specificity:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #1pred:[[1. 0.] [0. 1.]]label:[[1. 1.] [0. 0.]]sensitivity: 0.5000specificity: 0.5000</code></pre><h4 id="Expected-output-10"><a href="#Expected-output-10" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#1</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">sensitivity: <span class="number">0.5000</span></span><br><span class="line">specificity: <span class="number">0.5000</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Test Case #2"</span>)</span><br><span class="line"></span><br><span class="line">pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"pred:"</span>)</span><br><span class="line">print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">print(<span class="string">"label:"</span>)</span><br><span class="line">print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">sensitivity, specificity = compute_class_sens_spec(pred, label, <span class="number">0</span>)</span><br><span class="line">print(<span class="string">f"sensitivity: <span class="subst">&#123;sensitivity:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"specificity: <span class="subst">&#123;specificity:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #2pred:[[1. 0.] [0. 1.]]label:[[1. 1.] [0. 1.]]sensitivity: 0.6667specificity: 1.0000</code></pre><h4 id="Expected-output-11"><a href="#Expected-output-11" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#2</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">sensitivity: <span class="number">0.6667</span></span><br><span class="line">specificity: <span class="number">1.0000</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Note: we must explicity import 'display' in order for the autograder to compile the submitted code</span></span><br><span class="line"><span class="comment"># Even though we could use this function without importing it, keep this import in order to allow the grader to work</span></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display</span><br><span class="line">print(<span class="string">"Test Case #3"</span>)</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">'y_test'</span>: [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                   <span class="string">'preds_test'</span>: [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                   <span class="string">'category'</span>: [<span class="string">'TP'</span>,<span class="string">'TP'</span>,<span class="string">'TN'</span>,<span class="string">'TN'</span>,<span class="string">'TN'</span>,<span class="string">'FP'</span>,<span class="string">'FP'</span>,<span class="string">'FP'</span>,<span class="string">'FP'</span>,<span class="string">'FN'</span>,<span class="string">'FN'</span>,<span class="string">'FN'</span>,<span class="string">'FN'</span>,<span class="string">'FN'</span>]</span><br><span class="line">                  &#125;)</span><br><span class="line"></span><br><span class="line">display(df)</span><br><span class="line">pred = np.array( [df[<span class="string">'preds_test'</span>]])</span><br><span class="line">label = np.array( [df[<span class="string">'y_test'</span>]])</span><br><span class="line"></span><br><span class="line">sensitivity, specificity = compute_class_sens_spec(pred, label, <span class="number">0</span>)</span><br><span class="line">print(<span class="string">f"sensitivity: <span class="subst">&#123;sensitivity:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"specificity: <span class="subst">&#123;specificity:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #3</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>y_test</th>      <th>preds_test</th>      <th>category</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>1</td>      <td>TP</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>1</td>      <td>TP</td>    </tr>    <tr>      <th>2</th>      <td>0</td>      <td>0</td>      <td>TN</td>    </tr>    <tr>      <th>3</th>      <td>0</td>      <td>0</td>      <td>TN</td>    </tr>    <tr>      <th>4</th>      <td>0</td>      <td>0</td>      <td>TN</td>    </tr>    <tr>      <th>5</th>      <td>0</td>      <td>1</td>      <td>FP</td>    </tr>    <tr>      <th>6</th>      <td>0</td>      <td>1</td>      <td>FP</td>    </tr>    <tr>      <th>7</th>      <td>0</td>      <td>1</td>      <td>FP</td>    </tr>    <tr>      <th>8</th>      <td>0</td>      <td>1</td>      <td>FP</td>    </tr>    <tr>      <th>9</th>      <td>1</td>      <td>0</td>      <td>FN</td>    </tr>    <tr>      <th>10</th>      <td>1</td>      <td>0</td>      <td>FN</td>    </tr>    <tr>      <th>11</th>      <td>1</td>      <td>0</td>      <td>FN</td>    </tr>    <tr>      <th>12</th>      <td>1</td>      <td>0</td>      <td>FN</td>    </tr>    <tr>      <th>13</th>      <td>1</td>      <td>0</td>      <td>FN</td>    </tr>  </tbody></table></div><pre><code>sensitivity: 0.2857specificity: 0.4286</code></pre><h4 id="Expected-Output-1"><a href="#Expected-Output-1" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Test case <span class="comment">#3</span></span><br><span class="line">...</span><br><span class="line">sensitivity: <span class="number">0.2857</span></span><br><span class="line">specificity: <span class="number">0.4286</span></span><br></pre></td></tr></table></figure><h4 id="Sensitivity-and-Specificity-for-the-patch-prediction"><a href="#Sensitivity-and-Specificity-for-the-patch-prediction" class="headerlink" title="Sensitivity and Specificity for the patch prediction"></a>Sensitivity and Specificity for the patch prediction</h4><p>Next let’s compute the sensitivity and specificity on that patch for expanding tumors. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sensitivity, specificity = compute_class_sens_spec(patch_pred[<span class="number">0</span>], y, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"Sensitivity: <span class="subst">&#123;sensitivity:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Specificity: <span class="subst">&#123;specificity:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Sensitivity: 0.8049Specificity: 0.9924</code></pre><h4 id="Expected-output-12"><a href="#Expected-output-12" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Sensitivity: <span class="number">0.7891</span></span><br><span class="line">Specificity: <span class="number">0.9960</span></span><br></pre></td></tr></table></figure><p>We can also display the sensitivity and specificity for each class.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sens_spec_df</span><span class="params">(pred, label)</span>:</span></span><br><span class="line">    patch_metrics = pd.DataFrame(</span><br><span class="line">        columns = [<span class="string">'Edema'</span>, </span><br><span class="line">                   <span class="string">'Non-Enhancing Tumor'</span>, </span><br><span class="line">                   <span class="string">'Enhancing Tumor'</span>], </span><br><span class="line">        index = [<span class="string">'Sensitivity'</span>,</span><br><span class="line">                 <span class="string">'Specificity'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i, class_name <span class="keyword">in</span> enumerate(patch_metrics.columns):</span><br><span class="line">        sens, spec = compute_class_sens_spec(pred, label, i)</span><br><span class="line">        patch_metrics.loc[<span class="string">'Sensitivity'</span>, class_name] = round(sens,<span class="number">4</span>)</span><br><span class="line">        patch_metrics.loc[<span class="string">'Specificity'</span>, class_name] = round(spec,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> patch_metrics</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = get_sens_spec_df(patch_pred[<span class="number">0</span>], y)</span><br><span class="line"></span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure><pre><code>              Edema Non-Enhancing Tumor Enhancing TumorSensitivity  0.8746              0.9419          0.8049Specificity    0.97              0.9957          0.9924</code></pre><h4 id="Expected-output-13"><a href="#Expected-output-13" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">              Edema Non-Enhancing Tumor Enhancing Tumor</span><br><span class="line">Sensitivity  <span class="number">0.9085</span>              <span class="number">0.9505</span>          <span class="number">0.7891</span></span><br><span class="line">Specificity  <span class="number">0.9848</span>              <span class="number">0.9961</span>           <span class="number">0.996</span></span><br></pre></td></tr></table></figure><p><a name="5-3"></a></p><h2 id="5-3-Running-on-entire-scans"><a href="#5-3-Running-on-entire-scans" class="headerlink" title="5.3 Running on entire scans"></a>5.3 Running on entire scans</h2><p>As of now, our model just runs on patches, but what we really want to see is our model’s result on a whole MRI scan. </p><ul><li>To do this, generate patches for the scan.</li><li>Then we run the model on the patches. </li><li>Then combine the results together to get a fully labeled MR image.</li></ul><p>The output of our model will be a 4D array with 3 probability values for each voxel in our data. </p><ul><li>We then can use a threshold (which you can find by a calibration process) to decide whether or not to report a label for each voxel. </li></ul><p>We have written a function that stitches the patches together:  <code>predict_and_viz(image, label, model, threshold)</code> </p><ul><li>Inputs: an image, label and model.</li><li>Ouputs: the model prediction over the whole image, and a visual of the ground truth and prediction. </li></ul><p>Run the following cell to see this function in action!</p><h4 id="Note-the-prediction-takes-some-time"><a href="#Note-the-prediction-takes-some-time" class="headerlink" title="Note: the prediction takes some time!"></a>Note: the prediction takes some time!</h4><ul><li>The first prediction will take about 7 to 8 minutes to run.</li><li>You can skip running this first prediction to save time.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># uncomment this code to run it</span></span><br><span class="line"><span class="comment"># image, label = load_case(DATA_DIR + "imagesTr/BRATS_001.nii.gz", DATA_DIR + "labelsTr/BRATS_001.nii.gz")</span></span><br><span class="line"><span class="comment"># pred = util.predict_and_viz(image, label, model, .5, loc=(130, 130, 77))</span></span><br></pre></td></tr></table></figure><p>Here’s a second prediction.</p><ul><li>Takes about 7 to 8 minutes to run</li></ul><p>Please run this second prediction so that we can check the predictions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.getsizeof(image) / <span class="number">1000</span>  / <span class="number">1000</span></span><br></pre></td></tr></table></figure><pre><code>285696144</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">image, label = load_case(DATA_DIR + <span class="string">"imagesTr/BRATS_003.nii.gz"</span>, DATA_DIR + <span class="string">"labelsTr/BRATS_003.nii.gz"</span>)</span><br><span class="line">pred = util.predict_and_viz(image, label, model, <span class="number">.5</span>, loc=(<span class="number">130</span>, <span class="number">130</span>, <span class="number">77</span>))</span><br></pre></td></tr></table></figure><p><img src="output_103_0.png" alt="png"></p><h4 id="Check-how-well-the-predictions-do"><a href="#Check-how-well-the-predictions-do" class="headerlink" title="Check how well the predictions do"></a>Check how well the predictions do</h4><p>We can see some of the discrepancies between the model and the ground truth visually. </p><ul><li>We can also use the functions we wrote previously to compute sensitivity and specificity for each class over the whole scan.</li><li>First we need to format the label and prediction to match our functions expect.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">whole_scan_label = keras.utils.to_categorical(label, num_classes = <span class="number">4</span>)</span><br><span class="line">whole_scan_pred = pred</span><br><span class="line"></span><br><span class="line"><span class="comment"># move axis to match shape expected in functions</span></span><br><span class="line">whole_scan_label = np.moveaxis(whole_scan_label, <span class="number">3</span> ,<span class="number">0</span>)[<span class="number">1</span>:<span class="number">4</span>]</span><br><span class="line">whole_scan_pred = np.moveaxis(whole_scan_pred, <span class="number">3</span>, <span class="number">0</span>)[<span class="number">1</span>:<span class="number">4</span>]</span><br></pre></td></tr></table></figure><p>Now we can compute sensitivity and specificity for each class just like before.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">whole_scan_df = get_sens_spec_df(whole_scan_pred, whole_scan_label)</span><br><span class="line"></span><br><span class="line">print(whole_scan_df)</span><br></pre></td></tr></table></figure><pre><code>              Edema Non-Enhancing Tumor Enhancing TumorSensitivity   0.902              0.2617          0.8496Specificity  0.9894              0.9998          0.9982</code></pre><h1 id="That’s-all-for-now"><a href="#That’s-all-for-now" class="headerlink" title="That’s all for now!"></a>That’s all for now!</h1><p>Congratulations on finishing this challenging assignment! You now know all the basics for building a neural auto-segmentation model for MRI images. We hope that you end up using these skills on interesting and challenging problems that you face in the real world.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/2652/1*eTkBMyqdg9JodNcG_O4-Kw.jpeg&quot; width=&quot;100%&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://medium.com/stanford-ai-fo
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Medicine" scheme="https://zhangruochi.com/tags/Medicine/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow.js Converter</title>
    <link href="https://zhangruochi.com/TensorFlow-js-Converter/2020/04/17/"/>
    <id>https://zhangruochi.com/TensorFlow-js-Converter/2020/04/17/</id>
    <published>2020-04-18T03:05:16.000Z</published>
    <updated>2020-04-18T03:07:31.945Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Converting-a-Keras-Model-to-JSON-Format"><a href="#Converting-a-Keras-Model-to-JSON-Format" class="headerlink" title="Converting a Keras Model to JSON Format"></a>Converting a Keras Model to JSON Format</h1><p>In the previous lesson you saw how to use a CNN to make your recognition of the handwriting digits more efficient. In this lesson you’ll take that to the next level, recognizing real images of Cats and Dogs in order to classify an incoming image as one or the other. In particular the handwriting recognition made your life a little easier by having all the images be the same size and shape, and they were all monochrome color. Real-world images aren’t like that — they’re in different shapes, aspect ratios etc, and they’re usually in color!</p><p>So, as part of the task you need to process your data — not least resizing it to be uniform in shape. </p><p>You’ll follow these steps:</p><ol><li>Explore the Example Data of Cats and Dogs.</li><li>Build and Train a Neural Network to recognize the difference between the two.</li><li>Evaluate the Training and Validation accuracy.</li><li>Save the trained model as a Keras HDF5 file.</li><li>Use the tensorflow.js converter to convert the saved Keras model into JSON format.</li></ol><h1 id="Import-Resources"><a href="#Import-Resources" class="headerlink" title="Import Resources"></a>Import Resources</h1><p>In order to use the tensorflow.js converter we need to install <code>tensorflowjs</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\u2022 Using TensorFlow Version:'</span>, tf.__version__)</span><br></pre></td></tr></table></figure><pre><code>• Using TensorFlow Version: 2.1.0</code></pre><h2 id="Explore-the-Example-Data"><a href="#Explore-the-Example-Data" class="headerlink" title="Explore the Example Data"></a>Explore the Example Data</h2><p>Let’s start by downloading our example data, a .zip of 2,000 JPG pictures of cats and dogs, and extracting it locally in <code>/tmp</code>.</p><p><strong>NOTE:</strong> The 2,000 images used in this exercise are excerpted from the <a href="https://www.kaggle.com/c/dogs-vs-cats/data" target="_blank" rel="noopener">“Dogs vs. Cats” dataset</a> available on Kaggle, which contains 25,000 images. Here, we use a subset of the full dataset to decrease training time for educational purposes.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!wget --no-check-certificate \</span><br><span class="line">  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \</span><br><span class="line">  -O /tmp/cats_and_dogs_filtered.zip</span><br></pre></td></tr></table></figure><pre><code>Warning: Failed to set locale category LC_NUMERIC to en_CN.Warning: Failed to set locale category LC_TIME to en_CN.Warning: Failed to set locale category LC_COLLATE to en_CN.Warning: Failed to set locale category LC_MONETARY to en_CN.Warning: Failed to set locale category LC_MESSAGES to en_CN.--2020-02-14 21:04:38--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zipResolving storage.googleapis.com (storage.googleapis.com)... 172.217.12.240Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.12.240|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 68606236 (65M) [application/zip]Saving to: ‘/tmp/cats_and_dogs_filtered.zip’/tmp/cats_and_dogs_ 100%[===================&gt;]  65.43M  26.3MB/s    in 2.5s    2020-02-14 21:04:40 (26.3 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]</code></pre><p>The following python code will use the OS library to use Operating System libraries, giving you access to the file system, and the zipfile library allowing you to unzip the data. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line">local_zip = <span class="string">'/tmp/cats_and_dogs_filtered.zip'</span></span><br><span class="line"></span><br><span class="line">zip_ref = zipfile.ZipFile(local_zip, <span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">zip_ref.extractall(<span class="string">'/tmp'</span>)</span><br><span class="line">zip_ref.close()</span><br></pre></td></tr></table></figure><p>The contents of the .zip are extracted to the base directory <code>/tmp/cats_and_dogs_filtered</code>, which contains <code>train</code> and <code>validation</code> subdirectories for the training and validation datasets (see the <a href="https://developers.google.com/machine-learning/crash-course/validation/check-your-intuition" target="_blank" rel="noopener">Machine Learning Crash Course</a> for a refresher on training, validation, and test sets), which in turn each contain <code>cats</code> and <code>dogs</code> subdirectories.</p><p>In short: The training set is the data that is used to tell the neural network model that ‘this is what a cat looks like’, ‘this is what a dog looks like’ etc. The validation data set is images of cats and dogs that the neural network will not see as part of the training, so you can test how well or how badly it does in evaluating if an image contains a cat or a dog.</p><p>One thing to pay attention to in this sample: We do not explicitly label the images as cats or dogs. If you remember with the handwriting example earlier, we had labelled ‘this is a 1’, ‘this is a 7’ etc.  Later you’ll see something called an ImageGenerator being used — and this is coded to read images from subdirectories, and automatically label them from the name of that subdirectory. So, for example, you will have a ‘training’ directory containing a ‘cats’ directory and a ‘dogs’ one. ImageGenerator will label the images appropriately for you, reducing a coding step. </p><p>Let’s define each of these directories:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">base_dir = <span class="string">'/tmp/cats_and_dogs_filtered'</span></span><br><span class="line"></span><br><span class="line">train_dir = os.path.join(base_dir, <span class="string">'train'</span>)</span><br><span class="line">validation_dir = os.path.join(base_dir, <span class="string">'validation'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Directory with our training cat/dog pictures</span></span><br><span class="line">train_cats_dir = os.path.join(train_dir, <span class="string">'cats'</span>)</span><br><span class="line">train_dogs_dir = os.path.join(train_dir, <span class="string">'dogs'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Directory with our validation cat/dog pictures</span></span><br><span class="line">validation_cats_dir = os.path.join(validation_dir, <span class="string">'cats'</span>)</span><br><span class="line">validation_dogs_dir = os.path.join(validation_dir, <span class="string">'dogs'</span>)</span><br></pre></td></tr></table></figure><p>Now, let’s see what the filenames look like in the <code>cats</code> and <code>dogs</code> <code>train</code> directories (file naming conventions are the same in the <code>validation</code> directory):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_cat_fnames = os.listdir( train_cats_dir )</span><br><span class="line">train_dog_fnames = os.listdir( train_dogs_dir )</span><br><span class="line"></span><br><span class="line">print(train_cat_fnames[:<span class="number">10</span>])</span><br><span class="line">print(train_dog_fnames[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>[&#39;cat.952.jpg&#39;, &#39;cat.946.jpg&#39;, &#39;cat.6.jpg&#39;, &#39;cat.749.jpg&#39;, &#39;cat.991.jpg&#39;, &#39;cat.985.jpg&#39;, &#39;cat.775.jpg&#39;, &#39;cat.761.jpg&#39;, &#39;cat.588.jpg&#39;, &#39;cat.239.jpg&#39;][&#39;dog.775.jpg&#39;, &#39;dog.761.jpg&#39;, &#39;dog.991.jpg&#39;, &#39;dog.749.jpg&#39;, &#39;dog.985.jpg&#39;, &#39;dog.952.jpg&#39;, &#39;dog.946.jpg&#39;, &#39;dog.211.jpg&#39;, &#39;dog.577.jpg&#39;, &#39;dog.563.jpg&#39;]</code></pre><p>Let’s find out the total number of cat and dog images in the <code>train</code> and <code>validation</code> directories:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'total training cat images :'</span>, len(os.listdir(      train_cats_dir ) ))</span><br><span class="line">print(<span class="string">'total training dog images :'</span>, len(os.listdir(      train_dogs_dir ) ))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'total validation cat images :'</span>, len(os.listdir( validation_cats_dir ) ))</span><br><span class="line">print(<span class="string">'total validation dog images :'</span>, len(os.listdir( validation_dogs_dir ) ))</span><br></pre></td></tr></table></figure><pre><code>total training cat images : 1000total training dog images : 1000total validation cat images : 500total validation dog images : 500</code></pre><p>For both cats and dogs, we have 1,000 training images and 500 validation images.</p><p>Now let’s take a look at a few pictures to get a better sense of what the cat and dog datasets look like. First, we configure the <code>matplotlib</code> parameters:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters for our graph; we'll output images in a 4x4 configuration</span></span><br><span class="line">nrows = <span class="number">4</span></span><br><span class="line">ncols = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">pic_index = <span class="number">0</span> <span class="comment"># Index for iterating over images</span></span><br></pre></td></tr></table></figure><p>Now, we display a batch of 8 cat and 8 dog pictures. You can re-run the cell to see a fresh batch each time:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set up matplotlib fig, and size it to fit 4x4 pics</span></span><br><span class="line">fig = plt.gcf()</span><br><span class="line">fig.set_size_inches(ncols*<span class="number">4</span>, nrows*<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">pic_index+=<span class="number">8</span></span><br><span class="line"></span><br><span class="line">next_cat_pix = [os.path.join(train_cats_dir, fname) </span><br><span class="line">                <span class="keyword">for</span> fname <span class="keyword">in</span> train_cat_fnames[ pic_index<span class="number">-8</span>:pic_index] </span><br><span class="line">               ]</span><br><span class="line"></span><br><span class="line">next_dog_pix = [os.path.join(train_dogs_dir, fname) </span><br><span class="line">                <span class="keyword">for</span> fname <span class="keyword">in</span> train_dog_fnames[ pic_index<span class="number">-8</span>:pic_index]</span><br><span class="line">               ]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, img_path <span class="keyword">in</span> enumerate(next_cat_pix+next_dog_pix):</span><br><span class="line">    <span class="comment"># Set up subplot; subplot indices start at 1</span></span><br><span class="line">    sp = plt.subplot(nrows, ncols, i + <span class="number">1</span>)</span><br><span class="line">    sp.axis(<span class="string">'Off'</span>) <span class="comment"># Don't show axes (or gridlines)</span></span><br><span class="line">    </span><br><span class="line">    img = mpimg.imread(img_path)</span><br><span class="line">    plt.imshow(img)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_17_0.png" alt="png"></p><p>It may not be obvious from looking at the images in this grid, but an important note here, and a significant difference from the previous lesson is that these images come in all shapes and sizes. When you did the handwriting recognition example, you had 28x28 greyscale images to work with. These are color and in a variety of shapes. Before training a Neural network with them you’ll need to tweak the images. You’ll see that in the next section.</p><p>Ok, now that you have an idea for what your data looks like, the next step is to define the model that will be trained to recognize cats or dogs from these images </p><h2 id="Building-a-Small-Model-from-Scratch-to-Get-to-72-Accuracy"><a href="#Building-a-Small-Model-from-Scratch-to-Get-to-72-Accuracy" class="headerlink" title="Building a Small Model from Scratch to Get to ~72% Accuracy"></a>Building a Small Model from Scratch to Get to ~72% Accuracy</h2><p>In the previous section you saw that the images were in a variety of shapes and sizes. In order to train a neural network to handle them you’ll need them to be in a uniform size. We’ve chosen 150x150 for this, and you’ll see the code that preprocesses the images to that shape shortly. </p><p>But before we continue, let’s start defining the model. We will define a Sequential layer as before, adding some convolutional layers first. Note the input shape parameter this time. In the earlier example it was 28x28x1, because the image was 28x28 in greyscale (8 bits, 1 byte for color depth). This time it is 150x150 for the size and 3 (24 bits, 3 bytes) for the color depth. </p><p>We then add a couple of convolutional layers as in the previous example, and flatten the final result to feed into the densely connected layers.</p><p>Finally we add the densely connected layers. </p><p>Note that because we are facing a two-class classification problem, i.e. a <em>binary classification problem</em>, we will end our network with a <a href="https://wikipedia.org/wiki/Sigmoid_function" target="_blank" rel="noopener"><em>sigmoid</em> activation</a>, so that the output of our network will be a single scalar between 0 and 1, encoding the probability that the current image is class 1 (as opposed to class 0).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    <span class="comment"># Note the input shape is the desired size of the image 150x150 with 3 bytes color</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">16</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">'relu'</span>, input_shape=(<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>)),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>), </span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">'relu'</span>), </span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># Flatten the results to feed into a DNN</span></span><br><span class="line">    tf.keras.layers.Flatten(), </span><br><span class="line">    <span class="comment"># 512 neuron hidden layer</span></span><br><span class="line">    tf.keras.layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>), </span><br><span class="line">    <span class="comment"># Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')</span></span><br><span class="line">    tf.keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)  </span><br><span class="line">])</span><br></pre></td></tr></table></figure><p>The <code>model.summary()</code> method call prints a summary of the NN </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d (Conv2D)              (None, 148, 148, 16)      448       _________________________________________________________________max_pooling2d (MaxPooling2D) (None, 74, 74, 16)        0         _________________________________________________________________conv2d_1 (Conv2D)            (None, 72, 72, 32)        4640      _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 36, 36, 32)        0         _________________________________________________________________conv2d_2 (Conv2D)            (None, 34, 34, 64)        18496     _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 17, 17, 64)        0         _________________________________________________________________flatten (Flatten)            (None, 18496)             0         _________________________________________________________________dense (Dense)                (None, 512)               9470464   _________________________________________________________________dense_1 (Dense)              (None, 1)                 513       =================================================================Total params: 9,494,561Trainable params: 9,494,561Non-trainable params: 0_________________________________________________________________</code></pre><p>The “output shape” column shows how the size of your feature map evolves in each successive layer. The convolution layers reduce the size of the feature maps by a bit due to padding, and each pooling layer halves the dimensions.</p><p>Next, we’ll configure the specifications for model training. We will train our model with the <code>binary_crossentropy</code> loss, because it’s a binary classification problem and our final activation is a sigmoid. (For a refresher on loss metrics, see the <a href="https://developers.google.com/machine-learning/crash-course/descending-into-ml/video-lecture" target="_blank" rel="noopener">Machine Learning Crash Course</a>.) We will use the <code>rmsprop</code> optimizer with a learning rate of <code>0.001</code>. During training, we will want to monitor classification accuracy.</p><p><strong>NOTE</strong>: In this case, using the <a href="https://wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp" target="_blank" rel="noopener">RMSprop optimization algorithm</a> is preferable to <a href="https://developers.google.com/machine-learning/glossary/#SGD" target="_blank" rel="noopener">stochastic gradient descent</a> (SGD), because RMSprop automates learning-rate tuning for us. (Other optimizers, such as <a href="https://wikipedia.org/wiki/Stochastic_gradient_descent#Adam" target="_blank" rel="noopener">Adam</a> and <a href="https://developers.google.com/machine-learning/glossary/#AdaGrad" target="_blank" rel="noopener">Adagrad</a>, also automatically adapt the learning rate during training, and would work equally well here.)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics = [<span class="string">'acc'</span>])</span><br></pre></td></tr></table></figure><h3 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h3><p>Let’s set up data generators that will read pictures in our source folders, convert them to <code>float32</code> tensors, and feed them (with their labels) to our network. We’ll have one generator for the training images and one for the validation images. Our generators will yield batches of 20 images of size 150x150 and their labels (binary).</p><p>As you may already know, data that goes into neural networks should usually be normalized in some way to make it more amenable to processing by the network. (It is uncommon to feed raw pixels into a convnet.) In our case, we will preprocess our images by normalizing the pixel values to be in the <code>[0, 1]</code> range (originally all values are in the <code>[0, 255]</code> range).</p><p>In Keras this can be done via the <code>keras.preprocessing.image.ImageDataGenerator</code> class using the <code>rescale</code> parameter. This <code>ImageDataGenerator</code> class allows you to instantiate generators of augmented image batches (and their labels) via <code>.flow(data, labels)</code> or <code>.flow_from_directory(directory)</code>. These generators can then be used with the Keras model methods that accept data generators as inputs: <code>fit_generator</code>, <code>evaluate_generator</code>, and <code>predict_generator</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line"><span class="comment"># All images will be rescaled by 1./255.</span></span><br><span class="line">train_datagen = ImageDataGenerator( rescale = <span class="number">1.0</span>/<span class="number">255.</span> )</span><br><span class="line">test_datagen  = ImageDataGenerator( rescale = <span class="number">1.0</span>/<span class="number">255.</span> )</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------</span></span><br><span class="line"><span class="comment"># Flow training images in batches of 20 using train_datagen generator</span></span><br><span class="line"><span class="comment"># --------------------</span></span><br><span class="line">train_generator = train_datagen.flow_from_directory(train_dir,</span><br><span class="line">                                                    batch_size=<span class="number">20</span>,</span><br><span class="line">                                                    class_mode=<span class="string">'binary'</span>,</span><br><span class="line">                                                    target_size=(<span class="number">150</span>, <span class="number">150</span>))     </span><br><span class="line"><span class="comment"># --------------------</span></span><br><span class="line"><span class="comment"># Flow validation images in batches of 20 using test_datagen generator</span></span><br><span class="line"><span class="comment"># --------------------</span></span><br><span class="line">validation_generator =  test_datagen.flow_from_directory(validation_dir,</span><br><span class="line">                                                         batch_size=<span class="number">20</span>,</span><br><span class="line">                                                         class_mode  = <span class="string">'binary'</span>,</span><br><span class="line">                                                         target_size = (<span class="number">150</span>, <span class="number">150</span>))</span><br></pre></td></tr></table></figure><pre><code>Found 2000 images belonging to 2 classes.Found 1000 images belonging to 2 classes.</code></pre><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>Let’s train on all 2,000 images available, for 15 epochs, and validate on all 1,000 test images. (This may take a few minutes to run.)</p><p>Do note the values per epoch.</p><p>You’ll see 4 values per epoch — Loss, Accuracy, Validation Loss and Validation Accuracy. </p><p>The Loss and Accuracy are a great indication of progress of training. It’s making a guess as to the classification of the training data, and then measuring it against the known label, calculating the result. Accuracy is the portion of correct guesses. The Validation accuracy is the measurement with the data that has not been used in training. As expected this would be a bit lower. You’ll learn about why this occurs in the section on overfitting later in this course.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit_generator(train_generator,</span><br><span class="line">                              validation_data=validation_generator,</span><br><span class="line">                              steps_per_epoch=<span class="number">100</span>,</span><br><span class="line">                              epochs=<span class="number">15</span>,</span><br><span class="line">                              validation_steps=<span class="number">50</span>,</span><br><span class="line">                              verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><pre><code>WARNING:tensorflow:From &lt;ipython-input-17-c57227122236&gt;:6: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.Instructions for updating:Please use Model.fit, which supports generators.WARNING:tensorflow:sample_weight modes were coerced from  ...    to    [&#39;...&#39;]WARNING:tensorflow:sample_weight modes were coerced from  ...    to    [&#39;...&#39;]Train for 100 steps, validate for 50 stepsEpoch 1/15100/100 - 24s - loss: 0.9314 - acc: 0.5660 - val_loss: 0.6656 - val_acc: 0.6120Epoch 2/15100/100 - 25s - loss: 0.6632 - acc: 0.6310 - val_loss: 0.7892 - val_acc: 0.5310Epoch 3/15100/100 - 24s - loss: 0.5777 - acc: 0.7025 - val_loss: 0.5888 - val_acc: 0.6900Epoch 4/15100/100 - 23s - loss: 0.4736 - acc: 0.7785 - val_loss: 0.6290 - val_acc: 0.6970Epoch 5/15100/100 - 23s - loss: 0.3826 - acc: 0.8310 - val_loss: 0.6550 - val_acc: 0.6990Epoch 6/15100/100 - 23s - loss: 0.3170 - acc: 0.8695 - val_loss: 0.7551 - val_acc: 0.6940Epoch 7/15100/100 - 23s - loss: 0.2174 - acc: 0.9075 - val_loss: 1.0175 - val_acc: 0.6640Epoch 8/15100/100 - 23s - loss: 0.1598 - acc: 0.9420 - val_loss: 1.3129 - val_acc: 0.6650Epoch 9/15100/100 - 23s - loss: 0.1329 - acc: 0.9570 - val_loss: 1.1711 - val_acc: 0.6890Epoch 10/15100/100 - 23s - loss: 0.0960 - acc: 0.9735 - val_loss: 1.3053 - val_acc: 0.7130Epoch 11/15100/100 - 23s - loss: 0.0709 - acc: 0.9800 - val_loss: 1.5509 - val_acc: 0.6970Epoch 12/15100/100 - 23s - loss: 0.0496 - acc: 0.9865 - val_loss: 5.7583 - val_acc: 0.5370Epoch 13/15100/100 - 25s - loss: 0.0885 - acc: 0.9800 - val_loss: 3.9976 - val_acc: 0.5910Epoch 14/15100/100 - 23s - loss: 0.0423 - acc: 0.9830 - val_loss: 1.8770 - val_acc: 0.7040Epoch 15/15100/100 - 23s - loss: 0.0480 - acc: 0.9900 - val_loss: 2.3820 - val_acc: 0.6860</code></pre><h3 id="Evaluating-Accuracy-and-Loss-for-the-Model"><a href="#Evaluating-Accuracy-and-Loss-for-the-Model" class="headerlink" title="Evaluating Accuracy and Loss for the Model"></a>Evaluating Accuracy and Loss for the Model</h3><p>Let’s plot the training/validation accuracy and loss as collected during training:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-----------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Retrieve a list of list results on training and test data</span></span><br><span class="line"><span class="comment"># sets for each training epoch</span></span><br><span class="line"><span class="comment">#-----------------------------------------------------------</span></span><br><span class="line">acc      = history.history[     <span class="string">'acc'</span> ]</span><br><span class="line">val_acc  = history.history[ <span class="string">'val_acc'</span> ]</span><br><span class="line">loss     = history.history[    <span class="string">'loss'</span> ]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span> ]</span><br><span class="line"></span><br><span class="line">epochs   = range(len(acc)) <span class="comment"># Get number of epochs</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#------------------------------------------------</span></span><br><span class="line"><span class="comment"># Plot training and validation accuracy per epoch</span></span><br><span class="line"><span class="comment">#------------------------------------------------</span></span><br><span class="line">plt.plot  ( epochs,     acc, label=<span class="string">'Training'</span>)</span><br><span class="line">plt.plot  ( epochs, val_acc, label=<span class="string">'Validation'</span>)</span><br><span class="line">plt.title (<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="comment">#------------------------------------------------</span></span><br><span class="line"><span class="comment"># Plot training and validation loss per epoch</span></span><br><span class="line"><span class="comment">#------------------------------------------------</span></span><br><span class="line">plt.plot  ( epochs,     loss, label=<span class="string">'Training'</span>)</span><br><span class="line">plt.plot  ( epochs, val_loss, label=<span class="string">'Validation'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title (<span class="string">'Training and validation loss'</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5, 1.0, &#39;Training and validation loss&#39;)</code></pre><p><img src="output_31_1.png" alt="png"></p><p><img src="output_31_2.png" alt="png"></p><p>As you can see, we are <strong>overfitting</strong> like it’s getting out of fashion. Our training accuracy (in blue) gets close to 100% (!) while our validation accuracy (in orange) stalls as 70%. Our validation loss reaches its minimum after only five epochs.</p><p>Since we have a relatively small number of training examples (2000), overfitting should be our number one concern. Overfitting happens when a model exposed to too few examples learns patterns that do not generalize to new data, i.e. when the model starts using irrelevant features for making predictions. For instance, if you, as a human, only see three images of people who are lumberjacks, and three images of people who are sailors, and among them the only person wearing a cap is a lumberjack, you might start thinking that wearing a cap is a sign of being a lumberjack as opposed to a sailor. You would then make a pretty lousy lumberjack/sailor classifier.</p><p>Overfitting is the central problem in machine learning: given that we are fitting the parameters of our model to a given dataset, how can we make sure that the representations learned by the model will be applicable to data never seen before? How do we avoid learning things that are specific to the training data?</p><p>In the next exercise, we’ll look at ways to prevent overfitting in the cat vs. dog classification model.</p><h2 id="Save-the-Model"><a href="#Save-the-Model" class="headerlink" title="Save the Model"></a>Save the Model</h2><p>In the cell below, save the trained model as a Keras model (<code>.h5</code> file).</p><p><strong>HINT</strong>: Use <code>model.save()</code>. Feel free to take a look at the <code>Linear-to-JavaScript.ipynb</code> example.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Save the trained model as a Keras HDF5 file. </span></span><br><span class="line"></span><br><span class="line">saved_model_path = <span class="string">"./my_model.h5"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># YOUR CODE HERE</span></span><br><span class="line">model.save(saved_model_path)</span><br></pre></td></tr></table></figure><h2 id="Run-the-TensorFlow-js-Converter-on-The-Saved-Keras-Model"><a href="#Run-the-TensorFlow-js-Converter-on-The-Saved-Keras-Model" class="headerlink" title="Run the TensorFlow.js Converter on The Saved Keras Model"></a>Run the TensorFlow.js Converter on The Saved Keras Model</h2><p>In the cell below, use the <code>tensorflowjs_converter</code> to convert the saved Keras model into JSON format.</p><p><strong>HINT</strong>: Make sure you specify the format of the input model as Keras by using the <code>--input_format</code> option. Feel free to take a look at the <code>Linear-to-JavaScript.ipynb</code> example and the <a href="https://github.com/tensorflow/tfjs/tree/master/tfjs-converter#step-1-converting-a-tensorflow-savedmodel-tensorflow-hub-module-keras-hdf5-or-tfkeras-savedmodel-to-a-web-friendly-format" target="_blank" rel="noopener">TensorFlow.js converter documentation</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Use the tensorflow.js converter to convert the saved Keras model into JSON format.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># YOUR CODE HERE</span></span><br><span class="line">! tensorflowjs_converter \</span><br><span class="line">    --input_format=keras \</span><br><span class="line">    &#123;saved_model_path&#125; \</span><br><span class="line">    <span class="string">"./"</span></span><br></pre></td></tr></table></figure><p>If you did things correctly, you should now have a <strong>JSON</strong> file named <code>model.json</code> and various <code>.bin</code> files, such as <code>group1-shard1of10.bin</code>. The number of <code>.bin</code> files will depend on the size of your model: the larger your model, the greater the number of <code>.bin</code> files. The <code>model.json</code> file contains the architecture of your model and the <code>.bin</code> files will contain the weights of your model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Converting-a-Keras-Model-to-JSON-Format&quot;&gt;&lt;a href=&quot;#Converting-a-Keras-Model-to-JSON-Format&quot; class=&quot;headerlink&quot; title=&quot;Converting a K
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Deployment" scheme="https://zhangruochi.com/categories/AI-Workflow/Deployment/"/>
    
    
  </entry>
  
  <entry>
    <title>Evaluation of Diagnostic Models</title>
    <link href="https://zhangruochi.com/Evaluation-of-Diagnostic-Models/2020/04/17/"/>
    <id>https://zhangruochi.com/Evaluation-of-Diagnostic-Models/2020/04/17/</id>
    <published>2020-04-17T22:54:09.000Z</published>
    <updated>2020-04-17T22:55:06.578Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Evaluation-of-Diagnostic-Models"><a href="#Evaluation-of-Diagnostic-Models" class="headerlink" title="Evaluation of Diagnostic Models"></a>Evaluation of Diagnostic Models</h1><p>Welcome to the second assignment of course 1. In this assignment, we will be working with the results of the X-ray classification model we developed in the previous assignment. In order to make the data processing a bit more manageable, we will be working with a subset of our training, and validation datasets. We will also use our manually labeled test dataset of 420 X-rays.  </p><p>As a reminder, our dataset contains X-rays from 14 different conditions diagnosable from an X-ray. We’ll evaluate our performance on each of these classes using the classification metrics we learned in lecture.</p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><p>Click on these links to jump to a particular section of this assignment!</p><ul><li><a href="#1">1. Packages</a></li><li><a href="#2">2. Overview</a></li><li><a href="#3">3. Metrics</a><ul><li><a href="#3-1">3.1 True Positives, False Positives, True Negatives, and False Negatives</a></li><li><a href="#3-2">3.2 Accuracy</a></li><li><a href="#3-3">3.3 Prevalence</a></li><li><a href="#3-4">3.4 Sensitivity and Specificity</a></li><li><a href="#3-5">3.5 PPV and NPV</a></li><li><a href="#3-6">3.6 ROC Curve</a></li></ul></li><li><a href="#4">4. Confidence Intervals</a></li><li><a href="#5">5. Precision-Recall Curve</a></li><li><a href="#6">6. F1 Score</a></li><li><a href="#7">7. Calibration</a></li></ul><p><strong>By the end of this assignment you will learn about:</strong></p><ol><li>Accuracy</li><li>Prevalence</li><li>Specificity &amp; Sensitivity</li><li>PPV and NPV</li><li>ROC curve and AUCROC (c-statistic)</li><li>Confidence Intervals</li></ol><p><a name="1"></a></p><h2 id="1-Packages"><a href="#1-Packages" class="headerlink" title="1. Packages"></a>1. Packages</h2><p>In this assignment, we’ll make use of the following packages:</p><ul><li><a href="https://docs.scipy.org/doc/numpy/" target="_blank" rel="noopener">numpy</a> is a popular library for scientific computing</li><li><a href="https://matplotlib.org/3.1.1/contents.html" target="_blank" rel="noopener">matplotlib</a> is a plotting library compatible with numpy</li><li><a href="https://pandas.pydata.org/docs/" target="_blank" rel="noopener">pandas</a> is what we’ll use to manipulate our data</li><li><a href="https://scikit-learn.org/stable/index.html" target="_blank" rel="noopener">sklearn</a> will be used to measure the performance of our model</li></ul><p>Run the next cell to import all the necessary packages as well as custom util functions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd  </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> util</span><br></pre></td></tr></table></figure><p><a name="2"></a></p><h2 id="2-Overview"><a href="#2-Overview" class="headerlink" title="2. Overview"></a>2. Overview</h2><p>We’ll go through our evaluation metrics in the following order.</p><ul><li>Metrics<ul><li>TP, TN, FP, FN</li><li>Accuracy</li><li>Prevalence</li><li>Sensitivity and Specificity</li><li>PPV and NPV</li><li>AUC</li></ul></li><li>Confidence Intervals</li></ul><p>Let’s take a quick peek at our dataset. The data is stored in two CSV files called <code>train_preds.csv</code> and <code>valid_preds.csv</code>. We have precomputed the model outputs for our test cases. We’ll work with these predictions and the true class labels throughout the assignment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">train_results = pd.read_csv(<span class="string">"train_preds.csv"</span>)</span><br><span class="line">valid_results = pd.read_csv(<span class="string">"valid_preds.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># the labels in our dataset</span></span><br><span class="line">class_labels = [<span class="string">'Cardiomegaly'</span>,</span><br><span class="line"> <span class="string">'Emphysema'</span>,</span><br><span class="line"> <span class="string">'Effusion'</span>,</span><br><span class="line"> <span class="string">'Hernia'</span>,</span><br><span class="line"> <span class="string">'Infiltration'</span>,</span><br><span class="line"> <span class="string">'Mass'</span>,</span><br><span class="line"> <span class="string">'Nodule'</span>,</span><br><span class="line"> <span class="string">'Atelectasis'</span>,</span><br><span class="line"> <span class="string">'Pneumothorax'</span>,</span><br><span class="line"> <span class="string">'Pleural_Thickening'</span>,</span><br><span class="line"> <span class="string">'Pneumonia'</span>,</span><br><span class="line"> <span class="string">'Fibrosis'</span>,</span><br><span class="line"> <span class="string">'Edema'</span>,</span><br><span class="line"> <span class="string">'Consolidation'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># the labels for prediction values in our dataset</span></span><br><span class="line">pred_labels = [l + <span class="string">"_pred"</span> <span class="keyword">for</span> l <span class="keyword">in</span> class_labels]</span><br></pre></td></tr></table></figure><p>Extract the labels (y) and the predictions (pred).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = valid_results[class_labels].values</span><br><span class="line">pred = valid_results[pred_labels].values</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.shape, pred.shape</span><br></pre></td></tr></table></figure><pre><code>((1000, 14), (1000, 14))</code></pre><p>Run the next cell to view them side by side.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># let's take a peek at our dataset</span></span><br><span class="line">valid_results[np.concatenate([class_labels, pred_labels])].head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Cardiomegaly</th>      <th>Emphysema</th>      <th>Effusion</th>      <th>Hernia</th>      <th>Infiltration</th>      <th>Mass</th>      <th>Nodule</th>      <th>Atelectasis</th>      <th>Pneumothorax</th>      <th>Pleural_Thickening</th>      <th>...</th>      <th>Infiltration_pred</th>      <th>Mass_pred</th>      <th>Nodule_pred</th>      <th>Atelectasis_pred</th>      <th>Pneumothorax_pred</th>      <th>Pleural_Thickening_pred</th>      <th>Pneumonia_pred</th>      <th>Fibrosis_pred</th>      <th>Edema_pred</th>      <th>Consolidation_pred</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0.256020</td>      <td>0.266928</td>      <td>0.312440</td>      <td>0.460342</td>      <td>0.079453</td>      <td>0.271495</td>      <td>0.276861</td>      <td>0.398799</td>      <td>0.015867</td>      <td>0.156320</td>    </tr>    <tr>      <th>1</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0.382199</td>      <td>0.176825</td>      <td>0.465807</td>      <td>0.489424</td>      <td>0.084595</td>      <td>0.377318</td>      <td>0.363582</td>      <td>0.638024</td>      <td>0.025948</td>      <td>0.144419</td>    </tr>    <tr>      <th>2</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0.427727</td>      <td>0.115513</td>      <td>0.249030</td>      <td>0.035105</td>      <td>0.238761</td>      <td>0.167095</td>      <td>0.166389</td>      <td>0.262463</td>      <td>0.007758</td>      <td>0.125790</td>    </tr>    <tr>      <th>3</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0.158596</td>      <td>0.259460</td>      <td>0.334870</td>      <td>0.266489</td>      <td>0.073371</td>      <td>0.229834</td>      <td>0.191281</td>      <td>0.344348</td>      <td>0.008559</td>      <td>0.119153</td>    </tr>    <tr>      <th>4</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0.536762</td>      <td>0.198797</td>      <td>0.273110</td>      <td>0.186771</td>      <td>0.242122</td>      <td>0.309786</td>      <td>0.411771</td>      <td>0.244666</td>      <td>0.126930</td>      <td>0.342409</td>    </tr>  </tbody></table><p>5 rows × 28 columns</p></div><p>To further understand our dataset details, here’s a histogram of the number of samples for each label in the validation dataset:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line">plt.bar(x = class_labels, height= y.sum(axis=<span class="number">0</span>));</span><br></pre></td></tr></table></figure><p><img src="output_14_0.png" alt="png"></p><p>It seem like our dataset has an imbalanced population of samples. Specifically, our dataset has a small number of patients diagnosed with a <code>Hernia</code>.</p><p><a name="3"></a></p><h2 id="3-Metrics"><a href="#3-Metrics" class="headerlink" title="3 Metrics"></a>3 Metrics</h2><p><a name="3-1"></a></p><h3 id="3-1-True-Positives-False-Positives-True-Negatives-and-False-Negatives"><a href="#3-1-True-Positives-False-Positives-True-Negatives-and-False-Negatives" class="headerlink" title="3.1 True Positives, False Positives, True Negatives, and False Negatives"></a>3.1 True Positives, False Positives, True Negatives, and False Negatives</h3><p>The most basic statistics to compute from the model predictions are the true positives, true negatives, false positives, and false negatives. </p><p>As the name suggests</p><ul><li>true positive (TP): The model classifies the example as positive, and the actual label also positive.</li><li>false positive (FP): The model classifies the example as positive, <strong>but</strong> the actual label is negative.</li><li>true negative (TN): The model classifies the example as negative, and the actual label is also negative.</li><li>false negative (FN): The model classifies the example as negative, <strong>but</strong> the label is actually positive.</li></ul><p>We will count the number of TP, FP, TN and FN in the given data.  All of our metrics can be built off of these four statistics. </p><p>Recall that the model outputs real numbers between 0 and 1.</p><ul><li>To compute binary class predictions, we need to convert these to either 0 or 1. </li><li>We’ll do this using a threshold value $th$.</li><li>Any model outputs above $th$ are set to 1, and below $th$ are set to 0. </li></ul><p>All of our metrics (except for AUC at the end) will depend on the choice of this threshold. </p><p>Fill in the functions to compute the TP, FP, TN, and FN for a given threshold below. </p><p>The first one has been done for you.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">true_positives</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Count true positives.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        TP (int): true positives</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    TP = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get thresholded predictions</span></span><br><span class="line">    thresholded_preds = pred &gt;= th</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute TP</span></span><br><span class="line">    TP = np.sum((y == <span class="number">1</span>) &amp; (thresholded_preds == <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> TP</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">true_negatives</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Count true negatives.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        TN (int): true negatives</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    TN = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get thresholded predictions</span></span><br><span class="line">    thresholded_preds = pred &gt;= th</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute TN</span></span><br><span class="line">    TN = np.sum((y == <span class="number">0</span>) &amp; (thresholded_preds == <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> TN</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">false_positives</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Count false positives.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        FP (int): false positives</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    FP = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get thresholded predictions</span></span><br><span class="line">    thresholded_preds = pred &gt;= th</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute FP</span></span><br><span class="line">    FP = np.sum((y == <span class="number">0</span>) &amp; (thresholded_preds == <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> FP</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">false_negatives</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Count false positives.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        FN (int): false negatives</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    FN = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get thresholded predictions</span></span><br><span class="line">    thresholded_preds = pred &gt;= th</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute FN</span></span><br><span class="line">    FN = np.sum((y == <span class="number">1</span>) &amp; (thresholded_preds == <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> FN</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Note: we must explicity import 'display' in order for the autograder to compile the submitted code</span></span><br><span class="line"><span class="comment"># Even though we could use this function without importing it, keep this import in order to allow the grader to work</span></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display</span><br><span class="line"><span class="comment"># Test</span></span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">'y_test'</span>: [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                   <span class="string">'preds_test'</span>: [<span class="number">0.8</span>,<span class="number">0.7</span>,<span class="number">0.4</span>,<span class="number">0.3</span>,<span class="number">0.2</span>,<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0</span>],</span><br><span class="line">                   <span class="string">'category'</span>: [<span class="string">'TP'</span>,<span class="string">'TP'</span>,<span class="string">'TN'</span>,<span class="string">'TN'</span>,<span class="string">'TN'</span>,<span class="string">'FP'</span>,<span class="string">'FP'</span>,<span class="string">'FP'</span>,<span class="string">'FP'</span>,<span class="string">'FN'</span>,<span class="string">'FN'</span>,<span class="string">'FN'</span>,<span class="string">'FN'</span>,<span class="string">'FN'</span>]</span><br><span class="line">                  &#125;)</span><br><span class="line"></span><br><span class="line">display(df)</span><br><span class="line"><span class="comment">#y_test = np.array([1, 0, 0, 1, 1])</span></span><br><span class="line">y_test = df[<span class="string">'y_test'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#preds_test = np.array([0.8, 0.8, 0.4, 0.6, 0.3])</span></span><br><span class="line">preds_test = df[<span class="string">'preds_test'</span>]</span><br><span class="line"></span><br><span class="line">threshold = <span class="number">0.5</span></span><br><span class="line">print(<span class="string">f"threshold: <span class="subst">&#123;threshold&#125;</span>\n"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"""Our functions calcualted: </span></span><br><span class="line"><span class="string">TP: <span class="subst">&#123;true_positives(y_test, preds_test, threshold)&#125;</span></span></span><br><span class="line"><span class="string">TN: <span class="subst">&#123;true_negatives(y_test, preds_test, threshold)&#125;</span></span></span><br><span class="line"><span class="string">FP: <span class="subst">&#123;false_positives(y_test, preds_test, threshold)&#125;</span></span></span><br><span class="line"><span class="string">FN: <span class="subst">&#123;false_negatives(y_test, preds_test, threshold)&#125;</span></span></span><br><span class="line"><span class="string">"""</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Expected results"</span>)</span><br><span class="line">print(<span class="string">f"There are <span class="subst">&#123;sum(df[<span class="string">'category'</span>] == <span class="string">'TP'</span>)&#125;</span> TP"</span>)</span><br><span class="line">print(<span class="string">f"There are <span class="subst">&#123;sum(df[<span class="string">'category'</span>] == <span class="string">'TN'</span>)&#125;</span> TN"</span>)</span><br><span class="line">print(<span class="string">f"There are <span class="subst">&#123;sum(df[<span class="string">'category'</span>] == <span class="string">'FP'</span>)&#125;</span> FP"</span>)</span><br><span class="line">print(<span class="string">f"There are <span class="subst">&#123;sum(df[<span class="string">'category'</span>] == <span class="string">'FN'</span>)&#125;</span> FN"</span>)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>y_test</th>      <th>preds_test</th>      <th>category</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>0.8</td>      <td>TP</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>0.7</td>      <td>TP</td>    </tr>    <tr>      <th>2</th>      <td>0</td>      <td>0.4</td>      <td>TN</td>    </tr>    <tr>      <th>3</th>      <td>0</td>      <td>0.3</td>      <td>TN</td>    </tr>    <tr>      <th>4</th>      <td>0</td>      <td>0.2</td>      <td>TN</td>    </tr>    <tr>      <th>5</th>      <td>0</td>      <td>0.5</td>      <td>FP</td>    </tr>    <tr>      <th>6</th>      <td>0</td>      <td>0.6</td>      <td>FP</td>    </tr>    <tr>      <th>7</th>      <td>0</td>      <td>0.7</td>      <td>FP</td>    </tr>    <tr>      <th>8</th>      <td>0</td>      <td>0.8</td>      <td>FP</td>    </tr>    <tr>      <th>9</th>      <td>1</td>      <td>0.1</td>      <td>FN</td>    </tr>    <tr>      <th>10</th>      <td>1</td>      <td>0.2</td>      <td>FN</td>    </tr>    <tr>      <th>11</th>      <td>1</td>      <td>0.3</td>      <td>FN</td>    </tr>    <tr>      <th>12</th>      <td>1</td>      <td>0.4</td>      <td>FN</td>    </tr>    <tr>      <th>13</th>      <td>1</td>      <td>0.0</td>      <td>FN</td>    </tr>  </tbody></table></div><pre><code>threshold: 0.5Our functions calcualted: TP: 2TN: 3FP: 4FN: 5Expected resultsThere are 2 TPThere are 3 TNThere are 4 FPThere are 5 FN</code></pre><p>Run the next cell to see a summary of evaluative metrics for the model predictions for each class. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.get_performance_metrics(y, pred, class_labels)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>TP</th>      <th>TN</th>      <th>FP</th>      <th>FN</th>      <th>Accuracy</th>      <th>Prevalence</th>      <th>Sensitivity</th>      <th>Specificity</th>      <th>PPV</th>      <th>NPV</th>      <th>AUC</th>      <th>F1</th>      <th>Threshold</th>    </tr>    <tr>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Cardiomegaly</th>      <td>16</td>      <td>814</td>      <td>169</td>      <td>1</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Emphysema</th>      <td>20</td>      <td>869</td>      <td>103</td>      <td>8</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Effusion</th>      <td>99</td>      <td>690</td>      <td>196</td>      <td>15</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Hernia</th>      <td>1</td>      <td>743</td>      <td>255</td>      <td>1</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Infiltration</th>      <td>114</td>      <td>543</td>      <td>265</td>      <td>78</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Mass</th>      <td>40</td>      <td>789</td>      <td>158</td>      <td>13</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Nodule</th>      <td>28</td>      <td>731</td>      <td>220</td>      <td>21</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Atelectasis</th>      <td>64</td>      <td>657</td>      <td>249</td>      <td>30</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumothorax</th>      <td>24</td>      <td>785</td>      <td>183</td>      <td>8</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pleural_Thickening</th>      <td>24</td>      <td>713</td>      <td>259</td>      <td>4</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumonia</th>      <td>14</td>      <td>661</td>      <td>320</td>      <td>5</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Fibrosis</th>      <td>10</td>      <td>725</td>      <td>261</td>      <td>4</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Edema</th>      <td>15</td>      <td>767</td>      <td>213</td>      <td>5</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Consolidation</th>      <td>36</td>      <td>658</td>      <td>297</td>      <td>9</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>  </tbody></table></div><p>Right now it only has TP, TN, FP, FN. Throughout this assignment we’ll fill in all the other metrics to learn more about our model performance.</p><p><a name="3-2"></a></p><h3 id="3-2-Accuracy"><a href="#3-2-Accuracy" class="headerlink" title="3.2 Accuracy"></a>3.2 Accuracy</h3><p>Let’s use a threshold of .5 for the probability cutoff for our predictions for all classes and calculate our model’s accuracy as we would normally do in a machine learning problem. </p><script type="math/tex; mode=display">accuracy = \frac{\text{true positives} + \text{true negatives}}{\text{true positives} + \text{true negatives} + \text{false positives} + \text{false negatives}}</script><p>Use this formula to compute accuracy below:</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Remember to set the value for the threshold when calling the functions.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_accuracy</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute accuracy of predictions at threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        accuracy (float): accuracy of predictions at threshold</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    accuracy = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get TP, FP, TN, FN using our previously defined functions</span></span><br><span class="line">    TP = true_positives(y, pred, th)</span><br><span class="line">    FP = false_positives(y, pred, th)</span><br><span class="line">    TN = true_negatives(y, pred, th)</span><br><span class="line">    FN = false_negatives(y, pred, th)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute accuracy using TP, FP, TN, FN</span></span><br><span class="line">    accuracy = (TP + TN)/(TP + TN + FP + FN)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test</span></span><br><span class="line">print(<span class="string">"Test case:"</span>)</span><br><span class="line"></span><br><span class="line">y_test = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">'test labels: &#123;y_test&#125;'</span>)</span><br><span class="line"></span><br><span class="line">preds_test = np.array([<span class="number">0.8</span>, <span class="number">0.8</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.3</span>])</span><br><span class="line">print(<span class="string">f'test predictions: <span class="subst">&#123;preds_test&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">threshold = <span class="number">0.5</span></span><br><span class="line">print(<span class="string">f"threshold: <span class="subst">&#123;threshold&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"computed accuracy: <span class="subst">&#123;get_accuracy(y_test, preds_test, threshold)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test case:test labels: {y_test}test predictions: [0.8 0.8 0.4 0.6 0.3]threshold: 0.5computed accuracy: 0.6</code></pre><h4 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test labels: &#123;y_test&#125;</span><br><span class="line">test predictions: [<span class="number">0.8</span> <span class="number">0.8</span> <span class="number">0.4</span> <span class="number">0.6</span> <span class="number">0.3</span>]</span><br><span class="line">threshold: <span class="number">0.5</span></span><br><span class="line">computed accuracy: <span class="number">0.6</span></span><br></pre></td></tr></table></figure><p>Run the next cell to see the accuracy of the model output for each class, as well as the number of true positives, true negatives, false positives, and false negatives.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>TP</th>      <th>TN</th>      <th>FP</th>      <th>FN</th>      <th>Accuracy</th>      <th>Prevalence</th>      <th>Sensitivity</th>      <th>Specificity</th>      <th>PPV</th>      <th>NPV</th>      <th>AUC</th>      <th>F1</th>      <th>Threshold</th>    </tr>    <tr>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Cardiomegaly</th>      <td>16</td>      <td>814</td>      <td>169</td>      <td>1</td>      <td>0.83</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Emphysema</th>      <td>20</td>      <td>869</td>      <td>103</td>      <td>8</td>      <td>0.889</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Effusion</th>      <td>99</td>      <td>690</td>      <td>196</td>      <td>15</td>      <td>0.789</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Hernia</th>      <td>1</td>      <td>743</td>      <td>255</td>      <td>1</td>      <td>0.744</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Infiltration</th>      <td>114</td>      <td>543</td>      <td>265</td>      <td>78</td>      <td>0.657</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Mass</th>      <td>40</td>      <td>789</td>      <td>158</td>      <td>13</td>      <td>0.829</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Nodule</th>      <td>28</td>      <td>731</td>      <td>220</td>      <td>21</td>      <td>0.759</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Atelectasis</th>      <td>64</td>      <td>657</td>      <td>249</td>      <td>30</td>      <td>0.721</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumothorax</th>      <td>24</td>      <td>785</td>      <td>183</td>      <td>8</td>      <td>0.809</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pleural_Thickening</th>      <td>24</td>      <td>713</td>      <td>259</td>      <td>4</td>      <td>0.737</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumonia</th>      <td>14</td>      <td>661</td>      <td>320</td>      <td>5</td>      <td>0.675</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Fibrosis</th>      <td>10</td>      <td>725</td>      <td>261</td>      <td>4</td>      <td>0.735</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Edema</th>      <td>15</td>      <td>767</td>      <td>213</td>      <td>5</td>      <td>0.782</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Consolidation</th>      <td>36</td>      <td>658</td>      <td>297</td>      <td>9</td>      <td>0.694</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>  </tbody></table></div><p>If we were to judge our model’s performance based on the accuracy metric, we would say that our model is not very accurate for detecting the <code>Infiltration</code> cases (accuracy of 0.657) but pretty accurate for detecting <code>Emphysema</code> (accuracy of 0.889). </p><p><strong>But is that really the case?…</strong></p><p>Let’s imagine a model that simply predicts that any patient does <strong>Not</strong> have <code>Emphysema</code>, regardless of patient’s measurements. Let’s calculate the accuracy for such a model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_accuracy(valid_results[<span class="string">"Emphysema"</span>].values, np.zeros(len(valid_results)))</span><br></pre></td></tr></table></figure><p>As you can see above, such a model would be 97% accurate! Even better than our deep learning based model. </p><p>But is this really a good model? Wouldn’t this model be wrong 100% of the time if the patient actually had this condition?</p><p>In the following sections, we will address this concern with more advanced model measures - <strong>sensitivity and specificity</strong> - that evaluate how well the model predicts positives for patients with the condition and negatives for cases that actually do not have the condition.</p><p><a name="3-3"></a></p><h3 id="3-3-Prevalence"><a href="#3-3-Prevalence" class="headerlink" title="3.3 Prevalence"></a>3.3 Prevalence</h3><p>Another important concept is <strong>prevalence</strong>. </p><ul><li>In a medical context, prevalence is the proportion of people in the population who have the disease (or condition, etc). </li><li>In machine learning terms, this is the proportion of positive examples. The expression for prevalence is:</li></ul><script type="math/tex; mode=display">prevalence = \frac{1}{N} \sum_{i} y_i</script><p>where $y_i = 1$ when the example is ‘positive’ (has the disease).</p><p>Let’s measure prevalence for each disease:</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>    You can use <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html" target="_blank" rel="noopener"> np.mean </a> to calculate the formula.</li>    <li>Actually, the automatic grader is expecting numpy.mean, so please use it instead of using an equally valid but different way of calculating the prevalence. =) </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_prevalence</span><span class="params">(y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute accuracy of predictions at threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        prevalence (float): prevalence of positive cases</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    prevalence = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    prevalence = np.mean(y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> prevalence</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test</span></span><br><span class="line">print(<span class="string">"Test case:\n"</span>)</span><br><span class="line"></span><br><span class="line">y_test = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">f'test labels: <span class="subst">&#123;y_test&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"computed prevalence: <span class="subst">&#123;get_prevalence(y_test)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test case:test labels: [1 0 0 1 1 0 0 0 0 1]computed prevalence: 0.4</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>TP</th>      <th>TN</th>      <th>FP</th>      <th>FN</th>      <th>Accuracy</th>      <th>Prevalence</th>      <th>Sensitivity</th>      <th>Specificity</th>      <th>PPV</th>      <th>NPV</th>      <th>AUC</th>      <th>F1</th>      <th>Threshold</th>    </tr>    <tr>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Cardiomegaly</th>      <td>16</td>      <td>814</td>      <td>169</td>      <td>1</td>      <td>0.83</td>      <td>0.017</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Emphysema</th>      <td>20</td>      <td>869</td>      <td>103</td>      <td>8</td>      <td>0.889</td>      <td>0.028</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Effusion</th>      <td>99</td>      <td>690</td>      <td>196</td>      <td>15</td>      <td>0.789</td>      <td>0.114</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Hernia</th>      <td>1</td>      <td>743</td>      <td>255</td>      <td>1</td>      <td>0.744</td>      <td>0.002</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Infiltration</th>      <td>114</td>      <td>543</td>      <td>265</td>      <td>78</td>      <td>0.657</td>      <td>0.192</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Mass</th>      <td>40</td>      <td>789</td>      <td>158</td>      <td>13</td>      <td>0.829</td>      <td>0.053</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Nodule</th>      <td>28</td>      <td>731</td>      <td>220</td>      <td>21</td>      <td>0.759</td>      <td>0.049</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Atelectasis</th>      <td>64</td>      <td>657</td>      <td>249</td>      <td>30</td>      <td>0.721</td>      <td>0.094</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumothorax</th>      <td>24</td>      <td>785</td>      <td>183</td>      <td>8</td>      <td>0.809</td>      <td>0.032</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pleural_Thickening</th>      <td>24</td>      <td>713</td>      <td>259</td>      <td>4</td>      <td>0.737</td>      <td>0.028</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumonia</th>      <td>14</td>      <td>661</td>      <td>320</td>      <td>5</td>      <td>0.675</td>      <td>0.019</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Fibrosis</th>      <td>10</td>      <td>725</td>      <td>261</td>      <td>4</td>      <td>0.735</td>      <td>0.014</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Edema</th>      <td>15</td>      <td>767</td>      <td>213</td>      <td>5</td>      <td>0.782</td>      <td>0.02</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Consolidation</th>      <td>36</td>      <td>658</td>      <td>297</td>      <td>9</td>      <td>0.694</td>      <td>0.045</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>  </tbody></table></div><p><code>Hernia</code> has a prevalence 0.002, which is the rarest among the studied conditions in our dataset.</p><p><a name="3-4"></a></p><h3 id="3-4-Sensitivity-and-Specificity"><a href="#3-4-Sensitivity-and-Specificity" class="headerlink" title="3.4 Sensitivity and Specificity"></a>3.4 Sensitivity and Specificity</h3><p><img src="sens_spec.png" width="30%"></p><p>Sensitivity and specificity are two of the most prominent numbers that are used to measure diagnostics tests.</p><ul><li>Sensitivity is the probability that our test outputs positive given that the case is actually positive.</li><li>Specificity is the probability that the test outputs negative given that the case is actually negative. </li></ul><p>We can phrase this easily in terms of true positives, true negatives, false positives, and false negatives: </p><script type="math/tex; mode=display">sensitivity = \frac{\text{true positives}}{\text{true positives} + \text{false negatives}}</script><script type="math/tex; mode=display">specificity = \frac{\text{true negatives}}{\text{true negatives} + \text{false positives}}</script><p>Let’s calculate sensitivity and specificity for our model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sensitivity</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute sensitivity of predictions at threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        sensitivity (float): probability that our test outputs positive given that the case is actually positive</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    sensitivity = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get TP and FN using our previously defined functions</span></span><br><span class="line">    TP = true_positives(y, pred, th)</span><br><span class="line">    FN = false_negatives(y, pred, th)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># use TP and FN to compute sensitivity</span></span><br><span class="line">    sensitivity = TP / (TP + FN)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> sensitivity</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_specificity</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute specificity of predictions at threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        specificity (float): probability that the test outputs negative given that the case is actually negative</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    specificity = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get TN and FP using our previously defined functions</span></span><br><span class="line">    TN = true_negatives(y, pred, th)</span><br><span class="line">    FP = false_positives(y , pred, th)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># use TN and FP to compute specificity </span></span><br><span class="line">    specificity = TN / (TN + FP)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> specificity</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test</span></span><br><span class="line">print(<span class="string">"Test case"</span>)</span><br><span class="line"></span><br><span class="line">y_test = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">f'test labels: <span class="subst">&#123;y_test&#125;</span>\n'</span>)</span><br><span class="line"></span><br><span class="line">preds_test = np.array([<span class="number">0.8</span>, <span class="number">0.8</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.3</span>])</span><br><span class="line">print(<span class="string">f'test predictions: <span class="subst">&#123;preds_test&#125;</span>\n'</span>)</span><br><span class="line"></span><br><span class="line">threshold = <span class="number">0.5</span></span><br><span class="line">print(<span class="string">f"threshold: <span class="subst">&#123;threshold&#125;</span>\n"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"computed sensitivity: <span class="subst">&#123;get_sensitivity(y_test, preds_test, threshold):<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"computed specificity: <span class="subst">&#123;get_specificity(y_test, preds_test, threshold):<span class="number">.2</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test casetest labels: [1 0 0 1 1]test predictions: [0.8 0.8 0.4 0.6 0.3]threshold: 0.5computed sensitivity: 0.67computed specificity: 0.50</code></pre><h4 id="Expected-output-1"><a href="#Expected-output-1" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Test case</span><br><span class="line">test labels: [<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">test predictions: [<span class="number">0.8</span> <span class="number">0.8</span> <span class="number">0.4</span> <span class="number">0.6</span> <span class="number">0.3</span>]</span><br><span class="line"></span><br><span class="line">threshold: <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">computed sensitivity: <span class="number">0.67</span></span><br><span class="line">computed specificity: <span class="number">0.50</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, </span><br><span class="line">                        sens=get_sensitivity, spec=get_specificity)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>TP</th>      <th>TN</th>      <th>FP</th>      <th>FN</th>      <th>Accuracy</th>      <th>Prevalence</th>      <th>Sensitivity</th>      <th>Specificity</th>      <th>PPV</th>      <th>NPV</th>      <th>AUC</th>      <th>F1</th>      <th>Threshold</th>    </tr>    <tr>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Cardiomegaly</th>      <td>16</td>      <td>814</td>      <td>169</td>      <td>1</td>      <td>0.83</td>      <td>0.017</td>      <td>0.941</td>      <td>0.828</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Emphysema</th>      <td>20</td>      <td>869</td>      <td>103</td>      <td>8</td>      <td>0.889</td>      <td>0.028</td>      <td>0.714</td>      <td>0.894</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Effusion</th>      <td>99</td>      <td>690</td>      <td>196</td>      <td>15</td>      <td>0.789</td>      <td>0.114</td>      <td>0.868</td>      <td>0.779</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Hernia</th>      <td>1</td>      <td>743</td>      <td>255</td>      <td>1</td>      <td>0.744</td>      <td>0.002</td>      <td>0.5</td>      <td>0.744</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Infiltration</th>      <td>114</td>      <td>543</td>      <td>265</td>      <td>78</td>      <td>0.657</td>      <td>0.192</td>      <td>0.594</td>      <td>0.672</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Mass</th>      <td>40</td>      <td>789</td>      <td>158</td>      <td>13</td>      <td>0.829</td>      <td>0.053</td>      <td>0.755</td>      <td>0.833</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Nodule</th>      <td>28</td>      <td>731</td>      <td>220</td>      <td>21</td>      <td>0.759</td>      <td>0.049</td>      <td>0.571</td>      <td>0.769</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Atelectasis</th>      <td>64</td>      <td>657</td>      <td>249</td>      <td>30</td>      <td>0.721</td>      <td>0.094</td>      <td>0.681</td>      <td>0.725</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumothorax</th>      <td>24</td>      <td>785</td>      <td>183</td>      <td>8</td>      <td>0.809</td>      <td>0.032</td>      <td>0.75</td>      <td>0.811</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pleural_Thickening</th>      <td>24</td>      <td>713</td>      <td>259</td>      <td>4</td>      <td>0.737</td>      <td>0.028</td>      <td>0.857</td>      <td>0.734</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumonia</th>      <td>14</td>      <td>661</td>      <td>320</td>      <td>5</td>      <td>0.675</td>      <td>0.019</td>      <td>0.737</td>      <td>0.674</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Fibrosis</th>      <td>10</td>      <td>725</td>      <td>261</td>      <td>4</td>      <td>0.735</td>      <td>0.014</td>      <td>0.714</td>      <td>0.735</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Edema</th>      <td>15</td>      <td>767</td>      <td>213</td>      <td>5</td>      <td>0.782</td>      <td>0.02</td>      <td>0.75</td>      <td>0.783</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Consolidation</th>      <td>36</td>      <td>658</td>      <td>297</td>      <td>9</td>      <td>0.694</td>      <td>0.045</td>      <td>0.8</td>      <td>0.689</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>  </tbody></table></div><p>Note that specificity and sensitivity do not depend on the prevalence of the positive class in the dataset. </p><ul><li>This is because the statistics are only computed within people of the same class</li><li>Sensitivity only considers output on people in the positive class</li><li>Similarly, specificity only considers output on people in the negative class.</li></ul><p><a name="3-5"></a></p><h3 id="3-5-PPV-and-NPV"><a href="#3-5-PPV-and-NPV" class="headerlink" title="3.5 PPV and NPV"></a>3.5 PPV and NPV</h3><p>Diagnostically, however, sensitivity and specificity are not helpful. Sensitivity, for example, tells us the probability our test outputs positive given that the person already has the condition. Here, we are conditioning on the thing we would like to find out (whether the patient has the condition)!</p><p>What would be more helpful is the probability that the person has the disease given that our test outputs positive. That brings us to positive predictive value (PPV) and negative predictive value (NPV).</p><ul><li>Positive predictive value (PPV) is the probability that subjects with a positive screening test truly have the disease.</li><li>Negative predictive value (NPV) is the probability that subjects with a negative screening test truly don’t have the disease.</li></ul><p>Again, we can formulate these in terms of true positives, true negatives, false positives, and false negatives: </p><script type="math/tex; mode=display">PPV = \frac{\text{true positives}}{\text{true positives} + \text{false positives}}</script><script type="math/tex; mode=display">NPV = \frac{\text{true negatives}}{\text{true negatives} + \text{false negatives}}</script><p>Let’s calculate PPV &amp; NPV for our model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ppv</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute PPV of predictions at threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        PPV (float): positive predictive value of predictions at threshold</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    PPV = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get TP and FP using our previously defined functions</span></span><br><span class="line">    TP = true_positives(y, pred, th)</span><br><span class="line">    FP = false_positives(y, pred, th)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># use TP and FP to compute PPV</span></span><br><span class="line">    PPV = TP / (TP + FP)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> PPV</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_npv</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute NPV of predictions at threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        NPV (float): negative predictive value of predictions at threshold</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    NPV = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get TN and FN using our previously defined functions</span></span><br><span class="line">    TN = true_negatives(y, pred, th)</span><br><span class="line">    FN = false_negatives(y, pred, th)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># use TN and FN to compute NPV</span></span><br><span class="line">    NPV = TN / (TN + FN)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> NPV</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test</span></span><br><span class="line">print(<span class="string">"Test case:\n"</span>)</span><br><span class="line"></span><br><span class="line">y_test = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">f'test labels: <span class="subst">&#123;y_test&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">preds_test = np.array([<span class="number">0.8</span>, <span class="number">0.8</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.3</span>])</span><br><span class="line">print(<span class="string">f'test predictions: <span class="subst">&#123;preds_test&#125;</span>\n'</span>)</span><br><span class="line"></span><br><span class="line">threshold = <span class="number">0.5</span></span><br><span class="line">print(<span class="string">f"threshold: <span class="subst">&#123;threshold&#125;</span>\n"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"computed ppv: <span class="subst">&#123;get_ppv(y_test, preds_test, threshold):<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"computed npv: <span class="subst">&#123;get_npv(y_test, preds_test, threshold):<span class="number">.2</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test case:test labels: [1 0 0 1 1]test predictions: [0.8 0.8 0.4 0.6 0.3]threshold: 0.5computed ppv: 0.67computed npv: 0.50</code></pre><h4 id="Expected-output-2"><a href="#Expected-output-2" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Test case:</span><br><span class="line"></span><br><span class="line">test labels: [<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line">test predictions: [<span class="number">0.8</span> <span class="number">0.8</span> <span class="number">0.4</span> <span class="number">0.6</span> <span class="number">0.3</span>]</span><br><span class="line"></span><br><span class="line">threshold: <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">computed ppv: <span class="number">0.67</span></span><br><span class="line">computed npv: <span class="number">0.50</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, </span><br><span class="line">                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>TP</th>      <th>TN</th>      <th>FP</th>      <th>FN</th>      <th>Accuracy</th>      <th>Prevalence</th>      <th>Sensitivity</th>      <th>Specificity</th>      <th>PPV</th>      <th>NPV</th>      <th>AUC</th>      <th>F1</th>      <th>Threshold</th>    </tr>    <tr>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Cardiomegaly</th>      <td>16</td>      <td>814</td>      <td>169</td>      <td>1</td>      <td>0.83</td>      <td>0.017</td>      <td>0.941</td>      <td>0.828</td>      <td>0.086</td>      <td>0.999</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Emphysema</th>      <td>20</td>      <td>869</td>      <td>103</td>      <td>8</td>      <td>0.889</td>      <td>0.028</td>      <td>0.714</td>      <td>0.894</td>      <td>0.163</td>      <td>0.991</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Effusion</th>      <td>99</td>      <td>690</td>      <td>196</td>      <td>15</td>      <td>0.789</td>      <td>0.114</td>      <td>0.868</td>      <td>0.779</td>      <td>0.336</td>      <td>0.979</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Hernia</th>      <td>1</td>      <td>743</td>      <td>255</td>      <td>1</td>      <td>0.744</td>      <td>0.002</td>      <td>0.5</td>      <td>0.744</td>      <td>0.004</td>      <td>0.999</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Infiltration</th>      <td>114</td>      <td>543</td>      <td>265</td>      <td>78</td>      <td>0.657</td>      <td>0.192</td>      <td>0.594</td>      <td>0.672</td>      <td>0.301</td>      <td>0.874</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Mass</th>      <td>40</td>      <td>789</td>      <td>158</td>      <td>13</td>      <td>0.829</td>      <td>0.053</td>      <td>0.755</td>      <td>0.833</td>      <td>0.202</td>      <td>0.984</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Nodule</th>      <td>28</td>      <td>731</td>      <td>220</td>      <td>21</td>      <td>0.759</td>      <td>0.049</td>      <td>0.571</td>      <td>0.769</td>      <td>0.113</td>      <td>0.972</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Atelectasis</th>      <td>64</td>      <td>657</td>      <td>249</td>      <td>30</td>      <td>0.721</td>      <td>0.094</td>      <td>0.681</td>      <td>0.725</td>      <td>0.204</td>      <td>0.956</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumothorax</th>      <td>24</td>      <td>785</td>      <td>183</td>      <td>8</td>      <td>0.809</td>      <td>0.032</td>      <td>0.75</td>      <td>0.811</td>      <td>0.116</td>      <td>0.99</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pleural_Thickening</th>      <td>24</td>      <td>713</td>      <td>259</td>      <td>4</td>      <td>0.737</td>      <td>0.028</td>      <td>0.857</td>      <td>0.734</td>      <td>0.085</td>      <td>0.994</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumonia</th>      <td>14</td>      <td>661</td>      <td>320</td>      <td>5</td>      <td>0.675</td>      <td>0.019</td>      <td>0.737</td>      <td>0.674</td>      <td>0.042</td>      <td>0.992</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Fibrosis</th>      <td>10</td>      <td>725</td>      <td>261</td>      <td>4</td>      <td>0.735</td>      <td>0.014</td>      <td>0.714</td>      <td>0.735</td>      <td>0.037</td>      <td>0.995</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Edema</th>      <td>15</td>      <td>767</td>      <td>213</td>      <td>5</td>      <td>0.782</td>      <td>0.02</td>      <td>0.75</td>      <td>0.783</td>      <td>0.066</td>      <td>0.994</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Consolidation</th>      <td>36</td>      <td>658</td>      <td>297</td>      <td>9</td>      <td>0.694</td>      <td>0.045</td>      <td>0.8</td>      <td>0.689</td>      <td>0.108</td>      <td>0.987</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>  </tbody></table></div><p>Notice that despite having very high sensitivity and accuracy, the PPV of the predictions could still be very low. </p><p>This is the case with <code>Edema</code>, for example. </p><ul><li>The sensitivity for <code>Edema</code> is 0.75.</li><li>However, given that the model predicted positive, the probability that a person has Edema (its PPV) is only 0.066!</li></ul><p><a name="3-6"></a></p><h3 id="3-6-ROC-Curve"><a href="#3-6-ROC-Curve" class="headerlink" title="3.6 ROC Curve"></a>3.6 ROC Curve</h3><p>So far we have been operating under the assumption that our model’s prediction of <code>0.5</code> and above should be treated as positive and otherwise it should be treated as negative. This however was a rather arbitrary choice. One way to see this, is to look at a very informative visualization called the receiver operating characteristic (ROC) curve.</p><p>The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The ideal point is at the top left, with a true positive rate of 1 and a false positive rate of 0. The various points on the curve are generated by gradually changing the threshold.</p><p>Let’s look at this curve for our model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.get_curve(y, pred, class_labels)</span><br></pre></td></tr></table></figure><p><img src="output_53_0.png" alt="png"></p><p>The area under the ROC curve is also called AUCROC or C-statistic and is a measure of goodness of fit. In medical literature this number also gives the probability that a randomly selected patient who experienced a condition had a higher risk score than a patient who had not experienced the event. This summarizes the model output across all thresholds, and provides a good sense of the discriminative power of a given model.</p><p>Let’s use the <code>sklearn</code> metric function of <code>roc_auc_score</code> to add this score to our metrics table.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, </span><br><span class="line">                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>TP</th>      <th>TN</th>      <th>FP</th>      <th>FN</th>      <th>Accuracy</th>      <th>Prevalence</th>      <th>Sensitivity</th>      <th>Specificity</th>      <th>PPV</th>      <th>NPV</th>      <th>AUC</th>      <th>F1</th>      <th>Threshold</th>    </tr>    <tr>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Cardiomegaly</th>      <td>16</td>      <td>814</td>      <td>169</td>      <td>1</td>      <td>0.83</td>      <td>0.017</td>      <td>0.941</td>      <td>0.828</td>      <td>0.086</td>      <td>0.999</td>      <td>0.933</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Emphysema</th>      <td>20</td>      <td>869</td>      <td>103</td>      <td>8</td>      <td>0.889</td>      <td>0.028</td>      <td>0.714</td>      <td>0.894</td>      <td>0.163</td>      <td>0.991</td>      <td>0.935</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Effusion</th>      <td>99</td>      <td>690</td>      <td>196</td>      <td>15</td>      <td>0.789</td>      <td>0.114</td>      <td>0.868</td>      <td>0.779</td>      <td>0.336</td>      <td>0.979</td>      <td>0.891</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Hernia</th>      <td>1</td>      <td>743</td>      <td>255</td>      <td>1</td>      <td>0.744</td>      <td>0.002</td>      <td>0.5</td>      <td>0.744</td>      <td>0.004</td>      <td>0.999</td>      <td>0.644</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Infiltration</th>      <td>114</td>      <td>543</td>      <td>265</td>      <td>78</td>      <td>0.657</td>      <td>0.192</td>      <td>0.594</td>      <td>0.672</td>      <td>0.301</td>      <td>0.874</td>      <td>0.696</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Mass</th>      <td>40</td>      <td>789</td>      <td>158</td>      <td>13</td>      <td>0.829</td>      <td>0.053</td>      <td>0.755</td>      <td>0.833</td>      <td>0.202</td>      <td>0.984</td>      <td>0.888</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Nodule</th>      <td>28</td>      <td>731</td>      <td>220</td>      <td>21</td>      <td>0.759</td>      <td>0.049</td>      <td>0.571</td>      <td>0.769</td>      <td>0.113</td>      <td>0.972</td>      <td>0.745</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Atelectasis</th>      <td>64</td>      <td>657</td>      <td>249</td>      <td>30</td>      <td>0.721</td>      <td>0.094</td>      <td>0.681</td>      <td>0.725</td>      <td>0.204</td>      <td>0.956</td>      <td>0.781</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumothorax</th>      <td>24</td>      <td>785</td>      <td>183</td>      <td>8</td>      <td>0.809</td>      <td>0.032</td>      <td>0.75</td>      <td>0.811</td>      <td>0.116</td>      <td>0.99</td>      <td>0.826</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pleural_Thickening</th>      <td>24</td>      <td>713</td>      <td>259</td>      <td>4</td>      <td>0.737</td>      <td>0.028</td>      <td>0.857</td>      <td>0.734</td>      <td>0.085</td>      <td>0.994</td>      <td>0.868</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumonia</th>      <td>14</td>      <td>661</td>      <td>320</td>      <td>5</td>      <td>0.675</td>      <td>0.019</td>      <td>0.737</td>      <td>0.674</td>      <td>0.042</td>      <td>0.992</td>      <td>0.762</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Fibrosis</th>      <td>10</td>      <td>725</td>      <td>261</td>      <td>4</td>      <td>0.735</td>      <td>0.014</td>      <td>0.714</td>      <td>0.735</td>      <td>0.037</td>      <td>0.995</td>      <td>0.801</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Edema</th>      <td>15</td>      <td>767</td>      <td>213</td>      <td>5</td>      <td>0.782</td>      <td>0.02</td>      <td>0.75</td>      <td>0.783</td>      <td>0.066</td>      <td>0.994</td>      <td>0.856</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Consolidation</th>      <td>36</td>      <td>658</td>      <td>297</td>      <td>9</td>      <td>0.694</td>      <td>0.045</td>      <td>0.8</td>      <td>0.689</td>      <td>0.108</td>      <td>0.987</td>      <td>0.799</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>  </tbody></table></div><p><a name="4"></a></p><h2 id="4-Confidence-Intervals"><a href="#4-Confidence-Intervals" class="headerlink" title="4. Confidence Intervals"></a>4. Confidence Intervals</h2><p>Of course our dataset is only a sample of the real world, and our calculated values for all above metrics is an estimate of the real world values. It would be good to quantify this uncertainty due to the sampling of our dataset. We’ll do this through the use of confidence intervals. A 95\% confidence interval for an estimate $\hat{s}$ of a parameter $s$ is an interval $I = (a, b)$ such that 95\% of the time when the experiment is run, the true value $s$ is contained in $I$. More concretely, if we were to run the experiment many times, then the fraction of those experiments for which $I$ contains the true parameter would tend towards 95\%.</p><p>While some estimates come with methods for computing the confidence interval analytically, more complicated statistics, such as the AUC for example, are difficult. For these we can use a method called the <em>bootstrap</em>. The bootstrap estimates the uncertainty by resampling the dataset with replacement. For each resampling $i$, we will get a new estimate, $\hat{s}_i$. We can then estimate the distribution of $\hat{s}$ by using the distribution of $\hat{s}_i$ for our bootstrap samples.</p><p>In the code below, we create bootstrap samples and compute sample AUCs from those samples. Note that we use stratified random sampling (sampling from the positive and negative classes separately) to make sure that members of each class are represented. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bootstrap_auc</span><span class="params">(y, pred, classes, bootstraps = <span class="number">100</span>, fold_size = <span class="number">1000</span>)</span>:</span></span><br><span class="line">    statistics = np.zeros((len(classes), bootstraps))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">        df = pd.DataFrame(columns=[<span class="string">'y'</span>, <span class="string">'pred'</span>])</span><br><span class="line">        df.loc[:, <span class="string">'y'</span>] = y[:, c]</span><br><span class="line">        df.loc[:, <span class="string">'pred'</span>] = pred[:, c]</span><br><span class="line">        <span class="comment"># get positive examples for stratified sampling</span></span><br><span class="line">        df_pos = df[df.y == <span class="number">1</span>]</span><br><span class="line">        df_neg = df[df.y == <span class="number">0</span>]</span><br><span class="line">        prevalence = len(df_pos) / len(df)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(bootstraps):</span><br><span class="line">            <span class="comment"># stratified sampling of positive and negative examples</span></span><br><span class="line">            pos_sample = df_pos.sample(n = int(fold_size * prevalence), replace=<span class="keyword">True</span>)</span><br><span class="line">            neg_sample = df_neg.sample(n = int(fold_size * (<span class="number">1</span>-prevalence)), replace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">            y_sample = np.concatenate([pos_sample.y.values, neg_sample.y.values])</span><br><span class="line">            pred_sample = np.concatenate([pos_sample.pred.values, neg_sample.pred.values])</span><br><span class="line">            score = roc_auc_score(y_sample, pred_sample)</span><br><span class="line">            statistics[c][i] = score</span><br><span class="line">    <span class="keyword">return</span> statistics</span><br><span class="line"></span><br><span class="line">statistics = bootstrap_auc(y, pred, class_labels)</span><br></pre></td></tr></table></figure><p>Now we can compute confidence intervals from the sample statistics that we computed.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.print_confidence_intervals(class_labels, statistics)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Mean AUC (CI 5%-95%)</th>    </tr>  </thead>  <tbody>    <tr>      <th>Cardiomegaly</th>      <td>0.93 (0.90-0.97)</td>    </tr>    <tr>      <th>Emphysema</th>      <td>0.93 (0.90-0.96)</td>    </tr>    <tr>      <th>Effusion</th>      <td>0.89 (0.87-0.91)</td>    </tr>    <tr>      <th>Hernia</th>      <td>0.67 (0.30-0.98)</td>    </tr>    <tr>      <th>Infiltration</th>      <td>0.69 (0.65-0.73)</td>    </tr>    <tr>      <th>Mass</th>      <td>0.89 (0.86-0.92)</td>    </tr>    <tr>      <th>Nodule</th>      <td>0.74 (0.68-0.80)</td>    </tr>    <tr>      <th>Atelectasis</th>      <td>0.78 (0.75-0.81)</td>    </tr>    <tr>      <th>Pneumothorax</th>      <td>0.83 (0.75-0.91)</td>    </tr>    <tr>      <th>Pleural_Thickening</th>      <td>0.87 (0.82-0.93)</td>    </tr>    <tr>      <th>Pneumonia</th>      <td>0.76 (0.65-0.84)</td>    </tr>    <tr>      <th>Fibrosis</th>      <td>0.81 (0.75-0.86)</td>    </tr>    <tr>      <th>Edema</th>      <td>0.85 (0.81-0.90)</td>    </tr>    <tr>      <th>Consolidation</th>      <td>0.80 (0.75-0.84)</td>    </tr>  </tbody></table></div><p>As you can see, our confidence intervals are much wider for some classes than for others. Hernia, for example, has an interval around (0.30 - 0.98), indicating that we can’t be certain it is better than chance (at 0.5). </p><p><a name="5"></a></p><h2 id="5-Precision-Recall-Curve"><a href="#5-Precision-Recall-Curve" class="headerlink" title="5. Precision-Recall Curve"></a>5. Precision-Recall Curve</h2><p>Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. </p><p>In information retrieval</p><ul><li>Precision is a measure of result relevancy and that is equivalent to our previously defined PPV. </li><li>Recall is a measure of how many truly relevant results are returned and that is equivalent to our previously defined sensitivity measure.</li></ul><p>The precision-recall curve (PRC) shows the trade-off between precision and recall for different thresholds. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. </p><p>High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).</p><p>Run the following cell to generate a PRC:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.get_curve(y, pred, class_labels, curve=<span class="string">'prc'</span>)</span><br></pre></td></tr></table></figure><p><img src="output_64_0.png" alt="png"></p><p><a name="6"></a></p><h2 id="6-F1-Score"><a href="#6-F1-Score" class="headerlink" title="6. F1 Score"></a>6. F1 Score</h2><p>F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. </p><p>Again, we can simply use <code>sklearn</code>‘s utility metric function of <code>f1_score</code> to add this measure to our performance table.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line">util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, </span><br><span class="line">                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score,f1=f1_score)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>TP</th>      <th>TN</th>      <th>FP</th>      <th>FN</th>      <th>Accuracy</th>      <th>Prevalence</th>      <th>Sensitivity</th>      <th>Specificity</th>      <th>PPV</th>      <th>NPV</th>      <th>AUC</th>      <th>F1</th>      <th>Threshold</th>    </tr>    <tr>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Cardiomegaly</th>      <td>16</td>      <td>814</td>      <td>169</td>      <td>1</td>      <td>0.83</td>      <td>0.017</td>      <td>0.941</td>      <td>0.828</td>      <td>0.086</td>      <td>0.999</td>      <td>0.933</td>      <td>0.158</td>      <td>0.5</td>    </tr>    <tr>      <th>Emphysema</th>      <td>20</td>      <td>869</td>      <td>103</td>      <td>8</td>      <td>0.889</td>      <td>0.028</td>      <td>0.714</td>      <td>0.894</td>      <td>0.163</td>      <td>0.991</td>      <td>0.935</td>      <td>0.265</td>      <td>0.5</td>    </tr>    <tr>      <th>Effusion</th>      <td>99</td>      <td>690</td>      <td>196</td>      <td>15</td>      <td>0.789</td>      <td>0.114</td>      <td>0.868</td>      <td>0.779</td>      <td>0.336</td>      <td>0.979</td>      <td>0.891</td>      <td>0.484</td>      <td>0.5</td>    </tr>    <tr>      <th>Hernia</th>      <td>1</td>      <td>743</td>      <td>255</td>      <td>1</td>      <td>0.744</td>      <td>0.002</td>      <td>0.5</td>      <td>0.744</td>      <td>0.004</td>      <td>0.999</td>      <td>0.644</td>      <td>0.008</td>      <td>0.5</td>    </tr>    <tr>      <th>Infiltration</th>      <td>114</td>      <td>543</td>      <td>265</td>      <td>78</td>      <td>0.657</td>      <td>0.192</td>      <td>0.594</td>      <td>0.672</td>      <td>0.301</td>      <td>0.874</td>      <td>0.696</td>      <td>0.399</td>      <td>0.5</td>    </tr>    <tr>      <th>Mass</th>      <td>40</td>      <td>789</td>      <td>158</td>      <td>13</td>      <td>0.829</td>      <td>0.053</td>      <td>0.755</td>      <td>0.833</td>      <td>0.202</td>      <td>0.984</td>      <td>0.888</td>      <td>0.319</td>      <td>0.5</td>    </tr>    <tr>      <th>Nodule</th>      <td>28</td>      <td>731</td>      <td>220</td>      <td>21</td>      <td>0.759</td>      <td>0.049</td>      <td>0.571</td>      <td>0.769</td>      <td>0.113</td>      <td>0.972</td>      <td>0.745</td>      <td>0.189</td>      <td>0.5</td>    </tr>    <tr>      <th>Atelectasis</th>      <td>64</td>      <td>657</td>      <td>249</td>      <td>30</td>      <td>0.721</td>      <td>0.094</td>      <td>0.681</td>      <td>0.725</td>      <td>0.204</td>      <td>0.956</td>      <td>0.781</td>      <td>0.314</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumothorax</th>      <td>24</td>      <td>785</td>      <td>183</td>      <td>8</td>      <td>0.809</td>      <td>0.032</td>      <td>0.75</td>      <td>0.811</td>      <td>0.116</td>      <td>0.99</td>      <td>0.826</td>      <td>0.201</td>      <td>0.5</td>    </tr>    <tr>      <th>Pleural_Thickening</th>      <td>24</td>      <td>713</td>      <td>259</td>      <td>4</td>      <td>0.737</td>      <td>0.028</td>      <td>0.857</td>      <td>0.734</td>      <td>0.085</td>      <td>0.994</td>      <td>0.868</td>      <td>0.154</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumonia</th>      <td>14</td>      <td>661</td>      <td>320</td>      <td>5</td>      <td>0.675</td>      <td>0.019</td>      <td>0.737</td>      <td>0.674</td>      <td>0.042</td>      <td>0.992</td>      <td>0.762</td>      <td>0.079</td>      <td>0.5</td>    </tr>    <tr>      <th>Fibrosis</th>      <td>10</td>      <td>725</td>      <td>261</td>      <td>4</td>      <td>0.735</td>      <td>0.014</td>      <td>0.714</td>      <td>0.735</td>      <td>0.037</td>      <td>0.995</td>      <td>0.801</td>      <td>0.07</td>      <td>0.5</td>    </tr>    <tr>      <th>Edema</th>      <td>15</td>      <td>767</td>      <td>213</td>      <td>5</td>      <td>0.782</td>      <td>0.02</td>      <td>0.75</td>      <td>0.783</td>      <td>0.066</td>      <td>0.994</td>      <td>0.856</td>      <td>0.121</td>      <td>0.5</td>    </tr>    <tr>      <th>Consolidation</th>      <td>36</td>      <td>658</td>      <td>297</td>      <td>9</td>      <td>0.694</td>      <td>0.045</td>      <td>0.8</td>      <td>0.689</td>      <td>0.108</td>      <td>0.987</td>      <td>0.799</td>      <td>0.19</td>      <td>0.5</td>    </tr>  </tbody></table></div><p><a name="7"></a></p><h2 id="7-Calibration"><a href="#7-Calibration" class="headerlink" title="7. Calibration"></a>7. Calibration</h2><p>When performing classification we often want not only to predict the class label, but also obtain a probability of each label. This probability would ideally give us some kind of confidence on the prediction. In order to observe how our model’s generated probabilities are aligned with the real probabilities, we can plot what’s called a <em>calibration curve</em>. </p><p>In order to generate a calibration plot, we first bucketize our predictions to a fixed number of separate bins (e.g. 5) between 0 and 1. We then calculate a point for each bin: the x-value for each point is the mean for the probability that our model has assigned to these points and the y-value for each point fraction of true positives in that bin. We then plot these points in a linear plot. A well-calibrated model has a calibration curve that almost aligns with the y=x line.</p><p>The <code>sklearn</code> library has a utility <code>calibration_curve</code> for generating a calibration plot. Let’s use it and take a look at our model’s calibration:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.calibration <span class="keyword">import</span> calibration_curve</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_calibration_curve</span><span class="params">(y, pred)</span>:</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">20</span>, <span class="number">20</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(class_labels)):</span><br><span class="line">        plt.subplot(<span class="number">4</span>, <span class="number">4</span>, i + <span class="number">1</span>)</span><br><span class="line">        fraction_of_positives, mean_predicted_value = calibration_curve(y[:,i], pred[:,i], n_bins=<span class="number">20</span>)</span><br><span class="line">        plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], linestyle=<span class="string">'--'</span>)</span><br><span class="line">        plt.plot(mean_predicted_value, fraction_of_positives, marker=<span class="string">'.'</span>)</span><br><span class="line">        plt.xlabel(<span class="string">"Predicted Value"</span>)</span><br><span class="line">        plt.ylabel(<span class="string">"Fraction of Positives"</span>)</span><br><span class="line">        plt.title(class_labels[i])</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_calibration_curve(y, pred)</span><br></pre></td></tr></table></figure><p><img src="output_71_0.png" alt="png"></p><p>As the above plots show, for most predictions our model’s calibration plot does not resemble a well calibrated plot. How can we fix that?…</p><p>Thankfully, there is a very useful method called <a href="https://en.wikipedia.org/wiki/Platt_scaling" target="_blank" rel="noopener">Platt scaling</a> which works by fitting a logistic regression model to our model’s scores. To build this model, we will be using the training portion of our dataset to generate the linear model and then will use the model to calibrate the predictions for our test portion.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="keyword">as</span> LR </span><br><span class="line"></span><br><span class="line">y_train = train_results[class_labels].values</span><br><span class="line">pred_train = train_results[pred_labels].values</span><br><span class="line">pred_calibrated = np.zeros_like(pred)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(class_labels)):</span><br><span class="line">    lr = LR(solver=<span class="string">'liblinear'</span>, max_iter=<span class="number">10000</span>)</span><br><span class="line">    lr.fit(pred_train[:, i].reshape(<span class="number">-1</span>, <span class="number">1</span>), y_train[:, i])    </span><br><span class="line">    pred_calibrated[:, i] = lr.predict_proba(pred[:, i].reshape(<span class="number">-1</span>, <span class="number">1</span>))[:,<span class="number">1</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_calibration_curve(y[:,], pred_calibrated)</span><br></pre></td></tr></table></figure><p><img src="output_74_0.png" alt="png"></p><h1 id="That’s-it"><a href="#That’s-it" class="headerlink" title="That’s it!"></a>That’s it!</h1><p>Congratulations! That was a lot of metrics to get familiarized with.<br>We hope that you feel a lot more confident in your understanding of medical diagnostic evaluation and test your models correctly in your future work :)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Evaluation-of-Diagnostic-Models&quot;&gt;&lt;a href=&quot;#Evaluation-of-Diagnostic-Models&quot; class=&quot;headerlink&quot; title=&quot;Evaluation of Diagnostic Model
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Medicine" scheme="https://zhangruochi.com/tags/Medicine/"/>
    
  </entry>
  
  <entry>
    <title>Chest X-Ray Medical Diagnosis with Deep Learning</title>
    <link href="https://zhangruochi.com/Chest-X-Ray-Medical-Diagnosis-with-Deep-Learning/2020/04/15/"/>
    <id>https://zhangruochi.com/Chest-X-Ray-Medical-Diagnosis-with-Deep-Learning/2020/04/15/</id>
    <published>2020-04-16T01:11:55.000Z</published>
    <updated>2020-04-16T01:49:43.444Z</updated>
    
    <content type="html"><![CDATA[<ul><li>This is the assignment of coursera course <a href="https://www.coursera.org/learn/ai-for-medical-diagnosis/" target="_blank" rel="noopener">Medical Diagnosis</a> from deeplearning.ai </li></ul><h1 id="Chest-X-Ray-Medical-Diagnosis-with-Deep-Learning"><a href="#Chest-X-Ray-Medical-Diagnosis-with-Deep-Learning" class="headerlink" title="Chest X-Ray Medical Diagnosis with Deep Learning"></a>Chest X-Ray Medical Diagnosis with Deep Learning</h1><p><img src="output_1.png" style="padding-top: 50px;width: 87%;left: 0px;margin-left: 0px;margin-right: 0px;"></p><p>Welcome to the first assignment of course 1</p><p>In this assignment! You will explore medical image diagnosis by building a state-of-the-art chest X-ray classifier using Keras. </p><p>The assignment will walk through some of the steps of building and evaluating this deep learning classifier model. In particular, you will:</p><ul><li>Pre-process and prepare a real-world X-ray dataset</li><li>Use transfer learning to retrain a DenseNet model for X-ray image classification</li><li>Learn a technique to handle class imbalance</li><li>Measure diagnostic performance by computing the AUC (Area Under the Curve) for the ROC (Receiver Operating Characteristic) curve</li><li>Visualize model activity using GradCAMs</li></ul><p>In completing this assignment you will learn about the following topics: </p><ul><li>Data preparation<ul><li>Visualizing data</li><li>Preventing data leakage</li></ul></li><li>Model Development<ul><li>Addressing class imbalance</li><li>Leveraging pre-trained models using transfer learning</li></ul></li><li>Evaluation<ul><li>AUC and ROC curves</li></ul></li></ul><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><p>Use these links to jump to specific sections of this assignment!</p><ul><li><a href="#1">1. Import Packages and Function</a></li><li><a href="#2">2. Load the Datasets</a><ul><li><a href="#2-1">2.1 Preventing Data Leakage</a><ul><li><a href="#Ex-1">Exercise 1 - Checking Data Leakage</a></li></ul></li><li><a href="#2-2">2.2 Preparing Images</a></li></ul></li><li><a href="#3">3. Model Development</a><ul><li><a href="#3-1">3.1 Addressing Class Imbalance</a><ul><li><a href="#Ex-2">Exercise 2 - Computing Class Frequencies</a></li><li><a href="#Ex-3">Exercise 3 - Weighted Loss</a></li></ul></li><li><a href="#3-3">3.3 DenseNet121</a></li></ul></li><li><a href="#4">4. Training [optional]</a><ul><li><a href="#4-1">4.1 Training on the Larger Dataset</a></li></ul></li><li><a href="#5">5. Prediction and Evaluation</a><ul><li><a href="#5-1">5.1 ROC Curve and AUROC</a></li><li><a href="#5-2">5.2 Visualizing Learning with GradCAM</a></li></ul></li></ul><p><a name="1"></a></p><h2 id="1-Import-Packages-and-Functions¶"><a href="#1-Import-Packages-and-Functions¶" class="headerlink" title="1. Import Packages and Functions¶"></a>1. Import Packages and Functions¶</h2><p>We’ll make use of the following packages:</p><ul><li><code>numpy</code> and <code>pandas</code> is what we’ll use to manipulate our data</li><li><code>matplotlib.pyplot</code> and <code>seaborn</code> will be used to produce plots for visualization</li><li><code>util</code> will provide the locally defined utility functions that have been provided for this assignment</li></ul><p>We will also use several modules from the <code>keras</code> framework for building deep learning models.</p><p>Run the next cell to import all the necessary packages.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"><span class="keyword">from</span> keras.applications.densenet <span class="keyword">import</span> DenseNet121</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, GlobalAveragePooling2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> util</span><br></pre></td></tr></table></figure><p><a name="2"></a></p><h2 id="2-Load-the-Datasets"><a href="#2-Load-the-Datasets" class="headerlink" title="2 Load the Datasets"></a>2 Load the Datasets</h2><p>For this assignment, we will be using the <a href="https://arxiv.org/abs/1705.02315" target="_blank" rel="noopener">ChestX-ray8 dataset</a> which contains 108,948 frontal-view X-ray images of 32,717 unique patients. </p><ul><li>Each image in the data set contains multiple text-mined labels identifying 14 different pathological conditions. </li><li>These in turn can be used by physicians to diagnose 8 different diseases. </li><li>We will use this data to develop a single model that will provide binary classification predictions for each of the 14 labeled pathologies. </li><li>In other words it will predict ‘positive’ or ‘negative’ for each of the pathologies.</li></ul><p>You can download the entire dataset for free <a href="https://nihcc.app.box.com/v/ChestXray-NIHCC" target="_blank" rel="noopener">here</a>. </p><ul><li>We have provided a ~1000 image subset of the images for you.</li><li>These can be accessed in the folder path stored in the <code>IMAGE_DIR</code> variable.</li></ul><p>The dataset includes a CSV file that provides the labels for each X-ray. </p><p>To make your job a bit easier, we have processed the labels for our small sample and generated three new files to get you started. These three files are:</p><ol><li><code>nih/train-small.csv</code>: 875 images from our dataset to be used for training.</li><li><code>nih/valid-small.csv</code>: 109 images from our dataset to be used for validation.</li><li><code>nih/test.csv</code>: 420 images from our dataset to be used for testing. </li></ol><p>This dataset has been annotated by consensus among four different radiologists for 5 of our 14 pathologies:</p><ul><li><code>Consolidation</code></li><li><code>Edema</code></li><li><code>Effusion</code></li><li><code>Cardiomegaly</code></li><li><code>Atelectasis</code></li></ul><h4 id="Sidebar-on-meaning-of-‘class’"><a href="#Sidebar-on-meaning-of-‘class’" class="headerlink" title="Sidebar on meaning of ‘class’"></a>Sidebar on meaning of ‘class’</h4><p>It is worth noting that the word <strong>‘class’</strong> is used in multiple ways is these discussions. </p><ul><li>We sometimes refer to each of the 14 pathological conditions that are labeled in our dataset as a class. </li><li>But for each of those pathologies we are attempting to predict whether a certain condition is present (i.e. positive result) or absent (i.e. negative result). <ul><li>These two possible labels of ‘positive’ or ‘negative’ (or the numerical equivalent of 1 or 0) are also typically referred to as classes. </li></ul></li><li>Moreover, we also use the term in reference to software code ‘classes’ such as <code>ImageDataGenerator</code>.</li></ul><p>As long as you are aware of all this though, it should not cause you any confusion as the term ‘class’ is usually clear from the context in which it is used.</p><h4 id="Read-in-the-data"><a href="#Read-in-the-data" class="headerlink" title="Read in the data"></a>Read in the data</h4><p>Let’s open these files using the <a href="https://pandas.pydata.org/" target="_blank" rel="noopener">pandas</a> library</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_df = pd.read_csv(<span class="string">"nih/train-small.csv"</span>)</span><br><span class="line">valid_df = pd.read_csv(<span class="string">"nih/valid-small.csv"</span>)</span><br><span class="line"></span><br><span class="line">test_df = pd.read_csv(<span class="string">"nih/test.csv"</span>)</span><br><span class="line"></span><br><span class="line">train_df.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Image</th>      <th>Atelectasis</th>      <th>Cardiomegaly</th>      <th>Consolidation</th>      <th>Edema</th>      <th>Effusion</th>      <th>Emphysema</th>      <th>Fibrosis</th>      <th>Hernia</th>      <th>Infiltration</th>      <th>Mass</th>      <th>Nodule</th>      <th>PatientId</th>      <th>Pleural_Thickening</th>      <th>Pneumonia</th>      <th>Pneumothorax</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>00027079_001.png</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>27079</td>      <td>1</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>00004477_001.png</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>4477</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>00018530_002.png</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>18530</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>00026928_001.png</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>26928</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>00016687_000.png</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>1</td>      <td>16687</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_df.shape</span><br></pre></td></tr></table></figure><pre><code>(875, 16)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">labels = [<span class="string">'Cardiomegaly'</span>, </span><br><span class="line">          <span class="string">'Emphysema'</span>, </span><br><span class="line">          <span class="string">'Effusion'</span>, </span><br><span class="line">          <span class="string">'Hernia'</span>, </span><br><span class="line">          <span class="string">'Infiltration'</span>, </span><br><span class="line">          <span class="string">'Mass'</span>, </span><br><span class="line">          <span class="string">'Nodule'</span>, </span><br><span class="line">          <span class="string">'Atelectasis'</span>,</span><br><span class="line">          <span class="string">'Pneumothorax'</span>,</span><br><span class="line">          <span class="string">'Pleural_Thickening'</span>, </span><br><span class="line">          <span class="string">'Pneumonia'</span>, </span><br><span class="line">          <span class="string">'Fibrosis'</span>, </span><br><span class="line">          <span class="string">'Edema'</span>, </span><br><span class="line">          <span class="string">'Consolidation'</span>]</span><br></pre></td></tr></table></figure><p><a name="2-1"></a></p><h3 id="2-1-Preventing-Data-Leakage"><a href="#2-1-Preventing-Data-Leakage" class="headerlink" title="2.1 Preventing Data Leakage"></a>2.1 Preventing Data Leakage</h3><p>It is worth noting that our dataset contains multiple images for each patient. This could be the case, for example, when a patient has taken multiple X-ray images at different times during their hospital visits. In our data splitting, we have ensured that the split is done on the patient level so that there is no data “leakage” between the train, validation, and test datasets.</p><p><a name="Ex-1"></a></p><h3 id="Exercise-1-Checking-Data-Leakage"><a href="#Exercise-1-Checking-Data-Leakage" class="headerlink" title="Exercise 1 - Checking Data Leakage"></a>Exercise 1 - Checking Data Leakage</h3><p>In the cell below, write a function to check whether there is leakage between two datasets. We’ll use this to make sure there are no patients in the test set that are also present in either the train or validation sets.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li> Make use of python's set.intersection() function. </li>    <li> In order to match the automatic grader's expectations, please start the line of code with <code>df1_patients_unique...[continue your code here]</code> </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_for_leakage</span><span class="params">(df1, df2, patient_col)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return True if there any patients are in both df1 and df2.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        df1 (dataframe): dataframe describing first dataset</span></span><br><span class="line"><span class="string">        df2 (dataframe): dataframe describing second dataset</span></span><br><span class="line"><span class="string">        patient_col (str): string name of column with patient IDs</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        leakage (bool): True if there is leakage, otherwise False</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    df1_patients_unique = set(df1[patient_col].unique().tolist())</span><br><span class="line">    df2_patients_unique = set(df2[patient_col].unique().tolist())</span><br><span class="line">    </span><br><span class="line">    patients_in_both_groups = df1_patients_unique.intersection(df2_patients_unique)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># leakage contains true if there is patient overlap, otherwise false.</span></span><br><span class="line">    leakage = len(patients_in_both_groups) &gt;= <span class="number">1</span> <span class="comment"># boolean (true if there is at least 1 patient in both groups)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> leakage</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test</span></span><br><span class="line">print(<span class="string">"test case 1"</span>)</span><br><span class="line">df1 = pd.DataFrame(&#123;<span class="string">'patient_id'</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]&#125;)</span><br><span class="line">df2 = pd.DataFrame(&#123;<span class="string">'patient_id'</span>: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]&#125;)</span><br><span class="line">print(<span class="string">"df1"</span>)</span><br><span class="line">print(df1)</span><br><span class="line">print(<span class="string">"df2"</span>)</span><br><span class="line">print(df2)</span><br><span class="line">print(<span class="string">f"leakage output: <span class="subst">&#123;check_for_leakage(df1, df2, <span class="string">'patient_id'</span>)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">"-------------------------------------"</span>)</span><br><span class="line">print(<span class="string">"test case 2"</span>)</span><br><span class="line">df1 = pd.DataFrame(&#123;<span class="string">'patient_id'</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]&#125;)</span><br><span class="line">df2 = pd.DataFrame(&#123;<span class="string">'patient_id'</span>: [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]&#125;)</span><br><span class="line">print(<span class="string">"df1:"</span>)</span><br><span class="line">print(df1)</span><br><span class="line">print(<span class="string">"df2:"</span>)</span><br><span class="line">print(df2)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"leakage output: <span class="subst">&#123;check_for_leakage(df1, df2, <span class="string">'patient_id'</span>)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>test case 1df1   patient_id0           01           12           2df2   patient_id0           21           32           4leakage output: True-------------------------------------test case 2df1:   patient_id0           01           12           2df2:   patient_id0           31           42           5leakage output: False</code></pre><h5 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">test case <span class="number">1</span></span><br><span class="line">df1</span><br><span class="line">   patient_id</span><br><span class="line"><span class="number">0</span>           <span class="number">0</span></span><br><span class="line"><span class="number">1</span>           <span class="number">1</span></span><br><span class="line"><span class="number">2</span>           <span class="number">2</span></span><br><span class="line">df2</span><br><span class="line">   patient_id</span><br><span class="line"><span class="number">0</span>           <span class="number">2</span></span><br><span class="line"><span class="number">1</span>           <span class="number">3</span></span><br><span class="line"><span class="number">2</span>           <span class="number">4</span></span><br><span class="line">leakage output: <span class="keyword">True</span></span><br><span class="line">-------------------------------------</span><br><span class="line">test case <span class="number">2</span></span><br><span class="line">df1:</span><br><span class="line">   patient_id</span><br><span class="line"><span class="number">0</span>           <span class="number">0</span></span><br><span class="line"><span class="number">1</span>           <span class="number">1</span></span><br><span class="line"><span class="number">2</span>           <span class="number">2</span></span><br><span class="line">df2:</span><br><span class="line">   patient_id</span><br><span class="line"><span class="number">0</span>           <span class="number">3</span></span><br><span class="line"><span class="number">1</span>           <span class="number">4</span></span><br><span class="line"><span class="number">2</span>           <span class="number">5</span></span><br><span class="line">leakage output: <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p>Run the next cell to check if there are patients in both train and test or in both valid and test.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"leakage between train and test: &#123;&#125;"</span>.format(check_for_leakage(train_df, test_df, <span class="string">'PatientId'</span>)))</span><br><span class="line">print(<span class="string">"leakage between valid and test: &#123;&#125;"</span>.format(check_for_leakage(valid_df, test_df, <span class="string">'PatientId'</span>)))</span><br></pre></td></tr></table></figure><pre><code>leakage between train and test: Falseleakage between valid and test: False</code></pre><p>If we get <code>False</code> for both, then we’re ready to start preparing the datasets for training. Remember to always check for data leakage!</p><p><a name="2-2"></a></p><h3 id="2-2-Preparing-Images"><a href="#2-2-Preparing-Images" class="headerlink" title="2.2 Preparing Images"></a>2.2 Preparing Images</h3><p>With our dataset splits ready, we can now proceed with setting up our model to consume them. </p><ul><li>For this we will use the off-the-shelf <a href="https://keras.io/preprocessing/image/" target="_blank" rel="noopener">ImageDataGenerator</a> class from the Keras framework, which allows us to build a “generator” for images specified in a dataframe. </li><li>This class also provides support for basic data augmentation such as random horizontal flipping of images.</li><li>We also use the generator to transform the values in each batch so that their mean is $0$ and their standard deviation is 1. <ul><li>This will facilitate model training by standardizing the input distribution. </li></ul></li><li>The generator also converts our single channel X-ray images (gray-scale) to a three-channel format by repeating the values in the image across all channels.<ul><li>We will want this because the pre-trained model that we’ll use requires three-channel inputs.</li></ul></li></ul><p>Since it is mainly a matter of reading and understanding Keras documentation, we have implemented the generator for you. There are a few things to note: </p><ol><li>We normalize the mean and standard deviation of the data</li><li>We shuffle the input after each epoch.</li><li>We set the image size to be 320px by 320px</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_train_generator</span><span class="params">(df, image_dir, x_col, y_cols, shuffle=True, batch_size=<span class="number">8</span>, seed=<span class="number">1</span>, target_w = <span class="number">320</span>, target_h = <span class="number">320</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return generator for training set, normalizing using batch</span></span><br><span class="line"><span class="string">    statistics.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      train_df (dataframe): dataframe specifying training data.</span></span><br><span class="line"><span class="string">      image_dir (str): directory where image files are held.</span></span><br><span class="line"><span class="string">      x_col (str): name of column in df that holds filenames.</span></span><br><span class="line"><span class="string">      y_cols (list): list of strings that hold y labels for images.</span></span><br><span class="line"><span class="string">      sample_size (int): size of sample to use for normalization statistics.</span></span><br><span class="line"><span class="string">      batch_size (int): images per batch to be fed into model during training.</span></span><br><span class="line"><span class="string">      seed (int): random seed.</span></span><br><span class="line"><span class="string">      target_w (int): final width of input images.</span></span><br><span class="line"><span class="string">      target_h (int): final height of input images.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        train_generator (DataFrameIterator): iterator over training set</span></span><br><span class="line"><span class="string">    """</span>        </span><br><span class="line">    print(<span class="string">"getting train generator..."</span>) </span><br><span class="line">    <span class="comment"># normalize images</span></span><br><span class="line">    image_generator = ImageDataGenerator(</span><br><span class="line">        samplewise_center=<span class="keyword">True</span>,</span><br><span class="line">        samplewise_std_normalization= <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># flow from directory with specified batch size</span></span><br><span class="line">    <span class="comment"># and target image size</span></span><br><span class="line">    generator = image_generator.flow_from_dataframe(</span><br><span class="line">            dataframe=df,</span><br><span class="line">            directory=image_dir,</span><br><span class="line">            x_col=x_col,</span><br><span class="line">            y_col=y_cols,</span><br><span class="line">            class_mode=<span class="string">"raw"</span>,</span><br><span class="line">            batch_size=batch_size,</span><br><span class="line">            shuffle=shuffle,</span><br><span class="line">            seed=seed,</span><br><span class="line">            target_size=(target_w,target_h))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> generator</span><br></pre></td></tr></table></figure><h4 id="Build-a-separate-generator-for-valid-and-test-sets"><a href="#Build-a-separate-generator-for-valid-and-test-sets" class="headerlink" title="Build a separate generator for valid and test sets"></a>Build a separate generator for valid and test sets</h4><p>Now we need to build a new generator for validation and testing data. </p><p><strong>Why can’t we use the same generator as for the training data?</strong></p><p>Look back at the generator we wrote for the training data. </p><ul><li>It normalizes each image <strong>per batch</strong>, meaning that it uses batch statistics. </li><li>We should not do this with the test and validation data, since in a real life scenario we don’t process incoming images a batch at a time (we process one image at a time). </li><li>Knowing the average per batch of test data would effectively give our model an advantage.  <ul><li>The model should not have any information about the test data.</li></ul></li></ul><p>What we need to do is normalize incoming test data using the statistics <strong>computed from the training set</strong>. </p><ul><li>We implement this in the function below. </li><li>There is one technical note. Ideally, we would want to compute our sample mean and standard deviation using the entire training set. </li><li>However, since this is extremely large, that would be very time consuming. </li><li>In the interest of time, we’ll take a random sample of the dataset and calcualte the sample mean and sample standard deviation.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_test_and_valid_generator</span><span class="params">(valid_df, test_df, train_df, image_dir, x_col, y_cols, sample_size=<span class="number">100</span>, batch_size=<span class="number">8</span>, seed=<span class="number">1</span>, target_w = <span class="number">320</span>, target_h = <span class="number">320</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return generator for validation set and test test set using </span></span><br><span class="line"><span class="string">    normalization statistics from training set.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      valid_df (dataframe): dataframe specifying validation data.</span></span><br><span class="line"><span class="string">      test_df (dataframe): dataframe specifying test data.</span></span><br><span class="line"><span class="string">      train_df (dataframe): dataframe specifying training data.</span></span><br><span class="line"><span class="string">      image_dir (str): directory where image files are held.</span></span><br><span class="line"><span class="string">      x_col (str): name of column in df that holds filenames.</span></span><br><span class="line"><span class="string">      y_cols (list): list of strings that hold y labels for images.</span></span><br><span class="line"><span class="string">      sample_size (int): size of sample to use for normalization statistics.</span></span><br><span class="line"><span class="string">      batch_size (int): images per batch to be fed into model during training.</span></span><br><span class="line"><span class="string">      seed (int): random seed.</span></span><br><span class="line"><span class="string">      target_w (int): final width of input images.</span></span><br><span class="line"><span class="string">      target_h (int): final height of input images.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        test_generator (DataFrameIterator) and valid_generator: iterators over test set and validation set respectively</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    print(<span class="string">"getting train and valid generators..."</span>)</span><br><span class="line">    <span class="comment"># get generator to sample dataset</span></span><br><span class="line">    raw_train_generator = ImageDataGenerator().flow_from_dataframe(</span><br><span class="line">        dataframe=train_df, </span><br><span class="line">        directory=IMAGE_DIR, </span><br><span class="line">        x_col=<span class="string">"Image"</span>, </span><br><span class="line">        y_col=labels, </span><br><span class="line">        class_mode=<span class="string">"raw"</span>, </span><br><span class="line">        batch_size=sample_size, </span><br><span class="line">        shuffle=<span class="keyword">True</span>, </span><br><span class="line">        target_size=(target_w, target_h))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get data sample</span></span><br><span class="line">    batch = raw_train_generator.next()</span><br><span class="line">    data_sample = batch[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># use sample to fit mean and std for test set generator</span></span><br><span class="line">    image_generator = ImageDataGenerator(</span><br><span class="line">        featurewise_center=<span class="keyword">True</span>,</span><br><span class="line">        featurewise_std_normalization= <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># fit generator to sample from training data</span></span><br><span class="line">    image_generator.fit(data_sample)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get test generator</span></span><br><span class="line">    valid_generator = image_generator.flow_from_dataframe(</span><br><span class="line">            dataframe=valid_df,</span><br><span class="line">            directory=image_dir,</span><br><span class="line">            x_col=x_col,</span><br><span class="line">            y_col=y_cols,</span><br><span class="line">            class_mode=<span class="string">"raw"</span>,</span><br><span class="line">            batch_size=batch_size,</span><br><span class="line">            shuffle=<span class="keyword">False</span>,</span><br><span class="line">            seed=seed,</span><br><span class="line">            target_size=(target_w,target_h))</span><br><span class="line"></span><br><span class="line">    test_generator = image_generator.flow_from_dataframe(</span><br><span class="line">            dataframe=test_df,</span><br><span class="line">            directory=image_dir,</span><br><span class="line">            x_col=x_col,</span><br><span class="line">            y_col=y_cols,</span><br><span class="line">            class_mode=<span class="string">"raw"</span>,</span><br><span class="line">            batch_size=batch_size,</span><br><span class="line">            shuffle=<span class="keyword">False</span>,</span><br><span class="line">            seed=seed,</span><br><span class="line">            target_size=(target_w,target_h))</span><br><span class="line">    <span class="keyword">return</span> valid_generator, test_generator</span><br></pre></td></tr></table></figure><p>With our generator function ready, let’s make one generator for our training data and one each of our test and  validation datasets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">IMAGE_DIR = <span class="string">"nih/images-small/"</span></span><br><span class="line">train_generator = get_train_generator(train_df, IMAGE_DIR, <span class="string">"Image"</span>, labels)</span><br><span class="line">valid_generator, test_generator= get_test_and_valid_generator(valid_df, test_df, train_df, IMAGE_DIR, <span class="string">"Image"</span>, labels)</span><br></pre></td></tr></table></figure><pre><code>getting train generator.../opt/conda/lib/python3.6/site-packages/keras_preprocessing/image/dataframe_iterator.py:273: UserWarning: Found 866 invalid image filename(s) in x_col=&quot;Image&quot;. These filename(s) will be ignored.  .format(n_invalid, x_col)/opt/conda/lib/python3.6/site-packages/keras_preprocessing/image/dataframe_iterator.py:273: UserWarning: Found 866 invalid image filename(s) in x_col=&quot;Image&quot;. These filename(s) will be ignored.  .format(n_invalid, x_col)Found 9 validated image filenames.getting train and valid generators...Found 9 validated image filenames./opt/conda/lib/python3.6/site-packages/keras_preprocessing/image/dataframe_iterator.py:273: UserWarning: Found 108 invalid image filename(s) in x_col=&quot;Image&quot;. These filename(s) will be ignored.  .format(n_invalid, x_col)Found 1 validated image filenames.Found 420 validated image filenames.</code></pre><p>Let’s peek into what the generator gives our model during training and validation. We can do this by calling the <code>__get_item__(index)</code> function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x, y = train_generator.__getitem__(<span class="number">0</span>)</span><br><span class="line">plt.imshow(x[<span class="number">0</span>]);</span><br></pre></td></tr></table></figure><pre><code>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</code></pre><p><img src="output_28_1.png" alt="png"></p><p><a name="3"></a></p><h2 id="3-Model-Development"><a href="#3-Model-Development" class="headerlink" title="3 Model Development"></a>3 Model Development</h2><p>Now we’ll move on to model training and development. We have a few practical challenges to deal with before actually training a neural network, though. The first is class imbalance.</p><p><a name="3-1"></a></p><h3 id="3-1-Addressing-Class-Imbalance"><a href="#3-1-Addressing-Class-Imbalance" class="headerlink" title="3.1 Addressing Class Imbalance"></a>3.1 Addressing Class Imbalance</h3><p>One of the challenges with working with medical diagnostic datasets is the large class imbalance present in such datasets. Let’s plot the frequency of each of the labels in our dataset:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line">plt.bar(x=labels, height=np.mean(train_generator.labels, axis=<span class="number">0</span>))</span><br><span class="line">plt.title(<span class="string">"Frequency of Each Class"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_31_0.png" alt="png"></p><p>We can see from this plot that the prevalance of positive cases varies significantly across the different pathologies. (These trends mirror the ones in the full dataset as well.) </p><ul><li>The <code>Hernia</code> pathology has the greatest imbalance with the proportion of positive training cases being about 0.2%. </li><li>But even the <code>Infiltration</code> pathology, which has the least amount of imbalance, has only 17.5% of the training cases labelled positive.</li></ul><p>Ideally, we would train our model using an evenly balanced dataset so that the positive and negative training cases would contribute equally to the loss. </p><p>If we use a normal cross-entropy loss function with a highly unbalanced dataset, as we are seeing here, then the algorithm will be incentivized to prioritize the majority class (i.e negative in our case), since it contributes more to the loss. </p><h4 id="Impact-of-class-imbalance-on-loss-function"><a href="#Impact-of-class-imbalance-on-loss-function" class="headerlink" title="Impact of class imbalance on loss function"></a>Impact of class imbalance on loss function</h4><p>Let’s take a closer look at this. Assume we would have used a normal cross-entropy loss for each pathology. We recall that the cross-entropy loss contribution from the $i^{th}$ training data case is:</p><script type="math/tex; mode=display">\mathcal{L}_{cross-entropy}(x_i) = -(y_i \log(f(x_i)) + (1-y_i) \log(1-f(x_i))),</script><p>where $x_i$ and $y_i$ are the input features and the label, and $f(x_i)$ is the output of the model, i.e. the probability that it is positive. </p><p>Note that for any training case, either $y_i=0$ or else $(1-y_i)=0$, so only one of these terms contributes to the loss (the other term is multiplied by zero, and becomes zero). </p><p>We can rewrite the overall average cross-entropy loss over the entire training set $\mathcal{D}$ of size $N$ as follows: </p><script type="math/tex; mode=display">\mathcal{L}_{cross-entropy}(\mathcal{D}) = - \frac{1}{N}\big( \sum_{\text{positive examples}} \log (f(x_i)) + \sum_{\text{negative examples}} \log(1-f(x_i)) \big).</script><p>Using this formulation, we can see that if there is a large imbalance with very few positive training cases, for example, then the loss will be dominated by the negative class. Summing the contribution over all the training cases for each class (i.e. pathological condition), we see that the contribution of each class (i.e. positive or negative) is: </p><script type="math/tex; mode=display">freq_{p} = \frac{\text{number of positive examples}}{N}</script><script type="math/tex; mode=display">\text{and}</script><script type="math/tex; mode=display">freq_{n} = \frac{\text{number of negative examples}}{N}.</script><p><a name="Ex-2"></a></p><h3 id="Exercise-2-Computing-Class-Frequencies"><a href="#Exercise-2-Computing-Class-Frequencies" class="headerlink" title="Exercise 2 - Computing Class Frequencies"></a>Exercise 2 - Computing Class Frequencies</h3><p>Complete the function below to calculate these frequences for each label in our dataset.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li> Use numpy.sum(a, axis=), and choose the axis (0 or 1) </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_class_freqs</span><span class="params">(labels)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute positive and negative frequences for each class.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        labels (np.array): matrix of labels, size (num_examples, num_classes)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        positive_frequencies (np.array): array of positive frequences for each</span></span><br><span class="line"><span class="string">                                         class, size (num_classes)</span></span><br><span class="line"><span class="string">        negative_frequencies (np.array): array of negative frequences for each</span></span><br><span class="line"><span class="string">                                         class, size (num_classes)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># total number of patients (rows)</span></span><br><span class="line">    N = labels.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    positive_frequencies = np.sum(labels, axis=<span class="number">0</span>) / labels.shape[<span class="number">0</span>]</span><br><span class="line">    negative_frequencies = <span class="number">1</span> - positive_frequencies</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> positive_frequencies, negative_frequencies</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test</span></span><br><span class="line">labels_matrix = np.array(</span><br><span class="line">    [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">     [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">     [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">     [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">     [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">)</span><br><span class="line">print(<span class="string">"labels:"</span>)</span><br><span class="line">print(labels_matrix)</span><br><span class="line"></span><br><span class="line">test_pos_freqs, test_neg_freqs = compute_class_freqs(labels_matrix)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"pos freqs: <span class="subst">&#123;test_pos_freqs&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"neg freqs: <span class="subst">&#123;test_neg_freqs&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>labels:[[1 0 0] [0 1 1] [1 0 1] [1 1 1] [1 0 1]]pos freqs: [0.8 0.4 0.8]neg freqs: [0.2 0.6 0.2]</code></pre><h5 id="Expected-output-1"><a href="#Expected-output-1" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">labels:</span><br><span class="line">[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]]</span><br><span class="line">pos freqs: [<span class="number">0.8</span> <span class="number">0.4</span> <span class="number">0.8</span>]</span><br><span class="line">neg freqs: [<span class="number">0.2</span> <span class="number">0.6</span> <span class="number">0.2</span>]</span><br></pre></td></tr></table></figure><p>Now we’ll compute frequencies for our training data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">freq_pos, freq_neg = compute_class_freqs(train_generator.labels)</span><br><span class="line">freq_pos</span><br></pre></td></tr></table></figure><pre><code>array([0.        , 0.11111111, 0.22222222, 0.        , 0.22222222,       0.11111111, 0.        , 0.11111111, 0.        , 0.        ,       0.        , 0.        , 0.        , 0.        ])</code></pre><p>Let’s visualize these two contribution ratios next to each other for each of the pathologies:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = pd.DataFrame(&#123;<span class="string">"Class"</span>: labels, <span class="string">"Label"</span>: <span class="string">"Positive"</span>, <span class="string">"Value"</span>: freq_pos&#125;)</span><br><span class="line">data = data.append([&#123;<span class="string">"Class"</span>: labels[l], <span class="string">"Label"</span>: <span class="string">"Negative"</span>, <span class="string">"Value"</span>: v&#125; <span class="keyword">for</span> l,v <span class="keyword">in</span> enumerate(freq_neg)], ignore_index=<span class="keyword">True</span>)</span><br><span class="line">plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line">f = sns.barplot(x=<span class="string">"Class"</span>, y=<span class="string">"Value"</span>, hue=<span class="string">"Label"</span> ,data=data)</span><br></pre></td></tr></table></figure><p><img src="output_42_0.png" alt="png"></p><p>As we see in the above plot, the contributions of positive cases is significantly lower than that of the negative ones. However, we want the contributions to be equal. One way of doing this is by multiplying each example from each class by a class-specific weight factor, $w_{pos}$ and $w_{neg}$, so that the overall contribution of each class is the same. </p><p>To have this, we want </p><script type="math/tex; mode=display">w_{pos} \times freq_{p} = w_{neg} \times freq_{n},</script><p>which we can do simply by taking </p><script type="math/tex; mode=display">w_{pos} = freq_{neg}</script><script type="math/tex; mode=display">w_{neg} = freq_{pos}</script><p>This way, we will be balancing the contribution of positive and negative labels.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pos_weights = freq_neg</span><br><span class="line">neg_weights = freq_pos</span><br><span class="line">pos_contribution = freq_pos * pos_weights </span><br><span class="line">neg_contribution = freq_neg * neg_weights</span><br></pre></td></tr></table></figure><p>Let’s verify this by graphing the two contributions next to each other again:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = pd.DataFrame(&#123;<span class="string">"Class"</span>: labels, <span class="string">"Label"</span>: <span class="string">"Positive"</span>, <span class="string">"Value"</span>: pos_contribution&#125;)</span><br><span class="line">data = data.append([&#123;<span class="string">"Class"</span>: labels[l], <span class="string">"Label"</span>: <span class="string">"Negative"</span>, <span class="string">"Value"</span>: v&#125; </span><br><span class="line">                        <span class="keyword">for</span> l,v <span class="keyword">in</span> enumerate(neg_contribution)], ignore_index=<span class="keyword">True</span>)</span><br><span class="line">plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line">sns.barplot(x=<span class="string">"Class"</span>, y=<span class="string">"Value"</span>, hue=<span class="string">"Label"</span> ,data=data);</span><br></pre></td></tr></table></figure><p><img src="output_46_0.png" alt="png"></p><p>As the above figure shows, by applying these weightings the positive and negative labels within each class would have the same aggregate contribution to the loss function. Now let’s implement such a loss function. </p><p>After computing the weights, our final weighted loss for each training case will be </p><script type="math/tex; mode=display">\mathcal{L}_{cross-entropy}^{w}(x) = - (w_{p} y \log(f(x)) + w_{n}(1-y) \log( 1 - f(x) ) ).</script><p><a name="Ex-3"></a></p><h3 id="Exercise-3-Weighted-Loss"><a href="#Exercise-3-Weighted-Loss" class="headerlink" title="Exercise 3 - Weighted Loss"></a>Exercise 3 - Weighted Loss</h3><p>Fill out the <code>weighted_loss</code> function below to return a loss function that calculates the weighted loss for each batch. Recall that for the multi-class loss, we add up the average loss for each individual class. Note that we also want to add a small value, $\epsilon$, to the predicted values before taking their logs. This is simply to avoid a numerical error that would otherwise occur if the predicted value happens to be zero.</p><h5 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h5><p>Please use Keras functions to calculate the mean and the log.</p><ul><li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/backend/mean" target="_blank" rel="noopener">Keras.mean</a></li><li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/backend/log" target="_blank" rel="noopener">Keras.log</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weighted_loss</span><span class="params">(pos_weights, neg_weights, epsilon=<span class="number">1e-7</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return weighted loss function given negative weights and positive weights.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      pos_weights (np.array): array of positive weights for each class, size (num_classes)</span></span><br><span class="line"><span class="string">      neg_weights (np.array): array of negative weights for each class, size (num_classes)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      weighted_loss (function): weighted loss function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weighted_loss</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Return weighted loss value. </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes)</span></span><br><span class="line"><span class="string">            y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            loss (Tensor): overall scalar loss summed across all classes</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># initialize loss to zero</span></span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(pos_weights)):</span><br><span class="line">            <span class="comment"># for each class, add average weighted loss for that class </span></span><br><span class="line">            loss += -(K.mean( pos_weights[i] * y_true[:,i] * K.log(y_pred[:,i] + epsilon) + \</span><br><span class="line">                                neg_weights[i] * (<span class="number">1</span> - y_true[:,i]) * K.log(<span class="number">1</span> - y_pred[:,i] + epsilon), axis = <span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> weighted_loss</span><br></pre></td></tr></table></figure><p>Now let’s test our function with some simple cases. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test</span></span><br><span class="line">sess = K.get_session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    print(<span class="string">"Test example:\n"</span>)</span><br><span class="line">    y_true = K.constant(np.array(</span><br><span class="line">        [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">    ))</span><br><span class="line">    print(<span class="string">"y_true:\n"</span>)</span><br><span class="line">    print(y_true.eval())</span><br><span class="line"></span><br><span class="line">    w_p = np.array([<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.5</span>])</span><br><span class="line">    w_n = np.array([<span class="number">0.75</span>, <span class="number">0.75</span>, <span class="number">0.5</span>])</span><br><span class="line">    print(<span class="string">"\nw_p:\n"</span>)</span><br><span class="line">    print(w_p)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"\nw_n:\n"</span>)</span><br><span class="line">    print(w_n)</span><br><span class="line"></span><br><span class="line">    y_pred_1 = K.constant(<span class="number">0.7</span>*np.ones(y_true.shape))</span><br><span class="line">    print(<span class="string">"\ny_pred_1:\n"</span>)</span><br><span class="line">    print(y_pred_1.eval())</span><br><span class="line"></span><br><span class="line">    y_pred_2 = K.constant(<span class="number">0.3</span>*np.ones(y_true.shape))</span><br><span class="line">    print(<span class="string">"\ny_pred_2:\n"</span>)</span><br><span class="line">    print(y_pred_2.eval())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># test with a large epsilon in order to catch errors</span></span><br><span class="line">    L = get_weighted_loss(w_p, w_n, epsilon=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"\nIf we weighted them correctly, we expect the two losses to be the same."</span>)</span><br><span class="line">    L1 = L(y_true, y_pred_1).eval()</span><br><span class="line">    L2 = L(y_true, y_pred_2).eval()</span><br><span class="line">    print(<span class="string">f"\nL(y_pred_1)= <span class="subst">&#123;L1:<span class="number">.4</span>f&#125;</span>, L(y_pred_2)= <span class="subst">&#123;L2:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">    print(<span class="string">f"Difference is L1 - L2 = <span class="subst">&#123;L1 - L2:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test example:y_true:[[1. 1. 1.] [1. 1. 0.] [0. 1. 0.] [1. 0. 1.]]w_p:[0.25 0.25 0.5 ]w_n:[0.75 0.75 0.5 ]y_pred_1:[[0.7 0.7 0.7] [0.7 0.7 0.7] [0.7 0.7 0.7] [0.7 0.7 0.7]]y_pred_2:[[0.3 0.3 0.3] [0.3 0.3 0.3] [0.3 0.3 0.3] [0.3 0.3 0.3]]If we weighted them correctly, we expect the two losses to be the same.L(y_pred_1)= -0.4956, L(y_pred_2)= -0.4956Difference is L1 - L2 = 0.0000</code></pre><h4 id="Additional-check"><a href="#Additional-check" class="headerlink" title="Additional check"></a>Additional check</h4><p>If you implemented the function correctly, then if the epsilon for the <code>get_weighted_loss</code> is set to <code>1</code>, the weighted losses will be as follows:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">L(y_pred_1)= <span class="number">-0.4956</span>, L(y_pred_2)= <span class="number">-0.4956</span></span><br></pre></td></tr></table></figure></p><p>If you are missing something in your implementation, you will see a different set of losses for L1 and L2 (even though L1 and L2 will be the same).</p><p><a name="3-3"></a></p><h3 id="3-3-DenseNet121"><a href="#3-3-DenseNet121" class="headerlink" title="3.3 DenseNet121"></a>3.3 DenseNet121</h3><p>Next, we will use a pre-trained <a href="https://www.kaggle.com/pytorch/densenet121" target="_blank" rel="noopener">DenseNet121</a> model which we can load directly from Keras and then add two layers on top of it:</p><ol><li>A <code>GlobalAveragePooling2D</code> layer to get the average of the last convolution layers from DenseNet121.</li><li>A <code>Dense</code> layer with <code>sigmoid</code> activation to get the prediction logits for each of our classes.</li></ol><p>We can set our custom loss function for the model by specifying the <code>loss</code> parameter in the <code>compile()</code> function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create the base pre-trained model</span></span><br><span class="line">base_model = DenseNet121(weights=<span class="string">'./nih/densenet.hdf5'</span>, include_top=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">x = base_model.output</span><br><span class="line"></span><br><span class="line"><span class="comment"># add a global spatial average pooling layer</span></span><br><span class="line">x = GlobalAveragePooling2D()(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># and a logistic layer</span></span><br><span class="line">predictions = Dense(len(labels), activation=<span class="string">"sigmoid"</span>)(x)</span><br><span class="line"></span><br><span class="line">model = Model(inputs=base_model.input, outputs=predictions)</span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=get_weighted_loss(pos_weights, neg_weights))</span><br></pre></td></tr></table></figure><pre><code>WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.Instructions for updating:If using Keras pass *_constraint arguments to layers.WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.</code></pre><p><a name="4"></a></p><h2 id="4-Training-optional"><a href="#4-Training-optional" class="headerlink" title="4 Training [optional]"></a>4 Training [optional]</h2><p>With our model ready for training, we will use the <code>model.fit()</code> function in Keras to train our model. </p><ul><li>We are training on a small subset of the dataset (~1%).  </li><li>So what we care about at this point is to make sure that the loss on the training set is decreasing.</li></ul><p>Since training can take a considerable time, for pedagogical purposes we have chosen not to train the model here but rather to load a set of pre-trained weights in the next section. However, you can use the code shown below to practice training the model locally on your machine or in Colab.</p><p><strong>NOTE:</strong> Do not run the code below on the Coursera platform as it will exceed the platform’s memory limitations.</p><p>Python Code for training the model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit_generator(train_generator, </span><br><span class="line">                              validation_data=valid_generator,</span><br><span class="line">                              steps_per_epoch=<span class="number">100</span>, </span><br><span class="line">                              validation_steps=<span class="number">25</span>, </span><br><span class="line">                              epochs = <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(history.history[<span class="string">'loss'</span>])</span><br><span class="line">plt.ylabel(<span class="string">"loss"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"epoch"</span>)</span><br><span class="line">plt.title(<span class="string">"Training Loss Curve"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><a name="4-1"></a></p><h3 id="4-1-Training-on-the-Larger-Dataset"><a href="#4-1-Training-on-the-Larger-Dataset" class="headerlink" title="4.1 Training on the Larger Dataset"></a>4.1 Training on the Larger Dataset</h3><p>Given that the original dataset is 40GB+ in size and the training process on the full dataset takes a few hours, we have trained the model on a GPU-equipped machine for you and provided the weights file from our model (with a batch size of 32 instead) to be used for the rest of this assignment. </p><p>The model architecture for our pre-trained model is exactly the same, but we used a few useful Keras “callbacks” for this training. Do spend time to read about these callbacks at your leisure as they will be very useful for managing long-running training sessions:</p><ol><li>You can use <code>ModelCheckpoint</code> callback to monitor your model’s <code>val_loss</code> metric and keep a snapshot of your model at the point. </li><li>You can use the <code>TensorBoard</code> to use the Tensorflow Tensorboard utility to monitor your runs in real-time. </li><li>You can use the <code>ReduceLROnPlateau</code> to slowly decay the learning rate for your model as it stops getting better on a metric such as <code>val_loss</code> to fine-tune the model in the final steps of training.</li><li>You can use the <code>EarlyStopping</code> callback to stop the training job when your model stops getting better in it’s validation loss. You can set a <code>patience</code> value which is the number of epochs the model does not improve after which the training is terminated. This callback can also conveniently restore the weights for the best metric at the end of training to your model.</li></ol><p>You can read about these callbacks and other useful Keras callbacks <a href="https://keras.io/callbacks/" target="_blank" rel="noopener">here</a>.</p><p>Let’s load our pre-trained weights into the model now:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_weights(<span class="string">"./nih/pretrained_model.h5"</span>)</span><br></pre></td></tr></table></figure><p><a name="5"></a></p><h2 id="5-Prediction-and-Evaluation"><a href="#5-Prediction-and-Evaluation" class="headerlink" title="5 Prediction and Evaluation"></a>5 Prediction and Evaluation</h2><p>Now that we have a model, let’s evaluate it using our test set. We can conveniently use the <code>predict_generator</code> function to generate the predictions for the images in our test set.</p><p><strong>Note:</strong> The following cell can take about 4 minutes to run.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predicted_vals = model.predict_generator(test_generator, steps = len(test_generator))</span><br></pre></td></tr></table></figure><pre><code>WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.</code></pre><p><a name="5-1"></a></p><h3 id="5-1-ROC-Curve-and-AUROC"><a href="#5-1-ROC-Curve-and-AUROC" class="headerlink" title="5.1 ROC Curve and AUROC"></a>5.1 ROC Curve and AUROC</h3><p>We’ll cover topic of model evaluation in much more detail in later weeks, but for now we’ll walk through computing a metric called the AUC (Area Under the Curve) from the ROC (<a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank" rel="noopener">Receiver Operating Characteristic</a>) curve. This is also referred to as the AUROC value, but you will see all three terms in reference to the technique, and often used almost interchangeably. </p><p>For now, what you need to know in order to interpret the plot is that a curve that is more to the left and the top has more “area” under it, and indicates that the model is performing better.</p><p>We will use the <code>util.get_roc_curve()</code> function which has been provided for you in <code>util.py</code>. Look through this function and note the use of the <code>sklearn</code> library functions to generate the ROC curves and AUROC values for our model. </p><ul><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html" target="_blank" rel="noopener">roc_curve</a></li><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html" target="_blank" rel="noopener">roc_auc_score</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">auc_rocs = util.get_roc_curve(labels, predicted_vals, test_generator)</span><br></pre></td></tr></table></figure><p><img src="output_62_0.png" alt="png"></p><p>You can compare the performance to the AUCs reported in the original ChexNeXt paper in the table below: </p><p>For reference, here’s the AUC figure from the ChexNeXt paper which includes AUC values for their model as well as radiologists on this dataset:</p><p><img src="https://journals.plos.org/plosmedicine/article/figure/image?size=large&id=10.1371/journal.pmed.1002686.t001" width="80%"></p><p>This method does take advantage of a few other tricks such as self-training and ensembling as well, which can give a significant boost to the performance.</p><p>For details about the best performing methods and their performance on this dataset, we encourage you to read the following papers:</p><ul><li><a href="https://arxiv.org/abs/1711.05225" target="_blank" rel="noopener">CheXNet</a></li><li><a href="https://arxiv.org/pdf/1901.07031.pdf" target="_blank" rel="noopener">CheXpert</a></li><li><a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002686" target="_blank" rel="noopener">ChexNeXt</a></li></ul><p><a name="5-2"></a></p><h3 id="5-2-Visualizing-Learning-with-GradCAM"><a href="#5-2-Visualizing-Learning-with-GradCAM" class="headerlink" title="5.2 Visualizing Learning with GradCAM"></a>5.2 Visualizing Learning with GradCAM</h3><p>One of the challenges of using deep learning in medicine is that the complex architecture used for neural networks makes them much harder to interpret compared to traditional machine learning models (e.g. linear models). </p><p>One of the most common approaches aimed at increasing the interpretability of models for computer vision tasks is to use Class Activation Maps (CAM). </p><ul><li>Class activation maps are useful for understanding where the model is “looking” when classifying an image. </li></ul><p>In this section we will use a <a href="https://arxiv.org/abs/1610.02391" target="_blank" rel="noopener">GradCAM’s</a> technique to produce a heatmap highlighting the important regions in the image for predicting the pathological condition. </p><ul><li>This is done by extracting the gradients of each predicted class, flowing into our model’s final convolutional layer. Look at the <code>util.compute_gradcam</code> which has been provided for you in <code>util.py</code> to see how this is done with the Keras framework. </li></ul><p>It is worth mentioning that GradCAM does not provide a full explanation of the reasoning for each classification probability. </p><ul><li>However, it is still a useful tool for “debugging” our model and augmenting our prediction so that an expert could validate that a prediction is indeed due to the model focusing on the right regions of the image.</li></ul><p>First we will load the small training set and setup to look at the 4 classes with the highest performing AUC measures.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">"nih/train-small.csv"</span>)</span><br><span class="line">IMAGE_DIR = <span class="string">"nih/images-small/"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># only show the lables with top 4 AUC</span></span><br><span class="line">labels_to_show = np.take(labels, np.argsort(auc_rocs)[::<span class="number">-1</span>])[:<span class="number">4</span>]</span><br></pre></td></tr></table></figure><p>Now let’s look at a few specific images.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.compute_gradcam(model, <span class="string">'00008270_015.png'</span>, IMAGE_DIR, df, labels, labels_to_show)</span><br></pre></td></tr></table></figure><pre><code>Loading original imageGenerating gradcam for class CardiomegalyGenerating gradcam for class MassGenerating gradcam for class PneumothoraxGenerating gradcam for class Edema</code></pre><p><img src="output_71_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.compute_gradcam(model, <span class="string">'00011355_002.png'</span>, IMAGE_DIR, df, labels, labels_to_show)</span><br></pre></td></tr></table></figure><pre><code>Loading original imageGenerating gradcam for class CardiomegalyGenerating gradcam for class MassGenerating gradcam for class PneumothoraxGenerating gradcam for class Edema</code></pre><p><img src="output_72_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.compute_gradcam(model, <span class="string">'00029855_001.png'</span>, IMAGE_DIR, df, labels, labels_to_show)</span><br></pre></td></tr></table></figure><pre><code>Loading original imageGenerating gradcam for class CardiomegalyGenerating gradcam for class MassGenerating gradcam for class PneumothoraxGenerating gradcam for class Edema</code></pre><p><img src="output_73_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.compute_gradcam(model, <span class="string">'00005410_000.png'</span>, IMAGE_DIR, df, labels, labels_to_show)</span><br></pre></td></tr></table></figure><pre><code>Loading original imageGenerating gradcam for class CardiomegalyGenerating gradcam for class MassGenerating gradcam for class PneumothoraxGenerating gradcam for class Edema</code></pre><p><img src="output_74_1.png" alt="png"></p><p>Congratulations, you’ve completed the first assignment of course one! You’ve learned how to preprocess data, check for data leakage, train a pre-trained model, and evaluate using the AUC. Great work!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;This is the assignment of coursera course &lt;a href=&quot;https://www.coursera.org/learn/ai-for-medical-diagnosis/&quot; target=&quot;_blank&quot; rel=&quot;n
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Medicine" scheme="https://zhangruochi.com/tags/Medicine/"/>
    
  </entry>
  
  <entry>
    <title>Deploy a keras model on IBM cloud</title>
    <link href="https://zhangruochi.com/Deploy-a-keras-model-on-IBM-cloud/2020/04/15/"/>
    <id>https://zhangruochi.com/Deploy-a-keras-model-on-IBM-cloud/2020/04/15/</id>
    <published>2020-04-15T23:11:05.000Z</published>
    <updated>2020-04-18T02:58:47.642Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Zero-to-Singularity-Create-Tune-Deploy-and-Scale-a-Deep-Neural-Network-in-90-Minutes"><a href="#Zero-to-Singularity-Create-Tune-Deploy-and-Scale-a-Deep-Neural-Network-in-90-Minutes" class="headerlink" title="Zero to Singularity: Create, Tune, Deploy and Scale a Deep Neural Network in 90 Minutes"></a>Zero to Singularity: Create, Tune, Deploy and Scale a Deep Neural Network in 90 Minutes</h1><p>This notebook is part of a masterclass held at IBM Think on 13th of February 2019 in San Fransisco<br>In this exercise you will train a Keras DeepLearning model running on top of TensorFlow.</p><p>Note: For sake of bringing the training runtime down we’ve done two things</p><p>1) Used a softmax regression model over a Convolutional Neural Network </p><p>2) Trained only for one epoch instead of 20</p><p>This leads to approx. 5% less accuracy</p><p>Authors</p><p>Romeo Kienzler - Chief Data Scientist, IBM Watson IoT</p><p>Krishnamurthy Arthanarisamy - Architect, Watson Machine Learning Software Lab, Bangalore</p><h1 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h1><p>Please make sure the currently installed version of Keras and Tensorflow are matching the requirememts, if not, please run the two PIP commands below in order to re-install. Please restart the kernal before proceeding, please re-check if the versions are matching.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line">print(<span class="string">'Current:\t'</span>, keras.__version__)</span><br><span class="line">print(<span class="string">'Expected:\t 2.2.5 '</span>)</span><br></pre></td></tr></table></figure><pre><code>Using TensorFlow backend.Current:     2.2.4Expected:    2.2.5 </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(<span class="string">'Current:\t'</span>, tf.__version__)</span><br><span class="line">print(<span class="string">'Expected:\t 1.15.0'</span>)</span><br></pre></td></tr></table></figure><pre><code>Current:     1.13.1Expected:    1.15.0</code></pre><h1 id="IMPORTANT"><a href="#IMPORTANT" class="headerlink" title="IMPORTANT !!!"></a>IMPORTANT !!!</h1><p>If you ran the two lines below please restart your kernel (Kernel-&gt;Restart &amp; Clear Output)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!pip install keras==<span class="number">2.2</span><span class="number">.5</span> </span><br><span class="line">!pip install tensorflow==<span class="number">1.15</span><span class="number">.0</span></span><br></pre></td></tr></table></figure><h1 id="1-0-Train-a-MNIST-digits-recognition-model"><a href="#1-0-Train-a-MNIST-digits-recognition-model" class="headerlink" title="1.0 Train a MNIST digits recognition model"></a>1.0 Train a MNIST digits recognition model</h1><p>We start with some global parameters and imports</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#some learners constantly reported 502 errors in Watson Studio. </span></span><br><span class="line"><span class="comment">#This is due to the limited resources in the free tier and the heavy resource consumption of Keras.</span></span><br><span class="line"><span class="comment">#This is a workaround to limit resource consumption</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"></span><br><span class="line">K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=<span class="number">1</span>, inter_op_parallelism_threads=<span class="number">1</span>)))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Dropout, Flatten</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D, MaxPooling2D</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential, load_model</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> LeakyReLU</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line">epochs = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the data, split between train and test sets</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line"></span><br><span class="line">x_train = x_train.reshape(<span class="number">60000</span>, <span class="number">784</span>)</span><br><span class="line">x_test = x_test.reshape(<span class="number">10000</span>, <span class="number">784</span>)</span><br><span class="line">x_train = x_train.astype(<span class="string">'float32'</span>)</span><br><span class="line">x_test = x_test.astype(<span class="string">'float32'</span>)</span><br><span class="line">x_train /= <span class="number">255</span></span><br><span class="line">x_test /= <span class="number">255</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># convert class vectors to binary class matrices</span></span><br><span class="line">y_train = keras.utils.to_categorical(y_train, num_classes)</span><br><span class="line">y_test = keras.utils.to_categorical(y_test, num_classes)</span><br></pre></td></tr></table></figure><pre><code>Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz11493376/11490434 [==============================] - 0s 0us/step</code></pre><h1 id="Training-a-simple-model"><a href="#Training-a-simple-model" class="headerlink" title="Training a simple model"></a>Training a simple model</h1><p>First we’ll train a simple softmax regressor and check what accuracy we get</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">512</span>, input_shape=(<span class="number">784</span>,)))</span><br><span class="line">model.add(Dense(num_classes, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">        optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">        metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(x_train, y_train,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        epochs=epochs,</span><br><span class="line">        verbose=<span class="number">1</span>,</span><br><span class="line">        validation_data=(x_test, y_test))</span><br><span class="line">        </span><br><span class="line">score = model.evaluate(x_test, y_test, verbose=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line">print(<span class="string">'Accuracy:'</span>,score[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><pre><code>WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.Instructions for updating:Colocations handled automatically by placer.WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.Instructions for updating:Use tf.cast instead.Train on 60000 samples, validate on 10000 samplesEpoch 1/160000/60000 [==============================] - 14s 230us/step - loss: 0.3857 - acc: 0.8886 - val_loss: 0.3232 - val_acc: 0.9110Accuracy: 0.911</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#some cleanup from the previous run</span></span><br><span class="line">!rm -f ker_*</span><br><span class="line">!rm -f my_best_model.tgz</span><br></pre></td></tr></table></figure><p>You should see an accuracy of approximately 90%. Now lets define a hyper-parameter grid including different activation functions and gradient descent optimizers. We’re optimizing over the grid using grid search (nested for loops) and store each model variant in a file. We then decide for the best one in order to deploy to IBM Watson Machine Learning.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#define parameter grid</span></span><br><span class="line"></span><br><span class="line">activation_functions_layer_1 = [<span class="string">'sigmoid'</span>,<span class="string">'tanh'</span>,<span class="string">'relu'</span>]</span><br><span class="line">opimizers = [<span class="string">'rmsprop'</span>,<span class="string">'adagrad'</span>,<span class="string">'adadelta'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#optimize over parameter grid (grid search)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> activation_function_layer_1 <span class="keyword">in</span> activation_functions_layer_1:</span><br><span class="line">    <span class="keyword">for</span> opimizer <span class="keyword">in</span> opimizers:</span><br><span class="line">        </span><br><span class="line">        model = Sequential()</span><br><span class="line">        model.add(Dense(<span class="number">512</span>, activation = activation_function_layer_1, input_shape=(<span class="number">784</span>,)))</span><br><span class="line">        model.add(Dense(num_classes, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              optimizer=opimizer,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">        model.fit(x_train, y_train,</span><br><span class="line">              batch_size=batch_size,</span><br><span class="line">              epochs=epochs,</span><br><span class="line">              verbose=<span class="number">1</span>,</span><br><span class="line">              validation_data=(x_test, y_test))</span><br><span class="line">        </span><br><span class="line">        score = model.evaluate(x_test, y_test, verbose=<span class="number">0</span>)</span><br><span class="line">        save_path = <span class="string">"ker_func_mnist_model_2.%s.%s.%s.h5"</span> % (activation_function_layer_1,opimizer,score[<span class="number">1</span>])</span><br><span class="line">        model.save(save_path)</span><br></pre></td></tr></table></figure><pre><code>Train on 60000 samples, validate on 10000 samplesEpoch 1/160000/60000 [==============================] - 16s 269us/step - loss: 0.4272 - acc: 0.8839 - val_loss: 0.2807 - val_acc: 0.9172Train on 60000 samples, validate on 10000 samplesEpoch 1/160000/60000 [==============================] - 14s 233us/step - loss: 0.4151 - acc: 0.8871 - val_loss: 0.2874 - val_acc: 0.9162Train on 60000 samples, validate on 10000 samplesEpoch 1/160000/60000 [==============================] - 16s 261us/step - loss: 0.5419 - acc: 0.8611 - val_loss: 0.3225 - val_acc: 0.9081Train on 60000 samples, validate on 10000 samplesEpoch 1/160000/60000 [==============================] - 14s 238us/step - loss: 0.3358 - acc: 0.9009 - val_loss: 0.2107 - val_acc: 0.9391Train on 60000 samples, validate on 10000 samplesEpoch 1/160000/60000 [==============================] - 14s 239us/step - loss: 0.3241 - acc: 0.9076 - val_loss: 0.2284 - val_acc: 0.9358Train on 60000 samples, validate on 10000 samplesEpoch 1/160000/60000 [==============================] - 18s 308us/step - loss: 0.3640 - acc: 0.8943 - val_loss: 0.2737 - val_acc: 0.9186Train on 60000 samples, validate on 10000 samplesEpoch 1/160000/60000 [==============================] - 22s 374us/step - loss: 0.2554 - acc: 0.9270 - val_loss: 0.1215 - val_acc: 0.9625Train on 60000 samples, validate on 10000 samplesEpoch 1/160000/60000 [==============================] - 22s 367us/step - loss: 0.2377 - acc: 0.9315 - val_loss: 0.1378 - val_acc: 0.9588Train on 60000 samples, validate on 10000 samplesEpoch 1/160000/60000 [==============================] - 24s 402us/step - loss: 0.2918 - acc: 0.9167 - val_loss: 0.1622 - val_acc: 0.9528</code></pre><h1 id="Model-evaluation"><a href="#Model-evaluation" class="headerlink" title="Model evaluation"></a>Model evaluation</h1><p>Let’s have a look at all the models and see which hyper parameter configuration was the best one. You should see that relu and rmsprop gives you &gt; 95% of accuracy on the validation set</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -ltr *</span><br></pre></td></tr></table></figure><pre><code>-rw-r----- 1 dsxuser dsxuser    2289 Apr 15 22:31 rklib.py-rw-r----- 1 dsxuser dsxuser 3276560 Apr 15 22:33 ker_func_mnist_model_2.sigmoid.rmsprop.0.9172.h5-rw-r----- 1 dsxuser dsxuser 3276560 Apr 15 22:33 ker_func_mnist_model_2.sigmoid.adagrad.0.9162.h5-rw-r----- 1 dsxuser dsxuser 4905120 Apr 15 22:34 ker_func_mnist_model_2.sigmoid.adadelta.0.9081.h5-rw-r----- 1 dsxuser dsxuser 3276560 Apr 15 22:34 ker_func_mnist_model_2.tanh.rmsprop.0.9391.h5-rw-r----- 1 dsxuser dsxuser 3276568 Apr 15 22:34 ker_func_mnist_model_2.tanh.adagrad.0.9358.h5-rw-r----- 1 dsxuser dsxuser 4905128 Apr 15 22:35 ker_func_mnist_model_2.tanh.adadelta.0.9186.h5-rw-r----- 1 dsxuser dsxuser 3276568 Apr 15 22:35 ker_func_mnist_model_2.relu.rmsprop.0.9625.h5-rw-r----- 1 dsxuser dsxuser 3276568 Apr 15 22:35 ker_func_mnist_model_2.relu.adagrad.0.9588.h5-rw-r----- 1 dsxuser dsxuser 4905128 Apr 15 22:36 ker_func_mnist_model_2.relu.adadelta.0.9528.h5-rw-r----- 1 dsxuser dsxuser 2887392 Apr 15 22:38 my_best_model.tgzsystemml:total 0__pycache__:total 4-rw-r----- 1 dsxuser dsxuser 1584 Apr 15 22:26 rklib.cpython-36.pycscratch_space:total 0mkdir :total 4drwxr-x--- 3 dsxuser dsxuser 4096 Apr 15 22:30</code></pre><p>Now it’s time to create a tarball out of your favorite model, please replace the name of your favorite model H5 file with “please-put-me-here”</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!tar -zcvf my_best_model.tgz ker_func_mnist_model_2.relu.rmsprop<span class="number">.0</span><span class="number">.9625</span>.h5</span><br></pre></td></tr></table></figure><pre><code>ker_func_mnist_model_2.relu.rmsprop.0.9625.h5</code></pre><h2 id="2-0-Save-the-trained-model-to-WML-Repository"><a href="#2-0-Save-the-trained-model-to-WML-Repository" class="headerlink" title="2.0 Save the trained model to WML Repository"></a>2.0 Save the trained model to WML Repository</h2><p>We will use <code>watson_machine_learning_client</code> python library to save the trained model to WML Repository, to deploy the saved model and to make predictions using the deployed model.&lt;/br&gt;</p><p><code>watson_machine_learning_client</code> can be installed using the following <code>pip</code> command in case you are running outside Watson Studio:</p><p><code>!pip install watson-machine-learning-client --upgrade</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> watson_machine_learning_client <span class="keyword">import</span> WatsonMachineLearningAPIClient</span><br></pre></td></tr></table></figure><pre><code>2020-04-15 22:57:03,361 - watson_machine_learning_client.metanames - WARNING - &#39;AUTHOR_EMAIL&#39; meta prop is deprecated. It will be ignored.</code></pre><p>Please go to <a href="https://cloud.ibm.com/" target="_blank" rel="noopener">https://cloud.ibm.com/</a>,  login, click on the “Create Resource” button. From the “AI” category, please choose “Machine Learning”. Wait for the “Create” button to activate and click on “Create”. Click on “Service Credentials”, then “New Credential”, then “Add”. From the new entry in the table, under “ACTIONS”, please click on “View Credentials”. Please copy the whole JSON object to your clipboard. Now just paste the JSON object below so that you are able to use your personal instance of Watson Machine Learning.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">wml_credentials=&#123;</span><br><span class="line">  <span class="string">"apikey"</span>: <span class="string">"4EdHtgVIK5DfjhpOmUT8Tr9i5ad-9oWYMiyClbj0PUxJ"</span>,</span><br><span class="line">  <span class="string">"iam_apikey_description"</span>: <span class="string">"Auto-generated for key 9fa77406-bf31-478f-bfc9-78d8d16a0bd4"</span>,</span><br><span class="line">  <span class="string">"iam_apikey_name"</span>: <span class="string">"Service credentials-1"</span>,</span><br><span class="line">  <span class="string">"iam_role_crn"</span>: <span class="string">"crn:v1:bluemix:public:iam::::serviceRole:Writer"</span>,</span><br><span class="line">  <span class="string">"iam_serviceid_crn"</span>: <span class="string">"crn:v1:bluemix:public:iam-identity::a/dd8b066c8db947e8a36d2ac8acffee25::serviceid:ServiceId-b4b64d34-d93c-42dc-b024-af27a9ade4da"</span>,</span><br><span class="line">  <span class="string">"instance_id"</span>: <span class="string">"07660d0f-85d9-48fe-bc53-fe800754f9f8"</span>,</span><br><span class="line">  <span class="string">"url"</span>: <span class="string">"https://us-south.ml.cloud.ibm.com"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">client = WatsonMachineLearningAPIClient(wml_credentials)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model_props = &#123;client.repository.ModelMetaNames.AUTHOR_NAME: <span class="string">"RCZHANG"</span>, </span><br><span class="line">               client.repository.ModelMetaNames.AUTHOR_EMAIL: <span class="string">"lvduzhen@gmail.com"</span>, </span><br><span class="line">               client.repository.ModelMetaNames.NAME: <span class="string">"KK3_clt_keras_mnist"</span>,</span><br><span class="line">               client.repository.ModelMetaNames.FRAMEWORK_NAME: <span class="string">"tensorflow"</span>,</span><br><span class="line">               client.repository.ModelMetaNames.FRAMEWORK_VERSION: <span class="string">"1.15"</span> ,</span><br><span class="line">               client.repository.ModelMetaNames.FRAMEWORK_LIBRARIES: [&#123;<span class="string">"name"</span>: <span class="string">"keras"</span>, <span class="string">"version"</span>: <span class="string">"2.2.5"</span>&#125;]</span><br><span class="line">              &#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">published_model = client.repository.store_model(model=<span class="string">"my_best_model.tgz"</span>, meta_props=model_props)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">published_model_uid = client.repository.get_model_uid(published_model)</span><br><span class="line">model_details = client.repository.get_details(published_model_uid)</span><br></pre></td></tr></table></figure><h2 id="3-0-Deploy-the-Keras-model"><a href="#3-0-Deploy-the-Keras-model" class="headerlink" title="3.0 Deploy the Keras model"></a>3.0 Deploy the Keras model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">client.deployments.list()</span><br></pre></td></tr></table></figure><pre><code>------------------------------------  -------------------  ------  --------------  ------------------------  ---------------  -------------GUID                                  NAME                 TYPE    STATE           CREATED                   FRAMEWORK        ARTIFACT TYPEe69bb10d-67ea-4fa9-9788-4bbf80edac85  k1_keras_mnist_clt1  online  DEPLOY_SUCCESS  2020-04-15T22:59:44.929Z  tensorflow-1.15  model------------------------------------  -------------------  ------  --------------  ------------------------  ---------------  -------------</code></pre><p>To keep your environment clean, just delete all deployments from previous runs</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># client.deployments.delete("PASTE_YOUR_GUID_HERE_IF_APPLICABLE")</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">created_deployment = client.deployments.create(published_model_uid, name=<span class="string">"k1_keras_mnist_clt1"</span>)</span><br></pre></td></tr></table></figure><pre><code>#######################################################################################Synchronous deployment creation for uid: &#39;cc79b7b9-ae8b-4cf1-b7a3-165b104867ab&#39; started#######################################################################################INITIALIZINGDEPLOY_IN_PROGRESS...DEPLOY_SUCCESS------------------------------------------------------------------------------------------------Successfully finished deployment creation, deployment_uid=&#39;e69bb10d-67ea-4fa9-9788-4bbf80edac85&#39;------------------------------------------------------------------------------------------------</code></pre><h2 id="Test-the-model"><a href="#Test-the-model" class="headerlink" title="Test the model"></a>Test the model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#scoring_endpoint = client.deployments.get_scoring_url(created_deployment)</span></span><br><span class="line">scoring_endpoint = created_deployment[<span class="string">'entity'</span>][<span class="string">'scoring_url'</span>]</span><br><span class="line">print(scoring_endpoint)</span><br></pre></td></tr></table></figure><pre><code>https://us-south.ml.cloud.ibm.com/v3/wml_instances/07660d0f-85d9-48fe-bc53-fe800754f9f8/deployments/e69bb10d-67ea-4fa9-9788-4bbf80edac85/online</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_score_1 = x_test[<span class="number">23</span>].tolist()</span><br><span class="line">print(<span class="string">'The answer should be: '</span>,np.argmax(y_test[<span class="number">23</span>]))</span><br><span class="line">scoring_payload = &#123;<span class="string">'values'</span>: [x_score_1]&#125;</span><br></pre></td></tr></table></figure><pre><code>The answer should be:  5</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predictions = client.deployments.score(scoring_endpoint, scoring_payload)</span><br><span class="line">print(<span class="string">'And the answer is!... '</span>,predictions[<span class="string">'values'</span>][<span class="number">0</span>][<span class="number">1</span>])</span><br></pre></td></tr></table></figure><pre><code>And the answer is!...  5</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Zero-to-Singularity-Create-Tune-Deploy-and-Scale-a-Deep-Neural-Network-in-90-Minutes&quot;&gt;&lt;a href=&quot;#Zero-to-Singularity-Create-Tune-Depl
      
    
    </summary>
    
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Deployment" scheme="https://zhangruochi.com/categories/AI-Workflow/Deployment/"/>
    
    
  </entry>
  
  <entry>
    <title>Fourier Transform</title>
    <link href="https://zhangruochi.com/Fourier-Transform/2020/04/14/"/>
    <id>https://zhangruochi.com/Fourier-Transform/2020/04/14/</id>
    <published>2020-04-15T00:24:47.000Z</published>
    <updated>2020-04-15T05:06:10.340Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Fourier-Transform"><a href="#Fourier-Transform" class="headerlink" title="Fourier Transform"></a>Fourier Transform</h2><p>The Fourier transform (FT) decomposes a function (often a function of time, or a signal) into its constituent frequencies.</p><p>One motivation for the Fourier transform comes from the study of Fourier series. In the study of <strong>Fourier series</strong>, complicated but periodic functions are written as the sum of simple waves mathematically represented by sines and cosines. The Fourier transform is an extension of the Fourier series that results when the period of the represented function is lengthened and allowed to approach infinity.</p><p>The Fourier transform of a function f is traditionally denoted $\hat{f}$, by adding a circumflex to the symbol of the function. There are several common conventions for defining the Fourier transform of an integrable function</p><script type="math/tex; mode=display">\hat{f}(\xi) = \int_{-\infty}^{\infty} f(x)\ e^{-2\pi i x \xi}\,dx</script><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%"></center><p>If we want to describe a signal, we need three things :</p><ul><li>The <strong>frequency</strong> of the signal which shows, how many occurrences in the period we have.</li><li><strong>Amplitude</strong> which shows the height of the signal or in other terms the strength of the signal.</li><li><strong>Phase shift</strong> as to where does the signal starts.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> numpy.fft <span class="keyword">import</span> fft</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_wave</span> <span class="params">(freq, amp, T, sr)</span>:</span></span><br><span class="line"></span><br><span class="line">    time = np.arange(<span class="number">0</span>,T,<span class="number">1</span>/sr)</span><br><span class="line">    </span><br><span class="line">    X = amp*np.sin(<span class="number">2</span>*np.pi*freq*time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> time,X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.style.use(<span class="string">"seaborn"</span>)</span><br><span class="line">plt.rcParams[<span class="string">"xtick.labelsize"</span>] = <span class="number">14</span></span><br><span class="line">plt.rcParams[<span class="string">"ytick.labelsize"</span>] = <span class="number">14</span></span><br><span class="line"></span><br><span class="line">f, axarr = plt.subplots(<span class="number">2</span>, figsize=(<span class="number">20</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">sr=<span class="number">50</span> <span class="comment">#in Hz</span></span><br><span class="line"></span><br><span class="line">x,y   = gen_wave(<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,sr)</span><br><span class="line">x,y2   = gen_wave(<span class="number">5</span>,<span class="number">3</span>,<span class="number">1</span>,sr)</span><br><span class="line"></span><br><span class="line">y = y + y2</span><br><span class="line"></span><br><span class="line">axarr[<span class="number">0</span>].plot(x, y)</span><br><span class="line"></span><br><span class="line">n = len(y) </span><br><span class="line">p = fft(y) <span class="comment"># take the fourier transform </span></span><br><span class="line"></span><br><span class="line">mag = np.sqrt(p.real**<span class="number">2</span> + p.imag**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">mag = mag * <span class="number">2</span> / n</span><br><span class="line"></span><br><span class="line">mag = mag[<span class="number">0</span>:math.ceil((n)/<span class="number">2.0</span>)]</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>, len(mag), <span class="number">1.0</span>) * (sr / n)</span><br><span class="line"></span><br><span class="line">axarr[<span class="number">1</span>].bar(x, mag, color=<span class="string">'b'</span>)</span><br><span class="line">axarr[<span class="number">1</span>].xaxis.set_ticks(np.arange(min(x), max(x)+<span class="number">1</span>, <span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_2_0.png" alt="png"></p><h2 id="Decompose-the-cord"><a href="#Decompose-the-cord" class="headerlink" title="Decompose the cord"></a>Decompose the cord</h2><p>A special case is the expression of a musical chord in terms of the volumes and frequencies of its constituent notes.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(C) 2018 Nikolay Manchev</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This work is licensed under the Creative Commons Attribution 4.0 International</span></span><br><span class="line"><span class="string">License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> wavfile</span><br><span class="line"></span><br><span class="line">fs, snd = wavfile.read(<span class="string">"output.wav"</span>)</span><br><span class="line"></span><br><span class="line">snd = snd / (<span class="number">2.</span>**<span class="number">15</span>)</span><br><span class="line">s1 = snd[:,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">8</span>))</span><br><span class="line">plt.style.use(<span class="string">"seaborn"</span>)</span><br><span class="line"></span><br><span class="line">time = np.arange(<span class="number">0</span>, s1.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">time = (time / fs) * <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">plt.plot(time, s1, color=<span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'Amplitude'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Time (ms)'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xticks(fontsize=<span class="number">14</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_5_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(C) 2018 Nikolay Manchev</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This work is licensed under the Creative Commons Attribution 4.0 International</span></span><br><span class="line"><span class="string">License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> wavfile</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> numpy.fft <span class="keyword">import</span> fft</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">"xtick.labelsize"</span>] = <span class="number">14</span></span><br><span class="line">plt.rcParams[<span class="string">"ytick.labelsize"</span>] = <span class="number">14</span></span><br><span class="line">plt.style.use(<span class="string">"seaborn"</span>)</span><br><span class="line"></span><br><span class="line">threshold = <span class="number">800</span></span><br><span class="line">fs, snd = wavfile.read(<span class="string">"output.wav"</span>)</span><br><span class="line">y = snd[:,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">n = len(y) </span><br><span class="line">p = fft(y) </span><br><span class="line"></span><br><span class="line">mag = np.sqrt(p.real**<span class="number">2</span> + p.imag**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">mag = mag * <span class="number">2</span> / n</span><br><span class="line"></span><br><span class="line">mag = mag[<span class="number">0</span>:math.ceil((n)/<span class="number">2.0</span>)]</span><br><span class="line"></span><br><span class="line">freq = np.arange(<span class="number">0</span>, len(mag), <span class="number">1.0</span>) * (fs / n)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> threshold != <span class="number">0</span>:</span><br><span class="line">    print(np.unique(np.rint(freq[np.in1d(mag, mag[mag&gt;threshold])])))</span><br><span class="line">    mag[mag&lt;threshold]=threshold</span><br><span class="line"></span><br><span class="line">plt.plot(freq/<span class="number">1000</span>, mag, color=<span class="string">'b'</span>)</span><br><span class="line">plt.xticks(np.arange(min(freq/<span class="number">1000</span>), max(freq/<span class="number">1000</span>)+<span class="number">1</span>, <span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>[329. 330. 415. 555.]</code></pre><p><img src="output_6_1.png" alt="png"></p><h2 id="Wavelet-transform"><a href="#Wavelet-transform" class="headerlink" title="Wavelet transform"></a>Wavelet transform</h2><p>Wavelets have some slight benefits over Fourier transforms in reducing computations when examining specific frequencies. However, they are rarely more sensitive, and indeed, the common Morlet wavelet is mathematically identical to a short-time Fourier transform using a Gaussian window function. The exception is when searching for signals of a known, non-sinusoidal shape (e.g., heartbeats); in that case, using matched wavelets can outperform standard STFT/Morlet analyses.</p><script type="math/tex; mode=display">X(a,b) = \frac{1}{\sqrt{a}}\int_{-\infty}^{\infty}\overline{\Psi\left(\frac{t - b}{a}\right)} x(t)\, dt</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> types</span><br><span class="line"><span class="keyword">import</span> ibm_boto3</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">from</span> zipfile <span class="keyword">import</span> ZipFile</span><br><span class="line"><span class="keyword">from</span> botocore.client <span class="keyword">import</span> Config</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> soundfile <span class="keyword">as</span> sf</span><br><span class="line"><span class="keyword">import</span> librosa</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pywt</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zip_file = ZipFile(<span class="string">"audio_data.zip"</span>)</span><br></pre></td></tr></table></figure><p>The data used for this demonstration comes from the Urban Sounds Dataset. This dataset and its taxonomy is presented in J. Salamon, C. Jacoby and J. P. Bello, A Dataset and Taxonomy for Urban Sound Research, 22nd ACM International Conference on Multimedia, Orlando USA, Nov. 2014.</p><p>For simplicity the dataset is sampled and a subset of 20 audio clips from two categories are used - air conditioner (AC) and drill.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ZipFile.namelist(zip_file)</span><br></pre></td></tr></table></figure><pre><code>[&#39;audio_data/&#39;, &#39;audio_data/ac/&#39;, &#39;audio_data/ac/101729-0-0-1.wav&#39;, &#39;audio_data/ac/101729-0-0-11.wav&#39;, &#39;audio_data/ac/101729-0-0-12.wav&#39;, &#39;audio_data/ac/101729-0-0-13.wav&#39;, &#39;audio_data/ac/101729-0-0-14.wav&#39;, &#39;audio_data/ac/101729-0-0-16.wav&#39;, &#39;audio_data/ac/101729-0-0-17.wav&#39;, &#39;audio_data/ac/101729-0-0-18.wav&#39;, &#39;audio_data/ac/101729-0-0-19.wav&#39;, &#39;audio_data/ac/101729-0-0-21.wav&#39;, &#39;audio_data/ac/101729-0-0-22.wav&#39;, &#39;audio_data/ac/101729-0-0-23.wav&#39;, &#39;audio_data/ac/101729-0-0-24.wav&#39;, &#39;audio_data/ac/101729-0-0-26.wav&#39;, &#39;audio_data/ac/101729-0-0-28.wav&#39;, &#39;audio_data/ac/101729-0-0-29.wav&#39;, &#39;audio_data/ac/101729-0-0-3.wav&#39;, &#39;audio_data/ac/101729-0-0-32.wav&#39;, &#39;audio_data/ac/101729-0-0-33.wav&#39;, &#39;audio_data/ac/101729-0-0-36.wav&#39;, &#39;audio_data/drill/&#39;, &#39;audio_data/drill/103199-4-0-0.wav&#39;, &#39;audio_data/drill/103199-4-0-3.wav&#39;, &#39;audio_data/drill/103199-4-0-4.wav&#39;, &#39;audio_data/drill/103199-4-0-5.wav&#39;, &#39;audio_data/drill/103199-4-0-6.wav&#39;, &#39;audio_data/drill/103199-4-1-0.wav&#39;, &#39;audio_data/drill/103199-4-2-0.wav&#39;, &#39;audio_data/drill/103199-4-2-1.wav&#39;, &#39;audio_data/drill/103199-4-2-10.wav&#39;, &#39;audio_data/drill/103199-4-2-11.wav&#39;, &#39;audio_data/drill/103199-4-2-2.wav&#39;, &#39;audio_data/drill/103199-4-2-3.wav&#39;, &#39;audio_data/drill/103199-4-2-4.wav&#39;, &#39;audio_data/drill/103199-4-2-5.wav&#39;, &#39;audio_data/drill/103199-4-2-6.wav&#39;, &#39;audio_data/drill/103199-4-2-7.wav&#39;, &#39;audio_data/drill/103199-4-2-8.wav&#39;, &#39;audio_data/drill/103199-4-2-9.wav&#39;, &#39;audio_data/drill/103199-4-4-0.wav&#39;, &#39;audio_data/drill/103199-4-6-0.wav&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">audio_data = []</span><br><span class="line">labels = []</span><br><span class="line">sampling_rate = []</span><br><span class="line">file_names = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file_name <span class="keyword">in</span> ZipFile.namelist(zip_file):</span><br><span class="line">    <span class="comment"># Skip directories</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.basename(file_name):</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line">    audio_file = <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">if</span> file_name.startswith(<span class="string">"audio_data/ac/"</span>):</span><br><span class="line">        labels.append(<span class="number">0</span>)</span><br><span class="line">        audio_file = zip_file.open(file_name)</span><br><span class="line">    <span class="keyword">elif</span> file_name.startswith(<span class="string">"audio_data/drill/"</span>):</span><br><span class="line">        labels.append(<span class="number">1</span>)</span><br><span class="line">        audio_file = zip_file.open(file_name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"Unknown file class. Skipping."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> audio_file <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        file_names.append(file_name)</span><br><span class="line">        tmp = BytesIO(audio_file.read())</span><br><span class="line">        data, samplerate = sf.read(tmp)</span><br><span class="line">        audio_data.append(data)</span><br><span class="line">        sampling_rate.append(samplerate)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(audio_data),audio_data[<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure><pre><code>(40, (192000, 2))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">Counter(labels)</span><br></pre></td></tr></table></figure><pre><code>Counter({0: 20, 1: 20})</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for index in range(len(audio_data)):</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#     if (sampling_rate[index] == 48000):</span></span><br><span class="line"><span class="comment">#         audio_data[index] = librosa.resample(audio_data[index], 48000, 44100)</span></span><br><span class="line"><span class="comment">#         sampling_rate[index] = 44100</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_mono</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> data.ndim &gt; <span class="number">1</span>:</span><br><span class="line">        data = np.mean(data, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> range(len(audio_data)):</span><br><span class="line">    audio_data[index] = to_mono(audio_data[index])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">14</span>,<span class="number">6</span>))</span><br><span class="line">plt.plot(audio_data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x159e6f6d8&gt;]</code></pre><p><img src="output_9_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">14</span>,<span class="number">6</span>))</span><br><span class="line">plt.plot(audio_data[<span class="number">21</span>])</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x12fce4be0&gt;]</code></pre><p><img src="output_10_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scales = np.arange(<span class="number">1</span>, <span class="number">101</span>)</span><br><span class="line">coeff1, freqs1 = pywt.cwt(audio_data[<span class="number">1</span>][:<span class="number">25000</span>], scales, <span class="string">'morl'</span>)</span><br><span class="line">coeff2, freqs2 = pywt.cwt(audio_data[<span class="number">21</span>][:<span class="number">25000</span>], scales, <span class="string">'morl'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">coeff1.shape, freqs1.shape</span><br></pre></td></tr></table></figure><pre><code>((100, 25000), (100,))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">20</span>,<span class="number">10</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">plt.imshow(coeff1, cmap=<span class="string">'coolwarm'</span>, aspect=<span class="string">'auto'</span>)  </span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">plt.imshow(coeff2, cmap=<span class="string">'coolwarm'</span>, aspect=<span class="string">'auto'</span>)  </span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_13_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">40</span>,<span class="number">15</span>))</span><br><span class="line"></span><br><span class="line">ax1 = fig.add_subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, projection=<span class="string">'3d'</span>)</span><br><span class="line"></span><br><span class="line">Y = np.arange(<span class="number">1</span>, <span class="number">101</span>, <span class="number">1</span>)</span><br><span class="line">X = np.arange(<span class="number">1</span>, <span class="number">25001</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X, Y = np.meshgrid(X, Y)</span><br><span class="line"></span><br><span class="line">ax1.plot_surface(X, Y, coeff1, cmap=cm.coolwarm, linewidth=<span class="number">0</span>, antialiased=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">ax1.set_xlabel(<span class="string">"Time"</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"Scale"</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax1.set_zlabel(<span class="string">"Amplitude"</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax1.set_zlim3d(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">ax2 = fig.add_subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, projection=<span class="string">'3d'</span>)</span><br><span class="line"></span><br><span class="line">ax2.plot_surface(X, Y, coeff2, cmap=cm.coolwarm, linewidth=<span class="number">0</span>, antialiased=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax2.set_xlabel(<span class="string">"Time"</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">"Scale"</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax2.set_zlabel(<span class="string">"Amplitude"</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax2.set_zlim3d(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_14_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line">pca = PCA(n_components=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">features = np.empty((<span class="number">0</span>,<span class="number">100</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ind <span class="keyword">in</span> range(len(audio_data)):</span><br><span class="line">    print(<span class="string">'.'</span>, end=<span class="string">''</span>)</span><br><span class="line">    coeff, freqs = pywt.cwt(audio_data[ind][:<span class="number">25000</span>], scales, <span class="string">'morl'</span>)    </span><br><span class="line">    features = np.vstack([features, pca.fit_transform(coeff).flatten()])</span><br></pre></td></tr></table></figure><pre><code>........................................</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=<span class="number">0.20</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clf = svm.SVC()</span><br><span class="line">clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><pre><code>SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,    decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;scale&#39;, kernel=&#39;rbf&#39;,    max_iter=-1, probability=False, random_state=None, shrinking=True,    tol=0.001, verbose=False)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line">print(<span class="string">"Accuracy : %.2f%%"</span> % (accuracy_score(y_test, y_pred) * <span class="number">100</span>))</span><br></pre></td></tr></table></figure><pre><code>Accuracy : 87.50%</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Fourier-Transform&quot;&gt;&lt;a href=&quot;#Fourier-Transform&quot; class=&quot;headerlink&quot; title=&quot;Fourier Transform&quot;&gt;&lt;/a&gt;Fourier Transform&lt;/h2&gt;&lt;p&gt;The Fourie
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Machine-Learning/"/>
    
    
      <category term="Signal Processing" scheme="https://zhangruochi.com/tags/Signal-Processing/"/>
    
  </entry>
  
  <entry>
    <title>PCA from scratch</title>
    <link href="https://zhangruochi.com/PCA-from-scratch/2020/04/14/"/>
    <id>https://zhangruochi.com/PCA-from-scratch/2020/04/14/</id>
    <published>2020-04-14T17:32:41.000Z</published>
    <updated>2020-04-14T17:33:44.799Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> decomposition</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_arrow</span><span class="params">(ax, start, stop)</span>:</span></span><br><span class="line">    ax.annotate(<span class="string">''</span>, xytext=start, xy=stop,                 </span><br><span class="line">                arrowprops=dict(facecolor=<span class="string">'red'</span>, width=<span class="number">2.0</span>))</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr_vars</span><span class="params">( start=<span class="number">-10</span>, stop=<span class="number">10</span>, step=<span class="number">0.5</span>, mu=<span class="number">0</span>, sigma=<span class="number">3</span>, func=lambda x: x )</span>:</span></span><br><span class="line">    x = np.arange(start, stop, step)    </span><br><span class="line">    e = np.random.normal(mu, sigma, x.size)</span><br><span class="line">    y = np.zeros(x.size)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> ind <span class="keyword">in</span> range(x.size):</span><br><span class="line">        y[ind] = func(x[ind]) + e[ind]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (x,y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">(x1,x2) = corr_vars(start=<span class="number">2</span>, stop=<span class="number">4</span>, step=<span class="number">0.2</span>, sigma=<span class="number">2</span>, func=<span class="keyword">lambda</span> x: <span class="number">2</span>*math.sin(x))</span><br><span class="line"></span><br><span class="line">A = np.column_stack((x1,x2))</span><br><span class="line"></span><br><span class="line">Aorig = A</span><br><span class="line"></span><br><span class="line">A</span><br></pre></td></tr></table></figure><pre><code>array([[ 2.        , -1.68093609],       [ 2.2       ,  2.30235361],       [ 2.4       ,  3.65699797],       [ 2.6       ,  0.52613067],       [ 2.8       ,  2.63261787],       [ 3.        ,  1.3106777 ],       [ 3.2       ,  0.32561105],       [ 3.4       , -2.65116887],       [ 3.6       , -1.26403255],       [ 3.8       , -0.71371289]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">A = (A-np.mean(A,axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the original matrix</span></span><br><span class="line">f, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, sharey=<span class="keyword">True</span>, figsize=(<span class="number">7</span>,<span class="number">4</span>))</span><br><span class="line">ax1.scatter(Aorig[:,<span class="number">0</span>], Aorig[:,<span class="number">1</span>])</span><br><span class="line">ax1.set_title(<span class="string">"Original data"</span>)</span><br><span class="line">ax1.grid(<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the centered data</span></span><br><span class="line">ax2.scatter(A[:,<span class="number">0</span>],A[:,<span class="number">1</span>])</span><br><span class="line">ax2.set_title(<span class="string">"Centered data"</span>)</span><br><span class="line">ax2.grid(<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">ax1.axhline(<span class="number">0</span>, color=<span class="string">"blue"</span>)</span><br><span class="line">ax1.axvline(<span class="number">0</span>, color=<span class="string">"blue"</span>)</span><br><span class="line">ax2.axhline(<span class="number">0</span>, color=<span class="string">"blue"</span>)</span><br><span class="line">ax2.axvline(<span class="number">0</span>, color=<span class="string">"blue"</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim([<span class="number">-2</span>,<span class="number">5</span>])</span><br><span class="line">plt.ylim([<span class="number">-4</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>(-4, 5)</code></pre><p><img src="output_2_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">S = np.dot(A.T,A)/(A.shape[<span class="number">0</span>]<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"The covariance matrix is:"</span>)</span><br><span class="line">print(S,<span class="string">"\n"</span>)</span><br></pre></td></tr></table></figure><pre><code>The covariance matrix is:[[ 0.36666667 -0.55248919] [-0.55248919  4.18798554]] </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">f, ax1 = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, sharey=<span class="keyword">True</span>, figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">V = np.array([[<span class="number">-1</span>],[<span class="number">0</span>]])</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">ax1.scatter(A[:,<span class="number">0</span>],A[:,<span class="number">1</span>])</span><br><span class="line">ax1.set_title(<span class="string">"Vector [-1,1]"</span>)</span><br><span class="line">ax1.grid(<span class="keyword">True</span>)</span><br><span class="line">ax1.plot([<span class="number">0</span>,V[<span class="number">0</span>]],[<span class="number">0</span>,V[<span class="number">1</span>]],c=<span class="string">'r'</span>)</span><br><span class="line">plot_arrow(ax1, (<span class="number">0</span>,<span class="number">0</span>),(V[<span class="number">0</span>],V[<span class="number">1</span>]))</span><br><span class="line">plt.xlim([<span class="number">-5</span>,<span class="number">5</span>])</span><br><span class="line">plt.ylim([<span class="number">-5</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>Vector slope:  [-0.](-5, 5)</code></pre><p><img src="output_4_2.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">f, ax1 = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, sharey=<span class="keyword">True</span>, figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">ax1.scatter(A[:,<span class="number">0</span>],A[:,<span class="number">1</span>])</span><br><span class="line">ax1.set_title(<span class="string">"Vector [-1,1]"</span>)</span><br><span class="line">ax1.grid(<span class="keyword">True</span>)</span><br><span class="line">ax1.plot([<span class="number">0</span>,V[<span class="number">0</span>]],[<span class="number">0</span>,V[<span class="number">1</span>]],c=<span class="string">'r'</span>)</span><br><span class="line">plot_arrow(ax1, (<span class="number">0</span>,<span class="number">0</span>),(V[<span class="number">0</span>],V[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">plt.xlim([<span class="number">-5</span>,<span class="number">5</span>])</span><br><span class="line">plt.ylim([<span class="number">-5</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>Vector slope:  [-1.50678871](-5, 5)</code></pre><p><img src="output_5_2.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">f, ax1 = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, sharey=<span class="keyword">True</span>, figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">ax1.scatter(A[:,<span class="number">0</span>],A[:,<span class="number">1</span>])</span><br><span class="line">ax1.set_title(<span class="string">"Vector [-1,1]"</span>)</span><br><span class="line">ax1.grid(<span class="keyword">True</span>)</span><br><span class="line">ax1.plot([<span class="number">0</span>,V[<span class="number">0</span>]],[<span class="number">0</span>,V[<span class="number">1</span>]],c=<span class="string">'r'</span>)</span><br><span class="line">plot_arrow(ax1, (<span class="number">0</span>,<span class="number">0</span>),(V[<span class="number">0</span>],V[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">plt.xlim([<span class="number">-5</span>,<span class="number">5</span>])</span><br><span class="line">plt.ylim([<span class="number">-5</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>Vector slope:  [-5.72313052](-5, 5)</code></pre><p><img src="output_6_2.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">f, ax1 = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, sharey=<span class="keyword">True</span>, figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">ax1.scatter(A[:,<span class="number">0</span>],A[:,<span class="number">1</span>])</span><br><span class="line">ax1.set_title(<span class="string">"Vector [-1,1]"</span>)</span><br><span class="line">ax1.grid(<span class="keyword">True</span>)</span><br><span class="line">ax1.plot([<span class="number">0</span>,V[<span class="number">0</span>]],[<span class="number">0</span>,V[<span class="number">1</span>]],c=<span class="string">'r'</span>)</span><br><span class="line">plot_arrow(ax1, (<span class="number">0</span>,<span class="number">0</span>),(V[<span class="number">0</span>],V[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">plt.xlim([<span class="number">-5</span>,<span class="number">5</span>])</span><br><span class="line">plt.ylim([<span class="number">-5</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>Vector slope:  [-6.94911232](-5, 5)</code></pre><p><img src="output_7_2.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"The slope of the vector converges to the direction of greatest variance:\n"</span>)</span><br><span class="line"></span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br><span class="line">V = np.dot(S,V)</span><br><span class="line">print(<span class="string">"Vector slope: "</span>,V[<span class="number">1</span>]/V[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>The slope of the vector converges to the direction of greatest variance:Vector slope:  [-7.0507464]Vector slope:  [-7.0577219]Vector slope:  [-7.05819391]Vector slope:  [-7.05822582]Vector slope:  [-7.05822798]Vector slope:  [-7.05822813]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://en.wikipedia.org/wiki/Eigenvalue_algorithm</span></span><br><span class="line"></span><br><span class="line">l_1 = (S.trace() + np.sqrt(pow(S.trace(),<span class="number">2</span>) - <span class="number">4</span>*np.linalg.det(S))) / <span class="number">2</span></span><br><span class="line">l_2 = (S.trace() - np.sqrt(pow(S.trace(),<span class="number">2</span>) - <span class="number">4</span>*np.linalg.det(S))) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"The eigenvalues are:"</span>)</span><br><span class="line">print(<span class="string">"L1:"</span>,l_1)</span><br><span class="line">print(<span class="string">"L2:"</span>,l_2)</span><br></pre></td></tr></table></figure><pre><code>The eigenvalues are:L1: 4.266261447240239L2: 0.28839076171131417</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Cayley-Hamilton theorem</span></span><br><span class="line"><span class="comment"># (A - λ1I )(A - λ2I ) = (A - λ2I )(A - λ1I ) = 0</span></span><br><span class="line"></span><br><span class="line">A1 = S - l_1 * np.identity(<span class="number">2</span>)</span><br><span class="line">A2 = S - l_2 * np.identity(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">E1 = A2[:,<span class="number">1</span>]</span><br><span class="line">E2 = A1[:,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">E1 = E1 / np.linalg.norm(E1)</span><br><span class="line">E2 = E2 / np.linalg.norm(E2)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"The eigenvectors are:"</span>)</span><br><span class="line">print(<span class="string">"E1:"</span>, E1)</span><br><span class="line">print(<span class="string">"E2:"</span>, E2)</span><br></pre></td></tr></table></figure><pre><code>The eigenvectors are:E1: [-0.14027773  0.9901122 ]E2: [-0.9901122  -0.14027773]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">E = np.column_stack((E1,E2))</span><br><span class="line"></span><br><span class="line">E</span><br></pre></td></tr></table></figure><pre><code>array([[-0.14027773, -0.9901122 ],       [ 0.9901122 , -0.14027773]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">evals, evecs = np.linalg.eigh(S)</span><br><span class="line"></span><br><span class="line">print(evals)</span><br><span class="line">print(evecs)</span><br></pre></td></tr></table></figure><pre><code>[0.28839076 4.26626145][[-0.9901122  -0.14027773] [-0.14027773  0.9901122 ]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2, ax3) = plt.subplots(<span class="number">1</span>, <span class="number">3</span>, sharey=<span class="keyword">True</span>, figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax1.scatter(Aorig[:,<span class="number">0</span>],Aorig[:,<span class="number">1</span>])</span><br><span class="line">ax1.set_title(<span class="string">"Original Data"</span>)</span><br><span class="line">ax1.grid(<span class="keyword">True</span>)</span><br><span class="line">ax1.set_aspect(<span class="string">'equal'</span>)</span><br><span class="line"></span><br><span class="line">ax2.scatter(Aorig[:,<span class="number">0</span>],Aorig[:,<span class="number">1</span>])</span><br><span class="line">ax2.set_title(<span class="string">"E1"</span>)</span><br><span class="line">ax2.grid(<span class="keyword">True</span>)</span><br><span class="line">plot_arrow(ax2, np.mean(Aorig,axis=<span class="number">0</span>), np.mean(Aorig,axis=<span class="number">0</span>) + np.dot(Aorig, E).std(axis=<span class="number">0</span>).mean() * E1)</span><br><span class="line">ax2.set_aspect(<span class="string">'equal'</span>)               </span><br><span class="line"></span><br><span class="line">ax3.scatter(Aorig[:,<span class="number">0</span>],Aorig[:,<span class="number">1</span>])</span><br><span class="line">ax3.set_title(<span class="string">"E1 and E2"</span>)</span><br><span class="line">ax3.grid(<span class="keyword">True</span>)</span><br><span class="line">plot_arrow(ax3, np.mean(Aorig,axis=<span class="number">0</span>), np.mean(Aorig,axis=<span class="number">0</span>) + np.dot(Aorig, E).std(axis=<span class="number">0</span>).mean() * E1)</span><br><span class="line">plot_arrow(ax3, np.mean(Aorig,axis=<span class="number">0</span>), np.mean(Aorig,axis=<span class="number">0</span>) + np.dot(Aorig, E).std(axis=<span class="number">0</span>).mean() * E2)</span><br><span class="line">ax3.set_aspect(<span class="string">'equal'</span>)               </span><br><span class="line"></span><br><span class="line">plt.xlim([<span class="number">0</span>,<span class="number">5</span>])</span><br><span class="line">plt.ylim([<span class="number">-4</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>(-4, 5)</code></pre><p><img src="output_13_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">F1 = np.dot(A, E1)</span><br><span class="line">F2 = np.dot(A, E2)</span><br><span class="line"></span><br><span class="line">F = np.column_stack((F1, F2))</span><br><span class="line">F</span><br></pre></td></tr></table></figure><pre><code>array([[-1.97812455,  1.18924584],       [ 1.93772363,  0.43245658],       [ 3.25091797,  0.04440771],       [ 0.12295254,  0.28557622],       [ 2.18055566, -0.20793946],       [ 0.84363103, -0.22052313],       [-0.15975102, -0.28036266],       [-3.13515266, -0.06080918],       [-1.78978762, -0.45341595],       [-1.27296497, -0.72863598]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pca = decomposition.PCA(n_components=<span class="number">2</span>)</span><br><span class="line">print(pca.fit_transform(A))</span><br></pre></td></tr></table></figure><pre><code>[[-1.97812455  1.18924584] [ 1.93772363  0.43245658] [ 3.25091797  0.04440771] [ 0.12295254  0.28557622] [ 2.18055566 -0.20793946] [ 0.84363103 -0.22052313] [-0.15975102 -0.28036266] [-3.13515266 -0.06080918] [-1.78978762 -0.45341595] [-1.27296497 -0.72863598]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Machine-Learning/"/>
    
    
      <category term="Unsupervised Learning" scheme="https://zhangruochi.com/tags/Unsupervised-Learning/"/>
    
  </entry>
  
  <entry>
    <title>DBSCAN Clustering</title>
    <link href="https://zhangruochi.com/DBSCAN-Clustering/2020/04/14/"/>
    <id>https://zhangruochi.com/DBSCAN-Clustering/2020/04/14/</id>
    <published>2020-04-14T05:42:51.000Z</published>
    <updated>2020-04-14T05:54:17.806Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Intution"><a href="#Intution" class="headerlink" title="Intution"></a>Intution</h3><p>Partitioning methods (K-means, PAM clustering) and hierarchical clustering work for finding <strong>spherical-shaped</strong> clusters or convex clusters. In other words, they are suitable only for compact and well-separated clusters. Moreover, they are also severely affected by the presence of <strong>noise</strong> and <strong>outliers</strong> in the data.</p><p>Real life data may contain irregularities, like:</p><ul><li>Clusters can be of arbitrary shape such as those shown in the figure below.</li><li>Data may contain noise.</li></ul><p>The figure below shows a data set containing nonconvex clusters and outliers/noises. Given such data, k-means algorithm has difficulties for identifying these clusters with arbitrary shapes.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%"></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="50%" height="50%"></center><h3 id="DBSCAN-algorithm-requires-two-parameters"><a href="#DBSCAN-algorithm-requires-two-parameters" class="headerlink" title="DBSCAN algorithm requires two parameters"></a>DBSCAN algorithm requires two parameters</h3><ol><li><strong>eps</strong>: It defines the neighborhood around a data point i.e. if the distance between two points is lower or equal to <code>eps</code> then they are considered as neighbors. If the eps value is chosen too small then large part of the data will be considered as outliers. If it is chosen very large then the clusters will merge and majority of the data points will be in the same clusters. One way to find the eps value is based on the k-distance graph.</li><li><strong>MinPts</strong>: Minimum number of neighbors (data points) within <code>eps</code> radius. Larger the dataset, the larger value of MinPts must be chosen. As a general rule, the minimum MinPts can be derived from the number of dimensions D in the dataset as, MinPts &gt;= D+1. The minimum value of MinPts must be chosen at least 3.</li></ol><h3 id="In-this-algorithm-we-have-3-types-of-data-points"><a href="#In-this-algorithm-we-have-3-types-of-data-points" class="headerlink" title="In this algorithm, we have 3 types of data points."></a>In this algorithm, we have 3 types of data points.</h3><ul><li><strong>Core Point</strong>: A point is a core point if it has more than MinPts points within eps.</li><li><strong>Border Point</strong>: A point which has fewer than MinPts within eps but it is in the neighborhood of a core point.</li><li><strong>Noise or outlier</strong>: A point which is not a core point or border point.</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center><h4 id="DBSCAN-algorithm-can-be-abstracted-in-the-following-steps"><a href="#DBSCAN-algorithm-can-be-abstracted-in-the-following-steps" class="headerlink" title="DBSCAN algorithm can be abstracted in the following steps"></a>DBSCAN algorithm can be abstracted in the following steps</h4><ul><li>Find all the neighbor points within eps and identify the core points or visited with more than MinPts neighbors.</li><li>For each core point if it is not already assigned to a cluster, create a new cluster.</li><li>Find recursively all its density connected points and assign them to the same cluster as the core point.<ul><li>A point a and b are said to be density connected if there exist a point c which has a sufficient number of points in its neighbors and both the points a and b are within the eps distance. This is a chaining process. So, if b is neighbor of c, c is neighbor of d, d is neighbor of e, which in turn is neighbor of a implies that b is neighbor of a.</li></ul></li><li>Iterate through the remaining unvisited points in the dataset. Those points that do not belong to any cluster are noise.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Intution&quot;&gt;&lt;a href=&quot;#Intution&quot; class=&quot;headerlink&quot; title=&quot;Intution&quot;&gt;&lt;/a&gt;Intution&lt;/h3&gt;&lt;p&gt;Partitioning methods (K-means, PAM clustering)
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Machine-Learning/"/>
    
    
      <category term="Clustering" scheme="https://zhangruochi.com/tags/Clustering/"/>
    
  </entry>
  
  <entry>
    <title>SparkML Examples</title>
    <link href="https://zhangruochi.com/SparkML-Examples/2020/04/14/"/>
    <id>https://zhangruochi.com/SparkML-Examples/2020/04/14/</id>
    <published>2020-04-14T04:58:54.000Z</published>
    <updated>2020-04-14T05:59:09.690Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/IBM/coursera/raw/master/hmp.parquet</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a dataframe out of it</span></span><br><span class="line">df = spark.read.parquet(<span class="string">'hmp.parquet'</span>)</span><br><span class="line"><span class="comment"># two class</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">'df'</span>)</span><br><span class="line">df_two_class = spark.sql(<span class="string">"select * from df where class in ('Use_telephone','Standup_chair')"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train test split</span></span><br><span class="line">splits = df_two_class.randomSplit([<span class="number">0.8</span>, <span class="number">0.2</span>])</span><br><span class="line">df_train = splits[<span class="number">0</span>]</span><br><span class="line">df_test = splits[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StringIndexer</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># transformer</span></span><br><span class="line">indexer = StringIndexer(inputCol=<span class="string">"class"</span>, outputCol=<span class="string">"label"</span>)</span><br><span class="line">encoder = OneHotEncoder(inputCol=<span class="string">"label"</span>, outputCol=<span class="string">"labelVec"</span>)</span><br><span class="line">vectorAssembler = VectorAssembler(inputCols=[<span class="string">"x"</span>,<span class="string">"y"</span>,<span class="string">"z"</span>],</span><br><span class="line">                                  outputCol=<span class="string">"features"</span>)</span><br><span class="line">normalizer = MinMaxScaler(inputCol=<span class="string">"features"</span>, outputCol=<span class="string">"features_norm"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># modeling</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LinearSVC</span><br><span class="line">lsvc = LinearSVC(maxIter=<span class="number">10</span>, regParam=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## pipeline </span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line">pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer,lsvc])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># fit and predict</span></span><br><span class="line">model = pipeline.fit(df_train)</span><br><span class="line">prediction = model.transform(df_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> BinaryClassificationEvaluator</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate model</span></span><br><span class="line">evaluator = BinaryClassificationEvaluator(rawPredictionCol=<span class="string">"rawPrediction"</span>)</span><br><span class="line">evaluator.evaluate(prediction)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># testing</span></span><br><span class="line">prediction = model.transform(df_test)</span><br><span class="line">evaluator = BinaryClassificationEvaluator(rawPredictionCol=<span class="string">"rawPrediction"</span>)</span><br><span class="line">evaluator.evaluate(prediction)</span><br></pre></td></tr></table></figure><h3 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a dataframe out of it</span></span><br><span class="line">df = spark.read.parquet(<span class="string">'hmp.parquet'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># register a corresponding query table</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">'df'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line"></span><br><span class="line">vectorAssembler = VectorAssembler(inputCols=[<span class="string">"x"</span>,<span class="string">"y"</span>,<span class="string">"z"</span>],</span><br><span class="line">                                  outputCol=<span class="string">"features"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.clustering <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line">kmeans = KMeans().setK(<span class="number">13</span>).setSeed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line">pipeline = Pipeline(stages=[vectorAssembler, kmeans])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df.createOrReplaceTempView(<span class="string">'df'</span>)</span><br><span class="line">df = spark.sql(<span class="string">"select * from df where class in ('Brush_teeth','Climb_stairs')"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = pipeline.fit(df)</span><br><span class="line"></span><br><span class="line">wssse = model.stages[<span class="number">1</span>].computeCost(vectorAssembler.transform(df))</span><br><span class="line">print(<span class="string">"Within Set Sum of Squared Errors = "</span> + str(wssse))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Supervised-Learning&quot;&gt;&lt;a href=&quot;#Supervised-Learning&quot; class=&quot;headerlink&quot; title=&quot;Supervised Learning&quot;&gt;&lt;/a&gt;Supervised Learning&lt;/h3&gt;&lt;figu
      
    
    </summary>
    
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Pipeline" scheme="https://zhangruochi.com/categories/AI-Workflow/Pipeline/"/>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning Pipeline</title>
    <link href="https://zhangruochi.com/Machine-Learning-Pipeline/2020/04/13/"/>
    <id>https://zhangruochi.com/Machine-Learning-Pipeline/2020/04/13/</id>
    <published>2020-04-13T20:12:30.000Z</published>
    <updated>2020-04-13T23:11:28.964Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Scikit-learn-Pipeline"><a href="#Scikit-learn-Pipeline" class="headerlink" title="Scikit-learn Pipeline"></a>Scikit-learn Pipeline</h2><h3 id="Pipeline-1"><a href="#Pipeline-1" class="headerlink" title="Pipeline 1"></a>Pipeline 1</h3><p>In most machine learning projects the data that you have to work with is unlikely to be in the ideal format for producing the best performing model. There are quite often a number of transformational steps such as encoding categorical variables, feature scaling and normalisation that need to be performed. Scikit-learn has built in functions for most of these commonly used transformations in it’s <code>preprocessing</code> package.</p><p>However, in a typical machine learning workflow you will need to apply all these transformations at <code>least twice</code>. Once when training the model and again on any new data you want to predict on. Of course you could write a function to apply them and reuse that but you would still need to run this first and then call the model separately. Scikit-learn pipelines are a tool to simplify this process. They have several key benefits:</p><ul><li>They make your workflow much easier to read and understand.</li><li>They enforce the implementation and order of steps in your project.</li><li>These in turn make your work much more reproducible.</li></ul><p>Before building the pipeline I am splitting the training data into a train and test set so that I can validate the performance of the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = train.drop(<span class="string">'Target'</span>, axis=<span class="number">1</span>)</span><br><span class="line">y = train[<span class="string">'Target'</span>]</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><p>The first step in building the pipeline is to define each <code>transformer type</code>. The convention here is generally to create transformers for the different variable types.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, OneHotEncoder</span><br><span class="line"></span><br><span class="line">numeric_transformer = Pipeline(steps=[</span><br><span class="line">    (<span class="string">'imputer'</span>, SimpleImputer(strategy=<span class="string">'median'</span>)),</span><br><span class="line">    (<span class="string">'scaler'</span>, StandardScaler())])</span><br><span class="line"></span><br><span class="line">categorical_transformer = Pipeline(steps=[</span><br><span class="line">    (<span class="string">'imputer'</span>, SimpleImputer(strategy=<span class="string">'constant'</span>, fill_value=<span class="string">'missing'</span>)),</span><br><span class="line">    (<span class="string">'onehot'</span>, OneHotEncoder(handle_unknown=<span class="string">'ignore'</span>))])</span><br></pre></td></tr></table></figure><p>Next we use the <code>ColumnTransformer</code> to apply the transformations to the correct columns in the dataframe. Before building this I have stored lists of the numeric and categorical columns using the pandas dtype method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">numeric_features = train.select_dtypes(include=[<span class="string">'int64'</span>, <span class="string">'float64'</span>]).columns</span><br><span class="line"></span><br><span class="line">categorical_features = train.select_dtypes(include=[<span class="string">'object'</span>]).drop([<span class="string">'Loan_Status'</span>], axis=<span class="number">1</span>).columns</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer</span><br><span class="line"></span><br><span class="line">preprocessor = ColumnTransformer(</span><br><span class="line">    transformers=[</span><br><span class="line">        (<span class="string">'num'</span>, numeric_transformer, numeric_features),</span><br><span class="line">        (<span class="string">'cat'</span>, categorical_transformer, categorical_features)])</span><br></pre></td></tr></table></figure><p>The next step is to create a pipeline that combines the preprocessor created above with a classifier. In this case I have used a simple RandomForestClassifier to start with.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">rf = Pipeline(steps=[(<span class="string">'preprocessor'</span>, preprocessor),</span><br><span class="line">                      (<span class="string">'classifier'</span>, RandomForestClassifier())])</span><br></pre></td></tr></table></figure><p>Fitting the classifier</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rf.fit(X_train, y_train)</span><br><span class="line">y_pred = rf.predict(X_test)</span><br></pre></td></tr></table></figure><p>A pipeline can also be used during the model selection process. The following example code loops through a number of scikit-learn classifiers applying the transformations and training the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, log_loss</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC, LinearSVC, NuSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> QuadraticDiscriminantAnalysis</span><br><span class="line">classifiers = [</span><br><span class="line">    KNeighborsClassifier(<span class="number">3</span>),</span><br><span class="line">    SVC(kernel=<span class="string">"rbf"</span>, C=<span class="number">0.025</span>, probability=<span class="keyword">True</span>),</span><br><span class="line">    NuSVC(probability=<span class="keyword">True</span>),</span><br><span class="line">    DecisionTreeClassifier(),</span><br><span class="line">    RandomForestClassifier(),</span><br><span class="line">    AdaBoostClassifier(),</span><br><span class="line">    GradientBoostingClassifier()</span><br><span class="line">    ]</span><br><span class="line"><span class="keyword">for</span> classifier <span class="keyword">in</span> classifiers:</span><br><span class="line">    pipe = Pipeline(steps=[(<span class="string">'preprocessor'</span>, preprocessor),</span><br><span class="line">                      (<span class="string">'classifier'</span>, classifier)])</span><br><span class="line">    pipe.fit(X_train, y_train)   </span><br><span class="line">    print(classifier)</span><br><span class="line">    print(<span class="string">"model score: %.3f"</span> % pipe.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>The pipeline can also be used in grid search to find the best performing parameters. To do this you first need to create a parameter grid for your chosen model. One important thing to note is that you need to <code>append</code> the name that you have given the classifier part of your pipeline to each parameter name. In my code above I have called this ‘classifier’ so I have added <code>classifier__</code> to each parameter. Next I created a grid search object which includes the original pipeline. When I then call fit, the transformations are applied to the data, before a cross-validated grid-search is performed over the parameter grid.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">param_grid = &#123; </span><br><span class="line">    <span class="string">'classifier__n_estimators'</span>: [<span class="number">200</span>, <span class="number">500</span>],</span><br><span class="line">    <span class="string">'classifier__max_features'</span>: [<span class="string">'auto'</span>, <span class="string">'sqrt'</span>, <span class="string">'log2'</span>],</span><br><span class="line">    <span class="string">'classifier__max_depth'</span> : [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],</span><br><span class="line">    <span class="string">'classifier__criterion'</span> :[<span class="string">'gini'</span>, <span class="string">'entropy'</span>]&#125;</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line">CV = GridSearchCV(rf, param_grid, n_jobs= <span class="number">1</span>)</span><br><span class="line">                  </span><br><span class="line">CV.fit(X_train, y_train)  </span><br><span class="line">print(CV.best_params_)    </span><br><span class="line">print(CV.best_score_)</span><br></pre></td></tr></table></figure><h3 id="Pipeline-2"><a href="#Pipeline-2" class="headerlink" title="Pipeline 2"></a>Pipeline 2</h3><p>The example below demonstrates the pipeline defined with four steps:</p><ul><li>Feature Extraction with Principal Component Analysis (3 features)</li><li>Feature Extraction with Statistical Selection (6 features)</li><li>Feature Union</li><li>Learn a Logistic Regression Model</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> FeatureUnion</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">url = <span class="string">"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"</span></span><br><span class="line">names = [<span class="string">'preg'</span>, <span class="string">'plas'</span>, <span class="string">'pres'</span>, <span class="string">'skin'</span>, <span class="string">'test'</span>, <span class="string">'mass'</span>, <span class="string">'pedi'</span>, <span class="string">'age'</span>, <span class="string">'class'</span>]</span><br><span class="line">dataframe = read_csv(url, names=names)</span><br><span class="line">array = dataframe.values</span><br><span class="line">X = array[:,<span class="number">0</span>:<span class="number">8</span>]</span><br><span class="line">Y = array[:,<span class="number">8</span>]</span><br><span class="line"><span class="comment"># create feature union</span></span><br><span class="line">features = []</span><br><span class="line">features.append((<span class="string">'pca'</span>, PCA(n_components=<span class="number">3</span>)))</span><br><span class="line">features.append((<span class="string">'select_best'</span>, SelectKBest(k=<span class="number">6</span>)))</span><br><span class="line">feature_union = FeatureUnion(features)</span><br><span class="line"><span class="comment"># create pipeline</span></span><br><span class="line">estimators = []</span><br><span class="line">estimators.append((<span class="string">'feature_union'</span>, feature_union))</span><br><span class="line">estimators.append((<span class="string">'logistic'</span>, LogisticRegression()))</span><br><span class="line">model = Pipeline(estimators)</span><br><span class="line"><span class="comment"># evaluate pipeline</span></span><br><span class="line">seed = <span class="number">7</span></span><br><span class="line">kfold = KFold(n_splits=<span class="number">10</span>, random_state=seed)</span><br><span class="line">results = cross_val_score(model, X, Y, cv=kfold)</span><br><span class="line">print(results.mean())</span><br></pre></td></tr></table></figure><h3 id="Building-Scikit-Learn-transformers"><a href="#Building-Scikit-Learn-transformers" class="headerlink" title="Building Scikit-Learn transformers"></a>Building Scikit-Learn transformers</h3><p>Scikit-Learn’s API uses <code>duck typing</code>: if you want to write your own custom estimators (including transformers and predictors), you only need to implement the right methods, you don’t have to inherit from any particular class.</p><p>For example, all <strong>estimators</strong> must implement a <code>fit()</code> method, and <code>get_params()</code> and <code>set_params()</code> methods. All <strong>transformers</strong> must also implement <code>transform()</code> and <code>fit_transform()</code> methods. All <strong>predictors</strong> must implement a <code>predict()</code> method. And so on.</p><p>The most basic implementation of the <code>fit_transform()</code> method is just this:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.fit(X, y).transform(X, y)</span><br></pre></td></tr></table></figure></p><p>You don’t have to inherit from the <code>TransformerMixin</code> class, but that’s what you get if you do: if you implement the <code>fit()</code> method and the <code>transform()</code> method, it gives you the <code>fit_transform()</code> method for free, just like the above.</p><p>Similarly, the BaseEstimator class just gives you the <code>get_params()</code> and <code>set_params()</code> methods for free. By default, <code>get_params()</code> does some introspection to get the parameters of the constructor <strong>init</strong>(), and it assumes that the class has corresponding instance variables. For example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyEstimator</span><span class="params">(BaseEstimator)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a, b=<span class="number">2</span>)</span>:</span></span><br><span class="line">        self.a = a</span><br><span class="line">        self.b = b</span><br><span class="line"></span><br><span class="line"><span class="comment"># &gt;&gt;&gt; m = MyEstimator(1, 2)</span></span><br><span class="line"><span class="comment"># &gt;&gt;&gt; m.get_params()</span></span><br><span class="line"><span class="comment"># &#123;'a': 1, 'b': 2&#125;</span></span><br><span class="line"><span class="comment"># &gt;&gt;&gt; m.set_params(a=5, b=10)</span></span><br><span class="line"><span class="comment"># MyEstimator(a=5, b=10)</span></span><br><span class="line"><span class="comment"># &gt;&gt;&gt; m.a</span></span><br><span class="line"><span class="comment"># 5</span></span><br><span class="line"><span class="comment"># &gt;&gt;&gt; m.b</span></span><br><span class="line"><span class="comment"># 10</span></span><br></pre></td></tr></table></figure><p>Let’s say I have a lot of text and I want to extract certain data from it. I’m going to build a featurizer that takes a list of functions, calls each function with our text, and returns the results of all functions as a feature vector.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">longest_run_of_capitol_letters_feature</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="string">"""Find the longest run of capitol letters and return their length."""</span></span><br><span class="line">    runs = sorted(re.findall(<span class="string">r"[A-Z]+"</span>, text), key=len)</span><br><span class="line">    <span class="keyword">if</span> runs:</span><br><span class="line">        <span class="keyword">return</span> len(runs[<span class="number">-1</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">percent_character_feature</span><span class="params">(char)</span>:</span></span><br><span class="line">    <span class="string">"""Return percentage of text that is a particular char compared to total</span></span><br><span class="line"><span class="string">    text length."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feature_fn</span><span class="params">(text)</span>:</span></span><br><span class="line">        periods = text.count(char)</span><br><span class="line">        <span class="keyword">return</span> periods / len(text)</span><br><span class="line">    <span class="keyword">return</span> feature_fn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FunctionFeaturizer</span><span class="params">(BaseEstimator, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *featurizers)</span>:</span></span><br><span class="line">        self.featurizers = featurizers</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="string">"""All SciKit-Learn compatible transformers and classifiers have the</span></span><br><span class="line"><span class="string">        same interface. `fit` always returns the same object."""</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""Given a list of original data, return a list of feature vectors."""</span></span><br><span class="line">        fvs = []</span><br><span class="line">        <span class="keyword">for</span> datum <span class="keyword">in</span> X:</span><br><span class="line">            fv = [f(datum) <span class="keyword">for</span> f <span class="keyword">in</span> self.featurizers]</span><br><span class="line">            fvs.append(fv)</span><br><span class="line">        <span class="keyword">return</span> np.array(fvs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">sms_featurizer = FunctionFeaturizer(longest_run_of_capitol_letters_feature,</span><br><span class="line">                                    percent_character_feature(<span class="string">"."</span>))</span><br><span class="line"><span class="comment"># sms_featurizer.transform(sms_data[:10])</span></span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(sms_data, sms_results)</span><br><span class="line"></span><br><span class="line">pipe = make_pipeline(sms_featurizer, DecisionTreeClassifier())</span><br><span class="line">pipe.fit(X_train, y_train)</span><br><span class="line">pipe.score(X_test, y_test)</span><br><span class="line"><span class="comment"># =&gt; 0.91385498923187369</span></span><br></pre></td></tr></table></figure><h2 id="SparkML-Pipeline"><a href="#SparkML-Pipeline" class="headerlink" title="SparkML Pipeline"></a>SparkML Pipeline</h2><h3 id="EDA"><a href="#EDA" class="headerlink" title="EDA"></a>EDA</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a dataframe out of it</span></span><br><span class="line">df = spark.read.parquet(<span class="string">'hmp.parquet'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># register a corresponding query table</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">'df'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StringIndexer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">indexer = StringIndexer(inputCol=<span class="string">"class"</span>, outputCol=<span class="string">"classIndex"</span>)</span><br><span class="line">indexed = indexer.fit(df).transform(df)</span><br><span class="line">indexed.show()</span><br><span class="line">indexed.select(<span class="string">'classIndex'</span>).distinct().show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> OneHotEncoder, StringIndexer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">encoder = OneHotEncoder(inputCol=<span class="string">"classIndex"</span>, outputCol=<span class="string">"categoryVec"</span>)</span><br><span class="line">encoded = encoder.transform(indexed)</span><br><span class="line">encoded.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line"></span><br><span class="line">vectorAssembler = VectorAssembler(inputCols=[<span class="string">"x"</span>,<span class="string">"y"</span>,<span class="string">"z"</span>],</span><br><span class="line">                                  outputCol=<span class="string">"features"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># For your special case that has string instead of doubles you should cast them first.</span></span><br><span class="line"><span class="comment"># expr = [col(c).cast("Double").alias(c) </span></span><br><span class="line"><span class="comment">#         for c in vectorAssembler.getInputCols()]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># df2 = df2.select(*expr)</span></span><br><span class="line">features_vectorized = vectorAssembler.transform(encoded)</span><br><span class="line">features_vectorized.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"></span><br><span class="line">min_max_scaler = MinMaxScaler(inputCol=<span class="string">"features"</span>, outputCol=<span class="string">"features_norm"</span>)</span><br><span class="line">min_max_scaler_model = min_max_scaler.fit(features_vectorized)</span><br><span class="line">normalized_data = min_max_scaler_model.transform(features_vectorized)</span><br><span class="line">normalized_data.show()</span><br><span class="line"></span><br><span class="line">df_train = normalized_data.drop(<span class="string">"source"</span>).drop(<span class="string">"class"</span>).drop(<span class="string">"classIndex"</span>).drop(<span class="string">"features"</span>).drop(<span class="string">"x"</span>).drop(<span class="string">"y"</span>).drop(<span class="string">"z"</span>)</span><br><span class="line"></span><br><span class="line">df_train.show()</span><br></pre></td></tr></table></figure><h3 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line">pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, min_max_scaler_model])</span><br><span class="line">model = pipeline.fit(df)</span><br><span class="line">prediction = model.transform(df)</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf" target="_blank" rel="noopener">https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Scikit-learn-Pipeline&quot;&gt;&lt;a href=&quot;#Scikit-learn-Pipeline&quot; class=&quot;headerlink&quot; title=&quot;Scikit-learn Pipeline&quot;&gt;&lt;/a&gt;Scikit-learn Pipeline&lt;/
      
    
    </summary>
    
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Pipeline" scheme="https://zhangruochi.com/categories/AI-Workflow/Pipeline/"/>
    
    
  </entry>
  
  <entry>
    <title>Feedback loops</title>
    <link href="https://zhangruochi.com/Feedback-loops/2020/04/07/"/>
    <id>https://zhangruochi.com/Feedback-loops/2020/04/07/</id>
    <published>2020-04-07T19:04:27.000Z</published>
    <updated>2020-04-07T19:04:27.602Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Spark Fundamental</title>
    <link href="https://zhangruochi.com/Spark-Fundamental/2020/04/06/"/>
    <id>https://zhangruochi.com/Spark-Fundamental/2020/04/06/</id>
    <published>2020-04-07T02:12:46.000Z</published>
    <updated>2020-04-07T03:10:27.224Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Basic-concepts"><a href="#Basic-concepts" class="headerlink" title="Basic concepts"></a>Basic concepts</h1><h2 id="Work-with-the-SparkContext-object"><a href="#Work-with-the-SparkContext-object" class="headerlink" title="Work with the SparkContext object"></a>Work with the SparkContext object</h2><p>The Spark driver application uses the SparkContext object to allow a programming interface to interact with the driver application. The SparkContext object tells Spark how and where to access a cluster.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line">conf = SparkConf().setAppName(appName).setMaster(master)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br></pre></td></tr></table></figure><h2 id="Work-with-Resilient-Distributed-Datasets"><a href="#Work-with-Resilient-Distributed-Datasets" class="headerlink" title="Work with Resilient Distributed Datasets"></a>Work with Resilient Distributed Datasets</h2><p>Spark uses an abstraction for working with data called a Resilient Distributed Dataset (RDD). An RDD is a collection of elements that can be operated on in parallel. RDDs are immutable, so you can’t update the data in them. To update data in an RDD, you must create a new RDD. In Spark, all work is done by creating new RDDs, transforming existing RDDs, or using RDDs to compute results. When working with RDDs, the Spark driver application automatically distributes the work across the cluster.<br>You can construct RDDs by parallelizing existing Python collections (lists), by manipulating RDDs, or by manipulating files in HDFS or any other storage system.<br>You can run these types of methods on RDDs:</p><ul><li><strong>Actions</strong>: query the data and return values</li><li><strong>Transformations</strong>: manipulate data values and return pointers to new RDDs.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a Python collection of the numbers 1 - 10</span></span><br><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Put the collection into an RDD named x_nbr_rdd using the parallelize method</span></span><br><span class="line">x_nbr_rdd = sc.parallelize(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># View the first element in the RDD</span></span><br><span class="line"><span class="comment"># Each number in the collection is in a different element in the RDD. Because the first() method returned a value, it is an action.</span></span><br><span class="line">x_nbr_rdd.first()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now view the first five elements in the RDD</span></span><br><span class="line">x_nbr_rdd.take(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create another RDD</span></span><br><span class="line">y = [<span class="string">"Hello Human"</span>, <span class="string">"My Name is Spark"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_str_rdd = sc.parallelize(y)</span></span><br><span class="line">y_str_rdd.take(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="Manipulate-data-in-RDDs"><a href="#Manipulate-data-in-RDDs" class="headerlink" title="Manipulate data in RDDs"></a>Manipulate data in RDDs</h2><p>Remember that to manipulate data, you use transformation functions.<br>Here are some common Python transformation functions that you’ll be using in this notebook:</p><ul><li><code>map(func)</code>: returns a new RDD with the results of running the specified function on each element</li><li><code>filter(func)</code>: returns a new RDD with the elements for which the specified function returns true</li><li><code>distinct([numTasks]))</code>: returns a new RDD that contains the distinct elements of the source RDD</li><li><code>flatMap(func)</code>: returns a new RDD by first running the specified function on all elements, returning 0 or more results for each original element, and then flattening the results into individual elements<br>You can also create functions that run a single expression and don’t have a name with the Python lambda keyword. For example, this function returns the sum of its arguments: <code>lambda a , b : a + b</code>.</li></ul><h3 id="Update-numeric-values"><a href="#Update-numeric-values" class="headerlink" title="Update numeric values"></a>Update numeric values</h3><p>Run the <code>map()</code> function with the lambda keyword to replace each element, X, in your first RDD (the one that has numeric values) with X+1. Because RDDs are <strong>immutable</strong>, you need to specify a new RDD name.</p><p><strong>Be careful</strong> with the collect method! It returns all elements of the RDD to the driver. Returning a large data set might be not be very useful. No-one wants to scroll through a million rows!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_nbr_rdd_2 = x_nbr_rdd.map(<span class="keyword">lambda</span> x: x+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now look at the elements of the new RDD</span></span><br><span class="line">x_nbr_rdd_2.collect()</span><br></pre></td></tr></table></figure><h3 id="Add-numbers-in-an-array"><a href="#Add-numbers-in-an-array" class="headerlink" title="Add numbers in an array"></a>Add numbers in an array</h3><p>An array of values is a common data format where multiple values are contained in one element. You can manipulate the individual values if you split them up into separate elements.<br>Create an array of numbers by including quotation marks around the whole set of numbers. If you omit the quotation marks, you get a collection of numbers instead of an array.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X = [<span class="string">"1,2,3,4,5,6,7,8,9,10"</span>]</span><br><span class="line"></span><br><span class="line">y_rd = sc.parallelize(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the values at commas and add values in the positions 2 and 9 in the array. Keep in mind that an array starts with position 0. Use a backslash character, \, to break the line of code for clarity.</span></span><br><span class="line">Sum_rd = y_rd.map(<span class="keyword">lambda</span> y: y.split(<span class="string">","</span>)).\</span><br><span class="line">map(<span class="keyword">lambda</span> y: (int(y[<span class="number">2</span>])+int(y[<span class="number">9</span>])))</span><br><span class="line"></span><br><span class="line">Sum_rd.first()</span><br></pre></td></tr></table></figure><h3 id="Split-and-count-text-strings"><a href="#Split-and-count-text-strings" class="headerlink" title="Split and count text strings"></a>Split and count text strings</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Words = [<span class="string">"Hello Human. I'm Spark and I love running analysis on data."</span>]</span><br><span class="line">words_rd = sc.parallelize(Words)</span><br><span class="line">words_rd.first()</span><br><span class="line"></span><br><span class="line">Words_rd2 = words_rd.map(<span class="keyword">lambda</span> line: line.split(<span class="string">" "</span>))</span><br><span class="line">Words_rd2.first()</span><br><span class="line"></span><br><span class="line">Words_rd2.count()</span><br></pre></td></tr></table></figure><h3 id="Count-words-with-a-pair-RDD"><a href="#Count-words-with-a-pair-RDD" class="headerlink" title="Count words with a pair RDD"></a>Count words with a pair RDD</h3><p>A common way to count the number of instances of words in an RDD is to create a pair RDD. A pair RDD converts each word into a key-value pair: the word is the key and the number 1 is the value. Because the values are all 1, when you add the values for a particular word, you get the number of instances of that word.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">z = [<span class="string">"First,Line"</span>, <span class="string">"Second,Line"</span>, <span class="string">"and,Third,Line"</span>]</span><br><span class="line">z_str_rdd = sc.parallelize(z)</span><br><span class="line">z_str_rdd.first()</span><br><span class="line"></span><br><span class="line">z_str_rdd_split_flatmap = z_str_rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">","</span>))</span><br><span class="line">z_str_rdd_split_flatmap.collect()</span><br><span class="line"></span><br><span class="line">countWords = z_str_rdd_split_flatmap.map(<span class="keyword">lambda</span> word:(word,<span class="number">1</span>))</span><br><span class="line">countWords.collect()</span><br><span class="line"></span><br><span class="line"><span class="comment"># [('First', 1),</span></span><br><span class="line"><span class="comment">#  ('Line', 1),</span></span><br><span class="line"><span class="comment">#  ('Second', 1),</span></span><br><span class="line"><span class="comment">#  ('Line', 1),</span></span><br><span class="line"><span class="comment">#  ('and', 1),</span></span><br><span class="line"><span class="comment">#  ('Third', 1),</span></span><br><span class="line"><span class="comment">#  ('Line', 1)]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</span><br><span class="line">countWords2 = countWords.reduceByKey(add)</span><br><span class="line">countWords2.collect()</span><br><span class="line"></span><br><span class="line"><span class="comment"># [('Second', 1), ('Line', 3), ('First', 1), ('and', 1), ('Third', 1)]</span></span><br></pre></td></tr></table></figure><h3 id="Filter-data"><a href="#Filter-data" class="headerlink" title="Filter data"></a>Filter data</h3><p>The filter command creates a new RDD from another RDD based on a filter criteria. The filter syntax is:<br><code>.filter(lambda line: &quot;Filter Criteria Value&quot; in line)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">words_rd3 = z_str_rdd_split_flatmap.filter(<span class="keyword">lambda</span> line: <span class="string">"Second"</span> <span class="keyword">in</span> line) </span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"The count of words "</span> + str(words_rd3.first()))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Is: "</span> + str(words_rd3.count()))</span><br></pre></td></tr></table></figure><h1 id="Querying-data"><a href="#Querying-data" class="headerlink" title="Querying data"></a>Querying data</h1><h2 id="Enable-SQL-processing"><a href="#Enable-SQL-processing" class="headerlink" title="Enable SQL processing"></a>Enable SQL processing</h2><p>The preferred method to enable SQL processing with Spark 2.0 is to use the new SparkSession object, but you can also create a SQLContext object.<br>Use the predefined Spark Context, sc, which contains the connection information for Spark, to create an SQLContext:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SQLContext</span><br><span class="line">sqlContext = SQLContext(sc)</span><br></pre></td></tr></table></figure><h2 id="Create-a-DataFrame"><a href="#Create-a-DataFrame" class="headerlink" title="Create a DataFrame"></a>Create a DataFrame</h2><p>Instead of creating an RDD to read the file, you’ll create a Spark DataFrame. Unlike an RDD, a DataFrame creates a <code>schema</code> around the data, which supplies the necessary structure for SQL queries. A self-describing format like JSON is ideal for DataFrames, but many other file types are supported, including text (CSV) and Parquet.<br>Create a DataFrame:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">example1_df = sqlContext.read.json(<span class="string">"world_bank.json.gz"</span>)</span><br></pre></td></tr></table></figure></p><h2 id="Create-a-table"><a href="#Create-a-table" class="headerlink" title="Create a table"></a>Create a table</h2><p>SQL statements must be run against a table. Create a table that’s a pointer to the DataFrame:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">example1_df.registerTempTable(<span class="string">"world_bank"</span>)</span><br></pre></td></tr></table></figure><h2 id="Run-SQL-queries"><a href="#Run-SQL-queries" class="headerlink" title="Run SQL queries"></a>Run SQL queries</h2><p>You must define a new DataFrame for the results of the SQL query and put the SQL statement inside the sqlContext.sql() method.<br>Run the following cell to select all columns from the table and print information about the resulting DataFrame and schema of the data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">temp_df =  sqlContext.sql(<span class="string">"select * from world_bank"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (type(temp_df))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"*"</span> * <span class="number">20</span>)</span><br><span class="line"><span class="keyword">print</span> (temp_df)</span><br></pre></td></tr></table></figure><h3 id="Display-query-results-with-a-pandas-DataFrame"><a href="#Display-query-results-with-a-pandas-DataFrame" class="headerlink" title="Display query results with a pandas DataFrame"></a>Display query results with a pandas DataFrame</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">sqlContext.sql(<span class="string">"select id, borrower from world_bank limit 2"</span>).toPandas()</span><br></pre></td></tr></table></figure><h3 id="Run-a-group-by-query"><a href="#Run-a-group-by-query" class="headerlink" title="Run a group by query"></a>Run a group by query</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">"""</span></span><br><span class="line"><span class="string">select</span></span><br><span class="line"><span class="string">    regionname ,</span></span><br><span class="line"><span class="string">    count(*) as project_count</span></span><br><span class="line"><span class="string">from world_bank</span></span><br><span class="line"><span class="string">group by regionname </span></span><br><span class="line"><span class="string">order by count(*) desc</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">sqlContext.sql(query).toPandas()</span><br></pre></td></tr></table></figure><h3 id="Run-a-subselect-query"><a href="#Run-a-subselect-query" class="headerlink" title="Run a subselect query"></a>Run a subselect query</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">select * from</span></span><br><span class="line"><span class="string">    (select</span></span><br><span class="line"><span class="string">        regionname ,</span></span><br><span class="line"><span class="string">        count(*) as project_count</span></span><br><span class="line"><span class="string">    from world_bank</span></span><br><span class="line"><span class="string">    group by regionname </span></span><br><span class="line"><span class="string">    order by count(*) desc) table_alias</span></span><br><span class="line"><span class="string">limit 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">sqlContext.sql(query).toPandas()</span><br></pre></td></tr></table></figure><h2 id="Convert-RDDs-to-DataFrames"><a href="#Convert-RDDs-to-DataFrames" class="headerlink" title="Convert RDDs to DataFrames"></a>Convert RDDs to DataFrames</h2><p>If you want to run SQL queries on an existing RDD, you must convert the RDD to a DataFrame. The main difference between RDDs and DataFrames is whether the columns are named.<br>You’ll create an RDD and then convert it to a DataFrame in two different ways:</p><ul><li>Apply a schema</li><li>Create rows with named columns</li></ul><h3 id="Create-a-simple-RDD"><a href="#Create-a-simple-RDD" class="headerlink" title="Create a simple RDD"></a>Create a simple RDD</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">data_e2 = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">6</span>):</span><br><span class="line">    random_int = int(random.random() * <span class="number">10</span>)</span><br><span class="line">    data_e2.append([x, random_int, random_int^<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">rdd_example2 = sc.parallelize(data_e2)</span><br><span class="line"><span class="keyword">print</span> (rdd_example2.collect())</span><br><span class="line"></span><br><span class="line"><span class="comment"># [[1, 1, 3], [2, 3, 1], [3, 1, 3], [4, 8, 10], [5, 0, 2]]</span></span><br></pre></td></tr></table></figure><h3 id="Apply-a-schema"><a href="#Apply-a-schema" class="headerlink" title="Apply a schema"></a>Apply a schema</h3><p>You’ll use the <code>StructField</code> method to create a schema object that’s based on a string, apply the schema to the RDD to create a DataFrame, and then create a table to run SQL queries on.<br>Define your schema columns as a string:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">schemaString = <span class="string">"ID VAL1 VAL2"</span></span><br></pre></td></tr></table></figure><p>Assign header information with the StructField method and create the schema with the <code>StructType</code> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fields = [StructField(field_name, StringType(), <span class="keyword">True</span>) <span class="keyword">for</span> field_name <span class="keyword">in</span> schemaString.split()]</span><br><span class="line">schema = StructType(fields)</span><br></pre></td></tr></table></figure><p>Apply the schema to the RDD with the createDataFrame method</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">schemaExample = sqlContext.createDataFrame(rdd_example2, schema)</span><br></pre></td></tr></table></figure><p>Register the DataFrame as a table<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Register the DataFrame as a table</span></span><br><span class="line">schemaExample.registerTempTable(<span class="string">"example2"</span>)</span><br></pre></td></tr></table></figure></p><p>View the data</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (schemaExample.collect())</span><br><span class="line"></span><br><span class="line"><span class="comment"># [Row(ID='1', VAL1='1', VAL2='3'), Row(ID='2', VAL1='3', VAL2='1'), Row(ID='3', VAL1='1', VAL2='3'), Row(ID='4', VAL1='8', VAL2='10'), Row(ID='5', VAL1='0', VAL2='2')]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can reference the columns names in DataFrames</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> schemaExample.take(<span class="number">2</span>):</span><br><span class="line">    <span class="keyword">print</span> (row.ID, row.VAL1, row.VAL2)</span><br></pre></td></tr></table></figure><p>Run a simple SQL query</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqlContext.sql(<span class="string">"select * from example2"</span>).toPandas()</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="20%" height="20%"></center><h3 id="Create-rows-with-named-columns"><a href="#Create-rows-with-named-columns" class="headerlink" title="Create rows with named columns"></a>Create rows with named columns</h3><p>You’ll create an RDD with named columns and then convert it to a DataFrame and a table.<br>Create a new RDD and specify the names of the columns with the map method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line"></span><br><span class="line">rdd_example3 = rdd_example2.map(<span class="keyword">lambda</span> x: Row(id=x[<span class="number">0</span>], val1=x[<span class="number">1</span>], val2=x[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (rdd_example3.collect()) </span><br><span class="line"></span><br><span class="line"><span class="comment"># [Row(id=1, val1=1, val2=3), Row(id=2, val1=3, val2=1), Row(id=3, val1=1, val2=3), Row(id=4, val1=8, val2=10), Row(id=5, val1=0, val2=2)]</span></span><br></pre></td></tr></table></figure><p>Convert rdd_example3 to a DataFrame and register an associated table</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_example3 = rdd_example3.toDF()</span><br><span class="line">df_example3.registerTempTable(<span class="string">"example3"</span>)</span><br></pre></td></tr></table></figure><p>Run a simple SQL query</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqlContext.sql(<span class="string">"select * from example3"</span>).toPandas()</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="20%" height="20%"></center><h3 id="Join-tables"><a href="#Join-tables" class="headerlink" title="Join tables"></a>Join tables</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Join tables example2 and example3 on the ID column:</span></span><br><span class="line"></span><br><span class="line">query = <span class="string">"""</span></span><br><span class="line"><span class="string">select</span></span><br><span class="line"><span class="string">    *</span></span><br><span class="line"><span class="string">from</span></span><br><span class="line"><span class="string">    example2 e2</span></span><br><span class="line"><span class="string">inner join example3 e3 on</span></span><br><span class="line"><span class="string">    e2.ID = e3.id</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (sqlContext.sql(query).toPandas())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Alternatively, you can join DataFrames with a Python command instead of an SQL query:</span></span><br><span class="line"></span><br><span class="line">df_example4 = df_example3.join(schemaExample, schemaExample[<span class="string">"ID"</span>] == df_example3[<span class="string">"id"</span>] )</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df_example4.take(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">print</span> (row)</span><br></pre></td></tr></table></figure><h2 id="Create-SQL-functions"><a href="#Create-SQL-functions" class="headerlink" title="Create SQL functions"></a>Create SQL functions</h2><p>You can create functions that run in SQL queries.<br>First, create a Python function and test it:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">simple_function</span><span class="params">(v)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> int(v * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#test the function</span></span><br><span class="line"><span class="keyword">print</span> (simple_function(<span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>Next, register the function as an SQL function with the registerFunction method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqlContext.registerFunction(<span class="string">"simple_function"</span>, simple_function)</span><br></pre></td></tr></table></figure><p>Now run the function in an SQL Statement:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">"""</span></span><br><span class="line"><span class="string">select</span></span><br><span class="line"><span class="string">    ID,</span></span><br><span class="line"><span class="string">    VAL1,</span></span><br><span class="line"><span class="string">    VAL2,</span></span><br><span class="line"><span class="string">    simple_function(VAL1) as s_VAL1,</span></span><br><span class="line"><span class="string">    simple_function(VAL2) as s_VAL2</span></span><br><span class="line"><span class="string">from</span></span><br><span class="line"><span class="string"> example2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">sqlContext.sql(query).toPandas()</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%"></center><p>The values in the VAL1 and VAL2 columns look like strings (10 characters instead of a number multiplied by 10). That’s because string is the default data type for columns in Spark DataFrames.</p><h2 id="Convert-a-pandas-DataFrame-to-a-Spark-DataFrame"><a href="#Convert-a-pandas-DataFrame-to-a-Spark-DataFrame" class="headerlink" title="Convert a pandas DataFrame to a Spark DataFrame"></a>Convert a pandas DataFrame to a Spark DataFrame</h2><p>Although pandas DataFrames display data in a friendlier format, Spark DataFrames can be faster and more scalable.<br>You’ll get a new data set, create a pandas DataFrame for it, and then convert the pandas DataFrame to a Spark DataFrame.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pandas_df = pd.read_csv(<span class="string">"./GoSales_Tx.csv"</span>)</span><br><span class="line">pandas_df.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert the pandas DataFrame to a Spark DataFrame with the createDataFrame method. Remember using the createDataFrame method to convert an RDD to a Spark DataFrame</span></span><br><span class="line">spark_df = sqlContext.createDataFrame(pandas_df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Register the Spark DataFrame as a table</span></span><br><span class="line">spark_df.registerTempTable(<span class="string">"gosales_tx"</span>)</span><br><span class="line"></span><br><span class="line">sqlContext.sql(<span class="string">"select * from gosales_tx limit 10"</span>).collect()</span><br></pre></td></tr></table></figure><h1 id="Spark-machine-learning"><a href="#Spark-machine-learning" class="headerlink" title="Spark machine learning"></a>Spark machine learning</h1><p>The Spark machine learning library makes practical machine learning scalable and easy. The library consists of common machine learning algorithms and utilities, including classification, regression, clustering, collaborative filtering (this notebook!), dimensionality reduction, lower-level optimization primitives, and higher-level pipeline APIs.<br>The library has two packages:</p><ul><li>spark.mllib contains the original API that handles data in RDDs. It’s in maintenance mode, but fully supported.</li><li>spark.ml contains a newer API for constructing ML pipelines. It handles data in DataFrames. It’s being actively enhanced.</li></ul><h2 id="Alternating-least-squares-algorithm"><a href="#Alternating-least-squares-algorithm" class="headerlink" title="Alternating least squares algorithm"></a>Alternating least squares algorithm</h2><p>The alternating least squares (ALS) algorithm provides collaborative filtering between customers and products to find products that the customers might like, based on their previous purchases or ratings.<br>The ALS algorithm creates a matrix of all customers versus all products. Most cells in the matrix are empty, which means the customer hasn’t bought that product. The ALS algorithm then fills in the probability of customers buying products that they haven’t bought yet, based on similarities between customer purchases and similarities between products. The algorithm uses the least squares computation to minimize the estimation errors, and alternates between fixing the customer factors and solving for product factors and fixing the product factors and solving for customer factors.<br>You don’t, however, need to understand how the ALS algorithm works to use it! Spark machine learning algorithms have default values that work well in most cases.</p><h2 id="Get-the-data"><a href="#Get-the-data" class="headerlink" title="Get the data"></a>Get the data</h2><p>The data set contains the transactions of an online retailer of gift items for the period from 01/12/2010 to 09/12/2011. Many of the customers are wholesalers.<br>You’ll be using a slightly modified version of UCI’s Online Retail Data Set.<br>Here’s a glimpse of the data:</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="80%" height="80%"></center><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm <span class="string">'OnlineRetail.csv.gz'</span> -f</span><br><span class="line">wget https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/OnlineRetail.csv.gz</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">loadRetailData = sc.textFile(<span class="string">"OnlineRetail.csv.gz"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> loadRetailData.take(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">print</span> (row)</span><br><span class="line"></span><br><span class="line"><span class="comment"># InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country</span></span><br><span class="line"><span class="comment"># 536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/10 8:26,2.55,17850,United Kingdom</span></span><br><span class="line"><span class="comment"># 536365,71053,WHITE METAL LANTERN,6,12/1/10 8:26,3.39,17850,United Kingdom</span></span><br><span class="line"><span class="comment"># 536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/10 8:26,2.75,17850,United Kingdom</span></span><br><span class="line"><span class="comment"># 536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/10 8:26,3.39,17850,United Kingdom</span></span><br></pre></td></tr></table></figure><h2 id="Prepare-and-shape-the-data"><a href="#Prepare-and-shape-the-data" class="headerlink" title="Prepare and shape the data"></a>Prepare and shape the data</h2><p>It’s been said that preparing and shaping data is 80% of a data scientist’s job. Having the right data in the right format is critical for getting accurate results.<br>To get the data ready, complete these tasks:</p><ul><li>Format the data</li><li>Clean the data</li><li>Create a DataFrame</li><li>Remove unneeded columns</li></ul><h3 id="Format-the-data"><a href="#Format-the-data" class="headerlink" title="Format the data"></a>Format the data</h3><p>Remove the header from the RDD and split the string in each row with a comma:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">header = loadRetailData.first()</span><br><span class="line">loadRetailData = loadRetailData.filter(<span class="keyword">lambda</span> line: line != header).\</span><br><span class="line">                            map(<span class="keyword">lambda</span> l: l.split(<span class="string">","</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> loadRetailData.take(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">print</span> (row)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ['536365', '85123A', 'WHITE HANGING HEART T-LIGHT HOLDER', '6', '12/1/10 8:26', '2.55', '17850', 'United Kingdom']</span></span><br><span class="line"><span class="comment"># ['536365', '71053', 'WHITE METAL LANTERN', '6', '12/1/10 8:26', '3.39', '17850', 'United Kingdom']</span></span><br><span class="line"><span class="comment"># ['536365', '84406B', 'CREAM CUPID HEARTS COAT HANGER', '8', '12/1/10 8:26', '2.75', '17850', 'United Kingdom']</span></span><br><span class="line"><span class="comment"># ['536365', '84029G', 'KNITTED UNION FLAG HOT WATER BOTTLE', '6', '12/1/10 8:26', '3.39', '17850', 'United Kingdom']</span></span><br><span class="line"><span class="comment"># ['536365', '84029E', 'RED WOOLLY HOTTIE WHITE HEART.', '6', '12/1/10 8:26', '3.39', '17850', 'United Kingdom']</span></span><br></pre></td></tr></table></figure><h3 id="Clean-the-data"><a href="#Clean-the-data" class="headerlink" title="Clean the data"></a>Clean the data</h3><p>Remove the rows that have incomplete data. Keep only the rows that meet the following criteria:</p><ul><li>The purchase quantity is greater than 0</li><li>The customer ID not equal to 0</li><li>The stock code is not blank after you remove non-numeric characters</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">loadRetailData = loadRetailData.filter(<span class="keyword">lambda</span> l: int(l[<span class="number">3</span>]) &gt; <span class="number">0</span>\</span><br><span class="line">                                <span class="keyword">and</span> len(re.sub(<span class="string">"\D"</span>, <span class="string">""</span>, l[<span class="number">1</span>])) != <span class="number">0</span> \</span><br><span class="line">                                <span class="keyword">and</span> len(l[<span class="number">6</span>]) != <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (loadRetailData.take(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># [['536365', '85123A', 'WHITE HANGING HEART T-LIGHT HOLDER', '6', '12/1/10 8:26', '2.55', '17850', 'United Kingdom'], ['536365', '71053', 'WHITE METAL LANTERN', '6', '12/1/10 8:26', '3.39', '17850', 'United Kingdom']]</span></span><br></pre></td></tr></table></figure><h3 id="Create-a-DataFrame-1"><a href="#Create-a-DataFrame-1" class="headerlink" title="Create a DataFrame"></a>Create a DataFrame</h3><p>First, create an SQLContext and map each line to a row:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SQLContext, Row</span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Convert each line to a Row.</span></span><br><span class="line">loadRetailData = loadRetailData.map(<span class="keyword">lambda</span> l: Row(inv=int(l[<span class="number">0</span>]),\</span><br><span class="line">                                    stockCode=int(re.sub(<span class="string">"\D"</span>, <span class="string">""</span>, l[<span class="number">1</span>])),\</span><br><span class="line">                                    description=l[<span class="number">2</span>],\</span><br><span class="line">                                    quant=int(l[<span class="number">3</span>]),\</span><br><span class="line">                                    invDate=l[<span class="number">4</span>],\</span><br><span class="line">                                    price=float(l[<span class="number">5</span>]),\</span><br><span class="line">                                    custId=int(l[<span class="number">6</span>]),\</span><br><span class="line">                                    country=l[<span class="number">7</span>]))</span><br></pre></td></tr></table></figure><p>Create a DataFrame and show the inferred schema:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">retailDf = sqlContext.createDataFrame(loadRetailData)</span><br><span class="line"><span class="keyword">print</span> (retailDf.printSchema())</span><br><span class="line"></span><br><span class="line"><span class="comment"># root</span></span><br><span class="line"><span class="comment">#  |-- country: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- custId: long (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- description: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- inv: long (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- invDate: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- price: double (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- quant: long (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- stockCode: long (nullable = true)</span></span><br></pre></td></tr></table></figure></p><p>Register the DataFrame as a table so that you can run SQL queries on it and show the first two rows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">retailDf.registerTempTable(<span class="string">"retailPurchases"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"SELECT * FROM retailPurchases limit 2"</span>).toPandas()</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="80%" height="80%"></center><h3 id="Remove-unneeded-columns"><a href="#Remove-unneeded-columns" class="headerlink" title="Remove unneeded columns"></a>Remove unneeded columns</h3><p>The only columns you need are custId, stockCode, and a new column, purch, which has a value of 1 to indicate that the customer purchased the product:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">"""</span></span><br><span class="line"><span class="string">SELECT </span></span><br><span class="line"><span class="string">    custId, stockCode, 1 as purch</span></span><br><span class="line"><span class="string">FROM </span></span><br><span class="line"><span class="string">    retailPurchases </span></span><br><span class="line"><span class="string">group </span></span><br><span class="line"><span class="string">    by custId, stockCode"""</span></span><br><span class="line">retailDf = sqlContext.sql(query)</span><br><span class="line">retailDf.registerTempTable(<span class="string">"retailDf"</span>)</span><br><span class="line"></span><br><span class="line">sqlContext.sql(<span class="string">"select * from retailDf limit 10"</span>).toPandas()</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="40%" height="40%"></center><h2 id="Split-the-data-into-three-sets"><a href="#Split-the-data-into-three-sets" class="headerlink" title="Split the data into three sets"></a>Split the data into three sets</h2><p>You’ll split the data into three sets:</p><ul><li>a testing data set (10% of the data)</li><li>a cross-validation data set (10% of the data)</li><li>a training data set (80% of the data)</li></ul><p>Split the data randomly and create a DataFrame for each data set:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">testDf, cvDf, trainDf = retailDf.randomSplit([<span class="number">.1</span>,<span class="number">.1</span>,<span class="number">.8</span>],<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"trainDf count: "</span>, trainDf.count(), <span class="string">" example: "</span>)</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> trainDf.take(<span class="number">2</span>): <span class="keyword">print</span> (row)</span><br><span class="line"><span class="keyword">print</span> ()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"cvDf count: "</span>, cvDf.count(), <span class="string">" example: "</span>)</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> cvDf.take(<span class="number">2</span>): <span class="keyword">print</span> (row)</span><br><span class="line"><span class="keyword">print</span> ()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"testDf count: "</span>, testDf.count(), <span class="string">" example: "</span>)</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> testDf.take(<span class="number">2</span>): <span class="keyword">print</span> (row)</span><br><span class="line"><span class="keyword">print</span> ()</span><br></pre></td></tr></table></figure><h2 id="Build-recommendation-models"><a href="#Build-recommendation-models" class="headerlink" title="Build recommendation models"></a>Build recommendation models</h2><p>Machine learning algorithms have standard parameters and hyperparameters. Standard parameters specify data and options. Hyperparameters control the performance of the algorithm.<br>The ALS algorithm has these hyperparameters:</p><ul><li>The rank hyperparameter represents the number of features. The default value of rank is 10.</li><li>The maxIter hyperparameter represents the number of iterations to run the least squares computation. The default value of maxIter is 10.<br>Use the training DataFrame to train three models with the ALS algorithm with different values for the rank and maxIter hyperparameters. Assign the userCol, itemCol, and ratingCol parameters to the appropriate data columns. Set the implicitPrefs parameter to true so that the algorithm can predict latent factors.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.recommendation <span class="keyword">import</span> ALS</span><br><span class="line"></span><br><span class="line">als1 = ALS(rank=<span class="number">3</span>, maxIter=<span class="number">15</span>,userCol=<span class="string">"custId"</span>,itemCol=<span class="string">"stockCode"</span>,ratingCol=<span class="string">"purch"</span>,implicitPrefs=<span class="keyword">True</span>)</span><br><span class="line">model1 = als1.fit(trainDf)</span><br><span class="line"></span><br><span class="line">als2 = ALS(rank=<span class="number">15</span>, maxIter=<span class="number">3</span>,userCol=<span class="string">"custId"</span>,itemCol=<span class="string">"stockCode"</span>,ratingCol=<span class="string">"purch"</span>,implicitPrefs=<span class="keyword">True</span>)</span><br><span class="line">model2 = als2.fit(trainDf)</span><br><span class="line"></span><br><span class="line">als3 = ALS(rank=<span class="number">15</span>, maxIter=<span class="number">15</span>,userCol=<span class="string">"custId"</span>,itemCol=<span class="string">"stockCode"</span>,ratingCol=<span class="string">"purch"</span>,implicitPrefs=<span class="keyword">True</span>)</span><br><span class="line">model3 = als3.fit(trainDf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"The models are trained"</span>)</span><br></pre></td></tr></table></figure><h2 id="Test-the-models"><a href="#Test-the-models" class="headerlink" title="Test the models"></a>Test the models</h2><p>First, test the three models on the cross-validation data set, and then on the testing data set.<br>You’ll know the model is accurate when the prediction values for products that the customers have already bought are close to 1.</p><h3 id="Clean-the-cross-validation-data-set"><a href="#Clean-the-cross-validation-data-set" class="headerlink" title="Clean the cross validation data set"></a>Clean the cross validation data set</h3><p>Remove any of the customers or products in the cross-validation data set that are not in the training data set:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> UserDefinedFunction</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> BooleanType</span><br><span class="line">customers = set(trainDf.rdd.map(<span class="keyword">lambda</span> line: line.custId).collect())</span><br><span class="line">stock = set(trainDf.rdd.map(<span class="keyword">lambda</span> line: line.stockCode).collect())</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (cvDf.count())</span><br><span class="line">cvDf = cvDf.rdd.filter(<span class="keyword">lambda</span> line: line.stockCode <span class="keyword">in</span> stock <span class="keyword">and</span>\</span><br><span class="line">                                           line.custId <span class="keyword">in</span> customers).toDF()</span><br><span class="line"><span class="keyword">print</span> (cvDf.count())</span><br></pre></td></tr></table></figure><h3 id="Run-the-models-on-the-cross-validation-data-set"><a href="#Run-the-models-on-the-cross-validation-data-set" class="headerlink" title="Run the models on the cross-validation data set"></a>Run the models on the cross-validation data set</h3><p>Run the model with the cross-validation DataFrame by using the transform function and print the first two rows of each set of predictions:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">predictions1 = model1.transform(cvDf)</span><br><span class="line">predictions2 = model2.transform(cvDf)</span><br><span class="line">predictions3 = model3.transform(cvDf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (predictions1.take(<span class="number">2</span>))</span><br><span class="line"><span class="keyword">print</span> (predictions2.take(<span class="number">2</span>))</span><br><span class="line"><span class="keyword">print</span> (predictions3.take(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># [Row(custId=14606, stockCode=20735, purch=1, prediction=0.02294829487800598), Row(custId=16464, stockCode=20735, purch=1, prediction=0.00998256541788578)]</span></span><br><span class="line"><span class="comment"># [Row(custId=14606, stockCode=20735, purch=1, prediction=0.0441482812166214), Row(custId=16464, stockCode=20735, purch=1, prediction=0.004716672468930483)]</span></span><br><span class="line"><span class="comment"># [Row(custId=14606, stockCode=20735, purch=1, prediction=0.10467907041311264), Row(custId=16464, stockCode=20735, purch=1, prediction=0.0019032559357583523)]</span></span><br></pre></td></tr></table></figure><h3 id="Calculate-the-accuracy-for-each-model"><a href="#Calculate-the-accuracy-for-each-model" class="headerlink" title="Calculate the accuracy for each model"></a>Calculate the accuracy for each model</h3><p>You’ll use the mean squared error calculation to determine accuracy by comparing the prediction values for products to the actual purchase values. Remember that if a customer purchased a product, the value in the purch column is 1. The mean squared error calculation measures the average of the squares of the errors between what is estimated and the existing data. The lower the mean squared error value, the more accurate the model.<br>For all predictions, subtract the prediction from the actual purchase value (1), square the result, and calculate the mean of all of the squared differences:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">meanSquaredError1 = predictions1.rdd.map(<span class="keyword">lambda</span> line: (line.purch - line.prediction)**<span class="number">2</span>).mean()</span><br><span class="line">meanSquaredError2 = predictions2.rdd.map(<span class="keyword">lambda</span> line: (line.purch - line.prediction)**<span class="number">2</span>).mean()</span><br><span class="line">meanSquaredError3 = predictions3.rdd.map(<span class="keyword">lambda</span> line: (line.purch - line.prediction)**<span class="number">2</span>).mean()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Mean squared error = %.4f for our first model'</span> % meanSquaredError1)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Mean squared error = %.4f for our second model'</span> % meanSquaredError2)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Mean squared error = %.4f for our third model'</span> % meanSquaredError3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Mean squared error = 0.7393 for our first model</span></span><br><span class="line"><span class="comment"># Mean squared error = 0.7011 for our second model</span></span><br><span class="line"><span class="comment"># Mean squared error = 0.6683 for our third model</span></span><br></pre></td></tr></table></figure><p>The third model (model3) has the lowest mean squared error value, so it’s the most accurate.<br>Notice that of the three models, model3 has the highest values for the hyperparameters. At this point you might be tempted to run the model with even higher values for rank and maxIter. However, you might not get better results. Increasing the values of the hyperparameters increases the time for the model to run. Also, you don’t want to overfit the model so that it exactly fits the original data. In that case, you wouldn’t get any recommendations! For best results, keep the values of the hyperparameters close to the defaults.</p><h3 id="Confirm-the-best-model"><a href="#Confirm-the-best-model" class="headerlink" title="Confirm the best model"></a>Confirm the best model</h3><p>Now run model3 on the testing data set to confirm that it’s the best model. You want to make sure that the model is not over-matched to the cross-validation data. It’s possible for a model to match one subset of the data well but not another. If the values of the mean squared error for the testing data set and the cross-validation data set are close, then you’ve confirmed that the model works for all the data.<br>Clean the testing data set, run model3 on the testing data set, and calculate the mean squared error:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">filteredTestDf = testDf.rdd.filter(<span class="keyword">lambda</span> line: line.stockCode <span class="keyword">in</span> stock <span class="keyword">and</span>\</span><br><span class="line">                                              line.custId <span class="keyword">in</span> customers).toDF()</span><br><span class="line">predictions4 = model3.transform(filteredTestDf)</span><br><span class="line">meanSquaredError4 = predictions4.rdd.map(<span class="keyword">lambda</span> line: (line.purch - line.prediction)**<span class="number">2</span>).mean()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Mean squared error = %.4f for our best model'</span> % meanSquaredError4)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Mean squared error = 0.6693 for our best model</span></span><br><span class="line"><span class="comment"># That's pretty close. The model works for all the data.</span></span><br></pre></td></tr></table></figure><h2 id="Implement-the-model"><a href="#Implement-the-model" class="headerlink" title="Implement the model"></a>Implement the model</h2><p>Use the best model to predict which products a specific customer might be interested in purchasing.</p><h3 id="Create-a-DataFrame-for-the-customer-and-all-products"><a href="#Create-a-DataFrame-for-the-customer-and-all-products" class="headerlink" title="Create a DataFrame for the customer and all products"></a>Create a DataFrame for the customer and all products</h3><p>Create a DataFrame in which each row has the customer ID (15544) and a product ID</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lit</span><br><span class="line"></span><br><span class="line">stock15544 = set(trainDf.filter(trainDf[<span class="string">'custId'</span>] == <span class="number">15544</span>).rdd.map(<span class="keyword">lambda</span> line: line.stockCode).collect())</span><br><span class="line"></span><br><span class="line">userItems = trainDf.select(<span class="string">"stockCode"</span>).distinct().\</span><br><span class="line">            withColumn(<span class="string">'custId'</span>, lit(<span class="number">15544</span>)).\</span><br><span class="line">            rdd.filter(<span class="keyword">lambda</span> line: line.stockCode <span class="keyword">not</span> <span class="keyword">in</span> stock15544).toDF()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> userItems.take(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">print</span> (row.stockCode, row.custId)</span><br></pre></td></tr></table></figure><h3 id="Rate-each-product"><a href="#Rate-each-product" class="headerlink" title="Rate each product"></a>Rate each product</h3><p>Run the transform function to create a prediction value for each product:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">userItems = model3.transform(userItems)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> userItems.take(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">print</span> (row.stockCode, row.custId, row.prediction)</span><br></pre></td></tr></table></figure><h3 id="Find-the-top-recommendations"><a href="#Find-the-top-recommendations" class="headerlink" title="Find the top recommendations"></a>Find the top recommendations</h3><p>Print the top five product recommendations</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">userItems.registerTempTable(<span class="string">"predictions"</span>)</span><br><span class="line">query = <span class="string">"select * from predictions order by prediction desc limit 5"</span></span><br><span class="line"></span><br><span class="line">sqlContext.sql(query).toPandas()</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="40%" height="40%"></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Basic-concepts&quot;&gt;&lt;a href=&quot;#Basic-concepts&quot; class=&quot;headerlink&quot; title=&quot;Basic concepts&quot;&gt;&lt;/a&gt;Basic concepts&lt;/h1&gt;&lt;h2 id=&quot;Work-with-the-Spa
      
    
    </summary>
    
    
      <category term="Big Data Architecture" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/"/>
    
      <category term="Spark" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Stock Prices Series Problems</title>
    <link href="https://zhangruochi.com/Stock-Prices-Series-Problems/2020/04/05/"/>
    <id>https://zhangruochi.com/Stock-Prices-Series-Problems/2020/04/05/</id>
    <published>2020-04-06T00:34:38.000Z</published>
    <updated>2020-04-06T01:09:25.855Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Buy-and-sell-once"><a href="#Buy-and-sell-once" class="headerlink" title="Buy and sell once"></a>Buy and sell once</h2><h3 id="Leetcode-121-Best-Time-to-Buy-and-Sell-Stock"><a href="#Leetcode-121-Best-Time-to-Buy-and-Sell-Stock" class="headerlink" title="Leetcode 121. Best Time to Buy and Sell Stock"></a><a href="https://leetcode.com/problems/best-time-to-buy-and-sell-stock/" target="_blank" rel="noopener">Leetcode 121</a>. Best Time to Buy and Sell Stock</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Say you have an array for which the ith element is the price of a given stock on day i.</span><br><span class="line"></span><br><span class="line">If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit.</span><br><span class="line"></span><br><span class="line">Note that you cannot sell a stock before you buy one.</span><br><span class="line"></span><br><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">Input: [7,1,5,3,6,4]</span><br><span class="line">Output: 5</span><br><span class="line">Explanation: Buy on day 2 (price = 1) and sell on day 5 (price = 6), profit = 6-1 = 5.</span><br><span class="line">             Not 7-1 = 6, as selling price needs to be larger than buying price.</span><br><span class="line">Example 2:</span><br><span class="line"></span><br><span class="line">Input: [7,6,4,3,1]</span><br><span class="line">Output: 0</span><br><span class="line">Explanation: In this case, no transaction is done, i.e. max profit = 0.</span><br></pre></td></tr></table></figure><h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><p>The max profit of current state only relevent to the previous minimium price. So we can use one variable to record the previous minimium price and update the current max profit.</p><p>The state transfer function is: </p><script type="math/tex; mode=display">MaxProfit_{i} = Prices[i] - MinimumPrice</script><script type="math/tex; mode=display">MinimumPrice = min(MinimumPrice, Prices[i])</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices: List[int])</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> prices:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        mini = prices[<span class="number">0</span>]</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> prices[<span class="number">1</span>:]:</span><br><span class="line">            res = max(res, p - mini)</span><br><span class="line">            mini = min(mini, p)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><h2 id="Buy-and-sell-multiple-times"><a href="#Buy-and-sell-multiple-times" class="headerlink" title="Buy and sell multiple times"></a>Buy and sell multiple times</h2><h3 id="Leetocde-122-Best-Time-to-Buy-and-Sell-Stock-II"><a href="#Leetocde-122-Best-Time-to-Buy-and-Sell-Stock-II" class="headerlink" title="Leetocde 122  Best Time to Buy and Sell Stock II"></a><a href="https://leetcode.com/problems/best-time-to-buy-and-sell-stock-ii/" target="_blank" rel="noopener">Leetocde 122</a>  Best Time to Buy and Sell Stock II</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Say you have an array for which the ith element is the price of a given stock on day i.</span><br><span class="line"></span><br><span class="line">Design an algorithm to find the maximum profit. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times).</span><br><span class="line"></span><br><span class="line">Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">Input: [7,1,5,3,6,4]</span><br><span class="line">Output: 7</span><br><span class="line">Explanation: Buy on day 2 (price = 1) and sell on day 3 (price = 5), profit = 5-1 = 4.</span><br><span class="line">             Then buy on day 4 (price = 3) and sell on day 5 (price = 6), profit = 6-3 = 3.</span><br><span class="line"></span><br><span class="line">Example 2:</span><br><span class="line"></span><br><span class="line">Input: [1,2,3,4,5]</span><br><span class="line">Output: 4</span><br><span class="line">Explanation: Buy on day 1 (price = 1) and sell on day 5 (price = 5), profit = 5-1 = 4.</span><br><span class="line">             Note that you cannot buy on day 1, buy on day 2 and sell them later, as you are</span><br><span class="line">             engaging multiple transactions at the same time. You must sell before buying again.</span><br><span class="line"></span><br><span class="line">Example 3:</span><br><span class="line"></span><br><span class="line">Input: [7,6,4,3,1]</span><br><span class="line">Output: 0</span><br><span class="line">Explanation: In this case, no transaction is done, i.e. max profit = 0.</span><br></pre></td></tr></table></figure><h3 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution"></a>Solution</h3><ol><li>Mathemetical View</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center><p>If we analyze the graph, we notice that the points of interest are the consecutive valleys and peaks. For example, in the above case, if we skip $peak_i$ and $vally_j$, trying to obtain more profit by considering points with more difference in heights, the net profit obtained will always be <strong>lesser than</strong> the one obtained by including them, Since:</p><script type="math/tex; mode=display">C <= A + B</script><p>Therefore, we can find all the preak and valley pairs and calculate the total Profit:</p><script type="math/tex; mode=display">TotalProfit = \sum_{i}( height(peak_i) - height(valley_i))</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices: List[int])</span> -&gt; int:</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(prices)):</span><br><span class="line">            <span class="keyword">if</span> prices[i] &gt; prices[i<span class="number">-1</span>]:</span><br><span class="line">                res += (prices[i] - prices[i<span class="number">-1</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><ol><li>Dynamic Programming</li></ol><p>At the end of the $i_{th}$ day, we maintain <code>cash</code>, the maximum profit we could have if we did not have a share of stock, and hold, the maximum profit we could have if we owned a share of stock.</p><p>The intution is the state of today is only relevent to the yesterday. The State of yesterday can be expressed as :</p><script type="math/tex; mode=display">State_{yesterday} = [Profit, cash] \quad or \quad [Profit, hold]</script><p>Therefore , we can use two variables <code>cash</code>, <code>hold</code> to represent the above two states. The state transfer functions are:</p><ol><li>keep the same as day i-1, or sell from hold status at day i-1<script type="math/tex; mode=display">cash =  max(cash, hold + prices[i])</script></li><li>keep the same as day i-1, or buy from cash status at day i-1<script type="math/tex; mode=display">hold = max(hold, cash-prices[i])</script></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices: List[int])</span> -&gt; int:</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> prices:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># the maximum profit we could have if we did not have a share of stock</span></span><br><span class="line">        cash = <span class="number">0</span></span><br><span class="line">        <span class="comment"># the maximum profit we could have if we owned a share of stock.</span></span><br><span class="line">        hold = -prices[<span class="number">0</span>] </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(prices)):</span><br><span class="line">            cash = max(cash, hold+prices[i])</span><br><span class="line">            hold = max(hold, cash-prices[i])</span><br><span class="line">            </span><br><span class="line">            print(cash, hold)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> cash</span><br></pre></td></tr></table></figure><h3 id="Leetcode-714-Best-Time-to-Buy-and-Sell-Stock-with-Transaction-Fee"><a href="#Leetcode-714-Best-Time-to-Buy-and-Sell-Stock-with-Transaction-Fee" class="headerlink" title="Leetcode 714. Best Time to Buy and Sell Stock with Transaction Fee"></a><a href="https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-transaction-fee/" target="_blank" rel="noopener">Leetcode 714</a>. Best Time to Buy and Sell Stock with Transaction Fee</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Your are given an array of integers prices, for which the i-th element is the price of a given stock on day i; and a non-negative integer fee representing a transaction fee.</span><br><span class="line"></span><br><span class="line">You may complete as many transactions as you like, but you need to pay the transaction fee for each transaction. You may not buy more than 1 share of a stock at a time (ie. you must sell the stock share before you buy again.)</span><br><span class="line"></span><br><span class="line">Return the maximum profit you can make.</span><br><span class="line"></span><br><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">Input: prices = [1, 3, 2, 8, 4, 9], fee = 2</span><br><span class="line">Output: 8</span><br><span class="line">Explanation: The maximum profit can be achieved by:</span><br><span class="line">Buying at prices[0] = 1</span><br><span class="line">Selling at prices[3] = 8</span><br><span class="line">Buying at prices[4] = 4</span><br><span class="line">Selling at prices[5] = 9</span><br><span class="line">The total profit is ((8 - 1) - 2) + ((9 - 4) - 2) = 8.</span><br><span class="line">Note:</span><br><span class="line"></span><br><span class="line">0 &lt; prices.length &lt;= 50000.</span><br><span class="line">0 &lt; prices[i] &lt; 50000.</span><br><span class="line">0 &lt;= fee &lt; 50000.</span><br></pre></td></tr></table></figure><h3 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution"></a>Solution</h3><ol><li>Dynamic Programming</li></ol><p>See explanation above.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices: List[int], fee: int)</span> -&gt; int:</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># the maximum profit we could have if we did not have a share of stock</span></span><br><span class="line">        cash = <span class="number">0</span></span><br><span class="line">        <span class="comment"># the maximum profit we could have if we owned a share of stock.</span></span><br><span class="line">        hold = -prices[<span class="number">0</span>] </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(prices)):</span><br><span class="line">            cash = max(cash,hold+prices[i]-fee)</span><br><span class="line">            hold = max(hold, cash-prices[i])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> cash</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Buy-and-sell-once&quot;&gt;&lt;a href=&quot;#Buy-and-sell-once&quot; class=&quot;headerlink&quot; title=&quot;Buy and sell once&quot;&gt;&lt;/a&gt;Buy and sell once&lt;/h2&gt;&lt;h3 id=&quot;Leetc
      
    
    </summary>
    
    
      <category term="Data Structure and Algorithm" scheme="https://zhangruochi.com/categories/Data-Structure-and-Algorithm/"/>
    
    
  </entry>
  
  <entry>
    <title>Deploy Models with TensorFlow Serving and Flask</title>
    <link href="https://zhangruochi.com/Deploy-Models-with-TensorFlow-Serving-and-Flask/2020/04/03/"/>
    <id>https://zhangruochi.com/Deploy-Models-with-TensorFlow-Serving-and-Flask/2020/04/03/</id>
    <published>2020-04-03T14:22:09.000Z</published>
    <updated>2020-04-07T18:04:14.788Z</updated>
    
    <content type="html"><![CDATA[<p>A hands-on project from coursera course <a href="https://www.coursera.org/learn/deploy-models-tensorflow-serving-flask/home/welcome" target="_blank" rel="noopener">Deploy Models with TensorFlow Serving and Flask</a></p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Course Certificate</div></center><ol><li>Write a front view handle image uplaod</li></ol><blockquote><p>base.html</p></blockquote><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% extends "bootstrap/base.html" %&#125;</span><br><span class="line">&#123;% block title %&#125;Dog vs Cat&#123;% endblock %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% block content %&#125;</span><br><span class="line">&#123;% endblock %&#125;</span><br></pre></td></tr></table></figure><blockquote><p>index.html </p></blockquote><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;% extends "base.html" %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% block content %&#125;</span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"container"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">h1</span>&gt;</span>Upload File<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">form</span> <span class="attr">method</span>=<span class="string">"post"</span> <span class="attr">enctype</span>=<span class="string">"multipart/form-data"</span> <span class="attr">class</span>=<span class="string">"form-inline"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"form-group"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"file"</span> <span class="attr">name</span>=<span class="string">"file"</span> <span class="attr">class</span>=<span class="string">"btn btn-default"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"form-group"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">value</span>=<span class="string">"Upload"</span>, <span class="attr">class</span>=<span class="string">"btn btn-default"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">&#123;% endblock %&#125;</span><br></pre></td></tr></table></figure><ol><li>Use docker to deployment tensorflow serving.</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 8502:8501 --name=pets -v <span class="string">"/home/models/pets/:/models/pets/1"</span> -e MODEL_NAME=pets tensorflow/serving</span><br></pre></td></tr></table></figure><p>This will copy the model from <code>/home/models/pets/</code> which in your desktop to the path <code>/models/pets/1</code> in docker</p><p>The port <code>8501</code> is defined by docker and you can change the <code>8502</code> to any port you can you used.</p><ol><li>Use <code>Flask</code> to process HTTP Requests and do model inference in docker.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request, redirect, url_for, render_template</span><br><span class="line"><span class="keyword">from</span> flask_bootstrap <span class="keyword">import</span> Bootstrap</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line">Bootstrap(app)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Constants</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">MODEL_URI = <span class="string">'http://localhost:8502/v1/models/pets:predict'</span></span><br><span class="line">OUTPUT_DIR = <span class="string">'static'</span></span><br><span class="line">CLASSES = [<span class="string">'Cat'</span>, <span class="string">'Dog'</span>]</span><br><span class="line">SIZE = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Utility functions</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_filename</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(random.choices(string.ascii_lowercase, k=<span class="number">20</span>)) + <span class="string">'.jpg'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_prediction</span><span class="params">(image_path)</span>:</span></span><br><span class="line">    image = tf.keras.preprocessing.image.load_img(image_path, target_size=(SIZE, SIZE))</span><br><span class="line">    image = tf.keras.preprocessing.image.img_to_array(image)</span><br><span class="line">    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)</span><br><span class="line">    image = np.expand_dims(image, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    data = json.dumps(&#123;<span class="string">'instances'</span>: image.tolist() &#125;)</span><br><span class="line">    response = requests.post(MODEL_URI, data = data.encode())</span><br><span class="line">    result = json.loads(response.text)</span><br><span class="line">    prediction = result[<span class="string">'predictions'</span>][<span class="number">0</span>]</span><br><span class="line">    class_name = CLASSES[int(prediction &gt; <span class="number">0.5</span>)]</span><br><span class="line">    <span class="keyword">return</span> class_name</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Routes</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="meta">@app.route('/', methods=['GET', 'POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> request.method == <span class="string">'POST'</span>:</span><br><span class="line">        uploaded_file = request.files[<span class="string">'file'</span>]</span><br><span class="line">        <span class="keyword">if</span> uploaded_file.filename != <span class="string">''</span>:</span><br><span class="line">            <span class="keyword">if</span> uploaded_file.filename[<span class="number">-3</span>:] <span class="keyword">in</span> [<span class="string">'jpg'</span>, <span class="string">'png'</span>]:</span><br><span class="line">                image_path = os.path.join(OUTPUT_DIR, generate_filename())</span><br><span class="line">                uploaded_file.save(image_path)</span><br><span class="line">                class_name = get_prediction(image_path)</span><br><span class="line">                result = &#123;</span><br><span class="line">                    <span class="string">'class_name'</span>: class_name,</span><br><span class="line">                    <span class="string">'path_to_image'</span>: image_path,</span><br><span class="line">                    <span class="string">'size'</span>: SIZE</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> render_template(<span class="string">'show.html'</span>, result=result)</span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">'index.html'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    app.run(debug=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><ol><li>Rendering results in template</li></ol><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;% extends "base.html" %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% block content %&#125;</span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"container"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">h1</span>&gt;</span>Predicted Class: &#123;&#123; result.class_name &#125;&#125;<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span>&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"&#123;&#123; result.path_to_image &#125;&#125;"</span> <span class="attr">class</span>=<span class="string">"img-rounded"</span> <span class="attr">width</span>=<span class="string">"&#123;&#123; result.size &#125;&#125;"</span> <span class="attr">height</span>=<span class="string">"auto"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span> = <span class="string">"&#123;&#123; url_for('index') &#125;&#125;"</span> <span class="attr">class</span>=<span class="string">"btn btn-default"</span>&gt;</span>Go Back<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">&#123;% endblock %&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;A hands-on project from coursera course &lt;a href=&quot;https://www.coursera.org/learn/deploy-models-tensorflow-serving-flask/home/welcome&quot; targ
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Deployment" scheme="https://zhangruochi.com/categories/AI-Workflow/Deployment/"/>
    
    
  </entry>
  
  <entry>
    <title>Programming Language: New Types, Pattern Matching, Tail Recursion</title>
    <link href="https://zhangruochi.com/Programming-Language-New-Types-Pattern-Matching-Tail-Recursion/2020/03/29/"/>
    <id>https://zhangruochi.com/Programming-Language-New-Types-Pattern-Matching-Tail-Recursion/2020/03/29/</id>
    <published>2020-03-29T04:09:05.000Z</published>
    <updated>2020-04-03T14:21:55.457Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Conceptual-Ways-to-Build-New-Types"><a href="#Conceptual-Ways-to-Build-New-Types" class="headerlink" title="Conceptual Ways to Build New Types"></a>Conceptual Ways to Build New Types</h2><p>To create a compound type, there are really only three essential building blocks. Any decent programming language provides these building blocks in some way:</p><ul><li><strong>Each-of</strong>: A compound type t describes values that contain each of values of type t1, t2, …, and tn. Tuples are an example: int * bool describes values that contain an int and a bool. A Java class with fields is also an each-of sort of thing.</li><li><strong>One-of</strong>: A compound type t describes values that contain a value of one of the types t1, t2, …, or tn. For a type that contains an int or a bool in ML, we need <code>datatype bindings</code>. In object-oriented languages with classes like Java, one-of types are achieved with subclassing, but that is a topic for much later in the course.</li><li><strong>Self-reference</strong>: A compound type t may refer to itself in its definition in order to describe recursive data structures like lists and trees. This is useful in combination with each-of and one-of types. For example, int list describes values that either contain nothing or contain an int and another int list. </li></ul><h2 id="Records-Another-Approach-to-Each-of-Types"><a href="#Records-Another-Approach-to-Each-of-Types" class="headerlink" title="Records: Another Approach to Each-of Types"></a>Records: Another Approach to <strong>Each-of</strong> Types</h2><p>Record types are “each-of” types where each component is a <code>named field</code>.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;foo : <span class="built_in">int</span>, bar : <span class="built_in">int</span>*<span class="built_in">bool</span>, baz : <span class="built_in">bool</span>*<span class="built_in">int</span>&#125;</span><br></pre></td></tr></table></figure><p>In ML, we do not have to declare that we want a record type with particular field names and field types — we just write down a record expression and the type-checker gives it the right type.</p><p>Now that we know how to build record values, we need a way to access their pieces. For now, we will use <code>#foo e</code> where <code>foo</code> is a field name. </p><h3 id="The-truth-of-tuple"><a href="#The-truth-of-tuple" class="headerlink" title="The truth of tuple"></a>The truth of tuple</h3><p>In fact, this is how ML actually defines tuples: A tuple is a record. That is, all the syntax for tuples is just a convenient way to write down and use records. The REPL just always uses the tuple syntax where possible, so if you evaluate {2=1+2, 1=3+4} it will print the result as (7,3). Using the tuple syntax is better style, but we did not need to give tuples their own semantics: we can instead use the “another way of writing” rules above and then reuse the semantics for records.</p><p>This is the first of many examples we will see of <code>syntactic sugar</code>. We say, Tuples are just syntactic sugar for records with fields named 1, 2, …, n.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> z = (<span class="number">3</span>,<span class="number">7</span>) : <span class="built_in">int</span> * <span class="built_in">int</span></span><br><span class="line"><span class="keyword">val</span> z = &#123;<span class="number">1</span>=<span class="number">3</span>,<span class="number">3</span>=<span class="number">7</span>&#125; : &#123;<span class="number">1</span>:<span class="built_in">int</span>, <span class="number">3</span>:<span class="built_in">int</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="Datatype-Bindings-Our-Own-One-of-Types"><a href="#Datatype-Bindings-Our-Own-One-of-Types" class="headerlink" title="Datatype Bindings: Our Own One-of Types"></a>Datatype Bindings: Our Own <strong>One-of</strong> Types</h2><p>We now introduce datatype bindings, our third kind of binding after variable bindings and function bindings.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">datatype mytype = TwoInts of int * int</span><br><span class="line">                | Str of string</span><br><span class="line">                | Pizza</span><br></pre></td></tr></table></figure><p>Roughly, this defines a new type where values have an int * int or a string or nothing. Any value will also be <code>tagged</code> with information that lets us know which variant it is: These tags, which we will call constructors, are <code>TwoInts</code>, <code>Str</code>, and <code>Pizza</code>.</p><p>A constructor is two different things. First, it is either a function for creating values of the new type (if the variant has of t for some type t) or it is actually a value of the new type (otherwise). In our example, TwoInts is a function of type int*int -&gt; mytype, Str is a function of type string-&gt;mytype, and Pizza is a value of type mytype. Second, we use constructors in case-expressions as described further below.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">datatype</span> mytype = <span class="type">TwoInts</span> <span class="keyword">of</span> <span class="built_in">int</span> * <span class="built_in">int</span> </span><br><span class="line">                | <span class="type">Str</span> <span class="keyword">of</span> <span class="built_in">string</span> </span><br><span class="line">                | <span class="type">Pizza</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> a = <span class="type">Str</span> <span class="string">"hi"</span></span><br><span class="line"><span class="keyword">val</span> b = <span class="type">Str</span></span><br><span class="line"><span class="keyword">val</span> c = <span class="type">Pizza</span></span><br><span class="line"><span class="keyword">val</span> d = <span class="type">TwoInts</span>(<span class="number">1</span>+<span class="number">2</span>,<span class="number">3</span>+<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> e = a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">(* val a = Str "hi" : mytype</span></span><br><span class="line"><span class="comment">val b = fn : string -&gt; mytype</span></span><br><span class="line"><span class="comment">val c = Pizza : mytype</span></span><br><span class="line"><span class="comment">val d = TwoInts (3,7) : mytype</span></span><br><span class="line"><span class="comment">val e = Str "hi" : mytype *)</span></span><br></pre></td></tr></table></figure><h2 id="How-ML-Provides-Access-to-Datatype-Values-Case-Expressions"><a href="#How-ML-Provides-Access-to-Datatype-Values-Case-Expressions" class="headerlink" title="How ML Provides Access to Datatype Values: Case Expressions"></a>How ML Provides Access to Datatype Values: Case Expressions</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Conceptual-Ways-to-Build-New-Types&quot;&gt;&lt;a href=&quot;#Conceptual-Ways-to-Build-New-Types&quot; class=&quot;headerlink&quot; title=&quot;Conceptual Ways to Build
      
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
    
  </entry>
  
  <entry>
    <title>Bayesian networks</title>
    <link href="https://zhangruochi.com/Bayesian-networks/2020/03/23/"/>
    <id>https://zhangruochi.com/Bayesian-networks/2020/03/23/</id>
    <published>2020-03-23T20:03:48.000Z</published>
    <updated>2020-03-23T22:48:28.393Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Probabilistic-modeling-with-Bayesian-networks"><a href="#Probabilistic-modeling-with-Bayesian-networks" class="headerlink" title="Probabilistic modeling with Bayesian networks"></a>Probabilistic modeling with Bayesian networks</h2><p>Directed graphical models (a.k.a. Bayesian networks) are a family of probability distributions that admit a compact parametrization that can be naturally described using a directed graph.</p><p>The general idea behind this parametrization is surprisingly simple. Recall that by the chain rule, we can write any probability $p$ as:</p><script type="math/tex; mode=display">p(x_1, x_2, \dotsc, x_n) = p(x_1) p(x_2 \mid x_1) \cdots p(x_n \mid x_{n-1}, \dotsc, x_2, x_1).</script><p>A <strong>compact</strong> Bayesian network is a distribution in which each factor on the right hand side depends only on a small number of <em>ancestor variables</em> $x_{A_i}$:</p><script type="math/tex; mode=display">p(x_i \mid x_{i-1}, \dotsc, x_1) = p(x_i \mid x_{A_i}).</script><p>For example, in a model with five variables, we may choose to approximate the factor $p(x_5 \mid x_4, x_3, x_2, x_1)$ with $p(x_5 \mid x_4, x_3)$. In this case, we write $x_{A_5} = \{x_4, x_3\}$.</p><h2 id="Graphical-representation"><a href="#Graphical-representation" class="headerlink" title="Graphical representation"></a>Graphical representation</h2><p>As an example, consider a model of a student’s grade <script type="math/tex">g</script> on an exam. This grade depends on the exam’s difficulty $d$ and the student’s intelligence $i$; it also affects the quality $l$ of the reference letter from the professor who taught the course. The student’s intelligence $i$ affects the SAT score $s$ as well. Each variable is binary, except for $g$, which takes 3 possible values.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Bayes net model describing the performance of a student on an exam. The distribution can be represented a product of conditional probability distributions specified by tables. The form of these distributions is described by edges in the graph.</div></center><p>The joint probability distribution over the 5 variables naturally factorizes as follows:</p><script type="math/tex; mode=display">p(l, g, i, d, s) = p(l \mid g)\, p(g \mid i, d)\, p(i)\, p(d)\, p(s \mid i).</script><p>The graphical representation of this distribution is a DAG that visually specifies how random variables depend on each other. The graph clearly indicates that the letter depends on the grade, which in turn depends on the student’s intelligence and the difficulty of the exam.</p><p>Another way to interpret directed graphs is in terms of stories for how the data was generated. In the above example, to determine the quality of the reference letter, we may first sample an intelligence level and an exam difficulty; then, a student’s grade is sampled given these parameters; finally, the recommendation letter is generated based on that grade.</p><h2 id="Formal-definition"><a href="#Formal-definition" class="headerlink" title="Formal definition."></a>Formal definition.</h2><p>Formally, a Bayesian network is a directed graph $G = (V,E)$ together with</p><ul><li>A random variable $x_i$ for each node $i \in V$.</li><li>One conditional probability distribution (CPD) $p(x_i \mid x_{A_i})$ per node, specifying the probability of $x_i$ conditioned on its parents’ values.</li></ul><p>Thus, a Bayesian network defines a probability distribution $p$. Conversely, we say that a probability $p$ <strong>factorizes</strong> over a DAG $G$ if it can be decomposed into a product of factors, as specified by $G$.</p><p>It is not hard to see that a probability represented by a Bayesian network will be valid: clearly, it will be non-negative and one can show using an induction argument (and using the fact that the CPDs are valid probabilities) that the sum over all variable assignments will be one. Conversely, we can also show by counter-example that when <script type="math/tex">G</script> contains cycles, its associated probability may not sum to one.</p><h2 id="The-dependencies-of-a-Bayes-net"><a href="#The-dependencies-of-a-Bayes-net" class="headerlink" title="The dependencies of a Bayes net"></a>The dependencies of a Bayes net</h2><p>To summarize, Bayesian networks represent probability distributions that can be formed via products of smaller, local conditional probability distributions (one for each variable). By expressing a probability in this form, we are introducing into our model assumptions that certain variables are independent.</p><p>This raises the question: which independence assumptions are we exactly making by using a Bayesian network model with a given structure described by $G$? This question is important for two reasons: we should know precisely what model assumptions we are making (and whether they are correct); also, this information will help us design more efficient inference algorithms later on.</p><p>Let us use the notation $I(p)$ to denote the set of all independencies that hold for a joint distribution $p$. For example, if $p(x,y) = p(x) p(y)$, then we say that $x \perp y \in I(p)$.</p><h3 id="Independencies-described-by-directed-graphs"><a href="#Independencies-described-by-directed-graphs" class="headerlink" title="Independencies described by directed graphs"></a>Independencies described by directed graphs</h3><p>It turns out that a Bayesian network $p$ very elegantly describes many independencies in $I(p)$; these independencies can be recovered from the graph by looking at three types of structures.</p><p>For simplicity, let’s start by looking at a Bayes net $G$ with three nodes: $A$, $B$, and $C$. In this case, <script type="math/tex">G</script> essentially has only three possible structures, each of which leads to different independence assumptions. The interested reader can easily prove these results using a bit of algebra.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Bayesian networks over three variables, encoding different types of dependencies: cascade (a,b), common parent (c), and v-structure (d).</div></center><ul><li><p><strong>Common parent.</strong> If $G$ is of the form $A \leftarrow B \rightarrow C$, and $B$ is observed, then $A \perp C \mid B$. However, if $B$ is unobserved, then $A \not\perp C$. Intuitively this stems from the fact that $B$ contains all the information that determines the outcomes of $A$ and $C$; once it is observed, there is nothing else that affects these variables’ outcomes.</p></li><li><p><strong>Cascade.</strong>: If $G$ equals $A \rightarrow B \rightarrow C$, and $B$ is again observed, then, again $A \perp C \mid B$. However, if $B$ is unobserved, then $A \not\perp C$. Here, the intuition is again that $B$ holds all the information that determines the outcome of $C$; thus, it does not matter what value $A$ takes.</p></li><li><strong>V-structure.</strong> (also known as <em>explaining away</em>): If $G$ is $A \rightarrow C \leftarrow B$, then knowing $C$ couples $A$ and $B$. In other words, $A \perp B$ if $C$ is unobserved, but $A \not\perp B \mid C$ if $C$ is observed.</li></ul><p>The latter case requires additional explanation. Suppose that $C$ is a Boolean variable that indicates whether our lawn is wet one morning; $A$ and $B$ are two explanations for it being wet: either it rained (indicated by $A$), or the sprinkler turned on (indicated by $B$). If we know that the grass is wet ($C$ is true) and the sprinkler didn’t go on ($B$ is false), then the probability that $A$ is true must be one, because that is the only other possible explanation. Hence, $A$ and $B$ are not independent given $C$.</p><p>These structures clearly describe the independencies encoded by a three-variable Bayesian net. </p><h3 id="d-separation"><a href="#d-separation" class="headerlink" title="$d$-separation"></a>$d$-separation</h3><p>We can extend them to general networks by applying them recursively over any larger graph. This leads to a notion called $d$-separation (where $d$ stands for directed).</p><p>Let $Q$, $W$, and $O$ be three sets of nodes in a Bayesian Network $G$. We say that $Q$ and $W$ are $d$-separated given $O$ (<em>i.e.</em> the variables $O$ are observed) if $Q$ and $W$ are not connected by an <em>active path</em>. An undirected path in $G$ is called <em>active</em> given observed variables $O$ if for every consecutive triple of variables $X,Y,Z$ on the path, one of the following holds:</p><ul><li>$X \leftarrow Y \leftarrow Z$, and $Y$ is unobserved $Y \not\in O$</li><li>$X \rightarrow Y \rightarrow Z$, and $Y$ is unobserved $Y \not\in O$</li><li>$X \leftarrow Y \rightarrow Z$, and $Y$ is unobserved $Y \not\in O$</li><li>$X \rightarrow Y \leftarrow Z$, and $Y$ or any of its descendants are observed.</li></ul><p>In other words: A trail $X1, \cdots, X_n$ is active given Z if:</p><ul><li>for any v-structure we have that $X_i$ or one of its descendants<br>$\in$ Z</li><li>no other $X_i$ is in Z</li></ul><p>In this example, $X_1$ and $X_6$ are $d$-separated given $X_2, X_3$.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="7.png" width="70%" height="70%"></center><p>However, $X_2, X_3$ are not $d$-separated given $X_1, X_6$. There is an active pass which passed through the V-structure created when $X_6$ is observed.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="8.png" width="70%" height="70%"></center><p>For example, in the graph below, $X_1$ and $X_6$ are $d$-separated given $X_2, X_3$. However, $X_2, X_3$ are not $d$-separated given $X_1, X_6$, because we can find an active path $(X_2, X_6, X_5, X_3)$</p><p>The notion of $d$-separation is useful, because it lets us describe a large fraction of the dependencies that hold in our model. Let $I(G) = \{(X \perp Y \mid Z) : \text{$X,Y$ are $d$-sep given $Z$}\}$ be a set of variables that are $d$-separated in $G$.</p><h4 id="Two-theorem"><a href="#Two-theorem" class="headerlink" title="Two theorem"></a>Two theorem</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="9.png" width="70%" height="70%"></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="10.png" width="70%" height="70%"></center><h3 id="I-map"><a href="#I-map" class="headerlink" title="$I$-map"></a>$I$-map</h3><blockquote><p>If $p$ factorizes over $G$, then $I(G) \subseteq I(p)$. In this case, we say that $G$ is an $I$-map (independence map) for $p$.</p></blockquote><p>In other words, all the independencies encoded in $G$ are sound: variables that are $d$-separated in $G$ are truly independent in $p$. However, the converse is not true: a distribution may factorize over $G$, yet have independencies that are not captured in $G$.</p><p>In a way this is almost a trivial statement. If $p(x,y) = p(x)p(y)$, then this distribution still factorizes over the graph $y \rightarrow x$, since we can always write it as $p(x,y) = p(x\mid y)p(y)$ with a CPD $p(x\mid y)$ in which the probability of $x$ does not actually vary with $y$. However, we can construct a graph that matches the structure of $p$ by simply removing that unnecessary edge.</p><h4 id="Theorem"><a href="#Theorem" class="headerlink" title="Theorem"></a>Theorem</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="11.png" width="70%" height="70%"></center><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Two equivalent views of graph structure:</p><ul><li><strong>Factorization</strong>: $G$ allows $P$ to be represented</li><li><strong>I-map</strong>: Independencies encoded by G hold in P<ul><li>If $P$ factorizes over a graph $G$, we can read from the graph independencies that must hold in $P$ (an independency map)</li></ul></li></ul><h2 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="12.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Naive Bayes Probabilistic Graphical Model</div></center><p>From the graph,</p><script type="math/tex; mode=display">(x_i \perp x_j | c) \quad \text{for all} \quad x_i, x_j</script><p>Then, we can get</p><script type="math/tex; mode=display">P(C, x_i, \cdots, x_n) = P(c)\prod_{i=1}^{n}P(x_i | C)</script><p>Therefore, the raito of two class is:</p><script type="math/tex; mode=display">\frac{P(C = c^{1} | x_i, \cdots, x_n)}{P(C = c^{2} | x_i, \cdots, x_n)} = \frac{P(C = c^{1})}{P(C = c^{2})}\prod_{i=1}^{n}\frac{P(x_i | C = c^{1})}{P( x_i | C = c^{2})}</script><p>Indtroduce <code>Bernoulli</code>(or others) to calcute probabilities</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="13.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Bernoulli Naive Bayes for Text</div></center><ul><li>Simple approach for classification <ul><li>Computationally efficient</li><li>Easy to construct</li></ul></li><li>Surprisingly effective in domains with many <strong>weakly</strong> relevant features</li><li>Strong independence assumptions reduce performance when many features are strongly correlated</li></ul><h2 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h2><h4 id="Calculate-the-number-of-parameters-of-a-distribution-model"><a href="#Calculate-the-number-of-parameters-of-a-distribution-model" class="headerlink" title="Calculate the number of parameters of a distribution model"></a>Calculate the number of parameters of a distribution model</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="100%" height="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">The number of parameters</div></center><h4 id="Inter-causal-reasoning"><a href="#Inter-causal-reasoning" class="headerlink" title="Inter-causal reasoning"></a>Inter-causal reasoning</h4><center>    <img style="border-radius: 0.1em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="100%" height="100%">    <img style="border-radius: 0.1em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="100%" height="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Inter-causal reasoning</div></center><h4 id="Independencies-in-a-graph"><a href="#Independencies-in-a-graph" class="headerlink" title="Independencies in a graph"></a>Independencies in a graph</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="100%" height="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Independencies in a graph</div></center><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://ermongroup.github.io/cs228-notes/" target="_blank" rel="noopener">https://ermongroup.github.io/cs228-notes/</a></li><li>Course note from Coursera course <a href="https://www.coursera.org/learn/probabilistic-graphical-models/" target="_blank" rel="noopener">Probabilistic graphical models</a> lectured by Daphne Koller</li></ul>]]></content>
    
    <summary type="html">
    
      how do we choose a probability distribution to model some interesting aspect of the world?
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Probabilistic Graphical Models" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Probabilistic-Graphical-Models/"/>
    
    
  </entry>
  
  <entry>
    <title>Fucking distributions</title>
    <link href="https://zhangruochi.com/Fucking-distributions/2020/03/22/"/>
    <id>https://zhangruochi.com/Fucking-distributions/2020/03/22/</id>
    <published>2020-03-22T07:44:30.000Z</published>
    <updated>2020-03-22T07:50:53.670Z</updated>
    
    <content type="html"><![CDATA[<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="overview.png" width="100%" height="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">overview</div></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line"><span class="keyword">import</span> operator <span class="keyword">as</span> op</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations</span><br><span class="line"><span class="keyword">import</span> scipy.special <span class="keyword">as</span> sps</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure><h2 id="Uniform-distribution-continuous"><a href="#Uniform-distribution-continuous" class="headerlink" title="Uniform distribution(continuous)"></a>Uniform distribution(continuous)</h2><ul><li>Uniform distribution has same probaility value on [a, b], easy probability.</li></ul><script type="math/tex; mode=display">f(x)=\begin{cases}  \frac{1}{b - a} & \mathrm{for}\ a \le x \le b, \\[8pt]  0 & \mathrm{for}\ x<a\ \mathrm{or}\ x>b  \end{cases}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">uniform</span><span class="params">(x, a, b)</span>:</span></span><br><span class="line">    y = [<span class="number">1</span> / (b-a) <span class="keyword">if</span> a &lt;= val <span class="keyword">and</span> val &lt;= b <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> val <span class="keyword">in</span> x]</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-100</span>,<span class="number">100</span>)</span><br><span class="line"><span class="keyword">for</span> dis <span class="keyword">in</span> [(<span class="number">-50</span>, <span class="number">50</span>), (<span class="number">10</span>, <span class="number">20</span>)]:</span><br><span class="line">    a, b = dis[<span class="number">0</span>], dis[<span class="number">1</span>]</span><br><span class="line">    x, y, u, s = uniform(x, a, b)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$\mu=%.2f,\ \sigma=%.2f$'</span> % (u, s))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_5_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,<span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">15</span>, density=<span class="keyword">True</span>)</span><br><span class="line">plt.plot(bins, np.ones_like(bins), linewidth=<span class="number">2</span>, color=<span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x11eae0cc0&gt;]</code></pre><p><img src="output_6_1.png" alt="png"></p><h2 id="Bernoulli-distribution-discrete"><a href="#Bernoulli-distribution-discrete" class="headerlink" title="Bernoulli distribution(discrete)"></a>Bernoulli distribution(discrete)</h2><ul><li>Bernoulli distribution is not considered about prior probability P(X). Therefore, if we optimize to the maximum likelihood, we will be vulnerable to overfitting.</li><li>We use binary cross entropy to classify binary classification. It has same form like taking a negative log of the bernoulli distribution.</li></ul><script type="math/tex; mode=display">f(k;p) = \begin{cases}   p & \text{if }k=1, \\   q = 1-p & \text {if } k = 0. \end{cases}</script><ul><li>For Logistic Regression<script type="math/tex; mode=display">p=p(y|x,\theta)=p_{1}^{y_{i}}\ast p_{0}^{1-y_{i}}</script><script type="math/tex; mode=display">max \sum_{i=1}^{m}({y_{i}\log{p_{1}}+(1-y_{i})\log{p_{0})}}</script></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bernoulli</span><span class="params">(p, k)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> p <span class="keyword">if</span> k <span class="keyword">else</span> <span class="number">1</span> - p</span><br><span class="line"></span><br><span class="line">n_experiment = <span class="number">100</span></span><br><span class="line">p = <span class="number">0.6</span></span><br><span class="line">x = np.arange(n_experiment)</span><br><span class="line">y = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(n_experiment):</span><br><span class="line">    pick = bernoulli(p, k=bool(random.getrandbits(<span class="number">1</span>)))</span><br><span class="line">    y.append(pick)</span><br><span class="line"></span><br><span class="line">u, s = np.mean(y), np.std(y)</span><br><span class="line">plt.scatter(x, y, label=<span class="string">r'$p=%.2f,\ \mu=%.2f,\ \sigma=%.2f$'</span> % (p,u, s))</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_8_0.png" alt="png"></p><h2 id="Binomial-distribution-discrete"><a href="#Binomial-distribution-discrete" class="headerlink" title="Binomial distribution(discrete)"></a>Binomial distribution(discrete)</h2><ul><li>Binomial distribution with parameters <strong>n</strong> and <strong>p</strong> is the discrete probability distribution of the number of successes in a sequence of n independent experiments.</li><li>Binomial distribution is distribution considered prior probaility by specifying the number to be picked in advance.</li></ul><script type="math/tex; mode=display">f(k,n,p) = \Pr(k;n,p) = \Pr(X = k) = \binom{n}{k}p^k(1-p)^{n-k}</script><p>for k = 0, 1, 2, …, n, where</p><script type="math/tex; mode=display">\binom{n}{k} =\frac{n!}{k!(n-k)!}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">const</span><span class="params">(n, r)</span>:</span></span><br><span class="line">    r = min(r, n-r)</span><br><span class="line">    numer = reduce(op.mul, range(n, n-r, <span class="number">-1</span>), <span class="number">1</span>)</span><br><span class="line">    denom = reduce(op.mul, range(<span class="number">1</span>, r+<span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> numer / denom</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binomial</span><span class="params">(n, p)</span>:</span></span><br><span class="line">    q = <span class="number">1</span> - p</span><br><span class="line">    y = [const(n, k) * (p ** k) * (q ** (n-k)) <span class="keyword">for</span> k <span class="keyword">in</span> range(n)]</span><br><span class="line">    <span class="keyword">return</span> y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ls <span class="keyword">in</span> [(<span class="number">0.5</span>, <span class="number">20</span>), (<span class="number">0.7</span>, <span class="number">40</span>), (<span class="number">0.5</span>, <span class="number">40</span>)]:</span><br><span class="line">    p, n_experiment = ls[<span class="number">0</span>], ls[<span class="number">1</span>]</span><br><span class="line">    x = np.arange(n_experiment)</span><br><span class="line">    y, u, s = binomial(n_experiment, p)</span><br><span class="line">    plt.scatter(x, y, label=<span class="string">r'$n_&#123;experiment&#125;=%d,\ p=%.2f,\ \mu=%.2f,\ \sigma=%.2f$'</span> % (n_experiment,p, u, s))</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_10_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.binomial(<span class="number">10</span>, <span class="number">0.8</span>, <span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">20</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_11_0.png" alt="png"></p><h2 id="Multi-Bernoulli-distribution-Categorical-distribution-discrete"><a href="#Multi-Bernoulli-distribution-Categorical-distribution-discrete" class="headerlink" title="Multi-Bernoulli distribution, Categorical distribution(discrete)"></a>Multi-Bernoulli distribution, Categorical distribution(discrete)</h2><ul><li>Multi-bernoulli called categorical distribution, is a probability expanded more than 2.</li><li><strong>cross entopy</strong> has same form like taking a negative log of the Multi-Bernoulli distribution.</li></ul><script type="math/tex; mode=display">f(x\mid \boldsymbol{p} ) = \prod_{i=1}^k p_i^{[x=i]}</script><p>where $[x = i]$ evaluates to 1 if $x = i$, 0 otherwise. There are various advantages of this formulation</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categorical</span><span class="params">(p, k)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> p[k]</span><br><span class="line"></span><br><span class="line">n_experiment = <span class="number">100</span></span><br><span class="line">p = [<span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0.7</span>]</span><br><span class="line">x = np.arange(n_experiment)</span><br><span class="line">y = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(n_experiment):</span><br><span class="line">    pick = categorical(p, k = random.randint(<span class="number">0</span>, len(p) - <span class="number">1</span>))</span><br><span class="line">    y.append(pick)</span><br><span class="line"></span><br><span class="line">u, s = np.mean(y), np.std(y)</span><br><span class="line">plt.scatter(x, y, label=<span class="string">r'$p=[0.2, 0.1, 0.7],\mu=%.2f,\ \sigma=%.2f$'</span> % (u, s))</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_13_0.png" alt="png"></p><h2 id="Multinomial-distribution-discrete"><a href="#Multinomial-distribution-discrete" class="headerlink" title="Multinomial distribution(discrete)"></a>Multinomial distribution(discrete)</h2><ul><li>The multinomial distribution has the same relationship with the categorical distribution as the relationship between Bernoull and Binomial.</li><li>For example, it models the probability of counts for each side of a <strong>k-sided</strong> die rolled n times. For n independent trials each of which leads to a success for exactly one of k categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of <strong>any particular combination</strong> of numbers of successes for the various categories.</li><li>When k is 2 and n is 1, the multinomial distribution is the Bernoulli distribution. When k is 2 and n is bigger than 1, it is the binomial distribution. When k is bigger than 2 and n is 1, it is the categorical distribution.</li></ul><script type="math/tex; mode=display">\begin{align}f(x_1,\ldots,x_k;n,p_1,\ldots,p_k) & {} = \Pr(X_1 = x_1 \text{ and } \dots \text{ and } X_k = x_k) \\& {} = \begin{cases} { \displaystyle {n! \over x_1!\cdots x_k!}p_1^{x_1}\times\cdots\times p_k^{x_k}}, \quad &\text{when } \sum_{i=1}^k x_i=n \\  \\0 & \text{otherwise,} \end{cases}\end{align}</script><p>for non-negative integers $x_1, \cdots, x_k$.</p><p>The probability mass function can be expressed using the gamma function as:</p><script type="math/tex; mode=display">f(x_1,\dots, x_{k}; p_1,\ldots, p_k) = \frac{\Gamma(\sum_i x_i + 1)}{\prod_i \Gamma(x_i+1)} \prod_{i=1}^k p_i^{x_i}.</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">factorial</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> reduce(op.mul, range(<span class="number">1</span>, n + <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">const</span><span class="params">(n, a, b, c)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        return n! / a! b! c!, where a+b+c == n</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">assert</span>  a + b + c == n</span><br><span class="line"></span><br><span class="line">    numer = factorial(n)</span><br><span class="line">    denom = factorial(a) * factorial(b) * factorial(c)</span><br><span class="line">    <span class="keyword">return</span> numer / denom</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multinomial</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param x : list, sum(x) should be `n`</span></span><br><span class="line"><span class="string">    :param n : number of trial</span></span><br><span class="line"><span class="string">    :param p: list, sum(p) should be `1`</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># get all a,b,c where a+b+c == n, a&lt;b&lt;c</span></span><br><span class="line">    ls = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(j, n + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> i + j + k == n:</span><br><span class="line">                    ls.append([i, j, k])</span><br><span class="line"></span><br><span class="line">    y = [const(n, l[<span class="number">0</span>], l[<span class="number">1</span>], l[<span class="number">2</span>]) <span class="keyword">for</span> l <span class="keyword">in</span> ls]</span><br><span class="line">    x = np.arange(len(y))</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n_experiment <span class="keyword">in</span> [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>]:</span><br><span class="line">    x, y, u, s = multinomial(n_experiment)</span><br><span class="line">    plt.scatter(x, y, label=<span class="string">r'$trial=%d$'</span> % (n_experiment))</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_15_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.multinomial(<span class="number">20</span>, [<span class="number">1</span>/<span class="number">6.</span>]*<span class="number">6</span>, size=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>array([[1, 1, 2, 3, 8, 5],       [2, 4, 3, 3, 6, 2],       [1, 6, 3, 2, 3, 5],       [5, 3, 4, 4, 2, 2],       [3, 8, 4, 2, 0, 3],       [2, 4, 1, 5, 1, 7],       [6, 3, 2, 4, 3, 2],       [8, 2, 1, 1, 4, 4],       [3, 6, 4, 1, 4, 2],       [3, 2, 3, 3, 6, 3]])</code></pre><h2 id="Beta-distribution-continuous"><a href="#Beta-distribution-continuous" class="headerlink" title="Beta distribution(continuous)"></a>Beta distribution(continuous)</h2><ul><li>Beta distribution is conjugate to the binomial and Bernoulli distributions.</li><li>Using conjucation, we can get the posterior distribution more easily using the prior distribution we know.</li><li>Uniform distiribution is same when beta distribution met special case(alpha=1, beta=1).</li></ul><script type="math/tex; mode=display">\begin{align}f(x;\alpha,\beta) & = \mathrm{constant}\cdot x^{\alpha-1}(1-x)^{\beta-1} \\[3pt]& = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{\displaystyle \int_0^1 u^{\alpha-1} (1-u)^{\beta-1}\, du} \\[6pt]& = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\, x^{\alpha-1}(1-x)^{\beta-1} \\[6pt]& = \frac{1}{B(\alpha,\beta)} x^{\alpha-1}(1-x)^{\beta-1}\end{align}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma_function</span><span class="params">(n)</span>:</span></span><br><span class="line">    cal = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        cal *= i</span><br><span class="line">    <span class="keyword">return</span> cal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">beta</span><span class="params">(x, a, b)</span>:</span></span><br><span class="line"></span><br><span class="line">    gamma = gamma_function(a + b) / \</span><br><span class="line">            (gamma_function(a) * gamma_function(b))</span><br><span class="line">    y = gamma * (x ** (a - <span class="number">1</span>)) * ((<span class="number">1</span> - x) ** (b - <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ls <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">3</span>), (<span class="number">5</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">5</span>)]:</span><br><span class="line">    a, b = ls[<span class="number">0</span>], ls[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x in [0, 1], trial is 1/0.001 = 1000</span></span><br><span class="line">    x = np.arange(<span class="number">0</span>, <span class="number">1</span>, <span class="number">0.001</span>, dtype=np.float)</span><br><span class="line">    x, y, u, s = beta(x, a=a, b=b)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$\mu=%.2f,\ \sigma=%.2f,'</span></span><br><span class="line">                         <span class="string">r'\ \alpha=%d,\ \beta=%d$'</span> % (u, s, a, b))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_18_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.beta(<span class="number">2</span>, <span class="number">5</span>, size=<span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">20</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_19_0.png" alt="png"></p><h2 id="Gamma-distribution-continuous"><a href="#Gamma-distribution-continuous" class="headerlink" title="Gamma distribution(continuous)"></a>Gamma distribution(continuous)</h2><ul><li><p>Gamma distribution will be beta distribution, if $\frac{Gamma(a,1)}{Gamma(a,1) + Gamma(b,1)}$ is same with $Beta(a,b)$.</p></li><li><p>The exponential distribution and chi-squared distribution are special cases of the gamma distribution.</p></li></ul><p>A random variable X that is gamma-distributed with shape α and rate β is denoted:</p><script type="math/tex; mode=display">X \sim \Gamma(\alpha, \beta) \equiv \operatorname{Gamma}(\alpha,\beta)</script><p>The corresponding probability density function in the shape-rate parametrization is:</p><script type="math/tex; mode=display">\begin{align}f(x;\alpha,\beta) & = \frac{ \beta^\alpha x^{\alpha-1} e^{-\beta x}}{\Gamma(\alpha)} \quad \text{ for } x > 0 \quad \alpha, \beta > 0, \\[6pt]\end{align}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma_function</span><span class="params">(n)</span>:</span></span><br><span class="line">    cal = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        cal *= i</span><br><span class="line">    <span class="keyword">return</span> cal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma</span><span class="params">(x, a, b)</span>:</span></span><br><span class="line">    c = (b ** a) / gamma_function(a)</span><br><span class="line">    y = c * (x ** (a - <span class="number">1</span>)) * np.exp(-b * x)</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ls <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">1</span>), (<span class="number">3</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>)]:</span><br><span class="line">    a, b = ls[<span class="number">0</span>], ls[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    x = np.arange(<span class="number">0</span>, <span class="number">20</span>, <span class="number">0.01</span>, dtype=np.float)</span><br><span class="line">    x, y, u, s = gamma(x, a=a, b=b)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$\mu=%.2f,\ \sigma=%.2f,'</span></span><br><span class="line">                         <span class="string">r'\ \alpha=%d,\ \beta=%d$'</span> % (u, s, a, b))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_21_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a, b = <span class="number">2.</span>, <span class="number">2.</span></span><br><span class="line">s = np.random.gamma(a, b, <span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">50</span>, density=<span class="keyword">True</span>)</span><br><span class="line">y = bins**(a<span class="number">-1</span>)*(np.exp(-bins/b) /</span><br><span class="line">                      (sps.gamma(a)*b**b))</span><br><span class="line">plt.plot(bins, y, linewidth=<span class="number">2</span>, color=<span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x12cb80c50&gt;]</code></pre><p><img src="output_22_1.png" alt="png"></p><h2 id="Dirichlet-distribution-continuous"><a href="#Dirichlet-distribution-continuous" class="headerlink" title="Dirichlet distribution(continuous)"></a>Dirichlet distribution(continuous)</h2><ul><li>Dirichlet distribution is conjugate to the MultiNomial distributions. 即Dirichlet分布乘上一个多项分布的似然函数后，得到的后验分布仍然是一个Dirichlet分布。</li><li>If k=2, it will be Beta distribution.</li></ul><script type="math/tex; mode=display">f \left(x_1,\ldots, x_{K}; \alpha_1,\ldots, \alpha_K \right) = \frac{1}{\mathrm{B}(\boldsymbol\alpha)} \prod_{i=1}^K x_i^{\alpha_i - 1}</script><p>where $\{x_k\}_{k=1}^{k=K}$ belong to the standard $K-1$ simplex, or in other words: </p><script type="math/tex; mode=display">\sum_{i=1}^{K} x_i=1 \mbox{ and } x_i \ge 0 \mbox{ for all } i \in [1,K]</script><p>The normalizing constant is the multivariate beta function, which can be expressed in terms of the gamma function</p><script type="math/tex; mode=display">\mathrm{B}(\boldsymbol\alpha) = \frac{\prod_{i=1}^K \Gamma(\alpha_i)}{\Gamma\left(\sum_{i=1}^K \alpha_i\right)},\qquad\boldsymbol{\alpha}=(\alpha_1,\ldots,\alpha_K).</script><blockquote><p>Dirichlet分布可以看做是分布之上的分布。如何理解这句话，我们可以先举个例子：假设我们有一个骰子，其有六面，分别为{1,2,3,4,5,6}。现在我们做了10000次投掷的实验，得到的实验结果是六面分别出现了{2000,2000,2000,2000,1000,1000}次，如果用每一面出现的次数与试验总数的比值估计这个面出现的概率，则我们得到六面出现的概率，分别为{0.2,0.2,0.2,0.2,0.1,0.1}。现在，我们还不满足，我们想要做10000次试验，每次试验中我们都投掷骰子10000次。我们想知道，骰子六面出现概率为{0.2,0.2,0.2,0.2,0.1,0.1}的概率是多少（说不定下次试验统计得到的概率为{0.1, 0.1, 0.2, 0.2, 0.2, 0.2}这样了）。这样我们就在思考骰子六面出现概率分布这样的分布之上的分布。而这样一个分布就是Dirichlet分布。 </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalization</span><span class="params">(x, s)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :return: normalizated list, where sum(x) == s</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> [(i * s) / sum(x) <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sampling</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> normalization([random.randint(<span class="number">1</span>, <span class="number">100</span>),</span><br><span class="line">            random.randint(<span class="number">1</span>, <span class="number">100</span>), random.randint(<span class="number">1</span>, <span class="number">100</span>)], s=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma_function</span><span class="params">(n)</span>:</span></span><br><span class="line">    cal = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        cal *= i</span><br><span class="line">    <span class="keyword">return</span> cal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">beta_function</span><span class="params">(alpha)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param alpha: list, len(alpha) is k</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    numerator = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> alpha:</span><br><span class="line">        numerator *= gamma_function(a)</span><br><span class="line">    denominator = gamma_function(sum(alpha))</span><br><span class="line">    <span class="keyword">return</span> numerator / denominator</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dirichlet</span><span class="params">(x, a, n)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param x: list of [x[1,...,K], x[1,...,K], ...], shape is (n_trial, K)</span></span><br><span class="line"><span class="string">    :param a: list of coefficient, a_i &gt; 0</span></span><br><span class="line"><span class="string">    :param n: number of trial</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    c = (<span class="number">1</span> / beta_function(a))</span><br><span class="line">    y = [c * (xn[<span class="number">0</span>] ** (a[<span class="number">0</span>] - <span class="number">1</span>)) * (xn[<span class="number">1</span>] ** (a[<span class="number">1</span>] - <span class="number">1</span>))</span><br><span class="line">         * (xn[<span class="number">2</span>] ** (a[<span class="number">2</span>] - <span class="number">1</span>)) <span class="keyword">for</span> xn <span class="keyword">in</span> x]</span><br><span class="line">    x = np.arange(n)</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line">n_experiment = <span class="number">1200</span></span><br><span class="line"><span class="keyword">for</span> ls <span class="keyword">in</span> [(<span class="number">6</span>, <span class="number">2</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">7</span>, <span class="number">5</span>), (<span class="number">6</span>, <span class="number">2</span>, <span class="number">6</span>), (<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)]:</span><br><span class="line">    alpha = list(ls)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># random samping [x[1,...,K], x[1,...,K], ...], shape is (n_trial, K)</span></span><br><span class="line">    <span class="comment"># each sum of row should be one.</span></span><br><span class="line">    x = [sampling() <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>, n_experiment + <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    x, y, u, s = dirichlet(x, alpha, n=n_experiment)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$\alpha=(%d,%d,%d)$'</span> % (ls[<span class="number">0</span>], ls[<span class="number">1</span>], ls[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_24_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.dirichlet((<span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.5</span>), <span class="number">20</span>).transpose()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s.shape</span><br></pre></td></tr></table></figure><pre><code>(3, 20)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.barh(range(<span class="number">20</span>), s[<span class="number">0</span>])</span><br><span class="line">plt.barh(range(<span class="number">20</span>), s[<span class="number">1</span>], left=s[<span class="number">0</span>], color=<span class="string">'g'</span>)</span><br><span class="line">plt.barh(range(<span class="number">20</span>), s[<span class="number">2</span>], left=s[<span class="number">0</span>]+s[<span class="number">1</span>], color=<span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;BarContainer object of 20 artists&gt;</code></pre><p><img src="output_27_1.png" alt="png"></p><h2 id="Exponential-distribution-continuous"><a href="#Exponential-distribution-continuous" class="headerlink" title="Exponential distribution(continuous)"></a>Exponential distribution(continuous)</h2><ul><li>Exponential distribution is special cases of the gamma distribution when alpha is 1.</li></ul><script type="math/tex; mode=display">f(x;\lambda) = \begin{cases}\lambda e^{-\lambda x} & x \ge 0, \\0 & x < 0.\end{cases}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exponential</span><span class="params">(x, lamb)</span>:</span></span><br><span class="line">    y = lamb * np.exp(-lamb * x)</span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> lamb <span class="keyword">in</span> [<span class="number">0.5</span>, <span class="number">1</span>, <span class="number">1.5</span>]:</span><br><span class="line"></span><br><span class="line">    x = np.arange(<span class="number">0</span>, <span class="number">20</span>, <span class="number">0.01</span>, dtype=np.float)</span><br><span class="line">    x, y, u, s = exponential(x, lamb=lamb)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$\mu=%.2f,\ \sigma=%.2f,'</span></span><br><span class="line">                         <span class="string">r'\ \lambda=%d$'</span> % (u, s, lamb))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_29_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.exponential(scale = <span class="number">0.5</span>, size=<span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">20</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_30_0.png" alt="png"></p><h2 id="Gaussian-distribution-continuous"><a href="#Gaussian-distribution-continuous" class="headerlink" title="Gaussian distribution(continuous)"></a>Gaussian distribution(continuous)</h2><script type="math/tex; mode=display">f(x) = \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian</span><span class="params">(x, n)</span>:</span></span><br><span class="line">    u = x.mean()</span><br><span class="line">    s = x.std()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># divide [x.min(), x.max()] by n</span></span><br><span class="line">    x = np.linspace(x.min(), x.max(), n)</span><br><span class="line"></span><br><span class="line">    a = ((x - u) ** <span class="number">2</span>) / (<span class="number">2</span> * (s ** <span class="number">2</span>))</span><br><span class="line">    y = <span class="number">1</span> / (s * np.sqrt(<span class="number">2</span> * np.pi)) * np.exp(-a)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, y, x.mean(), x.std()</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-100</span>, <span class="number">100</span>) <span class="comment"># define range of x</span></span><br><span class="line">x, y, u, s = gaussian(x, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y, label=<span class="string">r'$\mu=%.2f,\ \sigma=%.2f$'</span> % (u, s))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_32_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mu, sigma = <span class="number">0</span>, <span class="number">0.1</span> <span class="comment"># mean and standard deviation</span></span><br><span class="line">s = np.random.default_rng().normal(mu, sigma, <span class="number">1000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">30</span>, density=<span class="keyword">True</span>)</span><br><span class="line">plt.plot(bins, <span class="number">1</span>/(sigma * np.sqrt(<span class="number">2</span> * np.pi)) *</span><br><span class="line">         np.exp( - (bins - mu)**<span class="number">2</span> / (<span class="number">2</span> * sigma**<span class="number">2</span>) ),</span><br><span class="line">         linewidth=<span class="number">2</span>, color=<span class="string">'r'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x11122d4e0&gt;]</code></pre><p><img src="output_33_1.png" alt="png"></p><h2 id="Poisson-distribution"><a href="#Poisson-distribution" class="headerlink" title="Poisson distribution"></a>Poisson distribution</h2><ul><li>在一个时间段内事件平均发生的次数服从泊松分布</li></ul><script type="math/tex; mode=display">\!f(k; \lambda)= \Pr(X = k)= \frac{\lambda^k e^{-\lambda}}{k!},</script><ul><li>e is Euler’s number (e = 2.71828…)</li><li>k! is the factorial of k.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.poisson(<span class="number">5</span>, <span class="number">10000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">14</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_35_0.png" alt="png"></p><h2 id="Chi-squared-distribution-continuous"><a href="#Chi-squared-distribution-continuous" class="headerlink" title="Chi-squared distribution(continuous)"></a>Chi-squared distribution(continuous)</h2><ul><li>Chi-square distribution with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables.</li><li>Chi-square distribution is special case of Beta distribution</li></ul><p>If $Z_1, \cdots, Z_k$ are independent, standard normal random variables, then the sum of their squares,</p><script type="math/tex; mode=display">Q\ = \sum_{i=1}^k Z_i^2 ,</script><p>is distributed according to the chi-square distribution with k degrees of freedom. This is usually denoted as</p><script type="math/tex; mode=display">Q\ \sim\ \chi^2(k)\ \ \text{or}\ \ Q\ \sim\ \chi^2_k .</script><p>The chi-square distribution has one parameter: a positive integer k that specifies the number of degrees of freedom (the number of $Z_i$ s).</p><p>The probability density function (pdf) of the chi-square distribution is</p><script type="math/tex; mode=display">f(x;\,k) =\begin{cases}  \dfrac{x^{\frac k 2 -1} e^{-\frac x 2}}{2^{\frac k 2} \Gamma\left(\frac k 2 \right)},  & x > 0; \\ 0, & \text{otherwise}.\end{cases}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma_function</span><span class="params">(n)</span>:</span></span><br><span class="line">    cal = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        cal *= i</span><br><span class="line">    <span class="keyword">return</span> cal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chi_squared</span><span class="params">(x, k)</span>:</span></span><br><span class="line"></span><br><span class="line">    c = <span class="number">1</span> / (<span class="number">2</span> ** (k/<span class="number">2</span>)) * gamma_function(k//<span class="number">2</span>)</span><br><span class="line">    y = c * (x ** (k/<span class="number">2</span> - <span class="number">1</span>)) * np.exp(-x /<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>]:</span><br><span class="line">    x = np.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">0.01</span>, dtype=np.float)</span><br><span class="line">    x, y, _, _ = chi_squared(x, k)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$k=%d$'</span> % (k))</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_37_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = np.random.chisquare(<span class="number">4</span>,<span class="number">10000</span>)</span><br><span class="line">count, bins, ignored = plt.hist(s, <span class="number">20</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_38_0.png" alt="png"></p><h2 id="Student-t-distribution-continuous"><a href="#Student-t-distribution-continuous" class="headerlink" title="Student-t distribution(continuous)"></a>Student-t distribution(continuous)</h2><ul><li>Definition</li></ul><p>Let $X_1, \cdots, X_n$ be independent and identically distributed as $N(\mu, \sigma^2)$, i.e. this is a sample of size $n$ from a normally distributed population with expected mean value $\mu$ and variance $\sigma^{2}$</p><p>Let</p><script type="math/tex; mode=display">\bar X = \frac 1 n \sum_{i=1}^n X_i</script><p>be the sample mean and let</p><script type="math/tex; mode=display">S^2 = \frac 1 {n-1} \sum_{i=1}^n (X_i - \bar X)^2</script><p>be the (Bessel-corrected) sample variance. </p><p>Then the random variable</p><script type="math/tex; mode=display">\frac{ \bar X - \mu} {S /\sqrt{n}}</script><p>has a standard normal distribution</p><p>Student’s t-distribution has the probability density function given by</p><script type="math/tex; mode=display">f(t) = \frac{\Gamma(\frac{\nu+1}{2})} {\sqrt{\nu\pi}\,\Gamma(\frac{\nu}{2})} \left(1+\frac{t^2}{\nu} \right)^{\!-\frac{\nu+1}{2}}</script><ul><li>$\nu$ is the number of degrees of freedom </li><li>$\Gamma$ is the gamma function.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gamma_function</span><span class="params">(n)</span>:</span></span><br><span class="line">    cal = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        cal *= i</span><br><span class="line">    <span class="keyword">return</span> cal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">student_t</span><span class="params">(x, freedom, n)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># divide [x.min(), x.max()] by n</span></span><br><span class="line">    x = np.linspace(x.min(), x.max(), n)</span><br><span class="line"></span><br><span class="line">    c = gamma_function((freedom + <span class="number">1</span>) // <span class="number">2</span>) \</span><br><span class="line">        / np.sqrt(freedom * np.pi) * gamma_function(freedom // <span class="number">2</span>)</span><br><span class="line">    y = c * (<span class="number">1</span> + x**<span class="number">2</span> / freedom) ** (-((freedom + <span class="number">1</span>) / <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, y, np.mean(y), np.std(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> freedom <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>]:</span><br><span class="line"></span><br><span class="line">    x = np.arange(<span class="number">-10</span>, <span class="number">10</span>) <span class="comment"># define range of x</span></span><br><span class="line">    x, y, _, _ = student_t(x, freedom=freedom, n=<span class="number">10000</span>)</span><br><span class="line">    plt.plot(x, y, label=<span class="string">r'$v=%d$'</span> % (freedom))</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_40_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Suppose the daily energy intake for 11 women in kilojoules (kJ) is:</span></span><br><span class="line"></span><br><span class="line">intake = np.array([<span class="number">5260.</span>, <span class="number">5470</span>, <span class="number">5640</span>, <span class="number">6180</span>, <span class="number">6390</span>, <span class="number">6515</span>, <span class="number">6805</span>, <span class="number">7515</span>, \</span><br><span class="line">                    <span class="number">7515</span>, <span class="number">8230</span>, <span class="number">8770</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## Does their energy intake deviate systematically from the recommended value of 7725 kJ?</span></span><br><span class="line"><span class="comment">## We have 10 degrees of freedom, so is the sample mean within 95% of the recommended value?</span></span><br><span class="line"></span><br><span class="line">s = np.random.standard_t(<span class="number">10</span>, size=<span class="number">100000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Calculate the t statistic, setting the ddof parameter to the unbiased value so the divisor in the standard deviation will be degrees of freedom, N-1.</span></span><br><span class="line"></span><br><span class="line">t = (np.mean(intake)<span class="number">-7725</span>)/(intake.std(ddof=<span class="number">1</span>)/np.sqrt(len(intake)))</span><br><span class="line"></span><br><span class="line">h = plt.hist(s, bins=<span class="number">100</span>, density=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_41_0.png" alt="png"></p><p>So the p-value is about 0.009, which says the null hypothesis has a probability of about 99% of being true.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.sum(s&lt;t) / float(len(s))</span><br></pre></td></tr></table></figure><pre><code>0.0086</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/graykode/distribution-is-all-you-need" target="_blank" rel="noopener">https://github.com/graykode/distribution-is-all-you-need</a></li><li><a href="https://blog.csdn.net/deropty/article/details/50266309" target="_blank" rel="noopener">https://blog.csdn.net/deropty/article/details/50266309</a></li></ul>]]></content>
    
    <summary type="html">
    
      Understanding distributions
    
    </summary>
    
    
      <category term="Math" scheme="https://zhangruochi.com/categories/Math/"/>
    
      <category term="Statistics" scheme="https://zhangruochi.com/categories/Math/Statistics/"/>
    
    
  </entry>
  
  <entry>
    <title>Gaussian Mixed Model Introduction</title>
    <link href="https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/"/>
    <id>https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/</id>
    <published>2020-03-15T04:44:46.000Z</published>
    <updated>2020-03-15T07:08:58.707Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Gaussian-Mixture-Models-高斯混合模型"><a href="#Gaussian-Mixture-Models-高斯混合模型" class="headerlink" title="Gaussian Mixture Models(高斯混合模型)"></a>Gaussian Mixture Models(高斯混合模型)</h2><p>高斯模型即正态分布，高斯混合模型就是几个正态分布的叠加，每一个正态分布代表一个类别，所以和K-means很像，高斯混合模型也可以用来做无监督的聚类分析。</p><h2 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h2><h3 id="Jensen’s-inequality"><a href="#Jensen’s-inequality" class="headerlink" title="Jensen’s inequality"></a>Jensen’s inequality</h3><p>For any concave function, we have</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Property of concave function</div></center><script type="math/tex; mode=display">f(\alpha a + (1-\alpha)b) \geq \alpha f(a) + (1 - \alpha) f(b)</script><p>Then, we have:</p><script type="math/tex; mode=display">f(\mathbb{E}_{p(t)}t) \geq \mathbb{E}_{p(t)}f(t)</script><h3 id="Kullback–Leibler-divergence"><a href="#Kullback–Leibler-divergence" class="headerlink" title="Kullback–Leibler divergence"></a>Kullback–Leibler divergence</h3><script type="math/tex; mode=display">\mathcal K \mathcal L (q || p) = \int q(x) log\frac{q(x)}{p(x)}dx</script><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><script type="math/tex; mode=display">max_{\theta} \prod_{i=1}^{N} p(x_i | \theta) = \prod_{i = 1}^{N} (\pi_1 \mathcal{N} (x_i | \mu_1, \mathbb{E_1}) + \cdots )</script><script type="math/tex; mode=display">\text{subject to} \qquad \pi_1 + \pi_2 + \pi_3 = 1; \pi_k \geq 0; k = 1,2,3</script><h3 id="Introducing-latent-variable"><a href="#Introducing-latent-variable" class="headerlink" title="Introducing latent variable"></a>Introducing latent variable</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="50%" height="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Latent Variable</div></center><script type="math/tex; mode=display">p(t=c| \theta) = \pi_c</script><script type="math/tex; mode=display">p(x | t = c , \theta) = \mathcal N ( x | \mu_c,\mathbb{E_c} )</script><h3 id="General-form-of-Expectation-Maximization"><a href="#General-form-of-Expectation-Maximization" class="headerlink" title="General form of Expectation Maximization"></a>General form of Expectation Maximization</h3><script type="math/tex; mode=display">p(x_i | \theta) = \sum_{c=1}^{3}p(x_i | t_i = c , \theta) p(t_i = c | \theta)</script><h2 id="基本步骤"><a href="#基本步骤" class="headerlink" title="基本步骤"></a>基本步骤</h2><h3 id="概率角度"><a href="#概率角度" class="headerlink" title="概率角度"></a>概率角度</h3><ol><li>初始化$\theta^{old}$</li><li>E step: 用 $\theta^{old}$计算样本对应隐变量的概率分布，即求后验概率：$p(Z|X,\theta^{old})$。然后计算完全数据的对数似然对后验概率的期望，它是变量$\theta$的函数:<script type="math/tex; mode=display">Q(\theta, \theta^{old}) = \sum_{Z}p(Z|X, \theta^{old})ln p(X,Z|\theta)</script></li><li>M step: 极大化Q函数,得到$\theta^{new}$</li><li>若不收敛、持续迭代。</li></ol><h3 id="程序角度"><a href="#程序角度" class="headerlink" title="程序角度"></a>程序角度</h3><ol><li>猜测有几个类别，既有几个高斯分布;</li><li>针对每一个高斯分布，随机给其均值和方差进行赋值;</li><li>针对每一个样本，计算其在各个高斯分布下的概率;<script type="math/tex; mode=display">f(x)=\frac{1 }{\sqrt\times\sigma}e^{\frac{1}{2}(\frac{x-\mu}{\sigma})^2}</script></li><li>针对每一个高斯分布，每一个样本对该高斯分布的贡献可以由其下的概率表示，如概率大则表示贡献大，反之亦然。这样把样本对该高斯分布的贡献作为权重来计算加权的均值和方差。之后替代其原本的均值和方差;</li><li>重复3~4直到每一个高斯分布的均值和方差收敛;</li><li>当高斯混合模型的特征值维数大于一维时，在计算加权的时候还要计算协方差，即要考虑不同维度之间的相互关联.</li></ol><blockquote><p>即通过模型来计算数据的期望值。通过更新参数μ和σ来让期望值最大化。这个过程可以不断迭代直到两次迭代中生成的参数变化非常小为止。该过程和k-means的算法训练过程很相似（k-means不断更新类中心来让结果最大化），只不过在这里的高斯模型中，我们需要同时更新两个参数：分布的均值和标准差。</p></blockquote><h2 id="GMM-VS-KMeans"><a href="#GMM-VS-KMeans" class="headerlink" title="GMM VS KMeans"></a>GMM VS KMeans</h2><p>KMeans 将样本分到离其最近的聚类中心所在的簇，也就是每个样本数据属于某簇的概率非零即1。对比KMeans，高斯混合的不同之处在于，样本点属于某簇的概率不是非零即1的，而是属于不同簇有不同的概率值。高斯混合模型假设所有样本点是由K个高斯分布混合而成的。</p><h2 id="Implementing-the-EM-Expectation-Maximization-algorithm-for-Gaussian-mixture-models"><a href="#Implementing-the-EM-Expectation-Maximization-algorithm-for-Gaussian-mixture-models" class="headerlink" title="Implementing the EM(Expectation Maximization) algorithm for Gaussian mixture models"></a>Implementing the EM(Expectation Maximization) algorithm for Gaussian mixture models</h2><h3 id="Log-likelihood"><a href="#Log-likelihood" class="headerlink" title="Log likelihood"></a>Log likelihood</h3><p>We provide a function to calculate log likelihood for mixture of Gaussians. The log likelihood quantifies the probability of observing a given set of data under a particular setting of the parameters in our model. We will use this to assess convergence of our EM algorithm; specifically, we will keep looping through EM update steps until the log likehood ceases to increase at a certain rate.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_sum_exp</span><span class="params">(Z)</span>:</span></span><br><span class="line">    <span class="string">""" Compute log(\sum_i exp(Z_i)) for some array Z."""</span></span><br><span class="line">    <span class="keyword">return</span> np.max(Z) + np.log(np.sum(np.exp(Z - np.max(Z))))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loglikelihood</span><span class="params">(data, weights, means, covs)</span>:</span></span><br><span class="line">    <span class="string">""" Compute the loglikelihood of the data for a Gaussian mixture model with the given parameters. """</span></span><br><span class="line">    num_clusters = len(means)</span><br><span class="line">    num_dim = len(data[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    ll = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">        </span><br><span class="line">        Z = np.zeros(num_clusters)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(num_clusters):</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Compute (x-mu)^T * Sigma^&#123;-1&#125; * (x-mu)</span></span><br><span class="line">            delta = np.array(d) - means[k]</span><br><span class="line">            exponent_term = np.dot(delta.T, np.dot(np.linalg.inv(covs[k]), delta))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Compute loglikelihood contribution for this data point and this cluster</span></span><br><span class="line">            Z[k] += np.log(weights[k])</span><br><span class="line">            Z[k] -= <span class="number">1</span>/<span class="number">2.</span> * (num_dim * np.log(<span class="number">2</span>*np.pi) + np.log(np.linalg.det(covs[k])) + exponent_term)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Increment loglikelihood contribution of this data point across all clusters</span></span><br><span class="line">        ll += log_sum_exp(Z)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> ll</span><br></pre></td></tr></table></figure><h3 id="E-step-assign-cluster-responsibilities-given-current-parameters"><a href="#E-step-assign-cluster-responsibilities-given-current-parameters" class="headerlink" title="E-step: assign cluster responsibilities, given current parameters"></a>E-step: assign cluster responsibilities, given current parameters</h3><p>The first step in the EM algorithm is to compute cluster responsibilities. Let $r_{ik}$ denote the responsibility of cluster $k$ for data point $i$. Note that cluster responsibilities are fractional parts: Cluster responsibilities for a single data point $i$ should sum to 1.</p><script type="math/tex; mode=display">r_{i1} + r_{i2} + \ldots + r_{iK} = 1</script><p>To figure how much a cluster is responsible for a given data point, we compute the likelihood of the data point under the  particular cluster assignment, multiplied by the weight of the cluster. For data point $i$ and cluster $k$, this quantity is</p><script type="math/tex; mode=display">r_{ik} \propto \pi_k N(x_i | \mu_k, \Sigma_k)</script><p>where $N(x_i | \mu_k, \Sigma_k)$ is the Gaussian distribution for cluster $k$ (with mean $\mu_k$ and covariance $\Sigma_k$).</p><p>We used $\propto$ because the quantity $N(x_i | \mu_k, \Sigma_k)$ is not yet the responsibility we want. To ensure that all responsibilities over each data point add up to 1, we add the normalization constant in the denominator:</p><script type="math/tex; mode=display">r_{ik} = \frac{\pi_k N(x_i | \mu_k, \Sigma_k)}{\sum_{k=1}^{K} \pi_k N(x_i | \mu_k, \Sigma_k)}.</script><p>Complete the following function that computes $r_{ik}$ for all data points $i$ and clusters $k$.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_responsibilities</span><span class="params">(data, weights, means, covariances)</span>:</span></span><br><span class="line">    <span class="string">'''E-step: compute responsibilities, given the current parameters'''</span></span><br><span class="line">    num_data = len(data)</span><br><span class="line">    num_clusters = len(means)</span><br><span class="line">    resp = np.zeros((num_data, num_clusters))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update resp matrix so that resp[i,k] is the responsibility of cluster k for data point i.</span></span><br><span class="line">    <span class="comment"># Hint: To compute likelihood of seeing data point i given cluster k, use multivariate_normal.pdf.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_data):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(num_clusters):</span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            resp[i, k] = weights[k]*multivariate_normal.pdf(data[i], mean=means[k], cov=covariances[k])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add up responsibilities over each data point and normalize</span></span><br><span class="line">    row_sums = resp.sum(axis=<span class="number">1</span>)[:, np.newaxis]</span><br><span class="line">    resp = resp / row_sums</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> resp</span><br></pre></td></tr></table></figure><h3 id="M-step-Update-parameters-given-current-cluster-responsibilities"><a href="#M-step-Update-parameters-given-current-cluster-responsibilities" class="headerlink" title="M-step: Update parameters, given current cluster responsibilities"></a>M-step: Update parameters, given current cluster responsibilities</h3><p>Once the cluster responsibilities are computed, we update the parameters (weights, means, and covariances) associated with the clusters.</p><p><strong>Computing soft counts</strong>. Before updating the parameters, we first compute what is known as “soft counts”. The soft count of a cluster is the sum of all cluster responsibilities for that cluster:</p><script type="math/tex; mode=display">N^{\text{soft}}_k = r_{1k} + r_{2k} + \ldots + r_{Nk} = \sum_{i=1}^{N} r_{ik}</script><p>where we loop over data points. Note that, unlike k-means, we must loop over every single data point in the dataset. This is because all clusters are represented in all data points, to a varying degree.</p><p>We provide the function for computing the soft counts:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_soft_counts</span><span class="params">(resp)</span>:</span></span><br><span class="line">    <span class="comment"># Compute the total responsibility assigned to each cluster, which will be useful when </span></span><br><span class="line">    <span class="comment"># implementing M-steps below. In the lectures this is called N^&#123;soft&#125;</span></span><br><span class="line">    counts = np.sum(resp, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> counts</span><br></pre></td></tr></table></figure><p><strong>Updating weights.</strong> The cluster weights show us how much each cluster is represented over all data points. The weight of cluster $k$ is given by the ratio of the soft count $N^{\text{soft}}_{k}$ to the total number of data points $N$:</p><script type="math/tex; mode=display">\hat{\pi}_k = \frac{N^{\text{soft}}_{k}}{N}</script><p>Notice that $N$ is equal to the sum over the soft counts $N^{\text{soft}}_{k}$ of all clusters.</p><p>Complete the following function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_weights</span><span class="params">(counts)</span>:</span></span><br><span class="line">    num_clusters = len(counts)</span><br><span class="line">    weights = [<span class="number">0.</span>] * num_clusters</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(num_clusters):</span><br><span class="line">        <span class="comment"># Update the weight for cluster k using the M-step update rule for the cluster weight, \hat&#123;\pi&#125;_k.</span></span><br><span class="line">        <span class="comment"># HINT: compute # of data points by summing soft counts.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        weights[k] = counts[k] / np.sum(counts)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure><p><strong>Updating means</strong>. The mean of each cluster is set to the <a href="https://en.wikipedia.org/wiki/Weighted_arithmetic_mean" target="_blank" rel="noopener">weighted average</a> of all data points, weighted by the cluster responsibilities:</p><script type="math/tex; mode=display">\hat{\mu}_k = \frac{1}{N_k^{\text{soft}}} \sum_{i=1}^N r_{ik}x_i</script><p>Complete the following function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_means</span><span class="params">(data, resp, counts)</span>:</span></span><br><span class="line">    num_clusters = len(counts)</span><br><span class="line">    num_data = len(data)</span><br><span class="line">    means = [np.zeros(len(data[<span class="number">0</span>]))] * num_clusters</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(num_clusters):</span><br><span class="line">        <span class="comment"># Update means for cluster k using the M-step update rule for the mean variables.</span></span><br><span class="line">        <span class="comment"># This will assign the variable means[k] to be our estimate for \hat&#123;\mu&#125;_k.</span></span><br><span class="line">        weighted_sum = <span class="number">0.</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_data):</span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            weighted_sum += data[i] * resp[i][k]</span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        means[k] = weighted_sum / counts[k]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> means</span><br></pre></td></tr></table></figure><p><strong>Updating covariances</strong>.  The covariance of each cluster is set to the weighted average of all <a href="https://people.duke.edu/~ccc14/sta-663/LinearAlgebraReview.html" target="_blank" rel="noopener">outer products</a>, weighted by the cluster responsibilities:</p><script type="math/tex; mode=display">\hat{\Sigma}_k = \frac{1}{N^{\text{soft}}_k}\sum_{i=1}^N r_{ik} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T</script><p>The “outer product” in this context refers to the matrix product</p><script type="math/tex; mode=display">(x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T.</script><p>Letting $(x_i - \hat{\mu}_k)$ to be $d \times 1$ column vector, this product is a $d \times d$ matrix. Taking the weighted average of all outer products gives us the covariance matrix, which is also $d \times d$.</p><p>Complete the following function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_covariances</span><span class="params">(data, resp, counts, means)</span>:</span></span><br><span class="line">    num_clusters = len(counts)</span><br><span class="line">    num_dim = len(data[<span class="number">0</span>])</span><br><span class="line">    num_data = len(data)</span><br><span class="line">    covariances = [np.zeros((num_dim,num_dim))] * num_clusters</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(num_clusters):</span><br><span class="line">        <span class="comment"># Update covariances for cluster k using the M-step update rule for covariance variables.</span></span><br><span class="line">        <span class="comment"># This will assign the variable covariances[k] to be the estimate for \hat&#123;\Sigma&#125;_k.</span></span><br><span class="line">        weighted_sum = np.zeros((num_dim, num_dim))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_data):</span><br><span class="line">            <span class="comment"># YOUR CODE HERE (Hint: Use np.outer on the data[i] and this cluster's mean)</span></span><br><span class="line">            weighted_sum += resp[i][k]*np.outer(data[i] - means[k], data[i] - means[k])</span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        covariances[k] = weighted_sum / counts[k]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> covariances</span><br></pre></td></tr></table></figure><h3 id="The-EM-algorithm"><a href="#The-EM-algorithm" class="headerlink" title="The EM algorithm"></a>The EM algorithm</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SOLUTION</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">EM</span><span class="params">(data, init_means, init_covariances, init_weights, maxiter=<span class="number">1000</span>, thresh=<span class="number">1e-4</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Make copies of initial parameters, which we will update during each iteration</span></span><br><span class="line">    means = init_means[:]</span><br><span class="line">    covariances = init_covariances[:]</span><br><span class="line">    weights = init_weights[:]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Infer dimensions of dataset and the number of clusters</span></span><br><span class="line">    num_data = len(data)</span><br><span class="line">    num_dim = len(data[<span class="number">0</span>])</span><br><span class="line">    num_clusters = len(means)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize some useful variables</span></span><br><span class="line">    resp = np.zeros((num_data, num_clusters))</span><br><span class="line">    ll = loglikelihood(data, weights, means, covariances)</span><br><span class="line">    ll_trace = [ll]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> range(maxiter):</span><br><span class="line">        <span class="keyword">if</span> it % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Iteration %s"</span> % it)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># E-step: compute responsibilities</span></span><br><span class="line">        resp = compute_responsibilities(data, weights, means, covariances)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># M-step</span></span><br><span class="line">        <span class="comment"># Compute the total responsibility assigned to each cluster, which will be useful when </span></span><br><span class="line">        <span class="comment"># implementing M-steps below. In the lectures this is called N^&#123;soft&#125;</span></span><br><span class="line">        counts = compute_soft_counts(resp)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the weight for cluster k using the M-step update rule for the cluster weight, \hat&#123;\pi&#125;_k.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        weights = compute_weights(counts)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update means for cluster k using the M-step update rule for the mean variables.</span></span><br><span class="line">        <span class="comment"># This will assign the variable means[k] to be our estimate for \hat&#123;\mu&#125;_k.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        means = compute_means(data, resp, counts)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update covariances for cluster k using the M-step update rule for covariance variables.</span></span><br><span class="line">        <span class="comment"># This will assign the variable covariances[k] to be the estimate for \hat&#123;\Sigma&#125;_k.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        covariances = compute_covariances(data, resp, counts, means)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the loglikelihood at this iteration</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        ll_latest = loglikelihood(data, weights, means, covariances)</span><br><span class="line">        ll_trace.append(ll_latest)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check for convergence in log-likelihood and store</span></span><br><span class="line">        <span class="keyword">if</span> (ll_latest - ll) &lt; thresh <span class="keyword">and</span> ll_latest &gt; -np.inf:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        ll = ll_latest</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> it % <span class="number">5</span> != <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Iteration %s"</span> % it)</span><br><span class="line">    </span><br><span class="line">    out = &#123;<span class="string">'weights'</span>: weights, <span class="string">'means'</span>: means, <span class="string">'covs'</span>: covariances, <span class="string">'loglik'</span>: ll_trace, <span class="string">'resp'</span>: resp&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p><strong>Reference from <a href="https://zhuanlan.zhihu.com/p/29538307" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29538307</a></strong><br><strong>Reference from <a href="https://zhuanlan.zhihu.com/p/31103654" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31103654</a></strong><br><strong>Reference from coursera course Machine Learning Foundation from University of Washington</strong>  </p>]]></content>
    
    <summary type="html">
    
      Gaussian Mixture Models Introduction
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Machine-Learning/"/>
    
    
      <category term="Machine Learning Algorithm" scheme="https://zhangruochi.com/tags/Machine-Learning-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Lifelong Learning</title>
    <link href="https://zhangruochi.com/Lifelong-Learning/2020/03/14/"/>
    <id>https://zhangruochi.com/Lifelong-Learning/2020/03/14/</id>
    <published>2020-03-14T06:28:06.000Z</published>
    <updated>2020-03-14T07:42:44.810Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Three-problem-need-to-solve-when-realize-lifelong-learnig"><a href="#Three-problem-need-to-solve-when-realize-lifelong-learnig" class="headerlink" title="Three problem need to solve when realize lifelong learnig"></a>Three problem need to solve when realize lifelong learnig</h2><ol><li>Knowledge Retention</li><li>Knowledge Transfer</li><li>Model Expansion</li></ol><h2 id="Knowledge-Retention"><a href="#Knowledge-Retention" class="headerlink" title="Knowledge Retention"></a>Knowledge Retention</h2><p>When a network learn a new task. It has the inclination to forget the skills it has learned. The phenomenon that the model will forget the previous skills is called <strong>Catastrophic Forgetting</strong>.</p><p>The reason for resulting the catastrophic forgetting is not the model’s size which has no enough capacity to learn the new skills. To prove this we can make the model to learn a multi-task problem and get a good results.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">catastrophic forgetting</div></center><h3 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h3><p>The ideas to solve knowledge retention</p><h3 id="Elastic-Weight-Consolidation"><a href="#Elastic-Weight-Consolidation" class="headerlink" title="Elastic Weight Consolidation"></a>Elastic Weight Consolidation</h3><script type="math/tex; mode=display">L^{\prime}(\theta) = L(\theta) + \lambda \sum_{i} b_i(\theta_i - \theta_i^{b})^2</script><ul><li>$L^{\prime}(\theta)$ : the loss to be optimized</li><li>$L(\theta)$ : the loss of current task</li><li>$\theta_i$ : the parameters to be learning</li><li>$\theta_i^{b}$ : the parameters leaned from previous tasks</li><li>$b_i$ how important the parameter is</li></ul><p>the idea of EWC is: learning the new parameters which are not far from the previous parameters</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">EWC</div></center><h4 id><a href="#" class="headerlink" title=" "></a> </h4><h4 id="How-to-set-b-i"><a href="#How-to-set-b-i" class="headerlink" title="How to set $b_i$ ?"></a>How to set $b_i$ ?</h4><ul><li>small 2nd derivative -&gt; set $b_i$ to small or large</li><li>large 2nd derivative -&gt;  set $b_i$ to small</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">important of parameters</div></center><h3 id="Generating-Data"><a href="#Generating-Data" class="headerlink" title="Generating Data"></a>Generating Data</h3><p>Conducting multi-task learning by generating pseudo-data using generative model. </p><p>We know the multi-task learning is a good way to solve life long task(sometimes it is the upper bound). If a new task come, we can regard it as a multi-task problem combined with previous tasks and build a model to solve it. But the premise of dong this is we have the data of previous tasks. In reality, we can not store all the dataset of previous tasks. Therefore, We can use generative model to generate previous dataset.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Generating Data</div></center><h2 id="Knowledge-Transfer"><a href="#Knowledge-Transfer" class="headerlink" title="Knowledge Transfer"></a>Knowledge Transfer</h2><p>The difference of knowledge transfer between lifelong learning with transfer learning is that <code>transfer learning</code> is just concentrate on new task while the lifelong learning shuild consider the catastrophic forgetting</p><h3 id="Gredient-Episodic-Memory"><a href="#Gredient-Episodic-Memory" class="headerlink" title="Gredient Episodic Memory"></a>Gredient Episodic Memory</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">catastrophic forgetting</div></center><p>The idea of GEM is: When we update the parameters by gredient descent we can find a direction which can benifits the previous tasks and new tasks to update. The disadvantages of GEM is we need store a little bit of data of previous tasks.</p><h2 id="Model-Expansion"><a href="#Model-Expansion" class="headerlink" title="Model Expansion"></a>Model Expansion</h2><h3 id="Progressive-Neural-Networks"><a href="#Progressive-Neural-Networks" class="headerlink" title="Progressive Neural Networks"></a>Progressive Neural Networks</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="6.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">catastrophic forgetting</div></center><p>An example of model expansion is the <code>Progressive Neural Networks</code> proposed in 2016. We fix the parameters after learning some tasks, and then train the new task. We build a new model and use the output of previous task as the input of new task. However, there is a disadvantage that you can not train too many new tasks because it will cause a lot of load.</p><h3 id="Expert-Gate"><a href="#Expert-Gate" class="headerlink" title="Expert Gate"></a>Expert Gate</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="7.png" width="80%" height="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">catastrophic forgetting</div></center><p>Exper Gate’s method: We still have one model for each task. For example, we have three tasks, and the fourth task is similar to the first one (we use Gate to determine which new task is similar to the old one), then we will use the model of the first task as the fourth Initialization of each task model, this has formed a certain migration effect. However, this method is still a task corresponding to a model, which still causes a lot of load on storage.</p><h3 id="Tasknomy"><a href="#Tasknomy" class="headerlink" title="Tasknomy"></a>Tasknomy</h3><p>The order of learning tasks is just like the order of our textbooks, which has a great impact on the final results. (This is a optimal order for the learning tasks).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Three-problem-need-to-solve-when-realize-lifelong-learnig&quot;&gt;&lt;a href=&quot;#Three-problem-need-to-solve-when-realize-lifelong-learnig&quot; clas
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Transfer Learning" scheme="https://zhangruochi.com/tags/Transfer-Learning/"/>
    
      <category term="Lifelong Learning" scheme="https://zhangruochi.com/tags/Lifelong-Learning/"/>
    
  </entry>
  
</feed>
