<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RUOCHI.AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhangruochi.com/"/>
  <updated>2020-05-26T09:04:34.001Z</updated>
  <id>https://zhangruochi.com/</id>
  
  <author>
    <name>Ruochi Zhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ML-Interview-Part4</title>
    <link href="https://zhangruochi.com/ML-Interview-Part4/2020/05/26/"/>
    <id>https://zhangruochi.com/ML-Interview-Part4/2020/05/26/</id>
    <published>2020-05-26T01:14:23.000Z</published>
    <updated>2020-05-26T09:04:34.001Z</updated>
    
    <content type="html"><![CDATA[<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="100%" height="100%"></center><h3 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h3><ol><li><p>简述 K-Means算法的具体步骤<br>输入是样本集$D=\{x_1,x_2,…x_m\}$,聚类的簇树k,最大迭代次数N。输出是簇划分$C=\{C_1,C_2,…C_k\}$</p><blockquote><ol><li>数据预处理,如归一化、离群点处理等<br>2.从数据集D中随机选择k个样本作为初始的k个质心向量：$\{\mu_1,\mu_2,…,\mu_k\}$</li><li>对于n=1,2,…,N<ul><li>将簇划分C初始化为 $C_t = \varnothing \;\; t =1,2…k$</li><li>计算样本$x_i$和各个质心向量$\mu_j(j=1,2,…k)$的距离: $d_{ij} = ||x_i - \mu_j||_2^2$，将$x_i$标记最小的为$d_{ij}$所对应的类别$\lambda_i$, 此时更新 $C_{\lambda_i} = C_{\lambda_i} \cup \{x_i\}$</li><li>对于j=1,2,…,k,对 $C_j$中所有的样本点重新计算新的质心$\mu_j = \frac{1}{|C_j|}\sum\limits_{x \in C_j}x$</li><li>如果所有的k个质心向量都没有发生变化，则转到步骤3）</li></ul></li><li>输出簇划分$C=\{C_1,C_2,…C_k\}$</li></ol></blockquote></li><li><p>简述K-Means++与 K-Means的区别</p><blockquote><p>K-Means中k个初始化的质心的位置选择对最后的聚类结果和运行时间都有很大的影响，因此需要选择合适的k个质心。如果仅仅是完全随机的选择，有可能导致算法收敛很慢。K-Means++算法就是对K-Means随机初始化质心的方法的优化。K-Means++的对于初始化质心的优化策略也很简单，如下：</p><ol><li>从输入的数据点集合中随机选择一个点作为第一个聚类中心$u_1$</li><li>对于数据集中的每一个点$x_i$,计算它与已选择的聚类中心中最近聚类中心的距离<script type="math/tex; mode=display">D(x_i) = arg\;min||x_i- \mu_r||_2^2\;\;r=1,2,...k_{selected}</script></li><li>选择一个新的数据点作为新的聚类中心，选择的原则是：$D(x)$较大的点，被选取作为聚类中心的概率较大.</li><li>重复b和c直到选择出k个聚类质心</li><li>利用这k个质心来作为初始化质心去运行标准的K-Means算法</li></ol></blockquote></li><li><p>K-Means均值算法的缺点是什么？</p><blockquote><ol><li>K值的选取不好把握</li><li>对于不是凸的或者球形的数据集比较难收敛</li><li>如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。</li><li>采用迭代方法，得到的结果只是局部最优。</li><li>对噪音和异常点比较的敏感。</li></ol></blockquote></li><li><p>如何选取 K-Means 的 K 值？</p><blockquote><p>K 值的选择一般基于经验和多次试验结果。比如可以采用手肘法，我们可以尝试不同的 K 值，并将不同的 K 值所对应的损失函数画成折线。拐点就是 K 的最佳值。</p></blockquote></li><li><p>什么是 Kernel K- Means ?</p><blockquote><p>还童的欧式距离度量方式，使得 K 均值算法本质上假设了各个数据簇的数据呈现球形或者高维球形，这种分布在实际生活中不常见。面对非凸的数据分布时，引入核函数来进行非线性映射，将输入空间中的数据点映射到高维的特征空间，并在新的特征中空间进行聚类。非线性映射增加了数据点线性可分的概率。</p></blockquote></li></ol><h3 id="DBSCANS-（密度聚类）"><a href="#DBSCANS-（密度聚类）" class="headerlink" title="DBSCANS （密度聚类）"></a>DBSCANS （密度聚类）</h3><blockquote><p><a href="https://zhangruochi.com/DBSCAN-Clustering/2020/04/14/">https://zhangruochi.com/DBSCAN-Clustering/2020/04/14/</a></p></blockquote><h3 id="Birch-层次聚类"><a href="#Birch-层次聚类" class="headerlink" title="Birch (层次聚类)"></a>Birch (层次聚类)</h3><ol><li>什么是层次聚类? 层次聚类的步骤是什么？<blockquote><p>层次聚类不指定具体的簇数，而只关注簇之间的远近，最终会形成一个树形图。<br><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="100%" height="100%"></center><br>根据聚类簇之间距离的计算方法的不同，层次聚类算法可以大致分为：单链接（Single-link）算法，全链接算法（complete-link）或均链接算法（average-link）。单链接算法用两个聚类簇中最近的样本距离作为两个簇之间的距离；而全链接使用计算两个聚类簇中最远的样本距离；均链接算法中两个聚类之间的距离由两个簇中所有的样本共同决定。</p><ol><li>每一个样本点视为一个簇；</li><li>计算各个簇之间的距离，最近的两个簇聚合成一个新簇；</li><li>重复以上过程直至最后只有一簇。</li></ol></blockquote></li></ol><h3 id="Gaussian-Mixed-Model-概率聚类"><a href="#Gaussian-Mixed-Model-概率聚类" class="headerlink" title="Gaussian Mixed Model (概率聚类)"></a>Gaussian Mixed Model (概率聚类)</h3><blockquote><p><a href="https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/">https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/</a></p></blockquote><ol><li>高斯混合模型的核心思想是什么？它是如何迭代计算的？<blockquote><p>高斯混合模型假设数据可以看作是从多个高斯分布中生成出来的。求解步骤如下:</p><ol><li>E step: 根据当前参数，计算每个点属于各个高斯分布的概率</li><li>M step: 使用上述 E step 求得的概率，计算每个高斯分布的加权平均参数。</li></ol></blockquote></li></ol><h3 id="聚类算法的评估"><a href="#聚类算法的评估" class="headerlink" title="聚类算法的评估"></a>聚类算法的评估</h3><ol><li>以聚类算法为例，假设没有外部标签数据，如何评估两个聚类算法的优劣？<blockquote><p>在无监督的情况下，我们可通过考察簇的分离情况和簇的紧凑情况来评估聚类的效果。</p><ol><li>轮廓系数：给定一个点p，改点的轮廓系数定义为<script type="math/tex; mode=display">s(p) = \frac{b(p) - a(p)}{max{a(p), b(p)}}</script>其中，$a(p)$是点$p$与同一簇中其他点$p\prime$之间的平均距离；$b(p)$是点$p$与另一不同簇中的点之间的最小平均距离（如果有n个簇，则只计算和点p最接近的一簇中的点与该点的平均距离). $a(p)$反应的是$p$所属的簇中数据的紧密程度，$b(p)$反应的是该簇与其他临近簇的分离程度。显然，$b(p)$越大，$a(p)$越小，对应的聚类的质量越好。</li></ol></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      K-Means
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Part3</title>
    <link href="https://zhangruochi.com/ML-Interview-Part3/2020/05/26/"/>
    <id>https://zhangruochi.com/ML-Interview-Part3/2020/05/26/</id>
    <published>2020-05-25T22:56:23.000Z</published>
    <updated>2020-05-26T01:15:29.877Z</updated>
    
    <content type="html"><![CDATA[<h2 id="PCA-最大方差理论"><a href="#PCA-最大方差理论" class="headerlink" title="PCA 最大方差理论"></a>PCA 最大方差理论</h2><ol><li><p>如何定义主成分？从这种定义出发，如何设计目标函数使得降维达到提取主成分的目的？针对这个目标函数，如何对 PCA 问题进行求解？</p><blockquote><p>在信号处理领域，我们认为信号具有较大的方差，噪声具有较小的方差，信号与噪声之比成为信噪比。信噪比越大意味着数据的质量越好。x 投影之后的方差就是协方差矩阵的特征值，最佳投影方向也就是协方差矩阵最大的特征值。至此，<br>PCA 的求解方法为：</p><ol><li>对样本数据进行中心化处理</li><li>求样本的协方差矩阵</li><li>对协方差矩阵进行特征值分解，将特征值从大到小排列</li><li>去特征值前$d$大对应的特征向量$w_1,w_2,…,w_d$,<br>通过以下映射将n维样本映射到$d$维度。<script type="math/tex; mode=display">x_i\prime = \left[\begin{matrix}& w_1^{T}x_i \\& w_2^{T}x_i \\& w_3^{T}x_i \\& \cdots \\& w_d^{T}x_i \end{matrix}\right]</script></li></ol></blockquote></li><li><p>PCA 的缺点是什么？<br>在 PCA 中，算法没有考虑数据的标签（类别），只是把数据映射到一些方差比较大的方向而已。如下图，PCA 算法会把两个类别的数据映射到y轴，使得分类效果特别差。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center></li></ol><h2 id="LDA-线性判别分析"><a href="#LDA-线性判别分析" class="headerlink" title="LDA 线性判别分析"></a>LDA 线性判别分析</h2><ol><li><p>对于具有类别标签的数据，映带如何设计目标函数使得降维的过程中不损失类别信息？在这种目标下，应当如何求解？</p><blockquote><p>投影后每类内部方差最小，类间方差最大<br><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center><br>类内散度矩阵$s_w$:</p><script type="math/tex; mode=display">S_w = \Sigma_0 + \Sigma_1 = \sum\limits_{x \in X_0}(x-\mu_0)(x-\mu_0)^T + \sum\limits_{x \in X_1}(x-\mu_1)(x-\mu_1)^T</script><p>类间散度矩阵$s_b$:</p><script type="math/tex; mode=display">S_b = (\mu_0-\mu_1)(\mu_0-\mu_1)^T</script><p>LDA 的优化目标：</p><script type="math/tex; mode=display">\underbrace{arg\;max}_w\;\;J(w) = \frac{w^TS_bw}{w^TS_ww}</script></blockquote></li><li><p>LDA 算法的步骤是什么？</p><blockquote><ol><li>计算类内散度矩阵$S_w$</li><li>计算类间散度矩阵$S_b$</li><li>计算矩阵$S_w^{-1}S_b$</li><li>计算$S_w^{-1}S_b$的最大的d个特征值和对应的d个特征向量$(w_1,w_2,…w_d)$得到投影矩阵$W$.</li><li>对样本集中的每一个样本特征$x_i$,转化为新的样本$z_i=W^Tx_i$.</li></ol></blockquote></li></ol><ol><li>LDA 与 PCA 作为经典的降维算法，如何从应用的角度分析其原理的异同？<blockquote><p>从目标出发，PCA 选择的是投影后数据方差最大的方向，由于它是无监督的，因此 PCA 假设方差越大，信息量越多，用主成分来表示原始数据可以去除用于的维度，达到降维。而 LDA选择的是投影后类内方差小、类间方差大的方向。其用到了类别信息，为了找到数据中具有判别性的维度，使得原始数据在这些方向上投影后，不同类 jin尽可能区分开。举例来说，我们想从一段音频中提取人的语音信号，这时可以使用 PCA 先进行降维，过滤掉一些固定频率的北京噪声。但如果我们的需求是从这段音频中区分出声音属于哪个人，那么我们应该使用 LDA 对数据进行降维，使得每个人的语音信号具有区分性。<br><strong>从应用的角度，我们可以掌握一个基本的原则—对无监督的任务使用 PCA 进行降维，对有监督的则应用 LDA</strong></p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      PCA
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Part2</title>
    <link href="https://zhangruochi.com/ML-Interview-Part2/2020/05/25/"/>
    <id>https://zhangruochi.com/ML-Interview-Part2/2020/05/25/</id>
    <published>2020-05-25T00:28:33.000Z</published>
    <updated>2020-05-26T09:16:29.898Z</updated>
    
    <content type="html"><![CDATA[<h2 id="经典算法"><a href="#经典算法" class="headerlink" title="经典算法"></a>经典算法</h2><h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><p>SVM的基本型：</p><script type="math/tex; mode=display">min_{w,b} = \frac{1}{2} ||w||^{2}</script><script type="math/tex; mode=display">s.t y_i(w^T x_i + b) \leq 1, i=1,2,3...m</script><ol><li><p>空间上线性可分的两点，分别向svm的超平面做投影，投影的点在超平面上依然线性可分吗？</p><blockquote><p>一定线性不可分</p></blockquote></li><li><p>硬间隔和软间隔是指什么？</p><blockquote><p>SVM的基本形态是一个硬间隔分类器，它要求所有样本都满足硬间隔约束(即函数间隔要大于1)，所以当数据集有噪声点时，SVM为了把噪声点也划分正确，超平面就会向另外一个类的样本靠拢，这就使得划分超平面的几何间距变小，降低模型的泛化性能。除此之外，当噪声点混入另外一个类时，对于硬间隔分类器而言，这就变成了一个线性不可分的问题，于是就使用核技巧，通过将样本映射到高维特征空间使得样本线性可分，这样得到一个复杂模型，并由此导致过拟合（原样本空间得到的划分超平面会是弯弯曲曲的，它确实可以把所有样本都划分正确，但得到的模型只对训练集有效）。<br>为了解决上述问题，SVM通过引入松弛变量构造了软间隔分类器，它允许分类器对一些样本犯错，允许一些样本不满足硬间隔约束条件，这样做可以避免SVM分类器过拟合，于是也就避免了模型过于复杂，降低了模型对噪声点的敏感性，提升了模型的泛化性能。<br>因为松弛变量是非负的，因此样本的函数间隔可以比1小。函数间隔比1小的样本被叫做离群点，我们放弃了对离群点的精确分类，这对我们的分类器来说是种损失。但是放弃这些点也带来了好处，那就是超平面不必向这些点的方向移动，因而可以得到更大的几何间隔（在低维空间看来，分类边界也更平滑）。显然我们必须权衡这种损失和好处。</p></blockquote></li><li><p>松弛变量和惩罚因子是什么？</p><blockquote><p>松弛变量：松弛变量表示样本离群的程度，松弛变量越大，离群越远，松弛变量为零，则样本没有离群。<br>惩罚因子：惩罚因子表示我们有多重视离群点带来的损失，当C取无穷大时，会迫使超平面将所有的样本都划分正确，这就退化成了硬间隔分类器。</p></blockquote></li><li><p>拉格朗日乘子法是什么？</p><blockquote><p>拉格朗日乘数法是一种优化算法，主要运用于解决优化问题，它的基本思想就是用过拉格朗日乘子来把含有m个变量和l个约束条件的约束优化问题转换成含有（m+l）个变量的无约束优化问题。</p></blockquote></li><li><p>什么是 kernel trick?</p><blockquote><p>$x_i$ 和 $x_j$ 在特征空间的內积等于它们在原始的样本空间通过 $k(x_i,x_j)$ 计算的结果。有了这样的函数，我们不必去计算高维甚至无穷维特征空间中的內积。 d<br>SVM 基本式的对偶问题为: </p><script type="math/tex; mode=display">max_{\alpha} \sum_{i=1}^{m}\alpha_{i} - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_i y_j \Phi{x_i}^{T}\Phi_{x_j}</script><script type="math/tex; mode=display">s.t. \sum_{i=1}^{m}\alpha_i y_i = 0</script><script type="math/tex; mode=display">\alpha_i \geq 0, i = 1,2,...,m.</script></blockquote></li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><ol><li><p>什么是逻辑回归？</p><blockquote><p>对数几率回归。对逻辑回归的公式进行整理，得到:</p><script type="math/tex; mode=display">log\frac{p}{1-p} = \theta^{T}x</script><script type="math/tex; mode=display">p = P(y=1 | x)</script><p>逻辑回归通过极大似然来得到最佳参数</p><script type="math/tex; mode=display">L(\theta) = \prod_{i:y_{i}=1}p(x_{i})\prod_{i^{\prime}:y_{i^{\prime}}=0}(1-p(x_{i^{\prime}}))</script></blockquote></li><li><p>使用逻辑回归处理多标签的分类问题时，有哪些常用做法？</p><blockquote><ol><li>如果一个样本只对应一个标签，那么可以使用 sofmax regression</li><li>当存在样本属于多个标签的情况，可以训练$i$个分类器，第$i$个分类器用以区分每个样本是否可以归为第i类。可以训练 softmax regression. 设定一个 threshold，判断每个类别的概率是否高于 threshold.</li></ol></blockquote></li></ol><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><ol><li><p>决策树有哪些启发函数？</p><blockquote><p>ID3（最大信息增益） 计算每个特征的信息增益，然后选择信息增益最大的特征来划分样本，完成决策树的增长。<br>C4.5（最大信息增益比）。<br>CART(最大基尼指数)</p></blockquote></li><li><p>信息熵、信息增益、信息增益比、最大基尼系数是什么？</p><blockquote><p>“信息熵”是度量样本集合不确定度（纯度）的最常用的指标。<br>当前样本集合 D 中第 k 类样本所占的比例为 pk ，则 D 的信息熵定义为</p><script type="math/tex; mode=display">Ent(D) = - \sum_{K=1}^{|y|}p_k log_2^{p_k}</script><p>信息增益表示得知属性 a 的信息而使得样本集合不确定度减少的程度<br>离散属性 a 有 V 个可能的取值 {a1,a2,…,aV}；样本集合中，属性 a 上取值为 av 的样本集合，记为 Dv。</p><script type="math/tex; mode=display">Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{D^v}{D}Ent(D^v)</script><p>信息增益率=信息增益/IV(a),说明信息增益率是信息增益除了一个属性a的固有值得来的。</p><script type="math/tex; mode=display">IV(a) = -sum_{v=1}^{v}\frac{D^v}{D}log_2\frac{D^v}{D}</script><p>Gini描述的是数据的纯度</p><script type="math/tex; mode=display">Gini(D) = 1 - sum_{k=1}^{n}(\frac{|C_k|}{|D|})^2</script><p>特征 A 的 Gini指数定义为:</p><script type="math/tex; mode=display">Gini(D|A) = \sum_{i=1}^{n}\frac{|D_i|}{|D|}Gini(D_i)</script></blockquote></li><li><p>ID3,C4.5,CART 各自的优缺点是什么？</p><blockquote><p>ID3倾向于取值较多的特征,因为信息增益放映的是给定条件以后不确定性减少的程度，特征取值越多就意味着确定性越高，也就是条件熵越小、信息增益越大。<br>C4.5实际上是对 ID3 进行优化，通过引入信息增益比，一定程度上对取值较多的特征进行惩罚、避免 ID3 出现过拟合。<br>CART 与 ID3,C4.5不同，它是一颗二叉树，采用二元分割法，每一步将数据按照特征 A 的取值切成两份，分别进入左右子树。</p></blockquote></li><li><p>Cart 在做 regression 和 classification 的区别是？</p><blockquote><p> 在分类问题中，CART 使用基尼指数（Gini index）作为选择特征（feature）和划分（split）的依据；在回归问题中，CART 使用 mse（mean square error）或者 mae（mean absolute error）作为选择 feature 和 split 的 criteria。<br>在分类问题中，CART的每一片叶子都代表的是一个class；在回归问题中，CART 的每一片叶子表示的是一个预测值，取值是连续的。预测值一般是该片叶子所含训练集元素输出的均值。</p></blockquote></li></ol><ol><li>决策树如何进行剪枝？<blockquote><p>预剪枝：1. 当树到达一定深度时，停止树的生长；2.当到达当前节点的样本数量小于某个阈值时，停止树的生长；3. 计算每次分裂时测试集的准确度提升，当小于某个阈值时不再继续扩展。<br>后剪枝：后剪枝的方法有很多，比如代价复杂度剪枝、悲观剪枝、最小误差剪枝等。</p></blockquote></li></ol><h3 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h3><ol><li><p>简述朴素贝叶斯的原理。</p><blockquote><p>朴素贝叶斯采用<code>属性条件独立性</code>的假设，对于给定的待分类观测数据X, 计算在X出现的条件下，各个目标类出现的概率（即后验概率）， 将该后验概率最大的类作为X所属的类。方法是根据已有样本进行贝叶斯估计学习出先验概率$P(Y)$和条件概率$P(X|Y)$，进而求出联合分布概率P(XY),最后利用贝叶斯定理求解后验概率P(Y|X).</p></blockquote></li><li><p>朴素贝叶斯“朴素”在哪里？</p><blockquote><p>利用贝叶斯定理求解联合概率P(XY)时，需要计算条件概率P(X|Y)。在计算P(X|Y)时，朴素贝叶斯做了一个很强的条件独立假设（当Y确定时，X的各个分量取值之间相互独立），即</p><script type="math/tex; mode=display">P(X_1=x_1,X_2=x_2,\cdots,X_j=x_j|Y=y_k) = P(X_1=x_1|Y=y_k)\*P(X_2=x_2|Y=y_k),\cdots,P(X_j=x_j|Y=y_k)</script></blockquote></li></ol><ol><li>什么是拉普拉斯平滑法?<blockquote><p>拉普拉斯平滑法是朴素贝叶斯中处理零概率问题的一种修正方式。在进行分类的时候，可能会出现某个属性在训练集中没有与某个类同时出现过的情况，如果直接基于朴素贝叶斯分类器的表达式进行计算的话就会出现零概率现象。为了避免其他属性所携带的信息被训练集中未出现过的属性值“抹去”，所以才使用拉普拉斯估计器进行修正。具体的方法是：在分子上加1,对于先验概率，在分母上加上训练集中可能的类别数；对于条件概率，则在分母上加上第i个属性可能的取值数</p></blockquote></li></ol><ol><li>朴素贝叶斯中有哪些不同的模型？<blockquote><p>朴素贝叶斯含有3种模型，分别是<strong>高斯模型</strong>，对连续型数据进行处理；<strong>多项式模型</strong>，对离散型数据进行处理，计算数据的条件概率(使用拉普拉斯估计器进行平滑的一个模型)；<strong>伯努利模型</strong>，伯努利模型的取值特征是布尔型，即出现为ture,不出现为false,在进行文档分类时，就是一个单词有没有在一个文档中出现过。</p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      SVM, Logistic Regression, Decision Tree
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ML-Interview-Part1</title>
    <link href="https://zhangruochi.com/ML-Interview-Part1/2020/05/24/"/>
    <id>https://zhangruochi.com/ML-Interview-Part1/2020/05/24/</id>
    <published>2020-05-24T08:02:06.000Z</published>
    <updated>2020-05-26T01:15:40.006Z</updated>
    
    <content type="html"><![CDATA[<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><ol><li><p>为什么需要对数值类型的特征做归一化？</p><blockquote><p>常用的归一化有：Min Max Scaler / Z-Score<br>当特征的 range 不同时，归一化特征可以加快梯度下降收敛的速度。PCA 等算法的假设有数据是均值均值为0,方差为1. </p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center></blockquote></li><li><p>应该怎样处理类别特征？</p><blockquote><p>Ordinal Encoding<br>One-hot Encoding<br>Binary Encoding</p></blockquote></li><li><p>什么是特征组合，如何处理高维组合特征？</p><blockquote><p><x1, x2> 等形成组合特征<br>特征选择，矩阵分解，PCA</x1,></p></blockquote></li><li><p>怎样有效地找到组合特征</p><blockquote><p>决策树从根节点到叶子结点的路径可以看成一种特征组合的方式</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center></blockquote></li><li><p>有哪些文本表示模型，它们各自的优缺点是什么？</p><blockquote><p>Word bag, 常用 TF-IDF表示词的权重（term frequency and inverse document frequency）。<br>N-gram. 提取词组.<br>因为相同的词可能有多种表示，经常会做词干提取word stemming<br>主题模型（得到每个主题上词的分布特征）<br>Word embedding<br>Contextual word embeddings</p></blockquote></li><li><p>word2vec是如何工作的</p><blockquote><p>CBOW 根据上下文来预测中心词，Skip-gram根据中心词来预测上下文。 CBOW 和 Skip-gram 都是由三层的神经网络组成。输入层为N维 one-hot encoding，隐藏层为 K 维。则输入层和隐藏层的 weight matrix （N*K）就是 embedding vector. word vector 可以由one-hot encoding 与 weight matrix 相乘得到。隐藏层到输出层的weightg matrix 为 （K*N）.输出也是一个N维向量，则可以根据softmax来求每个词的概率，然后应用梯度下降。<br>由于softmax需要对所有词进行遍历，计算量大。此时可以使用negtive sampling 或者 hierarchical softmax.</p></blockquote></li><li><p>图像分类时，训练数据不足如何处理。</p><blockquote><p>数据不足有过拟合风险，或者模型不能收敛。</p><ol><li>可以使用降低过拟合风险的措施。如l1/l2,继承学习,dropout 等</li><li>Data augmentation （旋转、平移、缩放、像素扰动、颜色变换、清晰度、对比度等）</li><li>Fine tuing or transfer learning</li><li>生成对抗模型生成新样本</li><li>对图像进行特征提取，使用传统的机器学习模型。</li></ol></blockquote></li></ol><h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2><ol><li><p>准确率的局限性是什么？</p><blockquote><p>当正负数据不平衡时会失去意义</p></blockquote></li><li><p>Precision 和 Recall 怎样权衡？</p><blockquote><p>Precision 是指分类正确的正样本/模型预测的正样本, Recall 是指分类正确的正样本/实际的正样本。P-R 曲线横轴是recall，纵轴是precision。P-R 曲线是将阙值从高到低滑动画出的。<br>使用 P-R 曲线来综合判定两个模型的好坏。 F1 和 ROC 也能反应排序模型的好坏。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="70%" height="70%"></center></blockquote></li><li><p>RMSE 的局限是什么？</p><blockquote><p>如果存在个别偏离程度大的异常值，RMSE的效果会很差。</p><ol><li>数据预处理清理outlier； 2. 建模考虑异常机制，如异常点检测；3. 使用更合适的指标如 MAPE</li></ol></blockquote></li><li><p>什么是 ROC 曲线？</p><blockquote><p>横轴是 FPR（FP/N）, 纵轴是 TPR（TP/P）。绘制 ROC 曲线，需要将模型的输出概率从大到小排序，然后动态地选择阈值。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%"></center></blockquote></li><li><p>AUC如何计算？</p><blockquote><p>AUC 是 ROC 曲线下的面积大小，计算时只用沿着ROC 曲线做积分就行了。AUC取值一般在[0.5,1]之间，越大越好。</p></blockquote></li><li><p>ROC 曲线相比 P-R 曲线有什么特点</p><blockquote><p>当正负样本的分别发生明显变化时，ROC曲线基本不变。因此 ROC 适用的场景更多。如下图是将负样本的数量增加 10 倍之后的结果。</p><center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%"></center></blockquote></li><li><p>为什么在一些场景中需要使用余弦相似度而不是欧氏距离。</p><blockquote><p>余弦相似度只关注向量的夹角，并不关心向量的绝对值大小，范围为[-1,1]。比如在度量两个文本的相似度时，以词频和词向量最为特征。文本越长则欧式距离一定越大，但是余弦相似度则可以保持不变。总的来说，关注相对差异，使用余弦相似度。关注数值绝对差异，使用欧式距离。</p></blockquote></li><li><p>如何进行线上 A/B 测试？</p><blockquote><p>用户分桶，在分桶过程中一定要保证独立性和采样的无偏性。</p></blockquote></li><li><p>为什么在进行了离线评估后还要进行线上评估？</p><blockquote><ol><li>离线评估无法完全消除过拟合的影响。2. 离线评估无法完全还原线上的工程环境。3. 线上系统的某些商业指标无法在离线环境中还原，如用户点击率，留存时长等。</li></ol></blockquote></li><li><p>模型评估时，有哪些主要的验证方法，他们的优缺点是什么？</p><blockquote><ol><li>holdout. 在验证集上计算出的评估指标与原始分组有很大的关系。</li><li>k-fold. 把k次评估的平均值作为最终的评估指标。</li><li>留一法. 每次留下 1 个样本作为验证集。开销大，实际工程中较少使用。</li><li>自助法. 基于自助采样的方法，对于总数为n的样本集合，进行n次有放回的随机采样，得到大小为n的训练集，没有被采样的样本作为测试集。</li></ol></blockquote></li><li><p>超参数有哪些调优方法？</p><blockquote><p>Grid Search。 十分消耗计算资源和时间，一般先使用较广的搜索范围和较大的步长，或者先确定对模型影响最大的参数。<br>随机搜索。 业界公认的Random search效果会比Grid search好。 例如前面的场景A有2种选择、B有3种、C有5种、连续值随机采样，那么每次分别在A、B、C中随机取值组合成新的超参数组合来训练。虽然有随机因素，但随机搜索可能出现效果特别差、也可能出现效果特别好，在尝试次数和Grid search相同的情况下一般最值会更大，当然variance也更大但这不影响最终结果。<br>贝叶斯优化算法. 是基于数据使用贝叶斯定理估计目标函数的后验分布，然后再根据分布选择下一个采样的超参数组合。它充分利用了前一个采样点的信息，其优化的工作方式是通过对目标函数形状的学习，并找到使结果向全局最大提升的参数</p></blockquote></li><li><p>过拟合、欠拟合具体是指什么现象？</p><blockquote><p>过拟合是指数据拟合过当，模型在训练集上表现好，但是测试集和新数据上表现差。欠拟合是模型在训练集和测试集上都表现不好。</p></blockquote></li><li><p>能否说出集中降低过拟合和欠拟合风险的方法？</p><blockquote><p>降低过拟合：获取更多数据、降低模型复杂度、正则化、集成学习<br>降低欠拟合：添加新特征、增加模型负责度、减少正则化系数</p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      特征工程,模型评估相关问题
    
    </summary>
    
    
      <category term="Interview" scheme="https://zhangruochi.com/categories/Interview/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Interview/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Time and Ordering</title>
    <link href="https://zhangruochi.com/Time-and-Ordering/2020/05/09/"/>
    <id>https://zhangruochi.com/Time-and-Ordering/2020/05/09/</id>
    <published>2020-05-08T17:10:01.000Z</published>
    <updated>2020-05-09T05:36:27.341Z</updated>
    
    <content type="html"><![CDATA[<p>分布式系统和传统的单机系统不同，彼此是通过网络而不是”主板”连接、消息通讯是不可靠的。因此如果没有任何同步机制，同一系统的成员之间无法确保时间戳的误差控制在某个范围内。这个基本条件的缺失，会给上层应用的设计带来很多的麻烦。比如，一个业务流程的两个阶段分别在两台机器上处理，而后在第三台机器上将处理记录join起来，就可能因为时间戳的问题引发混乱。如何做好时间同步的协议，成为了分布式系统中的一个基本的问题。</p><p>在系统对时的时候，有两类基本的协议，第一个是外部对时，简单的说，就是整个分布式系统中的所有成员，与外部某个指定的源头进行时间同步，确保与源头的时间的diff在某个误差范围$D$内; 另一种是内部对时，即内部通过广播等各种手段，确保系统内的成员俩俩间的时间误差在一定范围内。从这里可以看出，如果一个集群使用了外部对时，控制误差在$D$以内，那么这个集群内部的时间的误差，也一定能够控制在$2D$的范围内。但反过来不一定，因为有可能整个集群与外部的时间存在很大的整体偏差，尽管在内部彼此的偏差很小。</p><p>那么如何进行时间的同步呢？这里介绍两个经典的协议：Cristian和NTP。</p><h2 id="Cristian"><a href="#Cristian" class="headerlink" title="Cristian"></a>Cristian</h2><p>Cristian的基本过程是这样的，假定现在P进程要从授时服务器S获取时间，那么最朴素的做法就是P向S发送请求，S将自己的时间t返回给P，而后P设置自己的时间为t。这个做法存在一个很关键的问题，就是由于网络的通讯时间是不确定的，P拿到t的时候，已经经过了不确定多久了，无法估计结束后P与S的时间误差范围。因此，我们需要将网络通讯的时间，即RTT(Rount Trip Time)也考虑进来。在这个场景下，RTT指的是P进程发出请求，到得到S的回应消息的时间差，这个时间差是P进程自己可以记录求得的。假定我们知道从 $P \to S$的最小延时是 $min_1$, $S \to P$的最小延时是$min_2$,那么，我们可以推断，真实的时间在$[t+min_2, t+RTT-min_1]$间内，Cristian的做法就将对时结果设置为：$t’=t+\frac{RTT+min_2-min_1}{2}$ 这个中间位置上。那么，其误差就能控制在$\pm \frac{RTT-min_1-min_2}{2}$ 的范围内。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%"></center><h2 id="NTP"><a href="#NTP" class="headerlink" title="NTP"></a>NTP</h2><p>另外一个知名的时间同步协议是NTP，全称Network Time Protocol。NTP协议一般在某个大的机构内部署，将机构内的设备组织成树形结构，每个节点都从父节点处获取时间。整个同步过程分为两轮，第一轮父节点记录自己发送返回的时间点$ts_1$，子节点记录自己接收到返回消息的时间 $tr_1$；而后第二轮，子节点记录自己的发送时间$ts_2$；；父节点记录收到请求的时间$tr_2$后将$ts_1$ 和 $tr_2$返回。那么子节点可以计算出自己和父节点之间的时间偏差为: $o=\frac{(tr_{1}-tr_{2}+ts_{2}-ts{1})}{2}$，并以此为依据进行修正(一般需要确保时间不能“倒流”)。那么为什么$o$是这么计算呢？假定子节点与父节点的时间偏差(offset)为$o\prime$、父节点往子节点的通讯时延为$L_1$、子节点往父节点的通讯时延为$L_2$，那么:</p><script type="math/tex; mode=display">\begin{align*} & tr_{1}=ts_{1}+L_{1}+o' \\& tr_{2}=ts_{2}+L_{2}-o' \\\end{align*}</script><p>相减可以得到:</p><script type="math/tex; mode=display">o'= \frac{(tr_1-tr_2+ts_2-ts_1)}{2} + \frac{(L_2 - L1)}{2} = o + \frac{(L_2-L1)}{2}</script><p>因此:</p><script type="math/tex; mode=display">\lvert o'-o \rvert \leqslant \lvert \frac{(L_2-L_1)}{2}\rvert < \frac{(L_{1}+L_{2})}{2} = \frac{RTT}{2}</script><p>由此可知o的这个值也在RTT相关的一个误差范围内，是可估计的。<br>从上面两个协议可以看出，对时的误差是与RTT强相关的。由于消息的传递受制于光速、距离越远时间准确度的保证就越差。对于那些假定了时间误差在某个范围内的分布式协议，在跨越距离很大的时候，我们就必须要将这个误差对系统的影响考虑在内，这将显著增加分布式系统设计的复杂度、或者影响设计出来的系统的吞吐(尤其是有高一致性要求的事务型系统)。</p><p>最后，不论是Cristian还是NTP，都只描述了一次对时如何将时间的偏移(clow skew)控制在一定范围内。由于不同机器的时钟的行进速度(clock drift)是不同的，因此我们需要每隔一段时间，进行一次修正，以消除时钟节奏不同的影响。多久需要做一次同步呢? 这个做一个简单的计算就可以得到。假定系统整体时钟的行进速率与标准时钟的速率小于MDR(Max Drift Rate, 一般由时钟的实现方式决定)，那么系统内俩俩时钟的行进速率差小于2MDR。如果我们要求系统内时间差不能超过M，那就必须以不低于$\delta = \frac{M}{2 \times {MDR}}$的间隔进行时间同步。在现实的系统中，我们需要计算合理的M，以避免系统内出现过多的时间同步消息。</p><p>在上面部分，我们谈到了分布式系统里进程彼此的物理时间是如何进行同步的，并介绍了一些经典的时间同步算法。但静下心来仔细想想，我们希望进行时间同步，很多时候是希望不同的进程，对系统内事件的顺序达成一致。至于是否是使用真实世界的那个时间来排序，往往并不是那么重要。<br>那么，如何在一个分布式系统中，对发生在众多节点上的事件进行定序呢？目前已知的做法包括以下几种：</p><ul><li>使用物理时间同步的方法，确保众多节点的时间偏差在某个范围内。而后记录事件的发生时间及理论误差范围，比如将每个事件的发生时间登记为$(t \pm \Delta)$如果两个事件的时间范围没有overlap，那么就自然的可以排序判断；否则，则需要引入一个新的排序规则(比如以节点id)，对这两个事件约定一个排序。spanner中采用了这种方式。</li><li>采用Lamport Timestamp及其引申算法进行定序，确保事件满足causality consistency的性质，成为后续更高层次的分布式算法设计的基础。本文后面主要将展开这类算法，并引出分布式系统中一些基础概念。这些基础概念是理解分布式共识问题(consensus problem)的基础。</li></ul><h2 id="Lamport"><a href="#Lamport" class="headerlink" title="Lamport"></a>Lamport</h2><p>为明确这个问题，我们首先需要先对事件的序(happen-before)做出一个定义。在Lamport的体系中，事件的先后关系是按照如下原则设定的：</p><ul><li>规则一：如果A、B两个事件都发生在同一个进程内，那么，A、B之间的序自然可以由这个进程给出。假如进程先执行了A后执行了B，那么可以说A在B之前发生，记为$A \prec B$;</li><li>规则二：如果进程x往进程y发送了一条消息M；设在进程x的消息发送事件为A，进程y收到消息的事件为B，则显然我们应当认为A在B之前发生，同样记为$A \prec B$.</li></ul><p>由此引出了Lamport timestamp的算法，这个算法就是一种给事件打上逻辑时间戳、确保其满足causality的基本属性。这个算法的基本过程为：</p><ul><li>每个进程都记录自己的一个当前时间戳，初始的时候，大家都是0</li><li>如果进程内部发生了一个新的事件，那么将当前时间戳记为 $t’=t+1$，并认为事件发生于$t’$时刻</li><li>如果进程A向进程B通讯，则发送消息的时候，进程A的时间戳$t’_A = t_A + 1$并随消息发送到B，B更新自己的时间戳为$t’_B = max(t’_B, t’_A) + 1$.</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%"></center><h3 id="Concurrent-Events"><a href="#Concurrent-Events" class="headerlink" title="Concurrent Events"></a>Concurrent Events</h3><ul><li>A pair of concurrent events doesn’t have a causal path from one event to another (either way, in the pair)</li><li>Lamport timestamps not guaranteed to be ordered or unequal for concurrent events</li><li>Ok, since concurrent events are not causality related!</li><li>Remember</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">E1 -&gt; E2 =&gt; timestamp(E1) &lt; timestamp (E2), </span><br><span class="line">BUT timestamp(E1) &lt; timestamp (E2) =&gt; &#123;E1 -&gt; E2&#125; OR &#123;E1 and E2 concurrent&#125;</span><br></pre></td></tr></table></figure><h2 id="Vector-timestamps"><a href="#Vector-timestamps" class="headerlink" title="Vector timestamps"></a>Vector timestamps</h2><ul><li>Used in key-value stores like Riak</li><li>Each process uses a vector of integer clocks</li><li>Suppose there are N processes in the group 1…N</li><li>Each vector has N elements</li><li>Process i maintains vector Vi[1…N]</li><li>$j_{th}$ element of vector clock at process $i$, $V_i[j]$, is $i’s$ knowledge of latest events at process $j$</li></ul><p>Incrementing vector clocks</p><ol><li>On an instruction or send event at process $i$, it increments only its $i_{th}$ element of its vector clock.</li><li>Each message carries the send-event’s vector timestamp V_{message}[1…N]</li><li>On receiving a message at process $i$:</li></ol><script type="math/tex; mode=display">\begin{align*} &V_i[i] = V_i[i] + 1 \\& V_i[j] = max(V_{message}[j], V_i[j]) \quad for \ quad j \neq i \\\end{align*}</script><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="3.png" width="70%" height="70%"></center><h3 id="Causality"><a href="#Causality" class="headerlink" title="Causality"></a>Causality</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="4.png" width="70%" height="70%"></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="5.png" width="70%" height="70%"></center><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://lvsizhe.github.io/course/2018/09/time-in-distributed-systems-part1.html" target="_blank" rel="noopener">https://lvsizhe.github.io/course/2018/09/time-in-distributed-systems-part1.html</a></li><li>lecture slide from <a href="https://www.coursera.org/learn/cloud-computing/lecture/dy8wf/2-5-vector-clocks" target="_blank" rel="noopener">https://www.coursera.org/learn/cloud-computing/lecture/dy8wf/2-5-vector-clocks</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;分布式系统和传统的单机系统不同，彼此是通过网络而不是”主板”连接、消息通讯是不可靠的。因此如果没有任何同步机制，同一系统的成员之间无法确保时间戳的误差控制在某个范围内。这个基本条件的缺失，会给上层应用的设计带来很多的麻烦。比如，一个业务流程的两个阶段分别在两台机器上处理，而
      
    
    </summary>
    
    
      <category term="Big Data Architecture" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/"/>
    
      <category term="Distributed &amp; Cloud Computing" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/Distributed-Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>Programming Language: New Types, Pattern Matching, Tail Recursion</title>
    <link href="https://zhangruochi.com/Programming-Language-New-Types-Pattern-Matching-Tail-Recursion/2020/05/03/"/>
    <id>https://zhangruochi.com/Programming-Language-New-Types-Pattern-Matching-Tail-Recursion/2020/05/03/</id>
    <published>2020-05-02T19:29:05.000Z</published>
    <updated>2020-05-03T07:29:53.322Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Conceptual-Ways-to-Build-New-Types"><a href="#Conceptual-Ways-to-Build-New-Types" class="headerlink" title="Conceptual Ways to Build New Types"></a>Conceptual Ways to Build New Types</h2><p>To create a compound type, there are really only three essential building blocks. Any decent programming language provides these building blocks in some way:</p><ul><li><strong>Each-of</strong>: A compound type t describes values that contain each of values of type t1, t2, …, and tn. Tuples are an example: int * bool describes values that contain an int and a bool. A <strong>Java class</strong> with fields is also an each-of sort of thing.</li><li><strong>One-of</strong>: A compound type t describes values that contain a value of one of the types t1, t2, …, or tn. For a type that contains an int or a bool in ML, we need <code>datatype bindings</code>. In object-oriented languages with classes like Java, one-of types are achieved with <strong>subclassing</strong>, but that is a topic for much later in the course.</li><li><strong>Self-reference</strong>: A compound type t may refer to itself in its definition in order to describe recursive data structures like lists and trees. This is useful in combination with each-of and one-of types. For example, int list describes values that either contain nothing or contain an int and another int list. </li></ul><h2 id="Records-Another-Approach-to-Each-of-Types"><a href="#Records-Another-Approach-to-Each-of-Types" class="headerlink" title="Records: Another Approach to Each-of Types"></a>Records: Another Approach to <strong>Each-of</strong> Types</h2><p>Record types are “each-of” types where each component is a <code>named field</code>.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;foo : <span class="built_in">int</span>, bar : <span class="built_in">int</span>*<span class="built_in">bool</span>, baz : <span class="built_in">bool</span>*<span class="built_in">int</span>&#125;</span><br></pre></td></tr></table></figure><p>In ML, we do not have to declare that we want a record type with particular field names and field types — we just write down a record expression and the type-checker gives it the right type.</p><p>Now that we know how to build record values, we need a way to access their pieces. For now, we will use <code>#foo e</code> where <code>foo</code> is a field name. </p><h3 id="The-truth-of-tuple"><a href="#The-truth-of-tuple" class="headerlink" title="The truth of tuple"></a>The truth of tuple</h3><p>In fact, this is how ML actually defines tuples: A tuple is a record. That is, all the syntax for tuples is just a convenient way to write down and use records. The REPL just always uses the tuple syntax where possible, so if you evaluate {2=1+2, 1=3+4} it will print the result as (7,3). Using the tuple syntax is better style, but we did not need to give tuples their own semantics: we can instead use the “another way of writing” rules above and then reuse the semantics for records.</p><p>This is the first of many examples we will see of <code>syntactic sugar</code>. We say, Tuples are just syntactic sugar for records with fields named 1, 2, …, n.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> z = (<span class="number">3</span>,<span class="number">7</span>) : <span class="built_in">int</span> * <span class="built_in">int</span></span><br><span class="line"><span class="keyword">val</span> z = &#123;<span class="number">1</span>=<span class="number">3</span>,<span class="number">3</span>=<span class="number">7</span>&#125; : &#123;<span class="number">1</span>:<span class="built_in">int</span>, <span class="number">3</span>:<span class="built_in">int</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="Datatype-Bindings-Our-Own-One-of-Types"><a href="#Datatype-Bindings-Our-Own-One-of-Types" class="headerlink" title="Datatype Bindings: Our Own One-of Types"></a>Datatype Bindings: Our Own <strong>One-of</strong> Types</h2><p>We now introduce datatype bindings, our third kind of binding after variable bindings and function bindings.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">datatype mytype = TwoInts of int * int</span><br><span class="line">                | Str of string</span><br><span class="line">                | Pizza</span><br></pre></td></tr></table></figure><p>Roughly, this defines a new type where values have an int <em> int or a string or nothing. Any value will also be <code>tagged</code> with information that lets us know which variant it is: These tags, which we will call <em>*constructors</em></em>, are <code>TwoInts</code>, <code>Str</code>, and <code>Pizza</code>.</p><p>More precisely, the example above adds four things to the environment:</p><ul><li>A new type mytype that we can now use just like any other type</li><li>Three constructors TwoInts, Str, and Pizza</li></ul><p>A constructor is two different things. First, it is either a function for creating values of the new type (if the variant has of t for some type t) or it is actually a value of the new type (otherwise). In our example, TwoInts is a function of type int*int -&gt; mytype, Str is a function of type string-&gt;mytype, and Pizza is a value of type mytype. Second, we use constructors in case-expressions as described further below.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">datatype</span> mytype = <span class="type">TwoInts</span> <span class="keyword">of</span> <span class="built_in">int</span> * <span class="built_in">int</span> </span><br><span class="line">                | <span class="type">Str</span> <span class="keyword">of</span> <span class="built_in">string</span> </span><br><span class="line">                | <span class="type">Pizza</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> a = <span class="type">Str</span> <span class="string">"hi"</span></span><br><span class="line"><span class="keyword">val</span> b = <span class="type">Str</span></span><br><span class="line"><span class="keyword">val</span> c = <span class="type">Pizza</span></span><br><span class="line"><span class="keyword">val</span> d = <span class="type">TwoInts</span>(<span class="number">1</span>+<span class="number">2</span>,<span class="number">3</span>+<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> e = a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">(* val a = Str "hi" : mytype</span></span><br><span class="line"><span class="comment">val b = fn : string -&gt; mytype</span></span><br><span class="line"><span class="comment">val c = Pizza : mytype</span></span><br><span class="line"><span class="comment">val d = TwoInts (3,7) : mytype</span></span><br><span class="line"><span class="comment">val e = Str "hi" : mytype *)</span></span><br></pre></td></tr></table></figure><h2 id="How-ML-Provides-Access-to-Datatype-Values-Case-Expressions"><a href="#How-ML-Provides-Access-to-Datatype-Values-Case-Expressions" class="headerlink" title="How ML Provides Access to Datatype Values: Case Expressions"></a>How ML Provides Access to Datatype Values: Case Expressions</h2><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> f x = <span class="comment">(* f has type mytype -&gt; int*)</span> </span><br><span class="line">    <span class="keyword">case</span> x <span class="keyword">of</span></span><br><span class="line">      <span class="type">Pizza</span> =&gt; <span class="number">3</span></span><br><span class="line">    | <span class="type">TwoInts</span>(i1,i2) =&gt; i1 + i2 </span><br><span class="line">    | <span class="type">Str</span> s =&gt; <span class="type">String</span>.size s</span><br></pre></td></tr></table></figure><p>In one sense, a case-expression is like a more powerful if-then-else expression: Like a conditional expression, it evaluates two of its subexpressions: first the expression between the case and of keywords and second the expression in the first branch that matches. But instead of having two branches (one for true and one for false), we can have one branch for each variant of our datatype (and we will generalize this further below). Like conditional expressions, each branch’s expression must have the same type (int in the example above) because the type-checker cannot know what branch will be used.<br>Each branch has the form <code>p =&gt; e</code> where p is a pattern and e is an expression, and we separate the branches with the | character. Patterns look like expressions, but do not think of them as expressions. Instead they are used to match against the result of evaluating the case’s first expression (the part after case). This is why evaluating a case-expression is called pattern-matching.</p><h3 id="Datatype-Bindings-and-Case-Expressions-So-Far-Precisely"><a href="#Datatype-Bindings-and-Case-Expressions-So-Far-Precisely" class="headerlink" title="Datatype Bindings and Case Expressions So Far, Precisely"></a>Datatype Bindings and Case Expressions So Far, Precisely</h3><p>We can summarize what we know about datatypes and pattern matching so far as follows: The binding</p><blockquote><p>datatype t = C1 of t1 | C2 of t2 | … | Cn of tn</p></blockquote><p>introduces a new type t and each constructor Ci is a function of type ti-&gt;t. One omits the “of ti” for a variant that “carries nothing” and such a constructor just has type t. To “get at the pieces” of a t we use a case expression:</p><blockquote><p>case e of p1 =&gt; e1 | p2 =&gt; e2 | … | pn =&gt; en</p></blockquote><p>A case expression evaluates e to a value v, finds the first pattern pi that matches v, and evaluates ei to produce the result for the whole case expression. So far, patterns have looked like Ci(x1,…,xn) where Ci is a constructor of type t1 <em> … </em> tn -&gt; t (or just Ci if Ci carries nothing). Such a pattern matches a value of the form Ci(v1,…,vn) and binds each xi to vi for evaluating the corresponding ei.</p><h2 id="Type-Synonyms"><a href="#Type-Synonyms" class="headerlink" title="Type Synonyms"></a>Type Synonyms</h2><p>A <strong>type synonym</strong> simply creates another name for an existing type that is entirely interchangeable with the existing type.</p><p>For example, if we write:<br><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> foo = <span class="built_in">int</span></span><br></pre></td></tr></table></figure></p><p>then we can write foo wherever we write int and vice-versa.</p><p>for more complicated types, it can be convenient to create type synonyms. Here are some examples for types we created above:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> card = suit * rank</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> name_record = &#123; student_num : <span class="built_in">int</span> <span class="built_in">option</span>,</span><br><span class="line">                    first : <span class="built_in">string</span>,</span><br><span class="line">                    middle : <span class="built_in">string</span> <span class="built_in">option</span>,</span><br><span class="line">                    last : <span class="built_in">string</span> &#125;</span><br></pre></td></tr></table></figure><h2 id="Lists-and-Options-are-Datatypes"><a href="#Lists-and-Options-are-Datatypes" class="headerlink" title="Lists and Options are Datatypes"></a>Lists and Options are Datatypes</h2><p>Because datatype definitions can be recursive, we can use them to create our own types for lists. For example, this binding works well for a linked list of integers:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">datatype</span> my_int_list = <span class="type">Empty</span></span><br><span class="line">                        | <span class="type">Cons</span> <span class="keyword">of</span> <span class="built_in">int</span> * my_int_list</span><br></pre></td></tr></table></figure><p>We can use the constructors Empty and Cons to make values of my_int_list and we can use case expressions to use such values:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> one_two_three = <span class="type">Cons</span>(<span class="number">1</span>,<span class="type">Cons</span>(<span class="number">2</span>,<span class="type">Cons</span>(<span class="number">3</span>,<span class="type">Empty</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> append_mylist (xs,ys) = </span><br><span class="line">    <span class="keyword">case</span> xs <span class="keyword">of</span></span><br><span class="line">        <span class="type">Empty</span> =&gt; ys</span><br><span class="line">    | <span class="type">Cons</span>(x,xs’) =&gt; <span class="type">Cons</span>(x, append_mylist(xs’,ys))</span><br></pre></td></tr></table></figure><p>For options, all you need to know is SOME and NONE are constructors, which we use to create values (just like before) and in patterns to access the values. Here is a short example of the latter:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> inc_or_zero intoption = <span class="keyword">case</span> intoption <span class="keyword">of</span></span><br><span class="line">        <span class="type">NONE</span> =&gt; <span class="number">0</span></span><br><span class="line">      | <span class="type">SOME</span> i =&gt; i+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">&lt;!-- <span class="keyword">val</span> inc_or_zero = <span class="keyword">fn</span> : <span class="built_in">int</span> <span class="built_in">option</span> -&gt; <span class="built_in">int</span> --&gt;</span><br></pre></td></tr></table></figure><p>The story for lists is similar with a few convenient syntactic peculiarities: [] really is a constructor that carries nothing and :: really is a constructor that carries two things, but :: is unusual because it is an infix operator (it is placed between its two operands), both when creating things and in patterns:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> sum_list xs = </span><br><span class="line">    <span class="keyword">case</span> xs:</span><br><span class="line">        <span class="literal">[]</span> =&gt; <span class="number">0</span></span><br><span class="line">        | x::xs' =&gt; x + sum_list xs'</span><br></pre></td></tr></table></figure><p>Notice here x and xs’ are nothing but local variables introduced via pattern-matching. We can use any names for the variables we want. </p><h2 id="Pattern-Matching-for-Each-Of-Types-The-Truth-About-Val-Bindings"><a href="#Pattern-Matching-for-Each-Of-Types-The-Truth-About-Val-Bindings" class="headerlink" title="Pattern-Matching for Each-Of Types: The Truth About Val-Bindings"></a>Pattern-Matching for Each-Of Types: The Truth About Val-Bindings</h2><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> full_name (r : &#123;first:<span class="built_in">string</span>,middle:<span class="built_in">string</span>,last:<span class="built_in">string</span>&#125;) = <span class="keyword">case</span> r <span class="keyword">of</span></span><br><span class="line">        &#123;first=x,middle=y,last=z&#125; =&gt; x ^ <span class="string">" "</span> ^ y ^ <span class="string">" "</span> ^z</span><br></pre></td></tr></table></figure><p>However, a case-expression with one branch is poor style — it looks strange because the purpose of such expressions is to distinguish cases, plural. So how should we use pattern-matching for each-of types, when we know that a single pattern will definitely match so we are using pattern-matching just for the convenient <strong>extraction of values</strong>? It turns out you can use patterns in val-bindings too! So this approach is better style:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">fun</span> full_name (r : &#123;first:<span class="built_in">string</span>,middle:<span class="built_in">string</span>,last:<span class="built_in">string</span>&#125;) = </span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">val</span> &#123;first=x,middle=y,last=z&#125; = r</span><br><span class="line">    <span class="keyword">in</span></span><br><span class="line">        x ^ <span class="string">" "</span> ^ y ^ <span class="string">" "</span> ^z </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> sum_triple (triple : <span class="built_in">int</span>*<span class="built_in">int</span>*<span class="built_in">int</span>) = </span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">val</span> (x,y,z) = triple</span><br><span class="line">    <span class="keyword">in</span></span><br><span class="line">        x+y+z</span><br><span class="line">    <span class="keyword">end</span></span><br></pre></td></tr></table></figure><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> sum_triple (x,y,z) = x+y+z</span><br></pre></td></tr></table></figure><p>This version of sum_triple should intrigue you: It takes a triple as an argument and uses pattern-matching to bind three variables to the three pieces for use in the function body. But it looks exactly like a function that takes three arguments of type int. Indeed, is the type int<em>int</em>int-&gt;int for three-argument functions or for one argument functions that take triples?<br>It turns out we have been basically lying: There is no such thing as a multi-argument function in ML: <strong>Every function in ML takes exactly one argument!</strong> Every time we write a multi-argument function, we are really writing a one-argument function that takes a tuple as an argument and uses pattern-matching to extract the pieces. This is such a common idiom that it is easy to forget about and it is totally fine to talk about “multi-argument functions” when discussing your ML code with friends. But in terms of the actual language definition, it really is a one-argument function: syntactic sugar for expanding out to the first version of sum_triple with a one-arm case expression.</p><h2 id="Digression-Type-inference"><a href="#Digression-Type-inference" class="headerlink" title="Digression: Type inference"></a>Digression: Type inference</h2><p>In ML, every variable and function has a type (or your program fails to type-check) — type inference only means you do not need to write down the type.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> sum_triple triple = <span class="keyword">case</span> triple <span class="keyword">of</span></span><br><span class="line">    (x,y,z) =&gt; z + y + x</span><br></pre></td></tr></table></figure><p>In fact, type inference sometimes reveals that functions are more general than you might have thought. Consider this code, which does use part of a tuple/record:<br><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> partial_sum (x,y,z) = x + z</span><br><span class="line"><span class="keyword">fun</span> partial_name &#123;first=x, middle=y, last=z&#125; = x ^ <span class="string">" "</span> ^ z</span><br></pre></td></tr></table></figure></p><p>In both cases, the inferred function types reveal that the type of y can be any type, so we can call partial_sum (3,4,5) or partial_sum (3,false,5). This is okay because the polymorphism indicates that partial_sum has a more gen- eral type. If you can take a type containing ’a, ’b, ’c, etc. and replace each of these type variables consistently to get the type you “want,” then you have a more general type than the one you want.</p><h2 id="Nested-Patterns"><a href="#Nested-Patterns" class="headerlink" title="Nested Patterns"></a>Nested Patterns</h2><p>It turns out the definition of patterns is recursive: anywhere we have been putting a variable in our patterns, we can instead put another pattern. Roughly speaking, the semantics of pattern-matching is that the value being matched must have the same “shape” as the pattern and variables are bound to the “right pieces.” (This is very hand-wavy explanation which is why a precise definition is described below.) For example, the pattern a::(b::(c::d)) would match any list with at least 3 elements and it would bind a to the first element, b to the second, c to the third, and d to the list holding all the other elements (if any). The pattern a::(b::(c::[])) on the other hand, would match only lists with exactly three elements. Another nested patterns is (a,b,c)::d, which matches any non-empty list of triples, binding a to the first component of the head, b to the second component of the head, c to the third component of the head, and d to the tail of the list.</p><p>In general, pattern-matching is about taking a value and a pattern and (1) deciding if the pattern matches the value and (2) if so, binding variables to the right parts of the value. Here are some key parts to the elegant recursive definition of pattern matching:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">exception</span> <span class="type">BadTriple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> zip3 list_triple = <span class="keyword">case</span> list_triple <span class="keyword">of</span></span><br><span class="line">    (<span class="literal">[]</span>,<span class="literal">[]</span>,<span class="literal">[]</span>) =&gt; <span class="literal">[]</span></span><br><span class="line">        | (hd1::tl1,hd2::tl2,hd3::tl3) =&gt; (hd1,hd2,hd3)::zip3(tl1,tl2,tl3) </span><br><span class="line">        | _ =&gt; <span class="keyword">raise</span> <span class="type">BadTriple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> unzip3 lst =</span><br><span class="line">    <span class="keyword">case</span> lst <span class="keyword">of</span></span><br><span class="line">        <span class="literal">[]</span> =&gt; (<span class="literal">[]</span>,<span class="literal">[]</span>,<span class="literal">[]</span>)</span><br><span class="line">      | (a,b,c)::tl =&gt; <span class="keyword">let</span> <span class="keyword">val</span> (l1,l2,l3) = unzip3 tl</span><br><span class="line">                       <span class="keyword">in</span></span><br><span class="line">                           (a::l1,b::l2,c::l3)</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="Exceptions"><a href="#Exceptions" class="headerlink" title="Exceptions"></a>Exceptions</h2><p>ML has a built-in notion of exception. You can raise (also known as throw) an exception with the raise primitive. For example, the hd function in the standard library raises the List.Empty exception when called with []:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> hd xs =</span><br><span class="line">    <span class="keyword">case</span> xs <span class="keyword">of</span></span><br><span class="line"><span class="literal">[]</span> =&gt; <span class="keyword">raise</span> <span class="type">List</span>.<span class="type">Empty</span> | x::_ =&gt; x</span><br></pre></td></tr></table></figure><p>You can create your own kinds of exceptions with an exception binding. Exceptions can optionally carry values with them, which let the code raising the exception provide more information:</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">exception</span> <span class="type">MyUndesirableCondition</span></span><br><span class="line"><span class="keyword">exception</span> <span class="type">MyOtherException</span> <span class="keyword">of</span> <span class="built_in">int</span> * <span class="built_in">int</span></span><br></pre></td></tr></table></figure><p>Kinds of exceptions are a lot like constructors of a datatype binding. Indeed, they are functions (if they carry values) or values (if they don’t) that create values of type exn rather than the type of a datatype. So Empty, MyUndesirableCondition, and MyOtherException(3,9) are all values of type exn, whereas MyOtherException has type int*int-&gt;exn.</p><h2 id="Tail-Recursion-and-Accumulators"><a href="#Tail-Recursion-and-Accumulators" class="headerlink" title="Tail Recursion and Accumulators"></a>Tail Recursion and Accumulators</h2><p>This topic involves new programming idioms, but no new language constructs. It defines tail recursion, describes how it relates to writing efficient recursive functions in functional languages like ML, and presents how to use accumulators as a technique to make some functions tail recursive.</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">fun</span> sum1 xs =</span><br><span class="line">    <span class="keyword">case</span> xs <span class="keyword">of</span></span><br><span class="line">        <span class="literal">[]</span> =&gt; <span class="number">0</span></span><br><span class="line">      | i::xs’ =&gt; i + sum1 xs’</span><br><span class="line"></span><br><span class="line"><span class="keyword">fun</span> sum2 xs =</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">fun</span> f (xs,acc) =</span><br><span class="line">            <span class="keyword">case</span> xs <span class="keyword">of</span></span><br><span class="line">                <span class="literal">[]</span> =&gt; acc</span><br><span class="line">              | i::xs’ =&gt; f(xs’,i+acc)</span><br><span class="line">    <span class="keyword">in</span></span><br><span class="line">        f(xs,<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>Why might sum2 be preferred when it is clearly more complicated? To answer, we need to understand a little bit about how function calls are implemented. Conceptually, there is a <strong>call stack</strong>, which is a stack (the data structure with push and pop operations) with one element for each function call that has been started but has not yet completed. Each element stores things like the value of local variables and what part of the function has not been evaluated yet. When the evaluation of one function body calls another function, a new element is pushed on the call stack and it is popped off when the called function completes.</p><blockquote><p> there is nothing more for the caller to do after the callee returns except return the callee’s result.</p></blockquote><p>This situation is called a tail call (let’s not try to figure out why it’s called this) and functional languages like ML typically promise an essential optimization: When a call is a tail call, the caller’s stack-frame is popped before the call — the callee’s stack-frame just replaces the caller’s. This makes sense: the caller was just going to return the callee’s result anyway. Therefore, calls to sum2 never use more than 1 stack frame.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Conceptual-Ways-to-Build-New-Types&quot;&gt;&lt;a href=&quot;#Conceptual-Ways-to-Build-New-Types&quot; class=&quot;headerlink&quot; title=&quot;Conceptual Ways to Build
      
    
    </summary>
    
    
      <category term="Programming Language" scheme="https://zhangruochi.com/categories/Programming-Language/"/>
    
    
  </entry>
  
  <entry>
    <title>EDA Summary</title>
    <link href="https://zhangruochi.com/EDA-Summary/2020/04/30/"/>
    <id>https://zhangruochi.com/EDA-Summary/2020/04/30/</id>
    <published>2020-04-30T11:55:33.000Z</published>
    <updated>2020-05-01T00:02:49.967Z</updated>
    
    <content type="html"><![CDATA[<h2 id="The-main-goals-of-EDA-are"><a href="#The-main-goals-of-EDA-are" class="headerlink" title="The main goals of EDA are:"></a>The main goals of EDA are:</h2><ul><li>Provide summary level insight into a data set</li><li>Uncover underlying patterns and structure in the data</li><li>Identify outliers, missing data and class balance issues</li><li>Carry out quality control checks</li></ul><h2 id="The-principal-steps-in-the-process-of-EDA-are"><a href="#The-principal-steps-in-the-process-of-EDA-are" class="headerlink" title="The principal steps in the process of EDA are:"></a>The principal steps in the process of EDA are:</h2><ol><li>Summarize the data - Generally done using dataframes in R or Python</li><li>Tell the Story - Summarize the details of what connects the dataset to the business opportunity</li><li>Deal with missing data - Identify the strategy for dealing with missing data</li><li>Investigate - Using data visualization and hypothesis testing delve into the relationship between the dataset and the business opportunity</li><li>Communicate - Communicate the findings from the above steps</li></ol><h2 id="Data-visualization"><a href="#Data-visualization" class="headerlink" title="Data visualization"></a>Data visualization</h2><ol><li>Jupyter notebooks in combination with pandas and simple plots are the basis for modern EDA when using Python as a principal language</li></ol><h3 id="Advantages-of-Jupyter-notebooks"><a href="#Advantages-of-Jupyter-notebooks" class="headerlink" title="Advantages of Jupyter notebooks:"></a>Advantages of Jupyter notebooks:</h3><ul><li>They are portable: then can be used locally on private servers, public cloud, and as part of IBM Watson Studio</li><li>They work with dozens of languages</li><li>They mix markdown with executable code in a way that works naturally with storytelling and investigation</li><li>matplotlib itself and its numerous derivative works like seaborn are the core of the Python data visualization landscape</li><li>pandas and specifically the dataframe class works naturally with Jupyter, matplotlib and downstream modeling frameworks like sklearn</li></ul><h3 id="EDA-and-Data-Visualization-best-practices"><a href="#EDA-and-Data-Visualization-best-practices" class="headerlink" title="EDA and Data Visualization best practices"></a>EDA and Data Visualization best practices</h3><ol><li>The majority of code for any data science project should be contained within text files. This is a software engineering best practice that ensures re-usability, allows for unit testing and works naturally with version control. &gt;In Python the text files can be executable scripts, modules, a full Python package or some combination of these.</li><li>Keep a record of plots and visualization code that you create. It is difficult to remember all of the details of how visualizations were created. Extracting the visualization code to a specific place will ensure that similar plots for future projects will be quick to create.</li><li>Use you plots as a quality assurance tool. Given what you know about the data it can be useful to make an educated guess before you execute the cell or run the script. This habit is surprisingly useful for quality assurance of both data and code.</li></ol><h2 id="Missing-values"><a href="#Missing-values" class="headerlink" title="Missing values"></a>Missing values</h2><ul><li>Dealing with missing data sits at the intersection of EDA and data ingestion in the AI enterprise workflow</li><li>Ignoring missing data may have unintended consequences in terms of model performance that may not be easy to detect</li><li>Removing either complete rows or columns in a feature matrix that contain missing values is called complete case analysis</li><li>Complete case analysis, although commonly used, can lead to undesirable results—the extent to which depends on the category of missingness</li></ul><h3 id="The-categories-of-missingness-are"><a href="#The-categories-of-missingness-are" class="headerlink" title="The categories of missingness are:"></a>The categories of missingness are:</h3><ol><li>Missing completely at random or MCAR:</li></ol><p>When data are MCAR, missing cases are, on average, identical to non-missing cases, with respect to the feature matrix. Complete case analysis will reduce the power of the analysis, but will not affect bias.</p><ol><li>Missing at random or MAR:</li></ol><p>When data are MAR the missing data often have some dependence on measured values, and models can be used to help impute what the likely data would be. For example, in an MLB survey, there may be a gender bias when it comes to completing all of the questions.</p><ol><li>Missing not at random or MNAR:</li></ol><p>In this case the missing data depend on unmeasured or unknown variables. There is no information available to account for the missingness.</p><ul><li>The best case scenario is that the data are MCAR. It should be noted that imputing values under the other two types of missingness can result in an increase in bias.</li><li>In statistics the process of replacing missing data with substituted values is known as imputation.</li><li>It is a common practice to perform multiple imputations.</li><li>The practice of imputing missing values introduces uncertainty into the results of a data science project.</li><li>One way to deal with that additional uncertainty is to try a range of different values for imputation and measure how the results vary between each set of imputations. This technique is known as multiple imputation.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;The-main-goals-of-EDA-are&quot;&gt;&lt;a href=&quot;#The-main-goals-of-EDA-are&quot; class=&quot;headerlink&quot; title=&quot;The main goals of EDA are:&quot;&gt;&lt;/a&gt;The main g
      
    
    </summary>
    
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="EDA" scheme="https://zhangruochi.com/categories/AI-Workflow/EDA/"/>
    
    
  </entry>
  
  <entry>
    <title>Model Training Tricks (2)</title>
    <link href="https://zhangruochi.com/Model-Training-Tricks-2/2020/04/28/"/>
    <id>https://zhangruochi.com/Model-Training-Tricks-2/2020/04/28/</id>
    <published>2020-04-27T20:04:20.000Z</published>
    <updated>2020-04-28T21:09:54.664Z</updated>
    
    <content type="html"><![CDATA[<p>If you have unlimited data, unlimited memory, and unlimited time, then the advice is easy: train a huge model on all of your data for a really long time. The reason that deep learning is not straightforward is because your data, memory, and time is limited. If you are running out of memory or time, then the solution is to train a smaller model. If you are not able to train for long enough to overfit, then you are not taking advantage of the capacity of your model.</p><p>So step one is to get to the point that you can overfit. Then, the question is how to reduce that overfitting.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%"></center><p>Many practitioners when faced with an overfitting model start at exactly the wrong end of this diagram. Their starting point is to use a smaller model, or more regularisation. Using a smaller model should be absolutely the last step you take, unless your model is taking up too much time or memory. Reducing the size of your model as reducing the ability of your model to learn subtle relationships in your data.<br>Instead, your first step should be to seek to create more data. That could involve adding more labels to data that you already have in your organisation, finding additional tasks that your model could be asked to solve (or to think of it another way, identifying different kinds of labels that you could model), or creating additional synthetic data via using more or different data augmentation. Thanks to the development of mixup and similar approaches, effective data augmentation is now available for nearly all kinds of data.<br>Once you’ve got as much data as you think you can reasonably get a hold of, and are using it as effectively as possible by taking advantage of all of the labels that you can find, and all of the augmentation that make sense, if you are still overfitting and you should think about using more generalisable architectures. For instance, adding batch normalisation may improve generalisation.<br>If you are still overfitting after doing the best you can at using your data and tuning your architecture, then you can take a look at regularisation. Generally speaking, adding dropout to the last layer or two will do a good job of regularising your model. However, as we learnt from the story of the development of AWD-LSTM, it is often the case that adding dropout of different types throughout your model can help regularise even better. Generally speaking, a larger model with more regularisation is more flexible, and can therefore be more accurate than a smaller model with less regularisation.<br>Only after considering all of these options would be recommend that you try using smaller versions of your architectures.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://github.com/fastai/fastbook/blob/master/15_arch_details.ipynb" target="_blank" rel="noopener">https://github.com/fastai/fastbook/blob/master/15_arch_details.ipynb</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;If you have unlimited data, unlimited memory, and unlimited time, then the advice is easy: train a huge model on all of your data for a r
      
    
    </summary>
    
    
      <category term="Competition" scheme="https://zhangruochi.com/categories/Competition/"/>
    
      <category term="Tricks" scheme="https://zhangruochi.com/categories/Competition/Tricks/"/>
    
    
  </entry>
  
  <entry>
    <title>Model Training Tricks (1)</title>
    <link href="https://zhangruochi.com/Model-Training-Tricks-1/2020/04/27/"/>
    <id>https://zhangruochi.com/Model-Training-Tricks-1/2020/04/27/</id>
    <published>2020-04-27T08:21:11.000Z</published>
    <updated>2020-04-28T08:04:35.236Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dls</span><span class="params">(bs, size)</span>:</span></span><br><span class="line">    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),</span><br><span class="line">                   get_items=get_image_files,</span><br><span class="line">                   get_y=parent_label,</span><br><span class="line">                   item_tfms=Resize(<span class="number">460</span>),</span><br><span class="line">                   batch_tfms=[*aug_transforms(size=size, min_scale=<span class="number">0.75</span>),</span><br><span class="line">                               Normalize.from_stats(*imagenet_stats)])</span><br><span class="line">    <span class="keyword">return</span> dblock.dataloaders(path, bs=bs)</span><br></pre></td></tr></table></figure><p>Normalization becomes especially important when using pretrained models. The pretrained model only knows how to work with data of the type that it has seen before. If the average pixel was zero in the data it was trained with, but your data has zero as the minimum possible value of a pixel, then the model is going to be seeing something very different to what is intended.</p><p>This means that when you distribute a model, you need to also distribute the statistics used for normalization, since anyone using it for inference, or transfer learning, will need to use the same statistics. By the same token, if you’re using a model that someone else has trained, make sure you find out what normalization statistics they used, and match them.</p><h2 id="Progressive-resizing"><a href="#Progressive-resizing" class="headerlink" title="Progressive resizing"></a>Progressive resizing</h2><blockquote><p>Gradually using larger and larger images as you train</p></blockquote><p>Start training using small images, and end training using large images. By spending most of the epochs training with small images, training completed much faster. By completing training using large images, the final accuracy was much higher. We call this approach progressive resizing.</p><p>Note that for transfer learning, progressive resizing may actually hurt performance. This would happen if your pretrained model was quite <code>similar</code> to your transfer learning task and dataset, and was trained on similar sized images, so the weights don’t need to be changed much. In that case, training on smaller images may damage the pretrained weights. On the other hand, if the transfer learning task is going to be on images that are of different sizes, shapes, or style to those used in the pretraining tasks, progressive resizing will probably help. As always, the answer to “does it help?” is “try it!”.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">dls = get_dls(<span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">learn = Learner(dls, xresnet50(), loss_func=CrossEntropyLossFlat(), </span><br><span class="line">                metrics=accuracy)</span><br><span class="line">learn.fit_one_cycle(<span class="number">4</span>, <span class="number">3e-3</span>)</span><br><span class="line"></span><br><span class="line">learn.dls = get_dls(<span class="number">64</span>, <span class="number">224</span>)</span><br><span class="line">learn.fine_tune(<span class="number">5</span>, <span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure><h2 id="Test-time-augmentation"><a href="#Test-time-augmentation" class="headerlink" title="Test time augmentation"></a>Test time augmentation</h2><blockquote><p>During inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image</p></blockquote><p>Select a number of areas to crop from the original rectangular image, pass each of them through our model, and take the maximum or average of the predictions. In fact, we could do this not just for different crops, but for different values across all of our test time augmentation parameters.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">preds,targs = learn.tta()</span><br><span class="line">accuracy(preds, targs).item()</span><br></pre></td></tr></table></figure><h2 id="Mixup"><a href="#Mixup" class="headerlink" title="Mixup"></a>Mixup</h2><p>Mixup works as follows, for each image:</p><ol><li>Select another image from your dataset at random</li><li>Pick a weight at random</li><li>Take a weighted average (using the weight from step 2) of the selected image with your image; this will be your independent variable</li><li>Take a weighted average (with the same weight) of this image’s labels with your image’s labels; this will be your dependent variable</li></ol><p>In pseudo-code, we’re doing (where t is the weight for our weighted average):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">image2,target2 = dataset[randint(<span class="number">0</span>,len(dataset)]</span><br><span class="line">t = random_float(<span class="number">0.5</span>,<span class="number">1.0</span>)</span><br><span class="line">new_image = t * image1 + (<span class="number">1</span>-t) * image2</span><br><span class="line">new_target = t * target1 + (<span class="number">1</span>-t) * target2</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="80%" height="80%"></center><p>The third image is built by adding 0.3 times the first one and 0.7 times the second. In this example, should the model predict church? gas station? The right answer is 30% church and 70% gas station since that’s what we’ll get if we take the linear combination of the one-hot encoded targets. For instance, if church has for index 2 and gas station as for index 7, the one-hot-encoded representations are</p><blockquote><p>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0] and [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]<br>[0, 0, 0.3, 0, 0, 0, 0, 0.7, 0, 0]</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = xresnet50()</span><br><span class="line">learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), </span><br><span class="line">                metrics=accuracy, cbs=Mixup)</span><br><span class="line">learn.fit_one_cycle(<span class="number">5</span>, <span class="number">3e-3</span>)</span><br></pre></td></tr></table></figure><h2 id="Label-smoothing"><a href="#Label-smoothing" class="headerlink" title="Label smoothing"></a>Label smoothing</h2><p>In the theoretical expression of the loss, in classification problems, our targets are one-hot encoded (in practice we tend to avoid doing it to save memory, but what we compute is the same loss as if we had used one-hot encoding). That means the model is trained to return 0 for all categories but one, for which it is trained to return 1. Even 0.999 is not good enough, the model will get gradients and learn to predict activations that are even more confident. This encourages overfitting and gives you at inference time a model that is not going to give meaningful probabilities: it will always say 1 for the predicted category even if it’s not too sure, just because it was trained this way. <strong>It can become very harmful if your data is not perfectly labeled.</strong></p><p>This is how label smoothing works in practice: we start with one-hot encoded labels, then replace all zeros by</p><script type="math/tex; mode=display">\frac{\epsilon}{N}</script><p>where $N$ is the number of classes and $\epsilon$ is a parameter (usually 0.1, which would mean we are 10% unsure of our labels). Since you want the labels to add up to 1, replace the 1 by </p><p><script type="math/tex">1-\epsilon + \frac{\epsilon}{N}</script>. </p><p>This way, we don’t encourage the model to predict something overconfident: in our Imagenette example where we have 10 classes, the targets become something like:</p><blockquote><p>[0.01, 0.01, 0.01, 0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Normalization&quot;&gt;&lt;a href=&quot;#Normalization&quot; class=&quot;headerlink&quot; title=&quot;Normalization&quot;&gt;&lt;/a&gt;Normalization&lt;/h2&gt;&lt;figure class=&quot;highlight pyth
      
    
    </summary>
    
    
      <category term="Competition" scheme="https://zhangruochi.com/categories/Competition/"/>
    
      <category term="Tricks" scheme="https://zhangruochi.com/categories/Competition/Tricks/"/>
    
    
  </entry>
  
  <entry>
    <title>Empathize Stage</title>
    <link href="https://zhangruochi.com/Empathize-Stage/2020/04/21/"/>
    <id>https://zhangruochi.com/Empathize-Stage/2020/04/21/</id>
    <published>2020-04-21T15:47:10.000Z</published>
    <updated>2020-04-22T05:30:34.864Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Empathize-Process"><a href="#Empathize-Process" class="headerlink" title="Empathize Process"></a>Empathize Process</h2><ol><li>Get as close to the source of data as possible usually by interviewing the people involved</li><li>Identify the business problem</li><li>Obtain all of the relevant the data</li><li>Translate the business problem into a testable hypothesis or hypotheses</li></ol><h2 id="Identifying-the-business-opportunity-Through-the-eyes"><a href="#Identifying-the-business-opportunity-Through-the-eyes" class="headerlink" title="Identifying the business opportunity: Through the eyes"></a>Identifying the business opportunity: Through the eyes</h2><h3 id="Our-story"><a href="#Our-story" class="headerlink" title="Our story"></a>Our story</h3><p><strong>The first stage of any project in a large enterprise is to identify the business opportunity. In the world of design thinking, this begins with the Empathize stage</strong>. During this time, you and your team are gathering as much information as possible to understand the challenges faced by your AAVAIL.</p><p>You are suprised by the fact that you, a data scientist, are being asked to help out with interviews, observations, process mapping, and various design thinking sessions. These techniques as well as many others are used during the empathize stage to gather <strong>as much information as possible</strong> so that a problem may be defined.</p><p>As a data scientist, this process should be used to guide your investigative process. Ultimately, your top priority is to analyze the data coming out of Singapore, understand the problem and fix the situation. <strong>The involved parties are subscribers, data engineers, data scientists, marketing and management</strong>. You are going to need to talk everyone involved in the data generation process. This is why you’re spending time on interviews and observations.</p><p><strong>Asking questions is a critical part of getting the process started</strong>. You will want to be naturally curious gathering details about the product, the subscriber, and the interaction between the two. This information gathering stage provides both a perspective on the situation and it will help you formulate the business question.</p><p>In the short sections below, we provide guidelines for asking questions and beginning with an investigative mindset.</p><h3 id="Articulate-the-business-question"><a href="#Articulate-the-business-question" class="headerlink" title="Articulate the business question"></a>Articulate the business question</h3><p>There are generally many business questions that can be derived from a given situation. It is an important thought exercise to enumerate the possible questions, that way it makes the discussion easier when you work with the involved stakeholders in order to focus and prioritize. In this situation here are some ways of articulating the business case.</p><ul><li>Can we use marketing to reduce the rate of churn?</li><li>Can we salvage the Singapore market with new products?</li><li>Are there factors outside of our influence that caused the situation in Singapore and is it temporary?</li><li>Can we identify the underlying variables in Singapore that are related to churn and can we use the knowledge to remedy the situations?</li><li>The business problem in all of these examples is written shown in terms of the data we have.</li></ul><p>NOTE: This case study can be approached in many different ways and there may not be a clear right or wrong. During the various modules of this course, we will provide guidance when there are multiple paths to choose from.</p><h3 id="Prioritize"><a href="#Prioritize" class="headerlink" title="Prioritize"></a>Prioritize</h3><p>It is logical, but there is a need to prioritize If there are several distinct business objectives. In this case maybe one is related to reducing chrun directly and another is about profitability.</p><p>There are three major contributing factors when it comes to priority.</p><h4 id="Stakeholder-or-domain-expert-opinion"><a href="#Stakeholder-or-domain-expert-opinion" class="headerlink" title="Stakeholder or domain expert opinion"></a>Stakeholder or domain expert opinion</h4><p>In situations where considerable domain expertise is required to effectively prioritize (e.g. Physics, Medicine and Finance) prioritization will likely be driven by the people closest to the domain.</p><h4 id="Feasibility"><a href="#Feasibility" class="headerlink" title="Feasibility"></a>Feasibility</h4><ul><li>Do we have the necessary data to address the business questions?</li><li>Do we have clean enough data to address the business questions?</li><li>Do we have the technology infrastructure to deploy a solution once the data are modeled?</li></ul><h4 id="Impact"><a href="#Impact" class="headerlink" title="Impact"></a>Impact</h4><p>When looking at Impact we’re purely looking at expected dollar contribution and added value from a monetary perspective. When possible, calculating the back-of-the-envelope ROI is a crucial step that you can do. This is an expectation and not real ROI calculation, but it is a guiding principle nonetheless.</p><p>The ROI calculation should be an expected dollar value that you can generate based on all available information you currently have in your organization combined with any domain insight you can collect.</p><p>Measuring the back-of-the-envelope ROI calculation could make use of any of the following:</p><ul><li>Estimates for fully-loaded salaries of employees involved</li><li>Cost per unit item and/or time required to produce</li><li>Number of customers, clients, or users</li><li>Revenue and more</li></ul><h2 id="Scientific-Thinking-for-Business"><a href="#Scientific-Thinking-for-Business" class="headerlink" title="Scientific Thinking for Business"></a>Scientific Thinking for Business</h2><h3 id="Our-Story"><a href="#Our-Story" class="headerlink" title="Our Story"></a>Our Story</h3><p>Data science involves lots of investigation via trial and error. The investigations are based on evidence and this is one of the strongest reasons why data science is considered a “real” science.</p><p>You will be using a scientific process with your work at AAVAIL. This will help you to organize your work as well as be able to clearly explain everything you are doing to the AAVAIL leadership.</p><p>Let’s take a look now at some guidance and best practices for engaging with a <strong>scientific mindset</strong>.</p><h3 id="Science-is-a-process-and-the-route-to-solving-problems-is-not-always-direct"><a href="#Science-is-a-process-and-the-route-to-solving-problems-is-not-always-direct" class="headerlink" title="Science is a process and the route to solving problems is not always direct"></a>Science is a process and the route to solving problems is not always direct</h3><p>A common argument made by statisticians and mathematicians is that data science is not really a science. This is untrue, mainly because data science involves a lot of <strong>investigations</strong> through sometimes chaotic data sets, in search of meaningful patterns that might help in solving particular problems.</p><p>Since data science implies a scientific approach, it is important that all data scientists learn to adopt and use a scientific thought process. <strong>A scientific thought process of observation, developing hypotheses, testing hypotheses, and modifying hypotheses is critical to your success as a data scientist</strong>.</p><p>Pulling in data and jumping right into exploratory data analysis can make your work prone to exactly the types of negative issues that plague data science today. There are a number of well-discussed issues revolving around data science and data science teams not living up to promised potential.</p><p>At the heart of this problem is the process of communicating results to leadership. It should begin with a meaningful and well-articulated business opportunity. If that opportunity is stated too simply, as say, increasing overall revenue then the central talking point for communication is too vague to be meaningful from the data side.</p><blockquote><p>The business scenario needs to be communicated in a couple of ways:</p><ol><li>Stated in a testable way in terms of data</li><li>Stated in a clear way that minimizes the influence of confounding factors</li></ol></blockquote><h3 id="Testable-hypotheses"><a href="#Testable-hypotheses" class="headerlink" title="Testable hypotheses"></a>Testable hypotheses</h3><p>There is no one single best way to articulate a business opportunity as a testable hypothesis. In some cases the statement will be intuitive, but in other cases there will be some back and forth with stakeholders and domain experts.</p><h3 id="Guidelines-for-creating-testable-hypotheses"><a href="#Guidelines-for-creating-testable-hypotheses" class="headerlink" title="Guidelines for creating testable hypotheses"></a>Guidelines for creating testable hypotheses</h3><ol><li>Become a scientist of the business</li></ol><p>Spend a little bit less time learning new algorithms and Python packages and more time learning the levers that make your specific business go up or down and the variables that impact those levers.</p><ol><li>Make an effort to understand how the data are produced</li></ol><p>If it comes down to it, sources of variation can be explicitly accounted for in many types of models. If the data come from a database you should ask about the process by which the data are stored. If the data are compiled by another person then dig into the details and find out about the compiling process as well as the details of what happened before the data arrived on their desk.</p><ol><li>Make yourself part of the business</li></ol><p>Do not under any circumstances become siloed. Proactively get involved with the business unit as a partner, not a support function.</p><ol><li>Think about how to measure success</li></ol><p>When thinking about what course of action might be most appropriate, keep at the forefront of your mind how you will measure business value when said action is complete.</p><p><strong>IMPORTANT</strong>: Data Science is NOT Business Intelligence. BI analysts serve to derive business insights out of data. There is without a doubt some overlap, but the job of a data scientist is to investigate the business opportunity and solve it.</p><p>There is a balancing act to maintain between directly addressing the business need and ensuring that you have thoughtfully studied the problem enough to ensure that you can account for most of the likely contingencies. The scientific method can be of some guidance here.</p><h3 id="Thinking-scientifically-about-the-business-scenario"><a href="#Thinking-scientifically-about-the-business-scenario" class="headerlink" title="Thinking scientifically about the business scenario"></a>Thinking scientifically about the business scenario</h3><p>A major goal of this process is to make the business objectives clear to leadership. Some of these individuals are technical and some are not, so as a good rule-of-thumb get in the habit of articulating the business problem at a level that everyone can understand. Stakeholders and leadership need to know what you are trying to accomplish before you begin work. They also need to be aware from the start what success would look like. Science is an iterative process and many experiments produce results that some might consider a failure. However, experiments that are properly setup will not fail no matter the result–the result may not useful but you have gained valuable information along the way.</p><p>Experiments in this context could refer to an actual scientific experiment (e.g. A/B testing) or it could be more subtle. Let’s say you work for a company that collects tolls in an automated way, and you want to identify the make and model of each car in order to modify pricing models based on predicted vehicle weight. After talking with the stakeholders and the folks who implemented the image storage solution you are ready to begin. The experiment here has to do with how you begin. You may think that there is enough training data to implement a huge multi-class model and just solve most of the problem. If you approach it that way then you are hypothesizing that the solution will work.</p><p>For those of you who have done much image analysis work, you could guess that approach would likely result in a significant loss of time. If we take a step back and think scientifically, we could approach the solution from an evidence driven perspective. Before investing a significant amount of time you may try to see if you can distinguish one make and model from the rest before adding more classes. You may want to first pipe the images through an image segmentation algorithm to identify the make of the car. There are many possible ways to build towards a comprehensive solution, but it is important to determine if either of these piecemeal approaches would have any immediate business value.</p><p>This might be a good time for a reminder about the steps in the scientific method.</p><h3 id="The-Scientific-Method"><a href="#The-Scientific-Method" class="headerlink" title="The Scientific Method"></a>The Scientific Method</h3><p>It is the process by which science is carried out. The general idea is to build on previous knowledge to in order to improve an understanding of a given topic.</p><ol><li>Formulate the question</li><li>Generate a hypothesis to address the question</li><li>Make a prediction</li><li>Conduct an experiment</li><li>Analyze the data and draw a conclusion</li></ol><p>We will continue with an interactive example, but first it is important to note that <strong>Scientific experiments must be repeatable in order to become reliable evidence.</strong></p><h4 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h4><p>The question can be open-ended and generally it summarizes your business opportunity. Let’s say you work for a small business that manufactures sleds and other winter gear and you are not sure which cities to build your next retail locations. You have heard that Utah, Colorado and Vermont are all states that have high rates of snowfall, but it is unclear which one has the highest rate of snowfall.</p><h4 id="Hypothesis"><a href="#Hypothesis" class="headerlink" title="Hypothesis"></a>Hypothesis</h4><p>Because the Rocky mountains are higher in elevation and they are well-known for fresh powder on the ski slopes you hypothesize that both Utah and Colorado have more snow than Vermont.</p><h4 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h4><p>If you were to run a hypothesis test Vermont would have significantly less snow fall than Colorado or Utah</p><h4 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h4><p>You hit the NOAA weather API to get average annual snowfall by city. We have compiled these data for you in snowfall.csv. You could use a 1-way ANOVA to test the validity of your prediction, but let’s start by looking at the data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># First we read in the data</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">"../data/snowfall.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Next, subset the data to focus only on the states of interest</span></span><br><span class="line"></span><br><span class="line">mask = [<span class="keyword">True</span> <span class="keyword">if</span> s <span class="keyword">in</span> [<span class="string">'CO'</span>,<span class="string">'UT'</span>,<span class="string">'VT'</span>] <span class="keyword">else</span> <span class="keyword">False</span> <span class="keyword">for</span> s <span class="keyword">in</span> df[<span class="string">'state'</span>].values]</span><br><span class="line">df1 = df[mask]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Finally, create a pivot of the data that focuses only on the relevant summary data</span></span><br><span class="line"></span><br><span class="line">pivot = df1.groupby([<span class="string">'state'</span>])[<span class="string">'snowfall'</span>].describe()</span><br><span class="line">df1_pivot = pd.DataFrame(&#123;<span class="string">'count'</span>: pivot[<span class="string">'count'</span>],</span><br><span class="line">                          <span class="string">'avg_snowfall'</span>: pivot[<span class="string">'mean'</span>],</span><br><span class="line">                          <span class="string">'max_snowfall'</span>: pivot[<span class="string">'max'</span>]&#125;)</span><br><span class="line">print(df1_pivot)</span><br><span class="line"></span><br><span class="line"><span class="comment">#        count  avg_snowfall  max_snowfall</span></span><br><span class="line"><span class="comment"># state</span></span><br><span class="line"><span class="comment"># CO       5.0         37.76          59.6</span></span><br><span class="line"><span class="comment"># UT       2.0         51.65          58.2</span></span><br><span class="line"><span class="comment"># VT       1.0         80.90          80.9</span></span><br></pre></td></tr></table></figure><h4 id="Analyze"><a href="#Analyze" class="headerlink" title="Analyze"></a>Analyze</h4><p>There is not enough data to do a 1-way ANOVA. The experiment is not a failure; it has a few pieces of information.</p><p>There is not enough data<br>There is a small possibility that VT gets more snow on average than either CO or UT<br>Our degree of belief in the conclusion drawn from (2) is very small because of (1)<br>The notion of degree of belief is central to scientific thinking. It is somehow a part of our human nature to believe statements that have little to no supporting evidence. <strong>In science the word belief, with respect to a hypothesis is proportional to the evidence</strong>. With more evidence available, ideally, from repeated experiments, one’s degree of belief should change. Evidence is derived from the process described above and if we have none then we are stuck at the question stage and a proper scientific hypothesiscannot be made.</p><p>The other important side to degree of belief is that it never caps out at 100 percent certainty. Some hypotheses have become laws like Newton’s Law of Gravitation, but most natural phenomena in the world outside of physics cannot be explained as a law.</p><p>A hypothesis is the simplest explanation of a phenomenon. A scientific theory is an in-depth explanation of the observed phenomenon. Do not be mistaken with the word theory, there can be sufficient evidence that your degree of belief all but touches 100%, and is plenty for decision making purposes. A built-in safeguard for scientific thought is that our degree of belief does not reach 100%, which leaves some room to find new evidence that could move the dial in the other direction.</p><p>There are additional factors like external peer review that help ensure the integrity of the scientific method and in the case of implementing a model for a specific business task this could mean assigning reviewers for a pull request or simply asking other qualified individuals to check over your work.</p><h2 id="Gathering-Data"><a href="#Gathering-Data" class="headerlink" title="Gathering Data"></a>Gathering Data</h2><h3 id="Our-Story-1"><a href="#Our-Story-1" class="headerlink" title="Our Story"></a>Our Story</h3><p>Your first step at AAVAIL, just like everywhere else, it to look at the data sources. You soon discover that AAVAIL has data everywhere! There is no shortage of data. It looks like they have managed to store every type of transaction with their subscribers.</p><p>You will need a smart way of managing all of this data. Let’s take a look now at some best practices for managing data in a large enterprise.</p><h3 id="Documenting-your-data"><a href="#Documenting-your-data" class="headerlink" title="Documenting your data"></a>Documenting your data</h3><p>Too often data scientists will find themselves deep in the process of developing a solution, based on the data that was provided to them, before they realize that the data itself is flawed, inaccurate or in some other way non-ideal. Developing the habit of creating a simple document with at least a description of the ideal data needed to test the hypotheses around the business problem may seem like an unnecessary step, but it has potential to both:</p><ul><li>Streamline the modeling process</li><li>Help ensure that all future data come in an improved form</li></ul><h3 id="ETL"><a href="#ETL" class="headerlink" title="ETL"></a>ETL</h3><p>The process of gathering data is often referred to as Extract, Transform, Load (ETL). Data is generally gathered (or extracted) from heterogeneous sources, cleaned (transformed) and loaded into a single place that facilitates analysis. Before the advent of the modern data scientist’s toolkit data was often staged in a database,data lake or a data warehouse. Still today data is frequently staged to facilitate collaboration, but there are tools now that enable more possibilities today than ever before. Jupyter Lab has an extension called data grid that allows it to read delimited files with millions or even billions of rows. Then tools like Dask help you scale your analyses. To ensure that projects are completed in a reasonable amount of time the initial pass at ETL should use a simple format like CSV, then a more complex system can be built out once you have accomplished the Minimum Viable Product (MVP).</p><h3 id="Common-methods-of-gathering-data"><a href="#Common-methods-of-gathering-data" class="headerlink" title="Common methods of gathering data"></a>Common methods of gathering data</h3><h4 id="Plain-text-files"><a href="#Plain-text-files" class="headerlink" title="Plain text files"></a>Plain text files</h4><p>Plain text file can come in many forms and generally the open function is used to bring the data into a Python environment. This is a flexible format, but because no structure is imposed, custom scripts are generally needed to parse these files and these scripts do not always generalize to new files.</p><p>The large majority of data science projects involve a modeling step that requires input data in a tabular numeric format. In order to extract data from a plain text file you may need to identify patterns in the text and use regular expressions (regex) to pull out the relevant information. Python’s built-in regex library is known as re.</p><p>On the other hand if the data you are working with consists of natural language, then there are a number of libraries that can work directly with the text files. The two main libraries are:</p><ul><li>spaCy</li><li>NLTK</li></ul><p>Also, scikit-learn has become a standard tool in the overall workflow when processing these files.</p><ul><li>scikit-learn’s text tutorial</li></ul><p>These tools can be applied to unstructured text to generate things like word counts, and word frequencies. We saw an example of this in the Data science workflow combined with design thinking example.</p><h4 id="Delimited-files"><a href="#Delimited-files" class="headerlink" title="Delimited files"></a>Delimited files</h4><p>One of the most commonly encountered ways of storing structured data is in delimited files, where rows of tabular data are stored in lines of a text file and the columns within each row are separated by a special “delimiter” character such as a comma or a tab character.</p><p>This simple structure helps account for the popularity of these formats, with probably the most widely used being Comma-Separated Values (CSV). CSV files are both human and machine readable, and have minimal overhead in terms of the proportion of the file devoted to defining the structure of the data when compared to most other file formats. As such Pandas comes with methods for both reading and writingCSV files. (Note that these functions can also handle other delimiters like tab or the pipe character “|”, but commas are the default.)</p><p>Spreadsheet programs like Microsoft Excel that are used for analyzing tabular data also read from and write to files in CSV format. The native Excel file format (often with file extensions .xls or .xlsx) can also be considered a delimited file type. Though these files also contain a significant amount of extra information related to things like styling that are separate from the actual data. Nonetheless, since these files are commonly used to save datasets, Pandas also has a method for reading them: pandas.read_excel.</p><p>HINT: A best practice when loading data from plain text or delimited files is to separate the code for parsing into its own script. Because the files are read line by line in a separate Python call, it is more memory efficient and this separation of tasks helps with automation and maintenance.</p><p>It is a common mistake to try to read large files into pandas then use the date frame environment to parse. If your parsing (cleaning) task is simple then use a parser. Here is a simple example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/evn/python</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">simple example of a parser</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="comment">## specify the files</span></span><br><span class="line">file_in = os.path.join(<span class="string">".."</span>,<span class="string">"data"</span>,<span class="string">"snowfall.csv"</span>)</span><br><span class="line">file_out = os.path.join(<span class="string">".."</span>,<span class="string">"data"</span>,<span class="string">"snowfall_parsed.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## create an outfile handle (needs to be closed)</span></span><br><span class="line">fidout = open(file_out,<span class="string">"w"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## use the csv module to read/write</span></span><br><span class="line">writer = csv.writer(fidout)</span><br><span class="line"></span><br><span class="line"><span class="comment">## generic parsing function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_line</span><span class="params">(line)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> line[<span class="number">3</span>] <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">"HI"</span>,<span class="string">"NC"</span>,<span class="string">"OR"</span>]:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> line + [<span class="string">'new_data'</span>]</span><br><span class="line">    </span><br><span class="line"><span class="comment">## for each line in the file read in the first file that you need to reference</span></span><br><span class="line"><span class="keyword">with</span> open(file_in) <span class="keyword">as</span> csvfile:</span><br><span class="line">    reader = csv.reader(csvfile, delimiter=<span class="string">','</span>)</span><br><span class="line">    header_in = reader.__next__()</span><br><span class="line">    writer.writerow(header_in + [<span class="string">"new_column"</span>])</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> reader:</span><br><span class="line">        line = parse_line(line)</span><br><span class="line">        <span class="keyword">if</span> line:</span><br><span class="line">            writer.writerow(line)</span><br><span class="line">   </span><br><span class="line">fidout.close()</span><br><span class="line">print(<span class="string">"done parsing"</span>)</span><br></pre></td></tr></table></figure><p>The highlighted lines show where this parser changes the original data by filtering and adding an additional column.</p><h4 id="JSON-files"><a href="#JSON-files" class="headerlink" title="JSON files"></a>JSON files</h4><p>While delimited files are well suited for housing data in flat tables, datasets with more complex structures require different formats. The JavaScript Object Notation (JSON) file format can accommodate quite complex data hierarchies. Python’s built-in library handles reading/writing JSON files.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line">data = json.load(open(<span class="string">'some_file.json'</span>))</span><br></pre></td></tr></table></figure><p>In addition, pandas.read_json is also available for loading JSON files.</p><p>At its base a JSON object can be thought of as analogous to a Python dictionary or list of dictionaries. For example a table of data from a JSON file could be read into Python as a list of dictionaries where each dictionary represented a row of the table, and the keys of each dictionary were the column names. This formatting is somewhat inefficient for simple tabular data, with column information explicitly repeated with each row, but is useful when representing more intricate relationships in the data. JSON objects often have a highly nested structure that you can think of as dictionaries within dictionaries within dictionaries.</p><p>For example, modern websites track a great deal of information about users’ interactions with the site and the varied nature of these interactions make a table structure too rigid for recording them. In practice, most sites send JSON objects back and forth between the user’s computer and the website’s server. Many data scientists’ primary source of data are ultimately these JSON objects.</p><h4 id="Relational-databases"><a href="#Relational-databases" class="headerlink" title="Relational databases"></a>Relational databases</h4><p>Relational databases, i.e. those that impose a schema on datasets are a major source of data for data science projects. Database tables can naturally be converted into Python objects like Pandas DataFrames. Reading data into a Python environment requires opening a connection to a database and there are various libraries for managing this connection, depending on the type of database to be accessed. Some Relational DataBase Management System (RDBMS) and their corresponding interface utilities for Python:</p><div class="table-container"><table><thead><tr><th style="text-align:left">RDBMS</th><th style="text-align:left">Python Connector</th></tr></thead><tbody><tr><td style="text-align:left">MySQL</td><td style="text-align:left">MySQL Connector</td></tr><tr><td style="text-align:left">PostgreSQL</td><td style="text-align:left">Psycopg</td></tr><tr><td style="text-align:left">SQLite</td><td style="text-align:left">sqlite3</td></tr><tr><td style="text-align:left">Microsoft SQL</td><td style="text-align:left">pyodbc</td></tr></tbody></table></div><p>Each of these tools are designed with maintaining the integrity of the database in mind, including methods for rolling back updates, and ways to safeguard against SQL Injection vulnerabilities. As such, the process of querying the database and ingesting the results can seem fairly involved. For example, here is a basic flow for getting the contents of a table from a PostgreSQL database using psycopg2.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> psycopg2 <span class="keyword">as</span> pg2</span><br><span class="line">conn = pg2.connect(database=<span class="string">'my_db'</span>, user=<span class="string">'my_username'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a cursor to traverse the database</span></span><br><span class="line">cur = conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># cursor object executes a query, but does not automatically return results</span></span><br><span class="line">cur.execute(<span class="string">"SELECT * FROM my_table"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Return all query results</span></span><br><span class="line">results = cur.fetchall()</span><br><span class="line"><span class="comment"># WARNING: If the result set is large, it may overwhelm the memory</span></span><br><span class="line"><span class="comment"># resources on your machine.</span></span><br><span class="line"></span><br><span class="line">cur.close()</span><br><span class="line">conn.close()</span><br></pre></td></tr></table></figure><p>While the steps required to connect, query, and disconnect from a relational database are more involved than when loading in data from a file on your local machine, the table structure from the database schema basically guarantees that the data will fit cleanly into a Pandas DataFrame or NumPy Array.</p><h4 id="NoSQL-databases"><a href="#NoSQL-databases" class="headerlink" title="NoSQL databases"></a>NoSQL databases</h4><p>“NoSQL” is a catch-all term referring to “non SQL” or “non relational”, or more recently “not only SQL”. Usually meaning that the method for housing data does not impose a schema on it (or at least not as tightly constrained as in relational databases). This tradeoff gives greater flexibility in what and how data are stored at the cost of increased traversal times when searching the database. This tradeoff is similar to the one we encountered when working with delimited files like CSVs and with JSON files. When loading or dumping data between a file and a database, CSVs are a good match for tables in a relational database, whereas JSONs are more aligned with NoSQL databases.</p><p>The are many examples of NoSQL databases, each with different use cases, and most of which can be accessed with Python.</p><p>One flexible and popular example is MongoDB. MongoDB is a document-oriented database, where a “document” encapsulates and encodes data in a standard format. In the case of MongoDB, that format is JSON-like. Like the relational databases mentioned above, MongoDB has a client for querying it directly, as well as a connector for querying from within Python. These queries are constructed usingMongoDB’s query syntax.</p><p>The Python connector to MongoDB is PyMongo.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pymongo <span class="keyword">import</span> MongoClient</span><br><span class="line"><span class="comment"># By default a Mongo db running locally is accessible via port 27017</span></span><br><span class="line">client = MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line">db = client[<span class="string">'database_name'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Within a db, documents are grouped into "collections" -- roughly equivalent</span></span><br><span class="line"><span class="comment"># to tables in a relational db.</span></span><br><span class="line">coll = db[<span class="string">'collection_name'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Return all the documents within the collection</span></span><br><span class="line">docs = coll.find()</span><br></pre></td></tr></table></figure><h4 id="Web-scraping-and-APIs"><a href="#Web-scraping-and-APIs" class="headerlink" title="Web scraping and APIs"></a>Web scraping and APIs</h4><p>Automating the process of downloading content from websites is known as web scraping.</p><p><strong> IMPORTANT </strong></p><blockquote><p>Web scraping can be done in legitimate ways, but just as easily web scraping tools do not stop you from violating a websites terms of service. If a website encourages the sharing of their data then they will create a specific API endpoint that you will use. More often than not the API will require to have an identifying key.</p></blockquote><p>Various tools in Python are available for accessing and parsing webpage data.Requests is a user-friendly library for downloading web pages. It can also be used to retrieve files that are exposed through a URL. For a webpage the data returned from a call using Requests is the HyperText Markup Language (HTML) code that instructs a client such as a web browser how to display a page. This HTML code will often (but not always) contain the data you want to collect from the particular webpage.</p><p>Modern webpages tend to have a great deal of information in their HTML beyond what is shown to the user, so parsing through it all to collect the relevant data can be a daunting task. Fortunately, if a page is readable in your browser, then its HTML must have a coherent structure. Beautiful Soup is a Python library that provides tools for walking through that structure in a systematic and repeatable way. Thus, in the context of web scraping Beautiful Soup can be used to extract the relevant data from the soup of all the other information contained in an HTML file.</p><p>Many websites’ contents are dynamically rendered in such a way that the information displayed on a page never makes it directly into the page’s HTML. In such cases it may not be possible to download the data of interest with a tool like Requests. One option in this scenario is to move to a tool for browser automation, such as Selenium. Selenium’s Python interface is described here.</p><p>Another tool, specifically designed for web scraping in Python, is Scrapy.</p><p>Depending on your website of interest you may have to try several of these tools to successfully collect the relevant data in a scalable way. But a general rule of thumb is that if you can see what you want to collect in your browser, the the website sent it to you, so it should be retrievable.</p><h4 id="Streaming-data"><a href="#Streaming-data" class="headerlink" title="Streaming data"></a>Streaming data</h4><p>In the modern landscape of business data streams are becoming more common. A data stream is a sequence of digitally encoded signals. Data can be streamed for many purposes including storage and further processing (like modeling). Data streams become important when the data of a project or company becomes mature and the AI pipeline is connected to it. As we move into the portions of the AI enterprise workflow that focus on models in production we will be using Apache Spark’s streaming to connect deployed models with streaming data. Data collected from sensors or devices connected via the internet of things are oftent setup to produce streaming data. We will work specifically with these types of data in module 5.</p><h4 id="Apache-Hadoop-File-Share-HDFS"><a href="#Apache-Hadoop-File-Share-HDFS" class="headerlink" title="Apache Hadoop File Share (HDFS)"></a>Apache Hadoop File Share (HDFS)</h4><p>Apache Hadoop File Share (HDFS) is the core of Apache Hadoop , an open source system that is designed to use arrays of commodity hardware to store and manage very large datasets.</p><p>HDFS is the storage component of the system. Large datasets are divided into blocks, and those blocks are distributed and stored across the nodes in an HDFS cluster. Any code that is created to analyze the datasets stored in a Hadoop cluster is executed locally for each block of data, and in parallel. This parallel analysis of data blocks means that Hadoop can process very large data sets rapidly.</p><p>The Hadoop framework itself is written mostly in Java. However, any language, including Python, may be used to analyze the data stored in a Hadoop cluster. The Apache Foundation provides a number of other packages that may be installed alongside Hadoop to add additional relational database functionality and improve scalability.</p><p>IMPORTANT: Apache Hadoop is a de facto standard in many large enterprises today. It is often used with Apache Spark and a NoSQL database engine to provide data storage and management of data pipelines used by machine learning models.</p><h4 id="Other-sources-of-data-formats"><a href="#Other-sources-of-data-formats" class="headerlink" title="Other sources of data formats"></a>Other sources of data formats</h4><div class="table-container"><table><thead><tr><th style="text-align:left">Format</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left">HDF5</td><td style="text-align:left">There is a is a hierarchical format HDF5 used to store complex scientific data. The format is useful for storing and sharing large amounts of data.</td></tr><tr><td style="text-align:left">NumPy’s *.npy and *.nzp formats</td><td style="text-align:left">NumPy has its own binary format (NPY) and the NPZ format is an extension of it that allows multiple arrays and compression.</td></tr></tbody></table></div><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>In this module you should have learned:</p><ul><li>Stakeholder or domain expert opinion, feasibility and impact are three of the most important factors when prioritizing business opportunities</li><li>The practice of articulating a business opportunity, with the data in mind, as a testable hypothesis helps keep the overall project linked to the business needs</li><li>The notion of degree of belief is important when making statements both in science and in business. No statement has 100% degree of belief, it is some percentage of 100% that is a reflection of accumulated evidence</li><li>The scientific method helps formalize a process for rationalizing business decisions through experimentation and evidence</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Empathize-Process&quot;&gt;&lt;a href=&quot;#Empathize-Process&quot; class=&quot;headerlink&quot; title=&quot;Empathize Process&quot;&gt;&lt;/a&gt;Empathize Process&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Get 
      
    
    </summary>
    
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Process Model" scheme="https://zhangruochi.com/categories/AI-Workflow/Process-Model/"/>
    
    
  </entry>
  
  <entry>
    <title>Decision Thinking and Data Science Process</title>
    <link href="https://zhangruochi.com/Decision-Thinking-and-Data-Science-Process/2020/04/21/"/>
    <id>https://zhangruochi.com/Decision-Thinking-and-Data-Science-Process/2020/04/21/</id>
    <published>2020-04-21T09:18:18.000Z</published>
    <updated>2020-04-21T21:40:13.008Z</updated>
    
    <content type="html"><![CDATA[<p>Between design thinking and the above mentioned process models there are several 1:1 relationships between stages. The other relationships are generally straightforward to delineate. The design thinking process is consists of five stages and it has the distinct advantage of being applied outside of data science.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="1.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Design Thinking</div></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="2.png" width="70%" height="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Process Models</div></center><p>It is the details that keep you flowing from one stage to the next, while iterating in ways that are business driven that makes the contents of this course new. Let’s use a simple example to illustrate the basic process.</p><blockquote><p>A friend of yours just opened a new Sherlock Holmes themed café. Her café is state-of-the-art complete with monitors built into the tables. The business is off to a good start, but she has gotten some feedback that the games could use improvement. She knows that good games keep the customers around a little longer. The games are a way to keep customers entertained while they drink coffee and buy food items. She has some games already, but wants your help to create a few more games to keep customers both informed and entertained.</p></blockquote><p>Being a data scientist you would not just sit down and create a game—you are, of course, going to create based on your initial investigation of the business scenario.</p><h3 id="Empathize"><a href="#Empathize" class="headerlink" title="Empathize"></a>Empathize</h3><p><strong>In this stage time is dedicated to understanding the business opportunities.</strong></p><p>In this setting the frequency and duration of customer visits are going to be related to overall sales. The initial business opportunity here is How do you ensure new games drive revenue?. There are many other business opportunities, like what is the optimal menu for the customer-base and do seasonal variations of offerings help the business?, but lets focus on the initial one for this example. As part of this stage you would talk with your friend, her employees and some customers to do your best to fully understand the experience of the customer. The important thing here is to spend time on-site simulating the experience of a customer to obtain as genuine an understanding of the problem as possible. You may realize that most customers are there to work or most of them are just passing through. This domain knowledge is useful when making decisions like which new types of new games to create. After you have gathered your information and studied it you will generally articulate the business scenario using a scientific thought process—this means a statement that can be tested. The business opportunity should be stated in a way that minimizes the presence of confounding factors.</p><p>There are logical follow-up questions to ask to fully understand the problem, but the next two stages are the more appropriate places to get into these details. Now that you understand the problem it is time to gather the data.</p><p><strong>HINT</strong>: This is the stage where we gather all of the data and we make note of what would be ideal data.</p><ol><li><p>The data here are mostly sales and customer profiles. There are two important aspects of the data that would be ideal:</p></li><li><p>The data are at a transaction level (each purchase and its associated data are recorded)<br>We can associate game usage with transactions.</p></li></ol><p>Fortunately for us this is a modern cafe so customers order and play games through the same interface. Additionally, they are incentivized to login to the system and generate a customer profile. In this stage we go through the process of gathering the raw data. This may involve querying a database, gathering files, web-scraping and other mechanisms. It is important to gather <strong>all of the relevant data</strong> in this stage, because access and quality of the data may force you to modify the business question. It is very difficult to assess the quality of data when it is not in hand. If possible effort should be made to collect even marginally related data.</p><p>Lets assume that your initial investigation led you to understand that games that used quotations from the books in an interactive way were the most effective. So you have come up with the idea to develop a game that is built on a chatbot that has been trained to talk like Sherlock. This would involve Natural Language Processing (NLP) and we would need a corpus. As a start you might download The Adventures of Sherlock Holmes, by Arthur Conan Doyle from Project Gutenberg.</p><p><strong>HINT</strong>: This is a live coding example and we suggest that you open a Jupyter notebook either locally or within Watson Studio so that you may annotate and expand on the example freely.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">text = requests.get(<span class="string">'https://www.gutenberg.org/files/1661/1661-0.txt'</span>).text</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"sherlock-holmes.txt"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> text_file:</span><br><span class="line">    text_file.write(text)</span><br></pre></td></tr></table></figure><h3 id="Define"><a href="#Define" class="headerlink" title="Define"></a>Define</h3><p><strong>This is the data wrangling stage</strong></p><p>Given the data, an understanding of the business scenario and your gathered domain knowledge you will next perform your data cleaning and preliminary exploratory data analysis. To get to the point of preliminary investigation into the findings from the empathize stage it is frequently the case that we need to clean our data.</p><p>This could involve parsing JSON, manipulating SQL queries, reading CSV, cleaning a corpus of text, sifting through images, and so much more. One common goal of this part of the process is the creation of one or more Pandas dataframes or NumPy arrays that will be used for initial exploratory data analysis (EDA).</p><blockquote><p>Exploratory data analysis (EDA) is the process of analyzing data sets to create summaries and visualizations of the data. These summaries and visualizations are then used to guide the use of the data for solving business challenges.</p></blockquote><p><strong>HINT</strong>: This is the stage where we perform the initial EDA</p><p>Sometimes we need to perform a little EDA in order to determine how to best clean the data so these two steps are not necessarily distinct from each other. Visualization, basic hypothesis testing and simple feature engineering are among the most important tasks for EDA at this stage. An minimal example of a EDA plot is one where we look at the average number of words per sentence for the name mentions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## read in the book        </span></span><br><span class="line">text = open(<span class="string">'sherlock-holmes.txt'</span>, <span class="string">'r'</span>).read()</span><br><span class="line"></span><br><span class="line"><span class="comment">## do some basic parsing and cleaning of sentences</span></span><br><span class="line">stop_pattern = <span class="string">'\.|\?|\!'</span></span><br><span class="line">sentences = re.split(stop_pattern, text)</span><br><span class="line">sentences = [re.sub(<span class="string">"\r|\n"</span>,<span class="string">" "</span>,s.lower()) <span class="keyword">for</span> s <span class="keyword">in</span> sentences][<span class="number">3</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment">## extract a few features and create a pandas df</span></span><br><span class="line">has_sherlock =  [<span class="keyword">True</span> <span class="keyword">if</span> re.search(<span class="string">"sherlock|holmes"</span>,s) <span class="keyword">else</span> <span class="keyword">False</span> <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">has_watson = [<span class="keyword">True</span> <span class="keyword">if</span> re.search(<span class="string">"john|watson"</span>,s) <span class="keyword">else</span> <span class="keyword">False</span> <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">'text'</span>:sentences,<span class="string">'has_sherlock'</span>:has_sherlock,<span class="string">'has_watson'</span>:has_watson&#125;)</span><br><span class="line">df[<span class="string">'num_words'</span>] = df[<span class="string">'text'</span>].apply(<span class="keyword">lambda</span> x: len(x.split(<span class="string">" "</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">## make eda plot</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line"></span><br><span class="line">data1 = df[df[<span class="string">'has_sherlock'</span>]==<span class="keyword">True</span>]</span><br><span class="line">data2 = df[df[<span class="string">'has_watson'</span>]==<span class="keyword">True</span>]</span><br><span class="line"></span><br><span class="line">data = [df[df[col]==<span class="keyword">True</span>][<span class="string">'num_words'</span>].values <span class="keyword">for</span> col <span class="keyword">in</span> [<span class="string">'has_sherlock'</span>,<span class="string">'has_watson'</span>]]</span><br><span class="line"></span><br><span class="line">pos = [<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">ax1.violinplot(data, pos, points=<span class="number">40</span>, widths=<span class="number">0.5</span>,showextrema=<span class="keyword">True</span>, showmedians=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">labels = [<span class="string">'Sherlock'</span>, <span class="string">'Watson'</span>]</span><br><span class="line">ax1.set_xticks(np.arange(<span class="number">1</span>, len(labels) + <span class="number">1</span>))</span><br><span class="line">ax1.set_xticklabels(labels)</span><br><span class="line">ax1.set_xlim(<span class="number">0.25</span>, len(labels) + <span class="number">0.75</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">'Feature'</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">'# Words'</span>)</span><br><span class="line">ax1.set_title(<span class="string">"Words per sentence"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="Ideate"><a href="#Ideate" class="headerlink" title="Ideate"></a>Ideate</h3><p><strong>This is the stage where we modify our data and our features</strong></p><p>Now that you have clean data the data processing must continue until you are ready to input your data into a model. This stage contains all of the possible data manipulations you might perform before modeling. Perhaps the data need to be log transformed, standardized, reduced in dimensionality, kernel transformed, engineered to contain more features or transformed in some other way.</p><p>For our text data we would likely want to dig into the sentences themselves to make sure they fit the desired use case. If we were building a chatbot to engage with in a very Holmes manner then we would likely want to remove any sentences that were not said by Mr. Holmes, but his name was mentioned. If we were building a predictive model to determine which story a phrase would most likely have been generated, we would need to create a new column in our data frame representing the books themselves.</p><p>When working with text data many models that we might consider prefer a numeric representation of the data. This may be occurrences, frequencies, or another transformation of the original data. It is in this stage that these types of transformations are readied or carried out. For example here we import the necessary transformers for usage in the next stage.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># extract the data to be used in the model from the df</span></span><br><span class="line">labels = np.zeros(df.shape[<span class="number">0</span>])</span><br><span class="line">labels[(df[<span class="string">'has_sherlock'</span>] == <span class="keyword">True</span>)] = <span class="number">1</span></span><br><span class="line">labels[(df[<span class="string">'has_watson'</span>] == <span class="keyword">True</span>)] = <span class="number">2</span></span><br><span class="line">df[<span class="string">'labels'</span>] = labels</span><br><span class="line">df = df[df[<span class="string">'labels'</span>]!=<span class="number">0</span>]</span><br><span class="line">X = df[<span class="string">'text'</span>].values</span><br><span class="line">y = df[<span class="string">'labels'</span>].values</span><br></pre></td></tr></table></figure><p>There are a lot of ways to prepare data for different models. In some case you will not know the best transformation or series of transformations until you have run the different models and made a comparison. The concept of pipelines is extremely useful for iterating over different permutations of transformers and models. The following topics will be covered in detail during Module 3.</p><ul><li>Unsupervised learning</li><li>Feature engineering</li><li>Dimension Reduction</li><li>Simulation</li><li>Missing value imputation</li><li>Outlier detection</li></ul><p><strong>HINT</strong>: This is the stage where we enumerate the advantages and disadvantages of the possible modeling solutions</p><p>Once the transformations are carried or staged as part of some pipeline it is a valuable exercise to document what you know about the process so far. The form that this most commonly takes is a table of possible modeling strategies complete with the advantages and disadvantages of each.</p><h3 id="Prototype"><a href="#Prototype" class="headerlink" title="Prototype"></a>Prototype</h3><p><strong>This is the modeling stage</strong></p><p>The data have been cleaned, processed and staged (ideally in a pipeline) for modeling. The modeling (classic statistics and machine learning) is the bread and butter of data science. This is the stage where most data scientists want to spend the majority of their time. It is where you will interface with the most intriguing aspects of this discipline.</p><p>To illustrate the process to the end shown below is a Support Vector Machine with Stochastic gradient decent as a model. The process involves the use of a train-test split and a pipeline because we want you to be exposed from the very beginning of this course with best practices. Given this example we also see that there can be considerable overlap between the ideate and prototype stages. The overlap exists because transformations of data are generally specific to models–as you will explore which model fits the situation best you will be modifying the transformations of your data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment">## carry out the train test split</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line">text_clf = Pipeline([</span><br><span class="line">    (<span class="string">'vect'</span>, CountVectorizer()),</span><br><span class="line">    (<span class="string">'tfidf'</span>, TfidfTransformer()),</span><br><span class="line">    (<span class="string">'clf'</span>, SGDClassifier(loss=<span class="string">'hinge'</span>, penalty=<span class="string">'l2'</span>,</span><br><span class="line">                        alpha=<span class="number">1e-3</span>, random_state=<span class="number">42</span>,</span><br><span class="line">                        max_iter=<span class="number">5</span>, tol=<span class="keyword">None</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment">## train a model</span></span><br><span class="line">text_clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><h3 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h3><p><strong>This is the production, testing and feedback loop stage</strong></p><p>The model works and there are evaluation metrics to provide insight into how well it works. However, the process does not end here. Perhaps the model runs, but it is not yet in production or maybe you want to try different models and/or transformers. Once in production you might want to run some tests to determine if it will handle load or if it will scale well as the data grows. A working model with an impressive f-score does not mean it will be effective in practice. This stage is dedicated to all of the considerations that come after the initial modeling is carried out.</p><p>It is also the stage where you will determine how best to iterate. Design thinking like data science is an iterative process. Our model performed very well (see below), possibly because Dr. Holmes and Dr. Watson are described in very different ways in the stories, but it could be something else.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line"><span class="comment">## evaluate the model performance</span></span><br><span class="line">predicted = text_clf.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(metrics.classification_report(y_test, predicted,</span><br><span class="line">      target_names=[<span class="string">'sherlock'</span>,<span class="string">'watson'</span>]))</span><br></pre></td></tr></table></figure><p>As a scientist you always want to remain skeptical about your findings until you have multiple ways to corroborate them. You will also want to always be aware of the overall goal of why you are doing the work you are doing. This example is an interesting metaphor for what can happen as a data scientist. It is possible to go down a path that may only marginally be related to the central business question. Developing a game here is not unlike using a new model for deep-learning or incorporating a new technology into your workflow—it may be fun and it may to some degree help the business case, but you need to always ask yourself is this the best way for me or my team to address the business problem? The questions your ask here are going to guide how best to iterate on the entire workflow.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Between design thinking and the above mentioned process models there are several 1:1 relationships between stages. The other relationship
      
    
    </summary>
    
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Process Model" scheme="https://zhangruochi.com/categories/AI-Workflow/Process-Model/"/>
    
    
  </entry>
  
  <entry>
    <title>Develop a blockchain application from scratch in Python</title>
    <link href="https://zhangruochi.com/Develop-a-blockchain-application-from-scratch-in-Python/2020/04/21/"/>
    <id>https://zhangruochi.com/Develop-a-blockchain-application-from-scratch-in-Python/2020/04/21/</id>
    <published>2020-04-20T16:37:03.000Z</published>
    <updated>2020-04-21T06:02:15.358Z</updated>
    
    <content type="html"><![CDATA[<p>Blockchain is a way of storing digital data. The data can literally be anything. For Bitcoin, it’s the transactions (logs of transfers of Bitcoin from one account to another), but it can even be files; it doesn’t matter. The data is stored in the form of blocks, which are linked (or chained) together using cryptographic hashes — hence the name “blockchain.”</p><p>All of the magic lies in the way this data is stored and added to the blockchain. A blockchain is essentially a linked list that contains ordered data, with a few constraints such as:</p><ul><li>Blocks can’t be modified once added; in other words, it is append only.</li><li>There are specific rules for appending data to it.</li><li>Its architecture is distributed.</li></ul><p>Enforcing these constraints yields the following benefits:</p><ul><li>Immutability and durability of data</li><li>No single point of control or failure</li><li>A verifiable audit trail of the order in which data was added</li></ul><h3 id="Store-transactions-into-blocks"><a href="#Store-transactions-into-blocks" class="headerlink" title="Store transactions into blocks"></a>Store transactions into blocks</h3><p>We’ll be storing data in our blockchain in a format that’s widely used: JSON. Here’s what a post stored in blockchain will look like:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">  <span class="string">"author"</span>: <span class="string">"some_author_name"</span>, </span><br><span class="line">  <span class="string">"content"</span>: <span class="string">"Some thoughts that author wants to share"</span>, </span><br><span class="line">  <span class="string">"timestamp"</span>: <span class="string">"The time at which the content was created"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The generic term “data” is often replaced on the internet by the term “transactions.” So, just to avoid confusion and maintain consistency, we’ll be using the term “transaction” to refer to data in our example application.</p><p>The transactions are packed into blocks. A block can contain one or many transactions. The blocks containing the transactions are generated frequently and added to the blockchain. Because there can be multiple blocks, each block should have a unique ID.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, index, transactions, timestamp)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Constructor for the `Block` class.</span></span><br><span class="line"><span class="string">        :param index: Unique ID of the block.</span></span><br><span class="line"><span class="string">        :param transactions: List of transactions.</span></span><br><span class="line"><span class="string">        :param timestamp: Time of generation of the block.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.index = index </span><br><span class="line">        self.transactions = transactions </span><br><span class="line">        self.timestamp = timestamp</span><br></pre></td></tr></table></figure><h3 id="Add-digital-fingerprints-to-the-blocks"><a href="#Add-digital-fingerprints-to-the-blocks" class="headerlink" title="Add digital fingerprints to the blocks"></a>Add digital fingerprints to the blocks</h3><p>We’d like to prevent any kind of tampering in the data stored inside the block, and detection is the first step to that. To detect if the data in the block has been tampered with, you can use cryptographic hash functions.</p><p>A hash function is a function that takes data of any size and produces data of a fixed size from it (a hash), which is generally used to identify the input. The characteristics of an ideal hash function are:</p><ul><li>It should be easy to compute.</li><li>It should be deterministic, meaning the same data will always result in the same hash.</li><li>It should be uniformly random, meaning even a single bit change in the data should change the hash significantly.</li></ul><p>The consequence of this is:</p><ul><li>It is virtually impossible to guess the input data given the hash. (The only way is to try all possible input combinations.)</li><li>If you know both the input and the hash, you can simply pass the input through the hash function to verify the provided hash.</li></ul><p>This asymmetry of efforts that’s required to figure out the hash from an input (easy) vs. figuring out the input from a hash (almost impossible) is what blockchain leverages to obtain the desired characteristics.</p><p>We’ll store the hash of the block in a field inside our Block object, and it will act like a digital fingerprint (or signature) of data contained in it:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> sha256</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_hash</span><span class="params">(block)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns the hash of the block instance by first converting it</span></span><br><span class="line"><span class="string">    into JSON string.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    block_string = json.dumps(self.__dict__, sort_keys=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sha256(block_string.encode()).hexdigest()</span><br></pre></td></tr></table></figure><p><strong>Note</strong>: In most cryptocurrencies, even the individual transactions in the block are hashed and then stored to form a hash tree (also known as a merkle tree). The root of the tree usually represents the hash of the block. It’s not a necessary requirement for the functioning of the blockchain, so we’re omitting it to keep things simple.</p><h3 id="Chain-the-blocks"><a href="#Chain-the-blocks" class="headerlink" title="Chain the blocks"></a>Chain the blocks</h3><p>Okay, we’ve now set up the blocks. The blockchain is supposed to be a collection of blocks. We can store all the blocks in the Python list (the equivalent of an array). But this is not sufficient, because what if someone intentionally replaces an old block with a new block in the collection? Creating a new block with altered transactions, computing the hash, and replacing it with any older block is no big deal in our current implementation.</p><p>We need a way to make sure that any change in the previous blocks invalidates the entire chain. The Bitcoin way to do this is to create dependency among consecutive blocks by chaining them with the hash of the block immediately previous to them. By chaining here, we mean to include the hash of the previous block in the current block in a new field called previous_hash.</p><p>Okay, if every block is linked to the previous block through the previous_hash field, what about the very first block? That block is called the genesis block and it can be generated either manually or through some unique logic. Let’s add the previous_hash field to the Block class and implement the initial structure of our Blockchain class.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> sha256</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>:</span></span><br><span class="line">    def__init__(self, index, transactions, timestamp, previous_hash):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Constructor for the `Block` class.</span></span><br><span class="line"><span class="string">        :param index:         Unique ID of the block.</span></span><br><span class="line"><span class="string">        :param transactions:  List of transactions.</span></span><br><span class="line"><span class="string">        :param timestamp:     Time of generation of the block.</span></span><br><span class="line"><span class="string">        :param previous_hash: Hash of the previous block in the chain which this block is part of.                                        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.index = index</span><br><span class="line">        self.transactions = transactions</span><br><span class="line">        self.timestamp = timestamp</span><br><span class="line">        self.previous_hash = previous_hash <span class="comment"># Adding the previous hash field</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_hash</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Returns the hash of the block instance by first converting it</span></span><br><span class="line"><span class="string">        into JSON string.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        block_string = json.dumps(self.__dict__, sort_keys=<span class="keyword">True</span>) <span class="comment"># The string equivalent also considers the previous_hash field now</span></span><br><span class="line">        <span class="keyword">return</span> sha256(block_string.encode()).hexdigest()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Constructor for the `Blockchain` class.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.chain = []</span><br><span class="line">        self.create_genesis_block()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_genesis_block</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A function to generate genesis block and appends it to</span></span><br><span class="line"><span class="string">        the chain. The block has index 0, previous_hash as 0, and</span></span><br><span class="line"><span class="string">        a valid hash.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        genesis_block = Block(<span class="number">0</span>, [], time.time(), <span class="string">"0"</span>)</span><br><span class="line">        genesis_block.hash = genesis_block.compute_hash()</span><br><span class="line">        self.chain.append(genesis_block)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">last_block</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A quick pythonic way to retrieve the most recent block in the chain. Note that</span></span><br><span class="line"><span class="string">        the chain will always consist of at least one block (i.e., genesis block)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.chain[<span class="number">-1</span>]</span><br></pre></td></tr></table></figure><p>Now, if the content of any of the previous blocks changes:</p><ul><li>The hash of that previous block would change.</li><li>This will lead to a mismatch with the previous_hash field in the next block.</li><li>Since the input data to compute the hash of any block also consists of the previous_hash field, the hash of the next block will also change.</li></ul><p>Ultimately, the entire chain following the replaced block is invalidated, and the only way to fix it is to recompute the entire chain.</p><h3 id="Implement-a-proof-of-work-algorithm"><a href="#Implement-a-proof-of-work-algorithm" class="headerlink" title="Implement a proof of work algorithm"></a>Implement a proof of work algorithm</h3><p>There is one problem, though. If we change the previous block, the hashes of all the blocks that follow can be re-computed quite easily to create a different valid blockchain. To prevent this, we can exploit the asymmetry in efforts of hash functions that we discussed earlier to make the task of calculating the hash difficult and random. Here’s how we do this: Instead of accepting any hash for the block, we add some constraint to it. Let’s add a constraint that our hash should start with “n leading zeroes” where n can be any positive integer.</p><p>We know that unless we change the data of the block, the hash is not going to change, and of course we don’t want to change existing data. So what do we do? Simple! We’ll add some dummy data that we can change. Let’s introduce a new field in our block called nonce. A nonce is a number that we can keep on changing until we get a hash that satisfies our constraint. The nonce satisfying the constraint serves as proof that some computation has been performed. This technique is a simplified version of the Hashcash algorithm used in Bitcoin. The number of zeroes specified in the constraint determines the difficulty of our proof of work algorithm (the greater the number of zeroes, the harder it is to figure out the nonce).</p><p>Also, due to the asymmetry, proof of work is difficult to compute but very easy to verify once you figure out the nonce (you just have to run the hash function again):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span>:</span></span><br><span class="line">    <span class="comment"># difficulty of PoW algorithm</span></span><br><span class="line">    difficulty = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Previous code contd..</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">proof_of_work</span><span class="params">(self, block)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Function that tries different values of the nonce to get a hash</span></span><br><span class="line"><span class="string">        that satisfies our difficulty criteria.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        block.nonce = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        computed_hash = block.compute_hash()</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> computed_hash.startswith(<span class="string">'0'</span> * Blockchain.difficulty):</span><br><span class="line">            block.nonce += <span class="number">1</span></span><br><span class="line">            computed_hash = block.compute_hash()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> computed_hash</span><br></pre></td></tr></table></figure><p>Notice that there is no specific logic to figuring out the nonce quickly; it’s just brute force. The only definite improvement that you can make is to use hardware chips that are specially designed to compute the hash function in a smaller number of CPU instructions.</p><h3 id="Add-blocks-to-the-chain"><a href="#Add-blocks-to-the-chain" class="headerlink" title="Add blocks to the chain"></a>Add blocks to the chain</h3><p>To add a block to the chain, we’ll first have to verify that:</p><ul><li>The data has not been tampered with (the proof of work provided is correct).</li><li>The order of transactions is preserved (the previous_hash field of the block to be added points to the hash of the latest block in our chain).</li></ul><p>Let’s see the code for adding blocks into the chain:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Previous code contd..</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_block</span><span class="params">(self, block, proof)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A function that adds the block to the chain after verification.</span></span><br><span class="line"><span class="string">        Verification includes:</span></span><br><span class="line"><span class="string">        * Checking if the proof is valid.</span></span><br><span class="line"><span class="string">        * The previous_hash referred in the block and the hash of a latest block</span></span><br><span class="line"><span class="string">          in the chain match.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        previous_hash = self.last_block.hash</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> previous_hash != block.previous_hash:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> Blockchain.is_valid_proof(block, proof):</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        block.hash = proof</span><br><span class="line">        self.chain.append(block)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_valid_proof</span><span class="params">(self, block, block_hash)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Check if block_hash is valid hash of block and satisfies</span></span><br><span class="line"><span class="string">        the difficulty criteria.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> (block_hash.startswith(<span class="string">'0'</span> * Blockchain.difficulty) <span class="keyword">and</span></span><br><span class="line">                block_hash == block.compute_hash())</span><br></pre></td></tr></table></figure><h3 id="Mining"><a href="#Mining" class="headerlink" title="Mining"></a>Mining</h3><p>The transactions will be initially stored as a pool of unconfirmed transactions. The process of putting the unconfirmed transactions in a block and computing proof of work is known as the mining of blocks. Once the nonce satisfying our constraints is figured out, we can say that a block has been mined and it can be put into the blockchain.</p><p>In most of the cryptocurrencies (including Bitcoin), miners may be awarded some cryptocurrency as a reward for spending their computing power to compute a proof of work. Here’s what our mining function looks like:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.unconfirmed_transactions = [] <span class="comment"># data yet to get into blockchain</span></span><br><span class="line">        self.chain = []</span><br><span class="line">        self.create_genesis_block()</span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Previous code contd...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_new_transaction</span><span class="params">(self, transaction)</span>:</span></span><br><span class="line">        self.unconfirmed_transactions.append(transaction)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mine</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        This function serves as an interface to add the pending</span></span><br><span class="line"><span class="string">        transactions to the blockchain by adding them to the block</span></span><br><span class="line"><span class="string">        and figuring out proof of work.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.unconfirmed_transactions:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        last_block = self.last_block</span><br><span class="line"></span><br><span class="line">        new_block = Block(index=last_block.index + <span class="number">1</span>,</span><br><span class="line">                          transactions=self.unconfirmed_transactions,</span><br><span class="line">                          timestamp=time.time(),</span><br><span class="line">                          previous_hash=last_block.hash)</span><br><span class="line"></span><br><span class="line">        proof = self.proof_of_work(new_block)</span><br><span class="line">        self.add_block(new_block, proof)</span><br><span class="line">        self.unconfirmed_transactions = []</span><br><span class="line">        <span class="keyword">return</span> new_block.index</span><br></pre></td></tr></table></figure><h3 id="Combined-Code"><a href="#Combined-Code" class="headerlink" title="Combined Code"></a>Combined Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> sha256</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, index, transactions, timestamp, previous_hash)</span>:</span></span><br><span class="line">        self.index = index</span><br><span class="line">        self.transactions = transactions</span><br><span class="line">        self.timestamp = timestamp</span><br><span class="line">        self.previous_hash = previous_hash</span><br><span class="line">        self.nonce = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_hash</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A function that return the hash of the block contents.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        block_string = json.dumps(self.__dict__, sort_keys=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> sha256(block_string.encode()).hexdigest()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span>:</span></span><br><span class="line">    <span class="comment"># difficulty of our PoW algorithm</span></span><br><span class="line">    difficulty = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.unconfirmed_transactions = []</span><br><span class="line">        self.chain = []</span><br><span class="line">        self.create_genesis_block()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_genesis_block</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A function to generate genesis block and appends it to</span></span><br><span class="line"><span class="string">        the chain. The block has index 0, previous_hash as 0, and</span></span><br><span class="line"><span class="string">        a valid hash.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        genesis_block = Block(<span class="number">0</span>, [], time.time(), <span class="string">"0"</span>)</span><br><span class="line">        genesis_block.hash = genesis_block.compute_hash()</span><br><span class="line">        self.chain.append(genesis_block)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">last_block</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.chain[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_block</span><span class="params">(self, block, proof)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A function that adds the block to the chain after verification.</span></span><br><span class="line"><span class="string">        Verification includes:</span></span><br><span class="line"><span class="string">        * Checking if the proof is valid.</span></span><br><span class="line"><span class="string">        * The previous_hash referred in the block and the hash of latest block</span></span><br><span class="line"><span class="string">          in the chain match.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        previous_hash = self.last_block.hash</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> previous_hash != block.previous_hash:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.is_valid_proof(block, proof):</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        block.hash = proof</span><br><span class="line">        self.chain.append(block)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_valid_proof</span><span class="params">(self, block, block_hash)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Check if block_hash is valid hash of block and satisfies</span></span><br><span class="line"><span class="string">        the difficulty criteria.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> (block_hash.startswith(<span class="string">'0'</span> * Blockchain.difficulty) <span class="keyword">and</span></span><br><span class="line">                block_hash == block.compute_hash())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">proof_of_work</span><span class="params">(self, block)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Function that tries different values of nonce to get a hash</span></span><br><span class="line"><span class="string">        that satisfies our difficulty criteria.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        block.nonce = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        computed_hash = block.compute_hash()</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> computed_hash.startswith(<span class="string">'0'</span> * Blockchain.difficulty):</span><br><span class="line">            block.nonce += <span class="number">1</span></span><br><span class="line">            computed_hash = block.compute_hash()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> computed_hash</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_new_transaction</span><span class="params">(self, transaction)</span>:</span></span><br><span class="line">        self.unconfirmed_transactions.append(transaction)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mine</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        This function serves as an interface to add the pending</span></span><br><span class="line"><span class="string">        transactions to the blockchain by adding them to the block</span></span><br><span class="line"><span class="string">        and figuring out Proof Of Work.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.unconfirmed_transactions:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        new_block = Block(index=last_block.index + <span class="number">1</span>,</span><br><span class="line">                          transactions=self.unconfirmed_transactions,</span><br><span class="line">                          timestamp=time.time(),</span><br><span class="line">                          previous_hash=self.last_block.hash)</span><br><span class="line"></span><br><span class="line">        proof = self.proof_of_work(new_block)</span><br><span class="line">        self.add_block(new_block, proof)</span><br><span class="line"></span><br><span class="line">        self.unconfirmed_transactions = []</span><br><span class="line">        <span class="keyword">return</span> new_block.index</span><br></pre></td></tr></table></figure><h3 id="Create-interfaces"><a href="#Create-interfaces" class="headerlink" title="Create interfaces"></a>Create interfaces</h3><p>Okay, now it’s time to create interfaces for our blockchain node to interact with the application we’re going to build. We’ll be using a popular Python microframework called Flask to create a REST API that interacts with and invokes various operations in our blockchain node. If you’ve worked with any web framework before, the code below shouldn’t be difficult to follow along.</p><p>These REST endpoints can be used to play around with our blockchain by creating some transactions and then mining them.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize flask application</span></span><br><span class="line">app =  Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize a blockchain object.</span></span><br><span class="line">blockchain = Blockchain()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### We need an endpoint for our application to submit a new transaction. This will be used by our application to add new data (posts) to the blockchain:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Flask's way of declaring end-points</span></span><br><span class="line"><span class="meta">@app.route('/new_transaction', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">new_transaction</span><span class="params">()</span>:</span></span><br><span class="line">    tx_data = request.get_json()</span><br><span class="line">    required_fields = [<span class="string">"author"</span>, <span class="string">"content"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> field <span class="keyword">in</span> required_fields:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> tx_data.get(field):</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"Invalid transaction data"</span>, <span class="number">404</span></span><br><span class="line"></span><br><span class="line">    tx_data[<span class="string">"timestamp"</span>] = time.time()</span><br><span class="line"></span><br><span class="line">    blockchain.add_new_transaction(tx_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"Success"</span>, <span class="number">201</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### Here’s an endpoint to return the node’s copy of the chain. Our application will be using this endpoint to query all of the data to display:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/chain', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_chain</span><span class="params">()</span>:</span></span><br><span class="line">    chain_data = []</span><br><span class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> blockchain.chain:</span><br><span class="line">        chain_data.append(block.__dict__)</span><br><span class="line">    <span class="keyword">return</span> json.dumps(&#123;<span class="string">"length"</span>: len(chain_data),</span><br><span class="line">                       <span class="string">"chain"</span>: chain_data&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Here’s an endpoint to request the node to mine the unconfirmed transactions (if any). We’ll be using it to initiate a command to mine from our application itself:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/mine', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mine_unconfirmed_transactions</span><span class="params">()</span>:</span></span><br><span class="line">    result = blockchain.mine()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> result:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"No transactions to mine"</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"Block #&#123;&#125; is mined."</span>.format(result)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/pending_tx')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pending_tx</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(blockchain.unconfirmed_transactions)</span><br></pre></td></tr></table></figure><h3 id="Establish-consensus-and-decentralization"><a href="#Establish-consensus-and-decentralization" class="headerlink" title="Establish consensus and decentralization"></a>Establish consensus and decentralization</h3><p>Up to this point, the blockchain that we’ve implemented is meant to run on a single computer. Even though we’re linking block with hashes and applying the proof of work constraint, we still can’t trust a single entity (in our case, a single machine). We need the data to be distributed, we need multiple nodes maintaining the blockchain. So, to transition from a single node to a peer-to-peer network, let’s first create a mechanism to let a new node become aware of other peers in the network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Contains the host addresses of other participating members of the network</span></span><br><span class="line">peers = set()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Endpoint to add new peers to the network</span></span><br><span class="line"><span class="meta">@app.route('/register_node', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">register_new_peers</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># The host address to the peer node </span></span><br><span class="line">    node_address = request.get_json()[<span class="string">"node_address"</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> node_address:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Invalid data"</span>, <span class="number">400</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add the node to the peer list</span></span><br><span class="line">    peers.add(node_address)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the blockchain to the newly registered node so that it can sync</span></span><br><span class="line">    <span class="keyword">return</span> get_chain()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/register_with', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">register_with_existing_node</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Internally calls the `register_node` endpoint to</span></span><br><span class="line"><span class="string">    register current node with the remote node specified in the</span></span><br><span class="line"><span class="string">    request, and sync the blockchain as well with the remote node.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    node_address = request.get_json()[<span class="string">"node_address"</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> node_address:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Invalid data"</span>, <span class="number">400</span></span><br><span class="line"></span><br><span class="line">    data = &#123;<span class="string">"node_address"</span>: request.host_url&#125;</span><br><span class="line">    headers = &#123;<span class="string">'Content-Type'</span>: <span class="string">"application/json"</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Make a request to register with remote node and obtain information</span></span><br><span class="line">    response = requests.post(node_address + <span class="string">"/register_node"</span>,</span><br><span class="line">                             data=json.dumps(data), headers=headers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        <span class="keyword">global</span> blockchain</span><br><span class="line">        <span class="keyword">global</span> peers</span><br><span class="line">        <span class="comment"># update chain and the peers</span></span><br><span class="line">        chain_dump = response.json()[<span class="string">'chain'</span>]</span><br><span class="line">        blockchain = create_chain_from_dump(chain_dump)</span><br><span class="line">        peers.update(response.json()[<span class="string">'peers'</span>])</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Registration successful"</span>, <span class="number">200</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># if something goes wrong, pass it on to the API response</span></span><br><span class="line">        <span class="keyword">return</span> response.content, response.status_code</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_chain_from_dump</span><span class="params">(chain_dump)</span>:</span></span><br><span class="line">    blockchain = Blockchain()</span><br><span class="line">    <span class="keyword">for</span> idx, block_data <span class="keyword">in</span> enumerate(chain_dump):</span><br><span class="line">        block = Block(block_data[<span class="string">"index"</span>],</span><br><span class="line">                      block_data[<span class="string">"transactions"</span>],</span><br><span class="line">                      block_data[<span class="string">"timestamp"</span>],</span><br><span class="line">                      block_data[<span class="string">"previous_hash"</span>])</span><br><span class="line">        proof = block_data[<span class="string">'hash'</span>]</span><br><span class="line">        <span class="keyword">if</span> idx &gt; <span class="number">0</span>:</span><br><span class="line">            added = blockchain.add_block(block, proof)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> added:</span><br><span class="line">                <span class="keyword">raise</span> Exception(<span class="string">"The chain dump is tampered!!"</span>)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># the block is a genesis block, no verification needed</span></span><br><span class="line">            blockchain.chain.append(block)</span><br><span class="line">    <span class="keyword">return</span> blockchain</span><br></pre></td></tr></table></figure><p>A new node participating in the network can invoke the register_with_existing_node method (via the /register_with endpoint) to register with existing nodes in the network. This will help with the following:</p><ul><li>Asking the remote node to add a new peer to its list of known peers.</li><li>Initializing the blockchain of the new node with that of the remote node.</li><li>Resyncing the blockchain with the network if the node goes off-grid.</li></ul><p>However, there’s a problem with multiple nodes. Due to intentional manipulation or unintentional reasons (like network latency), the copy of chains of a few nodes can differ. In that case, the nodes need to agree upon some version of the chain to maintain the integrity of the entire system. In other words, we need to achieve consensus.</p><p>A simple consensus algorithm could be to agree upon the longest valid chain when the chains of different participating nodes in the network appear to diverge. The rationale behind this approach is that the longest chain is a good estimate of the most amount of work done (remember proof of work is difficult to compute):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blockchain</span></span></span><br><span class="line"><span class="class">    """</span></span><br><span class="line"><span class="class">    <span class="title">previous</span> <span class="title">code</span> <span class="title">continued</span>...</span></span><br><span class="line"><span class="class">    """</span></span><br><span class="line"><span class="class">    <span class="title">def</span> <span class="title">check_chain_validity</span><span class="params">(cls, chain)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        A helper method to check if the entire blockchain is valid.            </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        result = <span class="keyword">True</span></span><br><span class="line">        previous_hash = <span class="string">"0"</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate through every block</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> chain:</span><br><span class="line">            block_hash = block.hash</span><br><span class="line">            <span class="comment"># remove the hash field to recompute the hash again</span></span><br><span class="line">            <span class="comment"># using `compute_hash` method.</span></span><br><span class="line">            delattr(block, <span class="string">"hash"</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> cls.is_valid_proof(block, block.hash) <span class="keyword">or</span> \</span><br><span class="line">                    previous_hash != block.previous_hash:</span><br><span class="line">                result = <span class="keyword">False</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            block.hash, previous_hash = block_hash, block_hash</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consensus</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Our simple consensus algorithm. If a longer valid chain is</span></span><br><span class="line"><span class="string">    found, our chain is replaced with it.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">global</span> blockchain</span><br><span class="line"></span><br><span class="line">    longest_chain = <span class="keyword">None</span></span><br><span class="line">    current_len = len(blockchain.chain)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> peers:</span><br><span class="line">        response = requests.get(<span class="string">'&#123;&#125;/chain'</span>.format(node))</span><br><span class="line">        length = response.json()[<span class="string">'length'</span>]</span><br><span class="line">        chain = response.json()[<span class="string">'chain'</span>]</span><br><span class="line">        <span class="keyword">if</span> length &gt; current_len <span class="keyword">and</span> blockchain.check_chain_validity(chain):</span><br><span class="line">              <span class="comment"># Longer valid chain found!</span></span><br><span class="line">            current_len = length</span><br><span class="line">            longest_chain = chain</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> longest_chain:</span><br><span class="line">        blockchain = longest_chain</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p>Next, we need to develop a way for any node to announce to the network that it has mined a block so that everyone can update their blockchain and move on to mine other transactions. Other nodes can simply verify the proof of work and add the mined block to their respective chains (remember that verification is easy once the nonce is known):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># endpoint to add a block mined by someone else to</span></span><br><span class="line"><span class="comment"># the node's chain. The node first verifies the block</span></span><br><span class="line"><span class="comment"># and then adds it to the chain.</span></span><br><span class="line"><span class="meta">@app.route('/add_block', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">verify_and_add_block</span><span class="params">()</span>:</span></span><br><span class="line">    block_data = request.get_json()</span><br><span class="line">    block = Block(block_data[<span class="string">"index"</span>],</span><br><span class="line">                  block_data[<span class="string">"transactions"</span>],</span><br><span class="line">                  block_data[<span class="string">"timestamp"</span>],</span><br><span class="line">                  block_data[<span class="string">"previous_hash"</span>])</span><br><span class="line"></span><br><span class="line">    proof = block_data[<span class="string">'hash'</span>]</span><br><span class="line">    added = blockchain.add_block(block, proof)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> added:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"The block was discarded by the node"</span>, <span class="number">400</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"Block added to the chain"</span>, <span class="number">201</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">announce_new_block</span><span class="params">(block)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A function to announce to the network once a block has been mined.</span></span><br><span class="line"><span class="string">    Other blocks can simply verify the proof of work and add it to their</span></span><br><span class="line"><span class="string">    respective chains.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">for</span> peer <span class="keyword">in</span> peers:</span><br><span class="line">        url = <span class="string">"&#123;&#125;add_block"</span>.format(peer)</span><br><span class="line">        requests.post(url, data=json.dumps(block.__dict__, sort_keys=<span class="keyword">True</span>))</span><br></pre></td></tr></table></figure><p>The announce_new_block method should be called after every block is mined by the node so that peers can add it to their chains.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route('/mine', methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mine_unconfirmed_transactions</span><span class="params">()</span>:</span></span><br><span class="line">    result = blockchain.mine()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> result:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"No transactions to mine"</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Making sure we have the longest chain before announcing to the network</span></span><br><span class="line">        chain_length = len(blockchain.chain)</span><br><span class="line">        consensus()</span><br><span class="line">        <span class="keyword">if</span> chain_length == len(blockchain.chain):</span><br><span class="line">            <span class="comment"># announce the recently mined block to the network</span></span><br><span class="line">            announce_new_block(blockchain.last_block)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Block #&#123;&#125; is mined."</span>.format(blockchain.last_block.index)</span><br></pre></td></tr></table></figure><h3 id="Build-the-application"><a href="#Build-the-application" class="headerlink" title="Build the application"></a>Build the application</h3><p>Now, it’s time to start working on the interface of our application. We’ve used Jinja2 templating to render the web pages and some CSS to make things look nice.</p><p>Our application needs to connect to a node in the blockchain network to fetch the data and also to submit new data. There can also be multiple nodes, as well.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> render_template, redirect, request</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> app <span class="keyword">import</span> app</span><br><span class="line"></span><br><span class="line"><span class="comment"># Node in the blockchain network that our application will communicate with</span></span><br><span class="line"><span class="comment"># to fetch and add data.</span></span><br><span class="line">CONNECTED_NODE_ADDRESS = <span class="string">"http://127.0.0.1:8000"</span></span><br><span class="line"></span><br><span class="line">posts = []</span><br></pre></td></tr></table></figure><p>The fetch_posts function gets the data from the node’s /chain endpoint, parses the data, and stores it locally.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_posts</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Function to fetch the chain from a blockchain node, parse the</span></span><br><span class="line"><span class="string">    data, and store it locally.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    get_chain_address = <span class="string">"&#123;&#125;/chain"</span>.format(CONNECTED_NODE_ADDRESS)</span><br><span class="line">    response = requests.get(get_chain_address)</span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        content = []</span><br><span class="line">        chain = json.loads(response.content)</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> chain[<span class="string">"chain"</span>]:</span><br><span class="line">            <span class="keyword">for</span> tx <span class="keyword">in</span> block[<span class="string">"transactions"</span>]:</span><br><span class="line">                tx[<span class="string">"index"</span>] = block[<span class="string">"index"</span>]</span><br><span class="line">                tx[<span class="string">"hash"</span>] = block[<span class="string">"previous_hash"</span>]</span><br><span class="line">                content.append(tx)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">global</span> posts</span><br><span class="line">        posts = sorted(content,</span><br><span class="line">                       key=<span class="keyword">lambda</span> k: k[<span class="string">'timestamp'</span>],</span><br><span class="line">                       reverse=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>The application has an HTML form to take user input and then makes a POST request to a connected node to add the transaction into the unconfirmed transactions pool. The transaction is then mined by the network, and then finally fetched once we refresh our web page:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route('/submit', methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submit_textarea</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Endpoint to create a new transaction via our application</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    post_content = request.form[<span class="string">"content"</span>]</span><br><span class="line">    author = request.form[<span class="string">"author"</span>]</span><br><span class="line"></span><br><span class="line">    post_object = &#123;</span><br><span class="line">        <span class="string">'author'</span>: author,</span><br><span class="line">        <span class="string">'content'</span>: post_content,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Submit a transaction</span></span><br><span class="line">    new_tx_address = <span class="string">"&#123;&#125;/new_transaction"</span>.format(CONNECTED_NODE_ADDRESS)</span><br><span class="line"></span><br><span class="line">    requests.post(new_tx_address,</span><br><span class="line">                  json=post_object,</span><br><span class="line">                  headers=&#123;<span class="string">'Content-type'</span>: <span class="string">'application/json'</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return to the homepage</span></span><br><span class="line">    <span class="keyword">return</span> redirect(<span class="string">'/'</span>)</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://developer.ibm.com/tutorials/develop-a-blockchain-application-from-scratch-in-python/" target="_blank" rel="noopener">https://developer.ibm.com/tutorials/develop-a-blockchain-application-from-scratch-in-python/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Blockchain is a way of storing digital data. The data can literally be anything. For Bitcoin, it’s the transactions (logs of transfers of
      
    
    </summary>
    
    
      <category term="Big Data Architecture" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/"/>
    
      <category term="Blockchain" scheme="https://zhangruochi.com/categories/Big-Data-Architecture/Blockchain/"/>
    
    
  </entry>
  
  <entry>
    <title>Cox Proportional Hazards and Random Survival Forests</title>
    <link href="https://zhangruochi.com/Cox-Proportional-Hazards-and-Random-Survival-Forests/2020/04/19/"/>
    <id>https://zhangruochi.com/Cox-Proportional-Hazards-and-Random-Survival-Forests/2020/04/19/</id>
    <published>2020-04-19T05:54:32.000Z</published>
    <updated>2020-04-19T17:54:52.554Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Cox-Proportional-Hazards-and-Random-Survival-Forests"><a href="#Cox-Proportional-Hazards-and-Random-Survival-Forests" class="headerlink" title="Cox Proportional Hazards and Random Survival Forests"></a>Cox Proportional Hazards and Random Survival Forests</h1><p>Welcome to the final assignment in Course 2! In this assignment you’ll develop risk models using survival data and a combination of linear and non-linear techniques. We’ll be using a dataset with survival data of patients with Primary Biliary Cirrhosis (pbc). PBC is a progressive disease of the liver caused by a buildup of bile within the liver (cholestasis) that results in damage to the small bile ducts that drain bile from the liver. Our goal will be to understand the effects of different factors on the survival times of the patients. Along the way you’ll learn about the following topics: </p><ul><li>Cox Proportional Hazards<ul><li>Data Preprocessing for Cox Models.</li></ul></li><li>Random Survival Forests<ul><li>Permutation Methods for Interpretation.</li></ul></li></ul><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#1">1. Import Packages</a></li><li><a href="#2">2. Load the Dataset</a></li><li><a href="#3">3. Explore the Dataset</a></li><li><a href="#4">4. Cox Proportional Hazards</a><ul><li><a href="#Ex-1">Exercise 1</a></li></ul></li><li><a href="#5">5. Fitting and Interpreting a Cox Model</a></li><li><a href="#3">6. Hazard ratio</a><ul><li><a href="#Ex-2">Exercise 2</a></li></ul></li><li><a href="#7">7. Harrell’s C-Index</a><ul><li><a href="#Ex-3">Exercise 3</a></li></ul></li><li><a href="#8">8. Random Survival Forests</a></li><li><a href="#9">9. Permutation Method for Interpretation</a></li></ul><p><a name="1"></a></p><h2 id="1-Import-Packages"><a href="#1-Import-Packages" class="headerlink" title="1. Import Packages"></a>1. Import Packages</h2><p>We’ll first import all the packages that we need for this assignment. </p><ul><li><code>sklearn</code> is one of the most popular machine learning libraries.</li><li><code>numpy</code> is the fundamental package for scientific computing in python.</li><li><code>pandas</code> is what we’ll use to manipulate our data.</li><li><code>matplotlib</code> is a plotting library.</li><li><code>lifelines</code> is an open-source survival analysis library.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> lifelines <span class="keyword">import</span> CoxPHFitter</span><br><span class="line"><span class="keyword">from</span> lifelines.utils <span class="keyword">import</span> concordance_index <span class="keyword">as</span> cindex</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> util <span class="keyword">import</span> load_data</span><br></pre></td></tr></table></figure><p><a name="2"></a></p><h2 id="2-Load-the-Dataset"><a href="#2-Load-the-Dataset" class="headerlink" title="2. Load the Dataset"></a>2. Load the Dataset</h2><p>Run the next cell to load the data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = load_data()</span><br></pre></td></tr></table></figure><p><a name="3"></a></p><h2 id="3-Explore-the-Dataset"><a href="#3-Explore-the-Dataset" class="headerlink" title="3. Explore the Dataset"></a>3. Explore the Dataset</h2><p>In the lecture videos <code>time</code> was in months, however in this assignment, <code>time</code> will be converted into years. Also notice that we have assigned a numeric value to <code>sex</code>, where <code>female = 0</code> and <code>male = 1</code>.</p><p>Next, familiarize yourself with the data and the shape of it. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(df.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># df.head() only outputs the top few rows</span></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><pre><code>(258, 19)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>time</th>      <th>status</th>      <th>trt</th>      <th>age</th>      <th>sex</th>      <th>ascites</th>      <th>hepato</th>      <th>spiders</th>      <th>edema</th>      <th>bili</th>      <th>chol</th>      <th>albumin</th>      <th>copper</th>      <th>alk.phos</th>      <th>ast</th>      <th>trig</th>      <th>platelet</th>      <th>protime</th>      <th>stage</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1.095890</td>      <td>1.0</td>      <td>0.0</td>      <td>58.765229</td>      <td>0.0</td>      <td>1.0</td>      <td>1.0</td>      <td>1.0</td>      <td>1.0</td>      <td>14.5</td>      <td>261.0</td>      <td>2.60</td>      <td>156.0</td>      <td>1718.0</td>      <td>137.95</td>      <td>172.0</td>      <td>190.0</td>      <td>12.2</td>      <td>4.0</td>    </tr>    <tr>      <th>1</th>      <td>12.328767</td>      <td>0.0</td>      <td>0.0</td>      <td>56.446270</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>1.0</td>      <td>0.0</td>      <td>1.1</td>      <td>302.0</td>      <td>4.14</td>      <td>54.0</td>      <td>7394.8</td>      <td>113.52</td>      <td>88.0</td>      <td>221.0</td>      <td>10.6</td>      <td>3.0</td>    </tr>    <tr>      <th>2</th>      <td>2.772603</td>      <td>1.0</td>      <td>0.0</td>      <td>70.072553</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.5</td>      <td>1.4</td>      <td>176.0</td>      <td>3.48</td>      <td>210.0</td>      <td>516.0</td>      <td>96.10</td>      <td>55.0</td>      <td>151.0</td>      <td>12.0</td>      <td>4.0</td>    </tr>    <tr>      <th>3</th>      <td>5.273973</td>      <td>1.0</td>      <td>0.0</td>      <td>54.740589</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>1.0</td>      <td>0.5</td>      <td>1.8</td>      <td>244.0</td>      <td>2.54</td>      <td>64.0</td>      <td>6121.8</td>      <td>60.63</td>      <td>92.0</td>      <td>183.0</td>      <td>10.3</td>      <td>4.0</td>    </tr>    <tr>      <th>6</th>      <td>5.019178</td>      <td>0.0</td>      <td>1.0</td>      <td>55.534565</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>322.0</td>      <td>4.09</td>      <td>52.0</td>      <td>824.0</td>      <td>60.45</td>      <td>213.0</td>      <td>204.0</td>      <td>9.7</td>      <td>3.0</td>    </tr>  </tbody></table></div><p>Take a minute to examine particular cases.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">i = <span class="number">20</span></span><br><span class="line">df.iloc[i, :]</span><br></pre></td></tr></table></figure><pre><code>time          11.175342status         1.000000trt            0.000000age           44.520192sex            1.000000ascites        0.000000hepato         1.000000spiders        0.000000edema          0.000000bili           2.100000chol         456.000000albumin        4.000000copper       124.000000alk.phos    5719.000000ast          221.880000trig         230.000000platelet      70.000000protime        9.900000stage          2.000000Name: 23, dtype: float64</code></pre><p>Now, split your dataset into train, validation and test set using 60/20/20 split. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">df_dev, df_test = train_test_split(df, test_size = <span class="number">0.2</span>)</span><br><span class="line">df_train, df_val = train_test_split(df_dev, test_size = <span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Total number of patients:"</span>, df.shape[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">"Total number of patients in training set:"</span>, df_train.shape[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">"Total number of patients in validation set:"</span>, df_val.shape[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">"Total number of patients in test set:"</span>, df_test.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>Total number of patients: 258Total number of patients in training set: 154Total number of patients in validation set: 52Total number of patients in test set: 52</code></pre><p>Before proceeding to modeling, let’s normalize the continuous covariates to make sure they’re on the same scale. Again, we should normalize the test data using statistics from the train data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">continuous_columns = [<span class="string">'age'</span>, <span class="string">'bili'</span>, <span class="string">'chol'</span>, <span class="string">'albumin'</span>, <span class="string">'copper'</span>, <span class="string">'alk.phos'</span>, <span class="string">'ast'</span>, <span class="string">'trig'</span>, <span class="string">'platelet'</span>, <span class="string">'protime'</span>]</span><br><span class="line">mean = df_train.loc[:, continuous_columns].mean()</span><br><span class="line">std = df_train.loc[:, continuous_columns].std()</span><br><span class="line">df_train.loc[:, continuous_columns] = (df_train.loc[:, continuous_columns] - mean) / std</span><br><span class="line">df_val.loc[:, continuous_columns] = (df_val.loc[:, continuous_columns] - mean) / std</span><br><span class="line">df_test.loc[:, continuous_columns] = (df_test.loc[:, continuous_columns] - mean) / std</span><br></pre></td></tr></table></figure><p>Let’s check the summary statistics on our training dataset to make sure it’s standardized.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_train.loc[:, continuous_columns].describe()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>age</th>      <th>bili</th>      <th>chol</th>      <th>albumin</th>      <th>copper</th>      <th>alk.phos</th>      <th>ast</th>      <th>trig</th>      <th>platelet</th>      <th>protime</th>    </tr>  </thead>  <tbody>    <tr>      <th>count</th>      <td>1.540000e+02</td>      <td>1.540000e+02</td>      <td>1.540000e+02</td>      <td>1.540000e+02</td>      <td>1.540000e+02</td>      <td>1.540000e+02</td>      <td>1.540000e+02</td>      <td>1.540000e+02</td>      <td>1.540000e+02</td>      <td>1.540000e+02</td>    </tr>    <tr>      <th>mean</th>      <td>9.833404e-16</td>      <td>-3.258577e-16</td>      <td>1.153478e-16</td>      <td>1.153478e-16</td>      <td>5.767392e-18</td>      <td>1.326500e-16</td>      <td>-1.263059e-15</td>      <td>8.074349e-17</td>      <td>2.018587e-17</td>      <td>1.291896e-14</td>    </tr>    <tr>      <th>std</th>      <td>1.000000e+00</td>      <td>1.000000e+00</td>      <td>1.000000e+00</td>      <td>1.000000e+00</td>      <td>1.000000e+00</td>      <td>1.000000e+00</td>      <td>1.000000e+00</td>      <td>1.000000e+00</td>      <td>1.000000e+00</td>      <td>1.000000e+00</td>    </tr>    <tr>      <th>min</th>      <td>-2.304107e+00</td>      <td>-5.735172e-01</td>      <td>-1.115330e+00</td>      <td>-3.738104e+00</td>      <td>-9.856552e-01</td>      <td>-7.882167e-01</td>      <td>-1.489281e+00</td>      <td>-1.226674e+00</td>      <td>-2.058899e+00</td>      <td>-1.735556e+00</td>    </tr>    <tr>      <th>25%</th>      <td>-6.535035e-01</td>      <td>-4.895812e-01</td>      <td>-5.186963e-01</td>      <td>-5.697976e-01</td>      <td>-6.470611e-01</td>      <td>-5.186471e-01</td>      <td>-8.353982e-01</td>      <td>-6.884514e-01</td>      <td>-6.399831e-01</td>      <td>-7.382590e-01</td>    </tr>    <tr>      <th>50%</th>      <td>-6.443852e-03</td>      <td>-3.846612e-01</td>      <td>-2.576693e-01</td>      <td>5.663556e-02</td>      <td>-3.140636e-01</td>      <td>-3.416086e-01</td>      <td>-2.260984e-01</td>      <td>-2.495932e-01</td>      <td>-4.100373e-02</td>      <td>-1.398807e-01</td>    </tr>    <tr>      <th>75%</th>      <td>5.724289e-01</td>      <td>2.977275e-02</td>      <td>1.798617e-01</td>      <td>6.890921e-01</td>      <td>3.435366e-01</td>      <td>-4.620597e-03</td>      <td>6.061159e-01</td>      <td>3.755727e-01</td>      <td>6.617988e-01</td>      <td>3.587680e-01</td>    </tr>    <tr>      <th>max</th>      <td>2.654276e+00</td>      <td>5.239050e+00</td>      <td>6.243146e+00</td>      <td>2.140730e+00</td>      <td>5.495204e+00</td>      <td>4.869263e+00</td>      <td>3.058176e+00</td>      <td>5.165751e+00</td>      <td>3.190823e+00</td>      <td>4.447687e+00</td>    </tr>  </tbody></table></div><p><a name="4"></a></p><h2 id="4-Cox-Proportional-Hazards"><a href="#4-Cox-Proportional-Hazards" class="headerlink" title="4. Cox Proportional Hazards"></a>4. Cox Proportional Hazards</h2><p>Our goal is to build a risk score using the survival data that we have. We’ll begin by fitting a Cox Proportional Hazards model to your data.</p><p>Recall that the Cox Proportional Hazards model describes the hazard for an individual $i$ at time $t$ as </p><script type="math/tex; mode=display">\lambda(t, x) = \lambda_0(t)e^{\theta^T X_i}</script><p>The $\lambda_0$ term is a baseline hazard and incorporates the risk over time, and the other term incorporates the risk due to the individual’s covariates. After fitting the model, we can rank individuals using the person-dependent risk term $e^{\theta^T X_i}$. </p><p>Categorical variables cannot be used in a regression model as they are. In order to use them, conversion to a series of variables is required.</p><p>Since our data has a mix of categorical (<code>stage</code>) and continuous (<code>wblc</code>) variables, before we proceed further we need to do some data engineering. To tackle the issue at hand we’ll be using the <code>Dummy Coding</code> technique. In order to use Cox Proportional Hazards, we will have to turn the categorical data into one hot features so that we can fit our Cox model. Luckily, Pandas has a built-in function called <code>get_dummies</code> that will make it easier for us to implement our function. It turns categorical features into multiple binary features.</p><p><img src="1-hot-encode.png" style="padding-top: 5px;width: 60%;left: 0px;margin-left: 150px;margin-right: 0px;"></p><p><a name="Ex-1"></a></p><h3 id="Exercise-1"><a href="#Exercise-1" class="headerlink" title="Exercise 1"></a>Exercise 1</h3><p>In the cell below, implement the <code>to_one_hot(...)</code> function.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Remember to drop the first dummy for each each category to avoid convergence issues when fitting the proportional hazards model.</li>    <li> Check out the <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html" target="_blank" rel="noopener"> get_dummies() </a>  documentation. </li>    <li>Use <code>dtype=np.float64</code>.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_one_hot</span><span class="params">(dataframe, columns)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Convert columns in dataframe to one-hot encoding.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataframe (dataframe): pandas dataframe containing covariates</span></span><br><span class="line"><span class="string">        columns (list of strings): list categorical column names to one hot encode</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        one_hot_df (dataframe): dataframe with categorical columns encoded</span></span><br><span class="line"><span class="string">                            as binary variables</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    one_hot_df = pd.get_dummies(dataframe,columns=columns, drop_first = <span class="keyword">True</span>, dtype=np.float64)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> one_hot_df</span><br></pre></td></tr></table></figure><p>Now we’ll use the function you coded to transform the training, validation, and test sets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># List of categorical columns</span></span><br><span class="line">to_encode = [<span class="string">'edema'</span>, <span class="string">'stage'</span>]</span><br><span class="line"></span><br><span class="line">one_hot_train = to_one_hot(df_train, to_encode)</span><br><span class="line">one_hot_val = to_one_hot(df_val, to_encode)</span><br><span class="line">one_hot_test = to_one_hot(df_test, to_encode)</span><br><span class="line"></span><br><span class="line">print(one_hot_val.columns.tolist())</span><br><span class="line">print(<span class="string">f"There are <span class="subst">&#123;len(one_hot_val.columns)&#125;</span> columns"</span>)</span><br></pre></td></tr></table></figure><pre><code>[&#39;time&#39;, &#39;status&#39;, &#39;trt&#39;, &#39;age&#39;, &#39;sex&#39;, &#39;ascites&#39;, &#39;hepato&#39;, &#39;spiders&#39;, &#39;bili&#39;, &#39;chol&#39;, &#39;albumin&#39;, &#39;copper&#39;, &#39;alk.phos&#39;, &#39;ast&#39;, &#39;trig&#39;, &#39;platelet&#39;, &#39;protime&#39;, &#39;edema_0.5&#39;, &#39;edema_1.0&#39;, &#39;stage_2.0&#39;, &#39;stage_3.0&#39;, &#39;stage_4.0&#39;]There are 22 columns</code></pre><h4 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'time'</span>, <span class="string">'status'</span>, <span class="string">'trt'</span>, <span class="string">'age'</span>, <span class="string">'sex'</span>, <span class="string">'ascites'</span>, <span class="string">'hepato'</span>, <span class="string">'spiders'</span>, <span class="string">'bili'</span>, <span class="string">'chol'</span>, <span class="string">'albumin'</span>, <span class="string">'copper'</span>, <span class="string">'alk.phos'</span>, <span class="string">'ast'</span>, <span class="string">'trig'</span>, <span class="string">'platelet'</span>, <span class="string">'protime'</span>, <span class="string">'edema_0.5'</span>, <span class="string">'edema_1.0'</span>, <span class="string">'stage_2.0'</span>, <span class="string">'stage_3.0'</span>, <span class="string">'stage_4.0'</span>]</span><br><span class="line">There are <span class="number">22</span> columns</span><br></pre></td></tr></table></figure><h3 id="Look-for-new-features"><a href="#Look-for-new-features" class="headerlink" title="Look for new features"></a>Look for new features</h3><p>Now, let’s take a peek at one of the transformed data sets. Do you notice any new features?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(one_hot_train.shape)</span><br><span class="line">one_hot_train.head()</span><br></pre></td></tr></table></figure><pre><code>(154, 22)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>time</th>      <th>status</th>      <th>trt</th>      <th>age</th>      <th>sex</th>      <th>ascites</th>      <th>hepato</th>      <th>spiders</th>      <th>bili</th>      <th>chol</th>      <th>...</th>      <th>alk.phos</th>      <th>ast</th>      <th>trig</th>      <th>platelet</th>      <th>protime</th>      <th>edema_0.5</th>      <th>edema_1.0</th>      <th>stage_2.0</th>      <th>stage_3.0</th>      <th>stage_4.0</th>    </tr>  </thead>  <tbody>    <tr>      <th>279</th>      <td>3.868493</td>      <td>0.0</td>      <td>0.0</td>      <td>-0.414654</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>-0.300725</td>      <td>-0.096081</td>      <td>...</td>      <td>0.167937</td>      <td>0.401418</td>      <td>0.330031</td>      <td>0.219885</td>      <td>-1.137178</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>    </tr>    <tr>      <th>137</th>      <td>3.553425</td>      <td>1.0</td>      <td>0.0</td>      <td>0.069681</td>      <td>1.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.895363</td>      <td>0.406085</td>      <td>...</td>      <td>0.101665</td>      <td>0.472367</td>      <td>1.621764</td>      <td>-0.120868</td>      <td>-0.239610</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>    </tr>    <tr>      <th>249</th>      <td>4.846575</td>      <td>0.0</td>      <td>1.0</td>      <td>-0.924494</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>-0.510565</td>      <td>-0.225352</td>      <td>...</td>      <td>0.245463</td>      <td>1.899020</td>      <td>-0.580807</td>      <td>0.422207</td>      <td>0.159309</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>    </tr>    <tr>      <th>266</th>      <td>0.490411</td>      <td>1.0</td>      <td>0.0</td>      <td>1.938314</td>      <td>0.0</td>      <td>1.0</td>      <td>1.0</td>      <td>1.0</td>      <td>0.748475</td>      <td>-0.608191</td>      <td>...</td>      <td>-0.650254</td>      <td>-0.288898</td>      <td>-0.481443</td>      <td>-0.727833</td>      <td>1.356065</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>    </tr>    <tr>      <th>1</th>      <td>12.328767</td>      <td>0.0</td>      <td>0.0</td>      <td>0.563645</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>1.0</td>      <td>-0.405645</td>      <td>-0.210436</td>      <td>...</td>      <td>2.173526</td>      <td>-0.144699</td>      <td>-0.531125</td>      <td>-0.450972</td>      <td>-0.139881</td>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>    </tr>  </tbody></table><p>5 rows × 22 columns</p></div><p><a name="5"></a></p><h2 id="5-Fitting-and-Interpreting-a-Cox-Model"><a href="#5-Fitting-and-Interpreting-a-Cox-Model" class="headerlink" title="5. Fitting and Interpreting a Cox Model"></a>5. Fitting and Interpreting a Cox Model</h2><p>Run the following cell to fit your Cox Proportional Hazards model using the <code>lifelines</code> package.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cph = CoxPHFitter()</span><br><span class="line">cph.fit(one_hot_train, duration_col = <span class="string">'time'</span>, event_col = <span class="string">'status'</span>, step_size=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;lifelines.CoxPHFitter: fitted with 154 total observations, 90 right-censored observations&gt;</code></pre><p>You can use <code>cph.print_summary()</code> to view the coefficients associated with each covariate as well as confidence intervals. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cph.print_summary()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <tbody>    <tr>      <th>model</th>      <td>lifelines.CoxPHFitter</td>    </tr>    <tr>      <th>duration col</th>      <td>'time'</td>    </tr>    <tr>      <th>event col</th>      <td>'status'</td>    </tr>    <tr>      <th>number of observations</th>      <td>154</td>    </tr>    <tr>      <th>number of events observed</th>      <td>64</td>    </tr>    <tr>      <th>partial log-likelihood</th>      <td>-230.82</td>    </tr>    <tr>      <th>time fit was run</th>      <td>2020-04-19 16:30:56 UTC</td>    </tr>  </tbody></table></div><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>coef</th>      <th>exp(coef)</th>      <th>se(coef)</th>      <th>coef lower 95%</th>      <th>coef upper 95%</th>      <th>exp(coef) lower 95%</th>      <th>exp(coef) upper 95%</th>      <th>z</th>      <th>p</th>      <th>-log2(p)</th>    </tr>  </thead>  <tbody>    <tr>      <th>trt</th>      <td>-0.22</td>      <td>0.80</td>      <td>0.30</td>      <td>-0.82</td>      <td>0.37</td>      <td>0.44</td>      <td>1.45</td>      <td>-0.73</td>      <td>0.46</td>      <td>1.11</td>    </tr>    <tr>      <th>age</th>      <td>0.23</td>      <td>1.26</td>      <td>0.19</td>      <td>-0.13</td>      <td>0.60</td>      <td>0.88</td>      <td>1.82</td>      <td>1.26</td>      <td>0.21</td>      <td>2.27</td>    </tr>    <tr>      <th>sex</th>      <td>0.34</td>      <td>1.41</td>      <td>0.40</td>      <td>-0.45</td>      <td>1.14</td>      <td>0.64</td>      <td>3.11</td>      <td>0.84</td>      <td>0.40</td>      <td>1.33</td>    </tr>    <tr>      <th>ascites</th>      <td>-0.10</td>      <td>0.91</td>      <td>0.56</td>      <td>-1.20</td>      <td>1.01</td>      <td>0.30</td>      <td>2.75</td>      <td>-0.17</td>      <td>0.86</td>      <td>0.21</td>    </tr>    <tr>      <th>hepato</th>      <td>0.31</td>      <td>1.36</td>      <td>0.38</td>      <td>-0.44</td>      <td>1.06</td>      <td>0.64</td>      <td>2.89</td>      <td>0.81</td>      <td>0.42</td>      <td>1.26</td>    </tr>    <tr>      <th>spiders</th>      <td>-0.18</td>      <td>0.83</td>      <td>0.38</td>      <td>-0.94</td>      <td>0.57</td>      <td>0.39</td>      <td>1.77</td>      <td>-0.47</td>      <td>0.64</td>      <td>0.66</td>    </tr>    <tr>      <th>bili</th>      <td>0.05</td>      <td>1.05</td>      <td>0.18</td>      <td>-0.29</td>      <td>0.39</td>      <td>0.75</td>      <td>1.48</td>      <td>0.29</td>      <td>0.77</td>      <td>0.37</td>    </tr>    <tr>      <th>chol</th>      <td>0.19</td>      <td>1.20</td>      <td>0.15</td>      <td>-0.10</td>      <td>0.47</td>      <td>0.91</td>      <td>1.60</td>      <td>1.28</td>      <td>0.20</td>      <td>2.33</td>    </tr>    <tr>      <th>albumin</th>      <td>-0.40</td>      <td>0.67</td>      <td>0.18</td>      <td>-0.75</td>      <td>-0.06</td>      <td>0.47</td>      <td>0.94</td>      <td>-2.28</td>      <td>0.02</td>      <td>5.46</td>    </tr>    <tr>      <th>copper</th>      <td>0.30</td>      <td>1.35</td>      <td>0.16</td>      <td>-0.01</td>      <td>0.61</td>      <td>0.99</td>      <td>1.84</td>      <td>1.91</td>      <td>0.06</td>      <td>4.14</td>    </tr>    <tr>      <th>alk.phos</th>      <td>-0.22</td>      <td>0.80</td>      <td>0.14</td>      <td>-0.49</td>      <td>0.05</td>      <td>0.61</td>      <td>1.05</td>      <td>-1.62</td>      <td>0.11</td>      <td>3.24</td>    </tr>    <tr>      <th>ast</th>      <td>0.21</td>      <td>1.24</td>      <td>0.16</td>      <td>-0.10</td>      <td>0.53</td>      <td>0.91</td>      <td>1.69</td>      <td>1.34</td>      <td>0.18</td>      <td>2.48</td>    </tr>    <tr>      <th>trig</th>      <td>0.20</td>      <td>1.23</td>      <td>0.16</td>      <td>-0.11</td>      <td>0.52</td>      <td>0.89</td>      <td>1.68</td>      <td>1.27</td>      <td>0.21</td>      <td>2.28</td>    </tr>    <tr>      <th>platelet</th>      <td>0.14</td>      <td>1.15</td>      <td>0.15</td>      <td>-0.16</td>      <td>0.43</td>      <td>0.86</td>      <td>1.54</td>      <td>0.92</td>      <td>0.36</td>      <td>1.48</td>    </tr>    <tr>      <th>protime</th>      <td>0.36</td>      <td>1.43</td>      <td>0.17</td>      <td>0.03</td>      <td>0.69</td>      <td>1.03</td>      <td>1.99</td>      <td>2.15</td>      <td>0.03</td>      <td>4.97</td>    </tr>    <tr>      <th>edema_0.5</th>      <td>1.24</td>      <td>3.47</td>      <td>0.46</td>      <td>0.35</td>      <td>2.14</td>      <td>1.42</td>      <td>8.50</td>      <td>2.72</td>      <td>0.01</td>      <td>7.28</td>    </tr>    <tr>      <th>edema_1.0</th>      <td>2.02</td>      <td>7.51</td>      <td>0.60</td>      <td>0.84</td>      <td>3.20</td>      <td>2.31</td>      <td>24.43</td>      <td>3.35</td>      <td>&lt;0.005</td>      <td>10.28</td>    </tr>    <tr>      <th>stage_2.0</th>      <td>1.21</td>      <td>3.35</td>      <td>1.08</td>      <td>-0.92</td>      <td>3.33</td>      <td>0.40</td>      <td>28.06</td>      <td>1.11</td>      <td>0.27</td>      <td>1.91</td>    </tr>    <tr>      <th>stage_3.0</th>      <td>1.18</td>      <td>3.27</td>      <td>1.09</td>      <td>-0.96</td>      <td>3.33</td>      <td>0.38</td>      <td>27.86</td>      <td>1.08</td>      <td>0.28</td>      <td>1.84</td>    </tr>    <tr>      <th>stage_4.0</th>      <td>1.41</td>      <td>4.10</td>      <td>1.15</td>      <td>-0.85</td>      <td>3.67</td>      <td>0.43</td>      <td>39.43</td>      <td>1.22</td>      <td>0.22</td>      <td>2.18</td>    </tr>  </tbody></table><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <tbody>    <tr>      <th>Concordance</th>      <td>0.83</td>    </tr>    <tr>      <th>Log-likelihood ratio test</th>      <td>97.63 on 20 df, -log2(p)=38.13</td>    </tr>  </tbody></table></div><p><strong>Question:</strong></p><ul><li>According to the model, was treatment <code>trt</code> beneficial? </li><li>What was its associated hazard ratio? <ul><li>Note that the hazard ratio is how much an incremental increase in the feature variable changes the hazard.</li></ul></li></ul><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Check your answer!</b></font></summary></p><p><ul><ul>    <li>You should see that the treatment (trt) was beneficial because it has a negative impact on the hazard (the coefficient is negative, and exp(coef) is less than 1).</li>    <li>The associated hazard ratio is ~0.8, because this is the exp(coef) of treatment.</li></ul></ul></p><p>We can compare the predicted survival curves for treatment variables. Run the next cell to plot survival curves using the <code>plot_covariate_groups()</code> function. </p><ul><li>The y-axis is th survival rate</li><li>The x-axis is time</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cph.plot_covariate_groups(<span class="string">'trt'</span>, values=[<span class="number">0</span>, <span class="number">1</span>]);</span><br></pre></td></tr></table></figure><p>Notice how the group without treatment has a lower survival rate at all times (the x-axis is time) compared to the treatment group.</p><p><a name="6"></a></p><h2 id="6-Hazard-Ratio"><a href="#6-Hazard-Ratio" class="headerlink" title="6. Hazard Ratio"></a>6. Hazard Ratio</h2><p>Recall from the lecture videos that the Hazard Ratio between two patients was the likelihood of one patient (e.g smoker) being more at risk than the other (e.g non-smoker).</p><script type="math/tex; mode=display">\frac{\lambda_{smoker}(t)}{\lambda_{nonsmoker}(t)} = e^{\theta (X_{smoker} - X_{nonsmoker})^T}</script><p>Where</p><script type="math/tex; mode=display">\lambda_{smoker}(t) = \lambda_0(t)e^{\theta X_{smoker}^T}</script><p>and</p><script type="math/tex; mode=display">\lambda_{nonsmoker}(t) = \lambda_0(t)e^{\theta X_{nonsmoker}^T} \\</script><p><a name="Ex-2"></a></p><h3 id="Exercise-2"><a href="#Exercise-2" class="headerlink" title="Exercise 2"></a>Exercise 2</h3><p>In the cell below, write a function to compute the hazard ratio between two individuals given the model’s coefficients.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>use numpy.dot</li>    <li>use nump.exp</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hazard_ratio</span><span class="params">(case_1, case_2, cox_params)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return the hazard ratio of case_1 : case_2 using</span></span><br><span class="line"><span class="string">    the coefficients of the cox model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        case_1 (np.array): (1 x d) array of covariates</span></span><br><span class="line"><span class="string">        case_2 (np.array): (1 x d) array of covariates</span></span><br><span class="line"><span class="string">        model (np.array): (1 x d) array of cox model coefficients</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        hazard_ratio (float): hazard ratio of case_1 : case_2</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    hr = np.exp(np.dot(cox_params,(case_1 - case_2).T))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> hr</span><br></pre></td></tr></table></figure><p>Now, evaluate it on the following pair of indivduals: <code>i = 1</code> and <code>j = 5</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">i = <span class="number">1</span></span><br><span class="line">case_1 = one_hot_train.iloc[i, :].drop([<span class="string">'time'</span>, <span class="string">'status'</span>])</span><br><span class="line"></span><br><span class="line">j = <span class="number">5</span></span><br><span class="line">case_2 = one_hot_train.iloc[j, :].drop([<span class="string">'time'</span>, <span class="string">'status'</span>])</span><br><span class="line"></span><br><span class="line">print(hazard_ratio(case_1.values, case_2.values, cph.params_.values))</span><br></pre></td></tr></table></figure><pre><code>15.029017732492221</code></pre><h4 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output:"></a>Expected Output:</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15.029017732492221</span></span><br></pre></td></tr></table></figure><p><strong>Question:</strong> </p><p>Is <code>case_1</code> or <code>case_2</code> at greater risk? </p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Check your answer!</b></font></summary></p><p><ul><ul>    <li>You should see that `case_1` is at higher risk.</li>    <li>The hazard ratio of case 1 / case 2 is greater than 1, so case 1 had a higher hazard relative to case 2</li></ul></ul></p><p>Inspect different pairs, and see if you can figure out which patient is more at risk.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">i = <span class="number">4</span></span><br><span class="line">case_a = one_hot_train.iloc[i, :].drop([<span class="string">'time'</span>, <span class="string">'status'</span>])</span><br><span class="line"></span><br><span class="line">j = <span class="number">7</span></span><br><span class="line">case_b = one_hot_train.iloc[j, :].drop([<span class="string">'time'</span>, <span class="string">'status'</span>])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Case A\n\n"</span>, case_a, <span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">"Case B\n\n"</span>, case_b, <span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">"Hazard Ratio:"</span>, hazard_ratio(case_a.values, case_b.values, cph.params_.values))</span><br></pre></td></tr></table></figure><pre><code>Case A trt          0.000000age          0.563645sex          0.000000ascites      0.000000hepato       1.000000spiders      1.000000bili        -0.405645chol        -0.210436albumin      1.514297copper      -0.481961alk.phos     2.173526ast         -0.144699trig        -0.531125platelet    -0.450972protime     -0.139881edema_0.5    0.000000edema_1.0    0.000000stage_2.0    0.000000stage_3.0    1.000000stage_4.0    0.000000Name: 1, dtype: float64 Case B trt          0.000000age          0.463447sex          0.000000ascites      0.000000hepato       1.000000spiders      0.000000bili        -0.489581chol        -0.309875albumin     -1.232371copper      -0.504348alk.phos     2.870427ast         -0.936261trig        -0.150229platelet     3.190823protime     -0.139881edema_0.5    0.000000edema_1.0    0.000000stage_2.0    0.000000stage_3.0    0.000000stage_4.0    1.000000Name: 38, dtype: float64 Hazard Ratio: 0.1780450006997129</code></pre><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Check your answer!</b></font></summary></p><p><ul><ul>    <li>You should see that `case_2` is at higher risk.</li>    <li>The hazard ratio of case 1 / case 2 is less than 1, so case 2 had a higher hazard relative to case 1</li></ul></ul></p><p><a name="7"></a></p><h2 id="7-Harrell’s-C-index"><a href="#7-Harrell’s-C-index" class="headerlink" title="7. Harrell’s C-index"></a>7. Harrell’s C-index</h2><p>To evaluate how good our model is performing, we will write our own version of the C-index. Similar to the week 1 case, C-index in the survival context is the probability that, given a randomly selected pair of individuals, the one who died sooner has a higher risk score. </p><p>However, we need to take into account censoring. Imagine a pair of patients, $A$ and $B$. </p><h4 id="Scenario-1"><a href="#Scenario-1" class="headerlink" title="Scenario 1"></a>Scenario 1</h4><ul><li>A was censored at time $t_A$ </li><li>B died at $t_B$</li><li>$t_A &lt; t_B$. </li></ul><p>Because of censoring, we can’t say whether $A$ or $B$ should have a higher risk score. </p><h4 id="Scenario-2"><a href="#Scenario-2" class="headerlink" title="Scenario 2"></a>Scenario 2</h4><p>Now imagine that $t_A &gt; t_B$.</p><ul><li>A was censored at time $t_A$ </li><li>B died at $t_B$</li><li>$t_A &gt; t_B$</li></ul><p>Now we can definitively say that $B$ should have a higher risk score than $A$, since we know for a fact that $A$ lived longer. </p><p>Therefore, when we compute our C-index</p><ul><li>We should only consider pairs where at most one person is censored</li><li>If they are censored, then their censored time should occur <em>after</em> the other person’s time of death. </li></ul><p>The metric we get if we use this rule is called <strong>Harrel’s C-index</strong>.</p><p>Note that in this case, being censored at time $t$ means that the true death time was some time AFTER time $t$ and not at $t$. </p><ul><li>Therefore if $t_A = t_B$ and A was censored:<ul><li>Then $A$ actually lived longer than $B$. </li><li>This will effect how you deal with ties in the exercise below!</li></ul></li></ul><p><a name="Ex-3"></a></p><h3 id="Exercise-3"><a href="#Exercise-3" class="headerlink" title="Exercise 3"></a>Exercise 3</h3><p>Fill in the function below to compute Harrel’s C-index.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>If you get a division by zero error, consider checking how you count when a pair is permissible (in the case where one patient is censored and the other is not censored).</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">harrell_c</span><span class="params">(y_true, scores, event)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Compute Harrel C-index given true event/censoring times,</span></span><br><span class="line"><span class="string">    model output, and event indicators.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y_true (array): array of true event times</span></span><br><span class="line"><span class="string">        scores (array): model risk scores</span></span><br><span class="line"><span class="string">        event (array): indicator, 1 if event occurred at that index, 0 for censorship</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        result (float): C-index metric</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    n = len(y_true)</span><br><span class="line">    <span class="keyword">assert</span> (len(scores) == n <span class="keyword">and</span> len(event) == n)</span><br><span class="line">    </span><br><span class="line">    concordant = <span class="number">0.0</span></span><br><span class="line">    permissible = <span class="number">0.0</span></span><br><span class="line">    ties = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    result = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' and 'pass' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># use double for loop to go through cases</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="comment"># set lower bound on j to avoid double counting</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, n):</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># check if at most one is censored</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> (event[i] == <span class="number">0</span> <span class="keyword">and</span> event[j] == <span class="number">0</span>):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># check if neither are censored</span></span><br><span class="line">                <span class="keyword">if</span> event[i] == <span class="number">1</span> <span class="keyword">and</span> event[j] == <span class="number">1</span>:</span><br><span class="line">                    permissible += <span class="number">1</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># check if scores are tied</span></span><br><span class="line">                    <span class="keyword">if</span> y_true[i] == y_true[j]:</span><br><span class="line">                        ties += <span class="number">1</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># check for concordant</span></span><br><span class="line">                    <span class="keyword">elif</span> y_true[i] &gt; y_true[j] <span class="keyword">and</span> scores[i] &lt; scores[j]:</span><br><span class="line">                        concordant += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">elif</span> y_true[i] &lt; y_true[j] <span class="keyword">and</span> scores[i] &gt; scores[j]:</span><br><span class="line">                        concordant += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># check if one is censored</span></span><br><span class="line">                <span class="keyword">elif</span> event[i] == <span class="number">0</span> <span class="keyword">or</span> event[j] == <span class="number">0</span>:</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># get censored index</span></span><br><span class="line">                    censored = j</span><br><span class="line">                    uncensored = i</span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">if</span> event[i] == <span class="number">0</span>:</span><br><span class="line">                        censored = i</span><br><span class="line">                        uncensored = j</span><br><span class="line">                        </span><br><span class="line">                    <span class="comment"># check if permissible</span></span><br><span class="line">                    <span class="comment"># Note: in this case, we are assuming that censored at a time</span></span><br><span class="line">                    <span class="comment"># means that you did NOT die at that time. That is, if you</span></span><br><span class="line">                    <span class="comment"># live until time 30 and have event = 0, then you lived THROUGH</span></span><br><span class="line">                    <span class="comment"># time 30.</span></span><br><span class="line">                    <span class="keyword">if</span> y_true[censored] &gt;= y_true[uncensored]:</span><br><span class="line">                        permissible += <span class="number">1</span></span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># check if scores are tied</span></span><br><span class="line">                        <span class="keyword">if</span> scores[i] == scores[j]:</span><br><span class="line">                            <span class="comment"># update ties </span></span><br><span class="line">                            ties += <span class="number">1</span></span><br><span class="line">                            </span><br><span class="line">                        <span class="comment"># check if scores are concordant </span></span><br><span class="line">                        <span class="keyword">if</span> y_true[censored] &gt;= y_true[uncensored] <span class="keyword">and</span> scores[censored] &lt; scores[uncensored]:</span><br><span class="line">                            concordant += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># set result to c-index computed from number of concordant pairs,</span></span><br><span class="line">    <span class="comment"># number of ties, and number of permissible pairs (REPLACE 0 with your code) </span></span><br><span class="line">    result = (concordant + <span class="number">0.5</span> * ties) / permissible</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><p>You can test your function on the following test cases:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">y_true = [<span class="number">30</span>, <span class="number">12</span>, <span class="number">84</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 1</span></span><br><span class="line">event = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">scores = [<span class="number">0.5</span>, <span class="number">0.9</span>, <span class="number">0.1</span>, <span class="number">1.0</span>]</span><br><span class="line">print(<span class="string">"Case 1"</span>)</span><br><span class="line">print(<span class="string">"Expected: 1.0, Output: &#123;&#125;"</span>.format(harrell_c(y_true, scores, event)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 2</span></span><br><span class="line">scores = [<span class="number">0.9</span>, <span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">0.1</span>]</span><br><span class="line">print(<span class="string">"\nCase 2"</span>)</span><br><span class="line">print(<span class="string">"Expected: 0.0, Output: &#123;&#125;"</span>.format(harrell_c(y_true, scores, event)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 3</span></span><br><span class="line">event = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">scores = [<span class="number">0.5</span>, <span class="number">0.9</span>, <span class="number">0.1</span>, <span class="number">1.0</span>]</span><br><span class="line">print(<span class="string">"\nCase 3"</span>)</span><br><span class="line">print(<span class="string">"Expected: 1.0, Output: &#123;&#125;"</span>.format(harrell_c(y_true, scores, event)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 4</span></span><br><span class="line">y_true = [<span class="number">30</span>, <span class="number">30</span>, <span class="number">20</span>, <span class="number">20</span>]</span><br><span class="line">event = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">scores = [<span class="number">10</span>, <span class="number">5</span>, <span class="number">15</span>, <span class="number">20</span>]</span><br><span class="line">print(<span class="string">"\nCase 4"</span>)</span><br><span class="line">print(<span class="string">"Expected: 0.75, Output: &#123;&#125;"</span>.format(harrell_c(y_true, scores, event)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 5</span></span><br><span class="line">y_true = list(reversed([<span class="number">30</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">20</span>, <span class="number">20</span>]))</span><br><span class="line">event = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">scores = list(reversed([<span class="number">15</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">15</span>, <span class="number">20</span>]))</span><br><span class="line">print(<span class="string">"\nCase 5"</span>)</span><br><span class="line">print(<span class="string">"Expected: 0.583, Output: &#123;&#125;"</span>.format(harrell_c(y_true, scores, event)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 6</span></span><br><span class="line">y_true = [<span class="number">10</span>,<span class="number">10</span>]</span><br><span class="line">event = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">scores = [<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">print(<span class="string">"\nCase 6"</span>)</span><br><span class="line">print(<span class="string">f"Expected: 1.0 , Output:<span class="subst">&#123;harrell_c(y_true, scores, event):<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Case 1Expected: 1.0, Output: 1.0Case 2Expected: 0.0, Output: 0.0Case 3Expected: 1.0, Output: 1.0Case 4Expected: 0.75, Output: 0.75Case 5Expected: 0.583, Output: 0.5833333333333334Case 6Expected: 1.0 , Output:1.0000</code></pre><p>Now use the Harrell’s C-index function to evaluate the cox model on our data sets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train</span></span><br><span class="line">scores = cph.predict_partial_hazard(one_hot_train)</span><br><span class="line">cox_train_scores = harrell_c(one_hot_train[<span class="string">'time'</span>].values, scores.values, one_hot_train[<span class="string">'status'</span>].values)</span><br><span class="line"><span class="comment"># Validation</span></span><br><span class="line">scores = cph.predict_partial_hazard(one_hot_val)</span><br><span class="line">cox_val_scores = harrell_c(one_hot_val[<span class="string">'time'</span>].values, scores.values, one_hot_val[<span class="string">'status'</span>].values)</span><br><span class="line"><span class="comment"># Test</span></span><br><span class="line">scores = cph.predict_partial_hazard(one_hot_test)</span><br><span class="line">cox_test_scores = harrell_c(one_hot_test[<span class="string">'time'</span>].values, scores.values, one_hot_test[<span class="string">'status'</span>].values)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Train:"</span>, cox_train_scores)</span><br><span class="line">print(<span class="string">"Val:"</span>, cox_val_scores)</span><br><span class="line">print(<span class="string">"Test:"</span>, cox_test_scores)</span><br></pre></td></tr></table></figure><pre><code>Train: 0.8265139116202946Val: 0.8544776119402985Test: 0.8478543563068921</code></pre><p>What do these values tell us ?</p><p><a name="8"></a></p><h2 id="8-Random-Survival-Forests"><a href="#8-Random-Survival-Forests" class="headerlink" title="8. Random Survival Forests"></a>8. Random Survival Forests</h2><p>This performed well, but you have a hunch you can squeeze out better performance by using a machine learning approach. You decide to use a Random Survival Forest. To do this, you can use the <code>RandomForestSRC</code> package in R. To call R function from Python, we’ll use the <code>r2py</code> package. Run the following cell to import the necessary requirements. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">%load_ext rpy2.ipython</span><br><span class="line">%R require(ggplot2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rpy2.robjects.packages <span class="keyword">import</span> importr</span><br><span class="line"><span class="comment"># import R's "base" package</span></span><br><span class="line">base = importr(<span class="string">'base'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># import R's "utils" package</span></span><br><span class="line">utils = importr(<span class="string">'utils'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># import rpy2's package module</span></span><br><span class="line"><span class="keyword">import</span> rpy2.robjects.packages <span class="keyword">as</span> rpackages</span><br><span class="line"></span><br><span class="line">forest = rpackages.importr(<span class="string">'randomForestSRC'</span>, lib_loc=<span class="string">'R'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rpy2 <span class="keyword">import</span> robjects <span class="keyword">as</span> ro</span><br><span class="line">R = ro.r</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rpy2.robjects <span class="keyword">import</span> pandas2ri</span><br><span class="line">pandas2ri.activate()</span><br></pre></td></tr></table></figure><pre><code>R[write to console]: Loading required package: ggplot2</code></pre><p>Instead of encoding our categories as binary features, we can use the original dataframe since trees deal well with raw categorical data (can you think why this might be?).</p><p>Run the code cell below to build your forest.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = forest.rfsrc(ro.Formula(<span class="string">'Surv(time, status) ~ .'</span>), data=df_train, ntree=<span class="number">300</span>, nodedepth=<span class="number">5</span>, seed=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(model)</span><br></pre></td></tr></table></figure><pre><code>                         Sample size: 154                    Number of deaths: 64                     Number of trees: 300           Forest terminal node size: 15       Average no. of terminal nodes: 6.54No. of variables tried at each split: 5              Total no. of variables: 17       Resampling used to grow trees: swor    Resample size used to grow trees: 97                            Analysis: RSF                              Family: surv                      Splitting rule: logrank *random*       Number of random split points: 10                          Error rate: 19.07%</code></pre><p>Finally, let’s evaluate on our validation and test sets, and compare it with our Cox model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">result = R.predict(model, newdata=df_val)</span><br><span class="line">scores = np.array(result.rx(<span class="string">'predicted'</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Cox Model Validation Score:"</span>, cox_val_scores)</span><br><span class="line">print(<span class="string">"Survival Forest Validation Score:"</span>, harrell_c(df_val[<span class="string">'time'</span>].values, scores, df_val[<span class="string">'status'</span>].values))</span><br></pre></td></tr></table></figure><pre><code>Cox Model Validation Score: 0.8544776119402985Survival Forest Validation Score: 0.8296019900497512</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">result = R.predict(model, newdata=df_test)</span><br><span class="line">scores = np.array(result.rx(<span class="string">'predicted'</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Cox Model Test Score:"</span>, cox_test_scores)</span><br><span class="line">print(<span class="string">"Survival Forest Validation Score:"</span>, harrell_c(df_test[<span class="string">'time'</span>].values, scores, df_test[<span class="string">'status'</span>].values))</span><br></pre></td></tr></table></figure><pre><code>Cox Model Test Score: 0.8478543563068921Survival Forest Validation Score: 0.8621586475942783</code></pre><p>Your random forest model should be outperforming the Cox model slightly. Let’s dig deeper to see how they differ.</p><p><a name="9"></a></p><h2 id="9-Permutation-Method-for-Interpretation"><a href="#9-Permutation-Method-for-Interpretation" class="headerlink" title="9. Permutation Method for Interpretation"></a>9. Permutation Method for Interpretation</h2><p>We’ll dig a bit deeper into interpretation methods for forests a bit later, but for now just know that random surival forests come with their own built in variable importance feature. The method is referred to as VIMP, and for the purpose of this section you should just know that higher absolute value of the VIMP means that the variable generally has a larger effect on the model outcome.</p><p>Run the next cell to compute and plot VIMP for the random survival forest.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vimps = np.array(forest.vimp(model).rx(<span class="string">'importance'</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">y = np.arange(len(vimps))</span><br><span class="line">plt.barh(y, np.abs(vimps))</span><br><span class="line">plt.yticks(y, df_train.drop([<span class="string">'time'</span>, <span class="string">'status'</span>], axis=<span class="number">1</span>).columns)</span><br><span class="line">plt.title(<span class="string">"VIMP (absolute value)"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_67_0.png" alt="png"></p><h3 id="Question"><a href="#Question" class="headerlink" title="Question:"></a>Question:</h3><p>How does the variable importance compare to that of the Cox model? Which variable is important in both models? Which variable is important in the random survival forest but not in the Cox model? You should see that <code>edema</code> is important in both the random survival forest and the Cox model. You should also see that <code>bili</code> is important in the random survival forest but not the Cox model .</p><h1 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations!"></a>Congratulations!</h1><p>You’ve finished the last assignment in course 2! Take a minute to look back at the analysis you’ve done over the last four assignments. You’ve done a great job!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Cox-Proportional-Hazards-and-Random-Survival-Forests&quot;&gt;&lt;a href=&quot;#Cox-Proportional-Hazards-and-Random-Survival-Forests&quot; class=&quot;headerl
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Machine-Learning/"/>
    
    
      <category term="Medicine" scheme="https://zhangruochi.com/tags/Medicine/"/>
    
  </entry>
  
  <entry>
    <title>Survival Estimates that Vary with Time</title>
    <link href="https://zhangruochi.com/Survival-Estimates-that-Vary-with-Time/2020/04/18/"/>
    <id>https://zhangruochi.com/Survival-Estimates-that-Vary-with-Time/2020/04/18/</id>
    <published>2020-04-18T15:54:54.000Z</published>
    <updated>2020-04-19T15:09:46.620Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Survival-Estimates-that-Vary-with-Time"><a href="#Survival-Estimates-that-Vary-with-Time" class="headerlink" title="Survival Estimates that Vary with Time"></a>Survival Estimates that Vary with Time</h1><p>Welcome to the third assignment of Course 2. In this assignment, we’ll use Python to build some of the statistical models we learned this past week to analyze surivival estimates for a dataset of lymphoma patients. We’ll also evaluate these models and interpret their outputs. Along the way, you will be learning about the following: </p><ul><li>Censored Data</li><li>Kaplan-Meier Estimates</li><li>Subgroup Analysis</li></ul><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#1">1. Import Packages</a></li><li><a href="#2">2. Load the Dataset</a></li><li><a href="#">3. Censored Data</a><ul><li><a href="#Ex-1">Exercise 1</a></li></ul></li><li><a href="#4">4. Survival Estimates</a><ul><li><a href="#Ex-2">Exercise 2</a></li><li><a href="#Ex-3">Exercise 3</a></li></ul></li><li><a href="#5">5. Subgroup Analysis</a><ul><li><a href="#5-1">5.1 Bonus: Log Rank Test</a></li></ul></li></ul><p><a name="1"></a></p><h2 id="1-Import-Packages"><a href="#1-Import-Packages" class="headerlink" title="1. Import Packages"></a>1. Import Packages</h2><p>We’ll first import all the packages that we need for this assignment. </p><ul><li><code>lifelines</code> is an open-source library for data analysis.</li><li><code>numpy</code> is the fundamental package for scientific computing in python.</li><li><code>pandas</code> is what we’ll use to manipulate our data.</li><li><code>matplotlib</code> is a plotting library.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lifelines</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> util <span class="keyword">import</span> load_data</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> lifelines <span class="keyword">import</span> KaplanMeierFitter <span class="keyword">as</span> KM</span><br><span class="line"><span class="keyword">from</span> lifelines.statistics <span class="keyword">import</span> logrank_test</span><br></pre></td></tr></table></figure><p><a name="2"></a></p><h2 id="2-Load-the-Dataset"><a href="#2-Load-the-Dataset" class="headerlink" title="2. Load the Dataset"></a>2. Load the Dataset</h2><p>Run the next cell to load the lymphoma data set. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = load_data()</span><br></pre></td></tr></table></figure><p>As always, you first look over your data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"data shape: &#123;&#125;"</span>.format(data.shape))</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><pre><code>data shape: (80, 3)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Stage_group</th>      <th>Time</th>      <th>Event</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>6</td>      <td>1</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>19</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>1</td>      <td>32</td>      <td>1</td>    </tr>    <tr>      <th>3</th>      <td>1</td>      <td>42</td>      <td>1</td>    </tr>    <tr>      <th>4</th>      <td>1</td>      <td>42</td>      <td>1</td>    </tr>  </tbody></table></div><p>The column <code>Time</code> states how long the patient lived before they died or were censored.</p><p>The column <code>Event</code> says whether a death was observed or not. <code>Event</code> is 1 if the event is observed (i.e. the patient died) and 0 if data was censored.</p><p>Censorship here means that the observation has ended without any observed event.<br>For example, let a patient be in a hospital for 100 days at most. If a patient dies after only 44 days, their event will be recorded as <code>Time = 44</code> and <code>Event = 1</code>. If a patient walks out after 100 days and dies 3 days later (103 days total), this event is not observed in our process and the corresponding row has <code>Time = 100</code> and <code>Event = 0</code>. If a patient survives for 25 years after being admitted, their data for are still <code>Time = 100</code> and <code>Event = 0</code>.</p><p><a name="3"></a></p><h2 id="3-Censored-Data"><a href="#3-Censored-Data" class="headerlink" title="3. Censored Data"></a>3. Censored Data</h2><p>We can plot a histogram of the survival times to see in general how long cases survived before censorship or events.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data.Time.hist();</span><br><span class="line">plt.xlabel(<span class="string">"Observation time before death or censorship (days)"</span>);</span><br><span class="line">plt.ylabel(<span class="string">"Frequency (number of patients)"</span>);</span><br><span class="line"><span class="comment"># Note that the semicolon at the end of the plotting line</span></span><br><span class="line"><span class="comment"># silences unnecessary textual output - try removing it</span></span><br><span class="line"><span class="comment"># to observe its effect</span></span><br></pre></td></tr></table></figure><p><img src="output_11_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">"Event"</span>].unique()</span><br></pre></td></tr></table></figure><pre><code>array([1, 0])</code></pre><p><a name="Ex-1"></a></p><h3 id="Exercise-1"><a href="#Exercise-1" class="headerlink" title="Exercise 1"></a>Exercise 1</h3><p>In the next cell, write a function to compute the fraction ($\in [0, 1]$) of observations which were censored. </p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Summing up the <code>'Event'</code> column will give you the number of observations where censorship has NOT occurred.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">frac_censored</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return percent of observations which were censored.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        df (dataframe): dataframe which contains column 'Event' which is </span></span><br><span class="line"><span class="string">                        1 if an event occurred (death)</span></span><br><span class="line"><span class="string">                        0 if the event did not occur (censored)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        frac_censored (float): fraction of cases which were censored. </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    result = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    result = <span class="number">1</span>- df[<span class="string">'Event'</span>].sum(axis = <span class="number">0</span>) / df.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(frac_censored(data))</span><br></pre></td></tr></table></figure><pre><code>0.32499999999999996</code></pre><h4 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output:"></a>Expected Output:</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.325</span></span><br></pre></td></tr></table></figure><p>Run the next cell to see the distributions of survival times for censored and uncensored examples.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">df_censored = data[data.Event == <span class="number">0</span>]</span><br><span class="line">df_uncensored = data[data.Event == <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">df_censored.Time.hist()</span><br><span class="line">plt.title(<span class="string">"Censored"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Time (days)"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Frequency"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">df_uncensored.Time.hist()</span><br><span class="line">plt.title(<span class="string">"Uncensored"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Time (days)"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Frequency"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_19_0.png" alt="png"></p><p><img src="output_19_1.png" alt="png"></p><p><a name="4"></a></p><h2 id="4-Survival-Estimates"><a href="#4-Survival-Estimates" class="headerlink" title="4. Survival Estimates"></a>4. Survival Estimates</h2><p>We’ll now try to estimate the survival function:</p><script type="math/tex; mode=display">S(t) = P(T > t)</script><p>To illustrate the strengths of Kaplan Meier, we’ll start with a naive estimator of the above survival function. To estimate this quantity, we’ll divide the number of people who we know lived past time $t$ by the number of people who were not censored before $t$.</p><p>Formally, let $i$ = 1, …, $n$ be the cases, and let $t_i$ be the time when $i$ was censored or an event happened. Let $e_i= 1$ if an event was observed for $i$ and 0 otherwise. Then let $X_t = \{i : T_i &gt; t\}$, and let $M_t = \{i : e_i = 1 \text{ or } T_i &gt; t\}$. The estimator you will compute will be:</p><script type="math/tex; mode=display">\hat{S}(t) = \frac{|X_t|}{|M_t|}</script><p><a name="Ex-2"></a></p><h3 id="Exercise-2"><a href="#Exercise-2" class="headerlink" title="Exercise 2"></a>Exercise 2</h3><p>Write a function to compute this estimate for arbitrary $t$ in the cell below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_estimator</span><span class="params">(t, df)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return naive estimate for S(t), the probability</span></span><br><span class="line"><span class="string">    of surviving past time t. Given by number</span></span><br><span class="line"><span class="string">    of cases who survived past time t divided by the</span></span><br><span class="line"><span class="string">    number of cases who weren't censored before time t.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        t (int): query time</span></span><br><span class="line"><span class="string">        df (dataframe): survival data. Has a Time column,</span></span><br><span class="line"><span class="string">                        which says how long until that case</span></span><br><span class="line"><span class="string">                        experienced an event or was censored,</span></span><br><span class="line"><span class="string">                        and an Event column, which is 1 if an event</span></span><br><span class="line"><span class="string">                        was observed and 0 otherwise.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        S_t (float): estimator for survival function evaluated at t.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    S_t = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    S_t = df[df[<span class="string">'Time'</span>] &gt; t].shape[<span class="number">0</span>] / df[ (df[<span class="string">'Event'</span>] == <span class="number">1</span>) | (df[<span class="string">'Time'</span>] &gt; t) ].shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> S_t</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Test Cases"</span>)</span><br><span class="line"></span><br><span class="line">sample_df = pd.DataFrame(columns = [<span class="string">"Time"</span>, <span class="string">"Event"</span>])</span><br><span class="line">sample_df.Time = [<span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>]</span><br><span class="line">sample_df.Event = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">print(<span class="string">"Sample dataframe for testing code:"</span>)</span><br><span class="line">print(sample_df)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test Case 1: S(3)"</span>)</span><br><span class="line">print(<span class="string">"Output: &#123;&#125;, Expected: &#123;&#125;\n"</span>.format(naive_estimator(<span class="number">3</span>, sample_df), <span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test Case 2: S(12)"</span>)</span><br><span class="line">print(<span class="string">"Output: &#123;&#125;, Expected: &#123;&#125;\n"</span>.format(naive_estimator(<span class="number">12</span>, sample_df), <span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test Case 3: S(20)"</span>)</span><br><span class="line">print(<span class="string">"Output: &#123;&#125;, Expected: &#123;&#125;\n"</span>.format(naive_estimator(<span class="number">20</span>, sample_df), <span class="number">0.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test case 4</span></span><br><span class="line">sample_df = pd.DataFrame(&#123;<span class="string">'Time'</span>: [<span class="number">5</span>,<span class="number">5</span>,<span class="number">10</span>],</span><br><span class="line">                          <span class="string">'Event'</span>: [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line">                         &#125;)</span><br><span class="line">print(<span class="string">"Test case 4: S(5)"</span>)</span><br><span class="line">print(<span class="string">f"Output: <span class="subst">&#123;naive_estimator(<span class="number">5</span>, sample_df)&#125;</span>, Expected: 0.5"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test CasesSample dataframe for testing code:   Time  Event0     5      01    10      12    15      0Test Case 1: S(3)Output: 1.0, Expected: 1.0Test Case 2: S(12)Output: 0.5, Expected: 0.5Test Case 3: S(20)Output: 0.0, Expected: 0.0Test case 4: S(5)Output: 0.5, Expected: 0.5</code></pre><p>In the next cell, we will plot the naive estimator using the real data up to the maximum time in the dataset. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">max_time = data.Time.max()</span><br><span class="line">x = range(<span class="number">0</span>, max_time+<span class="number">1</span>)</span><br><span class="line">y = np.zeros(len(x))</span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(x):</span><br><span class="line">    y[i] = naive_estimator(t, data)</span><br><span class="line">    </span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.title(<span class="string">"Naive Survival Estimate"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Time"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Estimated cumulative survival rate"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_25_0.png" alt="png"></p><p><a name="Ex-3"></a></p><h3 id="Exercise-3"><a href="#Exercise-3" class="headerlink" title="Exercise 3"></a>Exercise 3</h3><p>Next let’s compare this with the Kaplan Meier estimate. In the cell below, write a function that computes the Kaplan Meier estimate of $S(t)$ at every distinct time in the dataset. </p><p>Recall the Kaplan-Meier estimate:</p><script type="math/tex; mode=display">S(t) = \prod_{t_i \leq t} (1 - \frac{d_i}{n_i})</script><p>where $t_i$ are the events observed in the dataset and $d_i$ is the number of deaths at time $t_i$ and $n_i$ is the number of people who we know have survived up to time $t_i$.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Try sorting by Time.</li>    <li>Use <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unique.html" target="_blank" rel="noopener">pandas.Series.unique<a> </a></a></li>    <li>If you get a division by zero error, please double-check how you calculated `n_t`</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">HomemadeKM</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return KM estimate evaluated at every distinct</span></span><br><span class="line"><span class="string">    time (event or censored) recorded in the dataset.</span></span><br><span class="line"><span class="string">    Event times and probabilities should begin with</span></span><br><span class="line"><span class="string">    time 0 and probability 1.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    input: </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">         Time  Censor</span></span><br><span class="line"><span class="string">    0     5       0</span></span><br><span class="line"><span class="string">    1    10       1</span></span><br><span class="line"><span class="string">    2    15       0</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    correct output: </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    event_times: [0, 5, 10, 15]</span></span><br><span class="line"><span class="string">    S: [1.0, 1.0, 0.5, 0.5]</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        df (dataframe): dataframe which has columns for Time</span></span><br><span class="line"><span class="string">                          and Event, defined as usual.</span></span><br><span class="line"><span class="string">                          </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        event_times (list of ints): array of unique event times</span></span><br><span class="line"><span class="string">                                      (begins with 0).</span></span><br><span class="line"><span class="string">        S (list of floats): array of survival probabilites, so that</span></span><br><span class="line"><span class="string">                            S[i] = P(T &gt; event_times[i]). This </span></span><br><span class="line"><span class="string">                            begins with 1.0 (since no one dies at time</span></span><br><span class="line"><span class="string">                            0).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># individuals are considered to have survival probability 1</span></span><br><span class="line">    <span class="comment"># at time 0</span></span><br><span class="line">    event_times = [<span class="number">0</span>]</span><br><span class="line">    p = <span class="number">1.0</span></span><br><span class="line">    S = [p]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get collection of unique observed event times</span></span><br><span class="line">    observed_event_times = df[<span class="string">'Time'</span>].unique().tolist()</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># sort event times</span></span><br><span class="line">    observed_event_times = sorted(observed_event_times)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># iterate through event times</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> observed_event_times:</span><br><span class="line">  </span><br><span class="line">        <span class="comment"># compute n_t, number of people who survive to time t</span></span><br><span class="line">        n_t = df[df[<span class="string">'Time'</span>] &gt;= t].shape[<span class="number">0</span>]</span><br><span class="line">  </span><br><span class="line">        <span class="comment"># compute d_t, number of people who die at time t</span></span><br><span class="line">        d_t = df[(df[<span class="string">'Time'</span>] == t) &amp; (df[<span class="string">'Event'</span>] == <span class="number">1</span>)].shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># update p</span></span><br><span class="line">        p = p * ( <span class="number">1</span> - d_t / n_t)</span><br><span class="line">  </span><br><span class="line">        <span class="comment"># update S and event_times (ADD code below)</span></span><br><span class="line">        <span class="comment"># hint: use append</span></span><br><span class="line">        S.append(p)</span><br><span class="line">        event_times.append(t)</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> event_times, S</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"TEST CASES:\n"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test Case 1\n"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test DataFrame:"</span>)</span><br><span class="line">sample_df = pd.DataFrame(columns = [<span class="string">"Time"</span>, <span class="string">"Event"</span>])</span><br><span class="line">sample_df.Time = [<span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>]</span><br><span class="line">sample_df.Event = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">print(sample_df.head())</span><br><span class="line">print(<span class="string">"\nOutput:"</span>)</span><br><span class="line">x, y = HomemadeKM(sample_df)</span><br><span class="line">print(<span class="string">"Event times: &#123;&#125;, Survival Probabilities: &#123;&#125;"</span>.format(x, y))</span><br><span class="line">print(<span class="string">"\nExpected:"</span>)</span><br><span class="line">print(<span class="string">"Event times: [0, 5, 10, 15], Survival Probabilities: [1.0, 1.0, 0.5, 0.5]"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\nTest Case 2\n"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test DataFrame:"</span>)</span><br><span class="line"></span><br><span class="line">sample_df = pd.DataFrame(columns = [<span class="string">"Time"</span>, <span class="string">"Event"</span>])</span><br><span class="line">sample_df.loc[:, <span class="string">"Time"</span>] = [<span class="number">2</span>, <span class="number">15</span>, <span class="number">12</span>, <span class="number">10</span>, <span class="number">20</span>]</span><br><span class="line">sample_df.loc[:, <span class="string">"Event"</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">print(sample_df.head())</span><br><span class="line">print(<span class="string">"\nOutput:"</span>)</span><br><span class="line">x, y = HomemadeKM(sample_df)</span><br><span class="line">print(<span class="string">"Event times: &#123;&#125;, Survival Probabilities: &#123;&#125;"</span>.format(x, y))</span><br><span class="line">print(<span class="string">"\nExpected:"</span>)</span><br><span class="line">print(<span class="string">"Event times: [0, 2, 10, 12, 15, 20], Survival Probabilities: [1.0, 1.0, 0.75, 0.5, 0.5, 0.0]"</span>)</span><br></pre></td></tr></table></figure><pre><code>TEST CASES:Test Case 1Test DataFrame:   Time  Event0     5      01    10      12    15      0Output:Event times: [0, 5, 10, 15], Survival Probabilities: [1.0, 1.0, 0.5, 0.5]Expected:Event times: [0, 5, 10, 15], Survival Probabilities: [1.0, 1.0, 0.5, 0.5]Test Case 2Test DataFrame:   Time  Event0     2      01    15      02    12      13    10      14    20      1Output:Event times: [0, 2, 10, 12, 15, 20], Survival Probabilities: [1.0, 1.0, 0.75, 0.5, 0.5, 0.0]Expected:Event times: [0, 2, 10, 12, 15, 20], Survival Probabilities: [1.0, 1.0, 0.75, 0.5, 0.5, 0.0]</code></pre><p>Now let’s plot the two against each other on the data to see the difference.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">max_time = data.Time.max()</span><br><span class="line">x = range(<span class="number">0</span>, max_time+<span class="number">1</span>)</span><br><span class="line">y = np.zeros(len(x))</span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(x):</span><br><span class="line">    y[i] = naive_estimator(t, data)</span><br><span class="line">    </span><br><span class="line">plt.plot(x, y, label=<span class="string">"Naive"</span>)</span><br><span class="line"></span><br><span class="line">x, y = HomemadeKM(data)</span><br><span class="line">plt.step(x, y, label=<span class="string">"Kaplan-Meier"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Time"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Survival probability estimate"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_31_0.png" alt="png"></p><h3 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h3><p>What differences do you observe between the naive estimator and Kaplan-Meier estimator? Do any of our earlier explorations of the dataset help to explain these differences?</p><p><a name="5"></a></p><h2 id="5-Subgroup-Analysis"><a href="#5-Subgroup-Analysis" class="headerlink" title="5. Subgroup Analysis"></a>5. Subgroup Analysis</h2><p>We see that along with Time and Censor, we have a column called <code>Stage_group</code>. </p><ul><li>A value of 1 in this column denotes a patient with stage III cancer</li><li>A value of 2 denotes stage IV. </li></ul><p>We want to compare the survival functions of these two groups.</p><p>This time we’ll use the <code>KaplanMeierFitter</code> class from <code>lifelines</code>. Run the next cell to fit and plot the Kaplan Meier curves for each group. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">S1 = data[data.Stage_group == <span class="number">1</span>]</span><br><span class="line">km1 = KM()</span><br><span class="line">km1.fit(S1.loc[:, <span class="string">'Time'</span>], event_observed = S1.loc[:, <span class="string">'Event'</span>], label = <span class="string">'Stage III'</span>)</span><br><span class="line"></span><br><span class="line">S2 = data[data.Stage_group == <span class="number">2</span>]</span><br><span class="line">km2 = KM()</span><br><span class="line">km2.fit(S2.loc[:, <span class="string">"Time"</span>], event_observed = S2.loc[:, <span class="string">'Event'</span>], label = <span class="string">'Stage IV'</span>)</span><br><span class="line"></span><br><span class="line">ax = km1.plot(ci_show=<span class="keyword">False</span>)</span><br><span class="line">km2.plot(ax = ax, ci_show=<span class="keyword">False</span>)</span><br><span class="line">plt.xlabel(<span class="string">'time'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Survival probability estimate'</span>)</span><br><span class="line">plt.savefig(<span class="string">'two_km_curves'</span>, dpi=<span class="number">300</span>)</span><br></pre></td></tr></table></figure><p><img src="output_34_0.png" alt="png"></p><p>Let’s compare the survival functions at 90, 180, 270, and 360 days</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">survivals = pd.DataFrame([<span class="number">90</span>, <span class="number">180</span>, <span class="number">270</span>, <span class="number">360</span>], columns = [<span class="string">'time'</span>])</span><br><span class="line">survivals.loc[:, <span class="string">'Group 1'</span>] = km1.survival_function_at_times(survivals[<span class="string">'time'</span>]).values</span><br><span class="line">survivals.loc[:, <span class="string">'Group 2'</span>] = km2.survival_function_at_times(survivals[<span class="string">'time'</span>]).values</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">survivals</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>time</th>      <th>Group 1</th>      <th>Group 2</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>90</td>      <td>0.736842</td>      <td>0.424529</td>    </tr>    <tr>      <th>1</th>      <td>180</td>      <td>0.680162</td>      <td>0.254066</td>    </tr>    <tr>      <th>2</th>      <td>270</td>      <td>0.524696</td>      <td>0.195436</td>    </tr>    <tr>      <th>3</th>      <td>360</td>      <td>0.524696</td>      <td>0.195436</td>    </tr>  </tbody></table></div><p>This makes clear the difference in survival between the Stage III and IV cancer groups in the dataset. </p><p><a name="5-1"></a></p><h2 id="5-1-Bonus-Log-Rank-Test"><a href="#5-1-Bonus-Log-Rank-Test" class="headerlink" title="5.1 Bonus: Log-Rank Test"></a>5.1 Bonus: Log-Rank Test</h2><p>To say whether there is a statistical difference between the survival curves we can run the log-rank test. This test tells us the probability that we could observe this data if the two curves were the same. The derivation of the log-rank test is somewhat complicated, but luckily <code>lifelines</code> has a simple function to compute it. </p><p>Run the next cell to compute a p-value using <code>lifelines.statistics.logrank_test</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logrank_p_value</span><span class="params">(group_1_data, group_2_data)</span>:</span></span><br><span class="line">    result = logrank_test(group_1_data.Time, group_2_data.Time,</span><br><span class="line">                          group_1_data.Event, group_2_data.Event)</span><br><span class="line">    <span class="keyword">return</span> result.p_value</span><br><span class="line"></span><br><span class="line">logrank_p_value(S1, S2)</span><br></pre></td></tr></table></figure><pre><code>0.009588929834755544</code></pre><p>If everything is correct, you should see a p value of less than <code>0.05</code>, which indicates that the difference in the curves is indeed statistically significant.</p><h1 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations!"></a>Congratulations!</h1><p>You’ve completed the third assignment of Course 2. You’ve learned about the Kaplan Meier estimator, a fundamental non-parametric estimator in survival analysis. Next week we’ll learn how to take into account patient covariates in our survival estimates!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Survival-Estimates-that-Vary-with-Time&quot;&gt;&lt;a href=&quot;#Survival-Estimates-that-Vary-with-Time&quot; class=&quot;headerlink&quot; title=&quot;Survival Estimat
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Machine-Learning/"/>
    
    
      <category term="Medicine" scheme="https://zhangruochi.com/tags/Medicine/"/>
    
  </entry>
  
  <entry>
    <title>Risk Models Using Tree-based Models</title>
    <link href="https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/"/>
    <id>https://zhangruochi.com/Risk-Models-Using-Tree-based-Models/2020/04/18/</id>
    <published>2020-04-18T15:53:23.000Z</published>
    <updated>2020-04-19T15:09:51.850Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Risk-Models-Using-Tree-based-Models"><a href="#Risk-Models-Using-Tree-based-Models" class="headerlink" title="Risk Models Using Tree-based Models"></a>Risk Models Using Tree-based Models</h1><p>Welcome to the second assignment of Course 2!</p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#1">1. Import Packages</a></li><li><a href="#2">2. Load the Dataset</a></li><li><a href="#3">3. Explore the Dataset</a></li><li><a href="#4">4. Dealing with Missing Data</a><ul><li><a href="#Ex-1">Exercise 1</a></li></ul></li><li><a href="#5">5. Decision Trees</a><ul><li><a href="#Ex-2">Exercise 2</a></li></ul></li><li><a href="#6">6. Random Forests</a><ul><li><a href="#Ex-3">Exercise 3</a></li></ul></li><li><a href="#7">7. Imputation</a></li><li><a href="#8">8. Error Analysis</a><ul><li><a href="#Ex-4">Exercise 4</a></li></ul></li><li><a href="#Ex-9">9. Imputation Approaches</a><ul><li><a href="#Ex-5">Exercise 5</a></li><li><a href="#Ex-6">Exercise 6</a></li></ul></li><li><a href="#10">10. Comparison</a></li><li><a href="#">11. Explanations: SHAP</a></li></ul><p>In this assignment, you’ll gain experience with tree based models by predicting the 10-year risk of death of individuals from the NHANES I epidemiology dataset (for a detailed description of this dataset you can check the <a href="https://wwwn.cdc.gov/nchs/nhanes/nhefs/default.aspx/" target="_blank" rel="noopener">CDC Website</a>). This is a challenging task and a great test bed for the machine learning methods we learned this week.</p><p>As you go through the assignment, you’ll learn about: </p><ul><li>Dealing with Missing Data<ul><li>Complete Case Analysis.</li><li>Imputation</li></ul></li><li>Decision Trees<ul><li>Evaluation.</li><li>Regularization.</li></ul></li><li>Random Forests <ul><li>Hyperparameter Tuning.</li></ul></li></ul><p><a name="1"></a></p><h2 id="1-Import-Packages"><a href="#1-Import-Packages" class="headerlink" title="1. Import Packages"></a>1. Import Packages</h2><p>We’ll first import all the common packages that we need for this assignment. </p><ul><li><code>shap</code> is a library that explains predictions made by machine learning models.</li><li><code>sklearn</code> is one of the most popular machine learning libraries.</li><li><code>itertools</code> allows us to conveniently manipulate iterable objects such as lists.</li><li><code>pydotplus</code> is used together with <code>IPython.display.Image</code> to visualize graph structures such as decision trees.</li><li><code>numpy</code> is a fundamental package for scientific computing in Python.</li><li><code>pandas</code> is what we’ll use to manipulate our data.</li><li><code>seaborn</code> is a plotting library which has some convenient functions for visualizing missing data.</li><li><code>matplotlib</code> is a plotting library.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> shap</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">import</span> pydotplus</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"><span class="keyword">from</span> sklearn.externals.six <span class="keyword">import</span> StringIO</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.experimental <span class="keyword">import</span> enable_iterative_imputer</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> IterativeImputer, SimpleImputer</span><br><span class="line"></span><br><span class="line"><span class="comment"># We'll also import some helper functions that will be useful later on.</span></span><br><span class="line"><span class="keyword">from</span> util <span class="keyword">import</span> load_data, cindex</span><br></pre></td></tr></table></figure><p><a name="2"></a></p><h2 id="2-Load-the-Dataset"><a href="#2-Load-the-Dataset" class="headerlink" title="2. Load the Dataset"></a>2. Load the Dataset</h2><p>Run the next cell to load in the NHANES I epidemiology dataset. This dataset contains various features of hospital patients as well as their outcomes, i.e. whether or not they died within 10 years.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_dev, X_test, y_dev, y_test = load_data(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>The dataset has been split into a development set (or dev set), which we will use to develop our risk models, and a test set, which we will use to test our models.</p><p>We further split the dev set into a training and validation set, respectively to train and tune our models, using a 75/25 split (note that we set a random state to make this split repeatable).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=<span class="number">0.25</span>, random_state=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><a name="3"></a></p><h2 id="3-Explore-the-Dataset"><a href="#3-Explore-the-Dataset" class="headerlink" title="3. Explore the Dataset"></a>3. Explore the Dataset</h2><p>The first step is to familiarize yourself with the data. Run the next cell to get the size of your training set and look at a small sample. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"X_train shape: &#123;&#125;"</span>.format(X_train.shape))</span><br><span class="line">X_train.head()</span><br></pre></td></tr></table></figure><pre><code>X_train shape: (5147, 18)</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Age</th>      <th>Diastolic BP</th>      <th>Poverty index</th>      <th>Race</th>      <th>Red blood cells</th>      <th>Sedimentation rate</th>      <th>Serum Albumin</th>      <th>Serum Cholesterol</th>      <th>Serum Iron</th>      <th>Serum Magnesium</th>      <th>Serum Protein</th>      <th>Sex</th>      <th>Systolic BP</th>      <th>TIBC</th>      <th>TS</th>      <th>White blood cells</th>      <th>BMI</th>      <th>Pulse pressure</th>    </tr>  </thead>  <tbody>    <tr>      <th>1599</th>      <td>43.0</td>      <td>84.0</td>      <td>637.0</td>      <td>1.0</td>      <td>49.3</td>      <td>10.0</td>      <td>5.0</td>      <td>253.0</td>      <td>134.0</td>      <td>1.59</td>      <td>7.7</td>      <td>1.0</td>      <td>NaN</td>      <td>490.0</td>      <td>27.3</td>      <td>9.1</td>      <td>25.803007</td>      <td>34.0</td>    </tr>    <tr>      <th>2794</th>      <td>72.0</td>      <td>96.0</td>      <td>154.0</td>      <td>2.0</td>      <td>43.4</td>      <td>23.0</td>      <td>4.3</td>      <td>265.0</td>      <td>106.0</td>      <td>1.66</td>      <td>6.8</td>      <td>2.0</td>      <td>208.0</td>      <td>301.0</td>      <td>35.2</td>      <td>6.0</td>      <td>33.394319</td>      <td>112.0</td>    </tr>    <tr>      <th>1182</th>      <td>54.0</td>      <td>78.0</td>      <td>205.0</td>      <td>1.0</td>      <td>43.8</td>      <td>12.0</td>      <td>4.2</td>      <td>206.0</td>      <td>180.0</td>      <td>1.67</td>      <td>6.6</td>      <td>2.0</td>      <td>NaN</td>      <td>363.0</td>      <td>49.6</td>      <td>5.9</td>      <td>20.278410</td>      <td>34.0</td>    </tr>    <tr>      <th>6915</th>      <td>59.0</td>      <td>90.0</td>      <td>417.0</td>      <td>1.0</td>      <td>43.4</td>      <td>9.0</td>      <td>4.5</td>      <td>327.0</td>      <td>114.0</td>      <td>1.65</td>      <td>7.6</td>      <td>2.0</td>      <td>NaN</td>      <td>347.0</td>      <td>32.9</td>      <td>6.1</td>      <td>32.917744</td>      <td>78.0</td>    </tr>    <tr>      <th>500</th>      <td>34.0</td>      <td>80.0</td>      <td>385.0</td>      <td>1.0</td>      <td>77.7</td>      <td>9.0</td>      <td>4.1</td>      <td>197.0</td>      <td>64.0</td>      <td>1.74</td>      <td>7.3</td>      <td>2.0</td>      <td>NaN</td>      <td>376.0</td>      <td>17.0</td>      <td>8.2</td>      <td>30.743489</td>      <td>30.0</td>    </tr>  </tbody></table></div><p>Our targets <code>y</code> will be whether or not the target died within 10 years. Run the next cell to see the target data series.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train.head(<span class="number">20</span>)</span><br></pre></td></tr></table></figure><pre><code>1599    False2794     True1182    False6915    False500     False1188     True9739    False3266    False6681    False8822    False5856     True3415    False9366    False7975    False1397    False6809    False9461    False9374    False1170     True158     FalseName: time, dtype: bool</code></pre><p>Use the next cell to examine individual cases and familiarize yourself with the features.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">i = <span class="number">10</span></span><br><span class="line">print(X_train.iloc[i,:])</span><br><span class="line">print(<span class="string">"\nDied within 10 years? &#123;&#125;"</span>.format(y_train.loc[y_train.index[i]]))</span><br></pre></td></tr></table></figure><pre><code>Age                    67.000000Diastolic BP           94.000000Poverty index         114.000000Race                    1.000000Red blood cells        43.800000Sedimentation rate     12.000000Serum Albumin           3.700000Serum Cholesterol     178.000000Serum Iron             73.000000Serum Magnesium         1.850000Serum Protein           7.000000Sex                     1.000000Systolic BP           140.000000TIBC                  311.000000TS                     23.500000White blood cells       4.300000BMI                    17.481227Pulse pressure         46.000000Name: 5856, dtype: float64Died within 10 years? True</code></pre><p><a name="4"></a></p><h2 id="4-Dealing-with-Missing-Data"><a href="#4-Dealing-with-Missing-Data" class="headerlink" title="4. Dealing with Missing Data"></a>4. Dealing with Missing Data</h2><p>Looking at our data in <code>X_train</code>, we see that some of the data is missing: some values in the output of the previous cell are marked as <code>NaN</code> (“not a number”).</p><p>Missing data is a common occurrence in data analysis, that can be due to a variety of reasons, such as measuring instrument malfunction, respondents not willing or not able to supply information, and errors in the data collection process.</p><p>Let’s examine the missing data pattern. <code>seaborn</code> is an alternative to <code>matplotlib</code> that has some convenient plotting functions for data analysis. We can use its <code>heatmap</code> function to easily visualize the missing data pattern.</p><p>Run the cell below to plot the missing data: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sns.heatmap(X_train.isnull(), cbar=<span class="keyword">False</span>)</span><br><span class="line">plt.title(<span class="string">"Training"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">sns.heatmap(X_val.isnull(), cbar=<span class="keyword">False</span>)</span><br><span class="line">plt.title(<span class="string">"Validation"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_17_0.png" alt="png"></p><p><img src="output_17_1.png" alt="png"></p><p>For each feature, represented as a column, values that are present are shown in black, and missing values are set in a light color.</p><p>From this plot, we can see that many values are missing for systolic blood pressure (<code>Systolic BP</code>).</p><p><a name="Ex-1"></a></p><h3 id="Exercise-1"><a href="#Exercise-1" class="headerlink" title="Exercise 1"></a>Exercise 1</h3><p>In the cell below, write a function to compute the fraction of cases with missing data. This will help us decide how we handle this missing data in the future.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li> The <code>pandas.DataFrame.isnull()</code> method is helpful in this case.</li>    <li> Use the <code>pandas.DataFrame.any()</code> method and set the <code>axis</code> parameter.</li>    <li> Divide the total number of rows with missing data by the total number of rows. Remember that in Python, <code>True</code> values are equal to 1.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fraction_rows_missing</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Return percent of rows with any missing</span></span><br><span class="line"><span class="string">    data in the dataframe. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        df (dataframe): a pandas dataframe with potentially missing data</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        frac_missing (float): fraction of rows with missing data</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE 'Pass' with your 'return' code) ###</span></span><br><span class="line">    <span class="keyword">return</span> df.isnull().any(axis = <span class="number">1</span>).sum() / df.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><p>Test your function by running the cell below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df_test = pd.DataFrame(&#123;<span class="string">'a'</span>:[<span class="keyword">None</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="keyword">None</span>], <span class="string">'b'</span>:[<span class="number">1</span>, <span class="keyword">None</span>, <span class="number">0</span>, <span class="number">1</span>]&#125;)</span><br><span class="line">print(<span class="string">"Example dataframe:\n"</span>)</span><br><span class="line">print(df_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\nComputed fraction missing: &#123;&#125;, expected: &#123;&#125;"</span>.format(fraction_rows_missing(df_test), <span class="number">0.75</span>))</span><br><span class="line">print(<span class="string">f"Fraction of rows missing from X_train: <span class="subst">&#123;fraction_rows_missing(X_train):<span class="number">.3</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Fraction of rows missing from X_val: <span class="subst">&#123;fraction_rows_missing(X_val):<span class="number">.3</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Fraction of rows missing from X_test: <span class="subst">&#123;fraction_rows_missing(X_test):<span class="number">.3</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Example dataframe:     a    b0  NaN  1.01  1.0  NaN2  1.0  0.03  NaN  1.0Computed fraction missing: 0.75, expected: 0.75Fraction of rows missing from X_train: 0.699Fraction of rows missing from X_val: 0.704Fraction of rows missing from X_test: 0.000</code></pre><p>We see that our train and validation sets have missing values, but luckily our test set has complete cases.</p><p>As a first pass, we will begin with a <strong>complete case analysis</strong>, dropping all of the rows with any missing data. Run the following cell to drop these rows from our train and validation sets. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_train_dropped = X_train.dropna(axis=<span class="string">'rows'</span>)</span><br><span class="line">y_train_dropped = y_train.loc[X_train_dropped.index]</span><br><span class="line">X_val_dropped = X_val.dropna(axis=<span class="string">'rows'</span>)</span><br><span class="line">y_val_dropped = y_val.loc[X_val_dropped.index]</span><br></pre></td></tr></table></figure><p><a name="5"></a></p><h2 id="5-Decision-Trees"><a href="#5-Decision-Trees" class="headerlink" title="5. Decision Trees"></a>5. Decision Trees</h2><p>Having just learned about decision trees, you choose to use a decision tree classifier. Use scikit-learn to build a decision tree for the hospital dataset using the train set.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dt = DecisionTreeClassifier(max_depth=<span class="keyword">None</span>, random_state=<span class="number">10</span>)</span><br><span class="line">dt.fit(X_train_dropped, y_train_dropped)</span><br></pre></td></tr></table></figure><pre><code>DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;,                       max_depth=None, max_features=None, max_leaf_nodes=None,                       min_impurity_decrease=0.0, min_impurity_split=None,                       min_samples_leaf=1, min_samples_split=2,                       min_weight_fraction_leaf=0.0, presort=&#39;deprecated&#39;,                       random_state=10, splitter=&#39;best&#39;)</code></pre><p>Next we will evaluate our model. We’ll use C-Index for evaluation.</p><blockquote><p>Remember from lesson 4 of week 1 that the C-Index evaluates the ability of a model to differentiate between different classes, by quantifying how often, when considering all pairs of patients (A, B), the model says that patient A has a higher risk score than patient B when, in the observed data, patient A actually died and patient B actually lived. In our case, our model is a binary classifier, where each risk score is either 1 (the model predicts that the patient will die) or 0 (the patient will live).</p><p>More formally, defining _permissible pairs_ of patients as pairs where the outcomes are different, _concordant pairs_ as permissible pairs where the patient that died had a higher risk score (i.e. our model predicted 1 for the patient that died and 0 for the one that lived), and _ties_ as permissible pairs where the risk scores were equal (i.e. our model predicted 1 for both patients or 0 for both patients), the C-Index is equal to:</p><script type="math/tex; mode=display">\text{C-Index} = \frac{\#\text{concordant pairs} + 0.5\times \#\text{ties}}{\#\text{permissible pairs}}</script></blockquote><p>Run the next cell to compute the C-Index on the train and validation set (we’ve given you an implementation this time).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_train_preds = dt.predict_proba(X_train_dropped)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"Train C-Index: <span class="subst">&#123;cindex(y_train_dropped.values, y_train_preds)&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y_val_preds = dt.predict_proba(X_val_dropped)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"Val C-Index: <span class="subst">&#123;cindex(y_val_dropped.values, y_val_preds)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Train C-Index: 1.0Val C-Index: 0.5629321808510638</code></pre><p>Unfortunately your tree seems to be overfitting: it fits the training data so closely that it doesn’t generalize well to other samples such as those from the validation set.</p><blockquote><p>The training C-index comes out to 1.0 because, when initializing <code>DecisionTreeClasifier</code>, we have left <code>max_depth</code> and <code>min_samples_split</code> unspecified. The resulting decision tree will therefore keep splitting as far as it can, which pretty much guarantees a pure fit to the training data.</p></blockquote><p>To handle this, you can change some of the hyperparameters of our tree. </p><p><a name="Ex-2"></a></p><h3 id="Exercise-2"><a href="#Exercise-2" class="headerlink" title="Exercise 2"></a>Exercise 2</h3><p>Try and find a set of hyperparameters that improves the generalization to the validation set and recompute the C-index. If you do it right, you should get C-index above 0.6 for the validation set. </p><p>You can refer to the documentation for the sklearn <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" target="_blank" rel="noopener">DecisionTreeClassifier</a>.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li> Try limiting the depth of the tree (<code>'max_depth'</code>).</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Experiment with different hyperparameters for the DecisionTreeClassifier</span></span><br><span class="line"><span class="comment"># until you get a c-index above 0.6 for the validation set</span></span><br><span class="line">dt_hyperparams = &#123;</span><br><span class="line">    <span class="comment"># set your own hyperparameters below, such as 'min_samples_split': 1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="string">"max_depth"</span>: <span class="number">3</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Run the next cell to fit and evaluate the regularized tree.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line">dt_reg = DecisionTreeClassifier(**dt_hyperparams, random_state=<span class="number">10</span>)</span><br><span class="line">dt_reg.fit(X_train_dropped, y_train_dropped)</span><br><span class="line"></span><br><span class="line">y_train_preds = dt_reg.predict_proba(X_train_dropped)[:, <span class="number">1</span>]</span><br><span class="line">y_val_preds = dt_reg.predict_proba(X_val_dropped)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"Train C-Index: <span class="subst">&#123;cindex(y_train_dropped.values, y_train_preds)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Val C-Index (expected &gt; 0.6): <span class="subst">&#123;cindex(y_val_dropped.values, y_val_preds)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Train C-Index: 0.688738755448391Val C-Index (expected &gt; 0.6): 0.6302692819148936</code></pre><p>If you used a low <code>max_depth</code> you can print the entire tree. This allows for easy interpretability. Run the next cell to print the tree splits. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dot_data = StringIO()</span><br><span class="line">export_graphviz(dt_reg, feature_names=X_train_dropped.columns, out_file=dot_data,  </span><br><span class="line">                filled=<span class="keyword">True</span>, rounded=<span class="keyword">True</span>, proportion=<span class="keyword">True</span>, special_characters=<span class="keyword">True</span>,</span><br><span class="line">                impurity=<span class="keyword">False</span>, class_names=[<span class="string">'neg'</span>, <span class="string">'pos'</span>], precision=<span class="number">2</span>)</span><br><span class="line">graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  </span><br><span class="line">Image(graph.create_png())</span><br></pre></td></tr></table></figure><p><img src="output_38_0.png" alt="png"></p><blockquote><p><strong>Overfitting, underfitting, and the bias-variance tradeoff</strong></p><p>If you tested several values of <code>max_depth</code>, you may have seen that a value of <code>3</code> gives training and validation C-Indices of about <code>0.689</code> and <code>0.630</code>, and that a <code>max_depth</code> of <code>2</code> gives better agreement with values of about <code>0.653</code> and <code>0.607</code>. In the latter case, we have further reduced overfitting, at the cost of a minor loss in predictive performance.</p><p>Contrast this with a <code>max_depth</code> value of <code>1</code>, which results in C-Indices of about <code>0.597</code> for the training set and <code>0.598</code> for the validation set: we have eliminated overfitting but with a much stronger degradation of predictive performance.</p><p>Lower predictive performance on the training and validation sets is indicative of the model _underfitting_ the data: it neither learns enough from the training data nor is able to generalize to unseen data (the validation data in our case).</p><p>Finding a model that minimizes and acceptably balances underfitting and overfitting (e.g. selecting the model with a <code>max_depth</code> of <code>2</code> over the other values) is a common problem in machine learning that is known as the _bias-variance tradeoff_.</p></blockquote><p><a name="6"></a></p><h2 id="6-Random-Forests"><a href="#6-Random-Forests" class="headerlink" title="6. Random Forests"></a>6. Random Forests</h2><p>No matter how you choose hyperparameters, a single decision tree is prone to overfitting. To solve this problem, you can try <strong>random forests</strong>, which combine predictions from many different trees to create a robust classifier. </p><p>As before, we will use scikit-learn to build a random forest for the data. We will use the default hyperparameters.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rf = RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">10</span>)</span><br><span class="line">rf.fit(X_train_dropped, y_train_dropped)</span><br></pre></td></tr></table></figure><pre><code>RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,                       criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;,                       max_leaf_nodes=None, max_samples=None,                       min_impurity_decrease=0.0, min_impurity_split=None,                       min_samples_leaf=1, min_samples_split=2,                       min_weight_fraction_leaf=0.0, n_estimators=100,                       n_jobs=None, oob_score=False, random_state=10, verbose=0,                       warm_start=False)</code></pre><p>Now compute and report the C-Index for the random forest on the training and validation set.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_train_rf_preds = rf.predict_proba(X_train_dropped)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"Train C-Index: <span class="subst">&#123;cindex(y_train_dropped.values, y_train_rf_preds)&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">y_val_rf_preds = rf.predict_proba(X_val_dropped)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"Val C-Index: <span class="subst">&#123;cindex(y_val_dropped.values, y_val_rf_preds)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Train C-Index: 1.0Val C-Index: 0.6660488696808511</code></pre><p>Training a random forest with the default hyperparameters results in a model that has better predictive performance than individual decision trees as in the previous section, but this model is overfitting.</p><p>We therefore need to tune (or optimize) the hyperparameters, to find a model that both has good predictive performance and minimizes overfitting.</p><p>The hyperparameters we choose to adjust will be:</p><ul><li><code>n_estimators</code>: the number of trees used in the forest.</li><li><code>max_depth</code>: the maximum depth of each tree.</li><li><code>min_samples_leaf</code>: the minimum number (if <code>int</code>) or proportion (if <code>float</code>) of samples in a leaf.</li></ul><p>The approach we implement to tune the hyperparameters is known as a grid search:</p><ul><li><p>We define a set of possible values for each of the target hyperparameters.</p></li><li><p>A model is trained and evaluated for every possible combination of hyperparameters.</p></li><li><p>The best performing set of hyperparameters is returned.</p></li></ul><p>The cell below implements a hyperparameter grid search, using the C-Index to evaluate each tested model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">holdout_grid_search</span><span class="params">(clf, X_train_hp, y_train_hp, X_val_hp, y_val_hp, hyperparams, fixed_hyperparams=&#123;&#125;)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Conduct hyperparameter grid search on hold out validation set. Use holdout validation.</span></span><br><span class="line"><span class="string">    Hyperparameters are input as a dictionary mapping each hyperparameter name to the</span></span><br><span class="line"><span class="string">    range of values they should iterate over. Use the cindex function as your evaluation</span></span><br><span class="line"><span class="string">    function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        clf: sklearn classifier</span></span><br><span class="line"><span class="string">        X_train_hp (dataframe): dataframe for training set input variables</span></span><br><span class="line"><span class="string">        y_train_hp (dataframe): dataframe for training set targets</span></span><br><span class="line"><span class="string">        X_val_hp (dataframe): dataframe for validation set input variables</span></span><br><span class="line"><span class="string">        y_val_hp (dataframe): dataframe for validation set targets</span></span><br><span class="line"><span class="string">        hyperparams (dict): hyperparameter dictionary mapping hyperparameter</span></span><br><span class="line"><span class="string">                            names to range of values for grid search</span></span><br><span class="line"><span class="string">        fixed_hyperparams (dict): dictionary of fixed hyperparameters that</span></span><br><span class="line"><span class="string">                                  are not included in the grid search</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        best_estimator (sklearn classifier): fitted sklearn classifier with best performance on</span></span><br><span class="line"><span class="string">                                             validation set</span></span><br><span class="line"><span class="string">        best_hyperparams (dict): hyperparameter dictionary mapping hyperparameter</span></span><br><span class="line"><span class="string">                                 names to values in best_estimator</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    best_estimator = <span class="keyword">None</span></span><br><span class="line">    best_hyperparams = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># hold best running score</span></span><br><span class="line">    best_score = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># get list of param values</span></span><br><span class="line">    lists = hyperparams.values()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get all param combinations</span></span><br><span class="line">    param_combinations = list(itertools.product(*lists))</span><br><span class="line">    total_param_combinations = len(param_combinations)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># iterate through param combinations</span></span><br><span class="line">    <span class="keyword">for</span> i, params <span class="keyword">in</span> enumerate(param_combinations, <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># fill param dict with params</span></span><br><span class="line">        param_dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> param_index, param_name <span class="keyword">in</span> enumerate(hyperparams):</span><br><span class="line">            param_dict[param_name] = params[param_index]</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># create estimator with specified params</span></span><br><span class="line">        estimator = clf(**param_dict, **fixed_hyperparams)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># fit estimator</span></span><br><span class="line">        estimator.fit(X_train_hp, y_train_hp)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get predictions on validation set</span></span><br><span class="line">        preds = estimator.predict_proba(X_val_hp)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># compute cindex for predictions</span></span><br><span class="line">        estimator_score = cindex(y_val_hp, preds[:,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        print(<span class="string">f'[<span class="subst">&#123;i&#125;</span>/<span class="subst">&#123;total_param_combinations&#125;</span>] <span class="subst">&#123;param_dict&#125;</span>'</span>)</span><br><span class="line">        print(<span class="string">f'Val C-Index: <span class="subst">&#123;estimator_score&#125;</span>\n'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if new high score, update high score, best estimator</span></span><br><span class="line">        <span class="comment"># and best params </span></span><br><span class="line">        <span class="keyword">if</span> estimator_score &gt;= best_score:</span><br><span class="line">                best_score = estimator_score</span><br><span class="line">                best_estimator = estimator</span><br><span class="line">                best_hyperparams = param_dict</span><br><span class="line"></span><br><span class="line">    <span class="comment"># add fixed hyperparamters to best combination of variable hyperparameters</span></span><br><span class="line">    best_hyperparams.update(fixed_hyperparams)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> best_estimator, best_hyperparams</span><br></pre></td></tr></table></figure><p><a name="Ex-3"></a></p><h3 id="Exercise-3"><a href="#Exercise-3" class="headerlink" title="Exercise 3"></a>Exercise 3</h3><p>In the cell below, define the values you want to run the hyperparameter grid search on, and run the cell to find the best-performing set of hyperparameters.</p><p>Your objective is to get a C-Index above <code>0.6</code> on both the train and validation set.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>n_estimators: try values greater than 100</li>    <li>max_depth: try values in the range 1 to 100</li>    <li>min_samples_leaf: try float values below .5 and/or int values greater than 2</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_forest_grid_search</span><span class="params">(X_train_dropped, y_train_dropped, X_val_dropped, y_val_dropped)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define ranges for the chosen random forest hyperparameters </span></span><br><span class="line">    hyperparams = &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### START CODE HERE (REPLACE array values with your code) ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># how many trees should be in the forest (int)</span></span><br><span class="line">        <span class="string">'n_estimators'</span>: [<span class="number">50</span>, <span class="number">200</span>, <span class="number">200</span>],</span><br><span class="line"></span><br><span class="line">        <span class="comment"># the maximum depth of trees in the forest (int)</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">'max_depth'</span>: [<span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>],</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># the minimum number of samples in a leaf as a fraction</span></span><br><span class="line">        <span class="comment"># of the total number of samples in the training set</span></span><br><span class="line">        <span class="comment"># Can be int (in which case that is the minimum number)</span></span><br><span class="line">        <span class="comment"># or float (in which case the minimum is that fraction of the</span></span><br><span class="line">        <span class="comment"># number of training set samples)</span></span><br><span class="line">        <span class="string">'min_samples_leaf'</span>: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    fixed_hyperparams = &#123;</span><br><span class="line">        <span class="string">'random_state'</span>: <span class="number">10</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    rf = RandomForestClassifier</span><br><span class="line"></span><br><span class="line">    best_rf, best_hyperparams = holdout_grid_search(rf, X_train_dropped, y_train_dropped,</span><br><span class="line">                                                    X_val_dropped, y_val_dropped, hyperparams,</span><br><span class="line">                                                    fixed_hyperparams)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f"Best hyperparameters:\n<span class="subst">&#123;best_hyperparams&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    y_train_best = best_rf.predict_proba(X_train_dropped)[:, <span class="number">1</span>]</span><br><span class="line">    print(<span class="string">f"Train C-Index: <span class="subst">&#123;cindex(y_train_dropped, y_train_best)&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    y_val_best = best_rf.predict_proba(X_val_dropped)[:, <span class="number">1</span>]</span><br><span class="line">    print(<span class="string">f"Val C-Index: <span class="subst">&#123;cindex(y_val_dropped, y_val_best)&#125;</span>"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># add fixed hyperparamters to best combination of variable hyperparameters</span></span><br><span class="line">    best_hyperparams.update(fixed_hyperparams)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> best_rf, best_hyperparams</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">best_rf, best_hyperparams = random_forest_grid_search(X_train_dropped, y_train_dropped, X_val_dropped, y_val_dropped)</span><br></pre></td></tr></table></figure><pre><code>[1/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.6724567819148937[2/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.6722240691489362[3/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3}Val C-Index: 0.6725066489361702[4/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.6637965425531915[5/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.6655585106382979[6/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3}Val C-Index: 0.659624335106383[7/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.6611535904255319[8/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.6647273936170213[9/27] {&#39;n_estimators&#39;: 50, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 3}Val C-Index: 0.6605884308510638[10/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.6811502659574468[11/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.6815159574468085[12/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3}Val C-Index: 0.6809175531914894[13/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.6765458776595744[14/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.6750831117021276[15/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3}Val C-Index: 0.6745844414893617[16/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.668467420212766[17/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.6737699468085107[18/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 3}Val C-Index: 0.667436835106383[19/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.6811502659574468[20/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.6815159574468085[21/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3}Val C-Index: 0.6809175531914894[22/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.6765458776595744[23/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.6750831117021276[24/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3}Val C-Index: 0.6745844414893617[25/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.668467420212766[26/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.6737699468085107[27/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 3}Val C-Index: 0.667436835106383Best hyperparameters:{&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 2, &#39;random_state&#39;: 10}Train C-Index: 0.7798145228600575Val C-Index: 0.6815159574468085</code></pre><p>Finally, evaluate the model on the test set. This is a crucial step, as trying out many combinations of hyperparameters and evaluating them on the validation set could result in a model that ends up overfitting the validation set. We therefore need to check if the model performs well on unseen data, which is the role of the test set, which we have held out until now.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line">y_test_best = best_rf.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"Test C-Index: <span class="subst">&#123;cindex(y_test.values, y_test_best)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test C-Index: 0.7013860174676174</code></pre><p>Your C-Index on the test set should be greater than <code>0.6</code>.</p><p><a name="7"></a></p><h2 id="7-Imputation"><a href="#7-Imputation" class="headerlink" title="7. Imputation"></a>7. Imputation</h2><p>You’ve now built and optimized a random forest model on our data. However, there was still a drop in test C-Index. This might be because you threw away more than half of the data of our data because of missing values for systolic blood pressure. Instead, we can try filling in, or imputing, these values. </p><p>First, let’s explore to see if our data is missing at random or not. Let’s plot histograms of the dropped rows against each of the covariates (aside from systolic blood pressure) to see if there is a trend. Compare these to the histograms of the feature in the entire dataset. Try to see if one of the covariates has a signficantly different distribution in the two subsets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dropped_rows = X_train[X_train.isnull().any(axis=<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">columns_except_Systolic_BP = [col <span class="keyword">for</span> col <span class="keyword">in</span> X_train.columns <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'Systolic BP'</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> columns_except_Systolic_BP:</span><br><span class="line">    sns.distplot(X_train.loc[:, col], norm_hist=<span class="keyword">True</span>, kde=<span class="keyword">False</span>, label=<span class="string">'full data'</span>)</span><br><span class="line">    sns.distplot(dropped_rows.loc[:, col], norm_hist=<span class="keyword">True</span>, kde=<span class="keyword">False</span>, label=<span class="string">'without missing data'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_54_0.png" alt="png"></p><p><img src="output_54_1.png" alt="png"></p><p><img src="output_54_2.png" alt="png"></p><p><img src="output_54_3.png" alt="png"></p><p><img src="output_54_4.png" alt="png"></p><p><img src="output_54_5.png" alt="png"></p><p><img src="output_54_6.png" alt="png"></p><p><img src="output_54_7.png" alt="png"></p><p><img src="output_54_8.png" alt="png"></p><p><img src="output_54_9.png" alt="png"></p><p><img src="output_54_10.png" alt="png"></p><p><img src="output_54_11.png" alt="png"></p><p><img src="output_54_12.png" alt="png"></p><p><img src="output_54_13.png" alt="png"></p><p><img src="output_54_14.png" alt="png"></p><p><img src="output_54_15.png" alt="png"></p><p><img src="output_54_16.png" alt="png"></p><p>Most of the covariates are distributed similarly whether or not we have discarded rows with missing data. In other words missingness of the data is independent of these covariates.</p><p>If this had been true across <em>all</em> covariates, then the data would have been said to be <strong>missing completely at random (MCAR)</strong>.</p><p>But when considering the age covariate, we see that much more data tends to be missing for patients over 65. The reason could be that blood pressure was measured less frequently for old people to avoid placing additional burden on them.</p><p>As missingness is related to one or more covariates, the missing data is said to be <strong>missing at random (MAR)</strong>.</p><p>Based on the information we have, there is however no reason to believe that the _values_ of the missing data — or specifically the values of the missing systolic blood pressures — are related to the age of the patients.<br>If this was the case, then this data would be said to be <strong>missing not at random (MNAR)</strong>.</p><p><a name="8"></a></p><h2 id="8-Error-Analysis"><a href="#8-Error-Analysis" class="headerlink" title="8. Error Analysis"></a>8. Error Analysis</h2><p><a name="Ex-4"></a></p><h3 id="Exercise-4"><a href="#Exercise-4" class="headerlink" title="Exercise 4"></a>Exercise 4</h3><p>Using the information from the plots above, try to find a subgroup of the test data on which the model performs poorly. You should be able to easily find a subgroup of at least 250 cases on which the model has a C-Index of less than 0.69.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li> Define a mask using a feature and a threshold, e.g. patients with a BMI below 20: <code>mask = X_test['BMI'] < 20 </code>. </li>    <li> Try to find a subgroup for which the model had little data.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bad_subset</span><span class="params">(forest, X_test, y_test)</span>:</span></span><br><span class="line">    <span class="comment"># define mask to select large subset with poor performance</span></span><br><span class="line">    <span class="comment"># currently mask defines the entire set</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE the code after 'mask =' with your code) ###</span></span><br><span class="line">    mask = X_test[<span class="string">'Age'</span>] &gt; <span class="number">67</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    X_subgroup = X_test[mask]</span><br><span class="line">    y_subgroup = y_test[mask]</span><br><span class="line">    subgroup_size = len(X_subgroup)</span><br><span class="line"></span><br><span class="line">    y_subgroup_preds = forest.predict_proba(X_subgroup)[:, <span class="number">1</span>]</span><br><span class="line">    performance = cindex(y_subgroup.values, y_subgroup_preds)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> performance, subgroup_size</span><br></pre></td></tr></table></figure><h4 id="Test-Your-Work"><a href="#Test-Your-Work" class="headerlink" title="Test Your Work"></a>Test Your Work</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">performance, subgroup_size = bad_subset(best_rf, X_test, y_test)</span><br><span class="line">print(<span class="string">"Subgroup size should greater than 250, performance should be less than 0.69 "</span>)</span><br><span class="line">print(<span class="string">f"Subgroup size: <span class="subst">&#123;subgroup_size&#125;</span>, C-Index: <span class="subst">&#123;performance&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Subgroup size should greater than 250, performance should be less than 0.69 Subgroup size: 320, C-Index: 0.670638197475522</code></pre><h4 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output"></a>Expected Output</h4><p>Note, your actual output will vary depending on the hyper-parameters that you chose and the mask that you chose.</p><ul><li>Make sure that the c-index is less than 0.69<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Subgroup size: <span class="number">586</span>, C-Index: <span class="number">0.6275</span></span><br></pre></td></tr></table></figure></li></ul><p><strong>Bonus</strong>: </p><ul><li>See if you can get a c-index as low as 0.53<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Subgroup size: 251, C-Index: 0.5331</span><br></pre></td></tr></table></figure></li></ul><p><a name="9"></a></p><h2 id="9-Imputation-Approaches"><a href="#9-Imputation-Approaches" class="headerlink" title="9. Imputation Approaches"></a>9. Imputation Approaches</h2><p>Seeing that our data is not missing completely at random, we can handle the missing values by replacing them with substituted values based on the other values that we have. This is known as imputation.</p><p>The first imputation strategy that we will use is <strong>mean substitution</strong>: we will replace the missing values for each feature with the mean of the available values. In the next cell, use the <code>SimpleImputer</code> from <code>sklearn</code> to use mean imputation for the missing values.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Impute values using the mean</span></span><br><span class="line">imputer = SimpleImputer(strategy=<span class="string">'mean'</span>)</span><br><span class="line">imputer.fit(X_train)</span><br><span class="line">X_train_mean_imputed = pd.DataFrame(imputer.transform(X_train), columns=X_train.columns)</span><br><span class="line">X_val_mean_imputed = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns)</span><br></pre></td></tr></table></figure><p><a name="Ex-5"></a></p><h3 id="Exercise-5"><a href="#Exercise-5" class="headerlink" title="Exercise 5"></a>Exercise 5</h3><p>Now perform a hyperparameter grid search to find the best-performing random forest model, and report results on the test set. </p><p>Define the parameter ranges for the hyperparameter search in the next cell, and run the cell.</p><h4 id="Target-performance"><a href="#Target-performance" class="headerlink" title="Target performance"></a>Target performance</h4><p>Make your test c-index at least 0.74 or higher</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>n_estimators: try values greater than 100</li>    <li>max_depth: try values in the range 1 to 100</li>    <li>min_samples_leaf: try float values below .5 and/or int values greater than 2</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define ranges for the random forest hyperparameter search </span></span><br><span class="line">hyperparams = &#123;</span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE array values with your code) ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># how many trees should be in the forest (int)</span></span><br><span class="line">    <span class="string">'n_estimators'</span>: [<span class="number">200</span>,<span class="number">500</span>],</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the maximum depth of trees in the forest (int)</span></span><br><span class="line">    <span class="string">'max_depth'</span>: [<span class="number">3</span>,<span class="number">5</span>],</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the minimum number of samples in a leaf as a fraction</span></span><br><span class="line">    <span class="comment"># of the total number of samples in the training set</span></span><br><span class="line">    <span class="comment"># Can be int (in which case that is the minimum number)</span></span><br><span class="line">    <span class="comment"># or float (in which case the minimum is that fraction of the</span></span><br><span class="line">    <span class="comment"># number of training set samples)</span></span><br><span class="line">    <span class="string">'min_samples_leaf'</span>: [<span class="number">1</span>,<span class="number">2</span>],</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line">rf = RandomForestClassifier</span><br><span class="line"></span><br><span class="line">rf_mean_imputed, best_hyperparams_mean_imputed = holdout_grid_search(rf, X_train_mean_imputed, y_train,</span><br><span class="line">                                                                     X_val_mean_imputed, y_val,</span><br><span class="line">                                                                     hyperparams, &#123;<span class="string">'random_state'</span>: <span class="number">10</span>&#125;)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Performance for best hyperparameters:"</span>)</span><br><span class="line"></span><br><span class="line">y_train_best = rf_mean_imputed.predict_proba(X_train_mean_imputed)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"- Train C-Index: <span class="subst">&#123;cindex(y_train, y_train_best):<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">y_val_best = rf_mean_imputed.predict_proba(X_val_mean_imputed)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"- Val C-Index: <span class="subst">&#123;cindex(y_val, y_val_best):<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">y_test_imp = rf_mean_imputed.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"- Test C-Index: <span class="subst">&#123;cindex(y_test, y_test_imp):<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>[1/8] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.7395345453913784[2/8] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.7397907669057344[3/8] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.7491450235484942[4/8] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.7481646505507676[5/8] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.740106701061148[6/8] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.7403585798379725[7/8] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.7496932941618408[8/8] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.7494088448535303Performance for best hyperparameters:- Train C-Index: 0.8137- Val C-Index: 0.7497- Test C-Index: 0.7819</code></pre><h4 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h4><p>Note, your actual c-index values will vary depending on the hyper-parameters that you choose.  </p><ul><li>Try to get a good Test c-index, similar these numbers below:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Performance <span class="keyword">for</span> best hyperparameters:</span><br><span class="line">- Train C-Index: <span class="number">0.8109</span></span><br><span class="line">- Val C-Index: <span class="number">0.7495</span></span><br><span class="line">- Test C-Index: <span class="number">0.7805</span></span><br></pre></td></tr></table></figure><p>Next, we will apply another imputation strategy, known as <strong>multivariate feature imputation</strong>, using scikit-learn’s <code>IterativeImputer</code> class (see the <a href="https://scikit-learn.org/stable/modules/impute.html#iterative-imputer" target="_blank" rel="noopener">documentation</a>).</p><p>With this strategy, for each feature that is missing values, a regression model is trained to predict observed values based on all of the other features, and the missing values are inferred using this model.<br>As a single iteration across all features may not be enough to impute all missing values, several iterations may be performed, hence the name of the class <code>IterativeImputer</code>.</p><p>In the next cell, use <code>IterativeImputer</code> to perform multivariate feature imputation.</p><blockquote><p>Note that the first time the cell is run, <code>imputer.fit(X_train)</code> may fail with the message <code>LinAlgError: SVD did not converge</code>: simply re-run the cell.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Impute using regression on other covariates</span></span><br><span class="line">imputer = IterativeImputer(random_state=<span class="number">0</span>, sample_posterior=<span class="keyword">False</span>, max_iter=<span class="number">1</span>, min_value=<span class="number">0</span>)</span><br><span class="line">imputer.fit(X_train)</span><br><span class="line">X_train_imputed = pd.DataFrame(imputer.transform(X_train), columns=X_train.columns)</span><br><span class="line">X_val_imputed = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns)</span><br></pre></td></tr></table></figure><p><a name="Ex-6"></a></p><h3 id="Exercise-6"><a href="#Exercise-6" class="headerlink" title="Exercise 6"></a>Exercise 6</h3><p>Perform a hyperparameter grid search to find the best-performing random forest model, and report results on the test set. Define the parameter ranges for the hyperparameter search in the next cell, and run the cell.</p><h4 id="Target-performance-1"><a href="#Target-performance-1" class="headerlink" title="Target performance"></a>Target performance</h4><p>Try to get a text c-index of at least 0.74 or higher.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>n_estimators: try values greater than 100</li>    <li>max_depth: try values in the range 1 to 100</li>    <li>min_samples_leaf: try float values below .5 and/or int values greater than 2</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define ranges for the random forest hyperparameter search </span></span><br><span class="line">hyperparams = &#123;</span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE array values with your code) ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># how many trees should be in the forest (int)</span></span><br><span class="line">    <span class="string">'n_estimators'</span>: [<span class="number">200</span>,<span class="number">500</span>],</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the maximum depth of trees in the forest (int)</span></span><br><span class="line">    <span class="string">'max_depth'</span>: [<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>],</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the minimum number of samples in a leaf as a fraction</span></span><br><span class="line">    <span class="comment"># of the total number of samples in the training set</span></span><br><span class="line">    <span class="comment"># Can be int (in which case that is the minimum number)</span></span><br><span class="line">    <span class="comment"># or float (in which case the minimum is that fraction of the</span></span><br><span class="line">    <span class="comment"># number of training set samples)</span></span><br><span class="line">    <span class="string">'min_samples_leaf'</span>: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line">rf = RandomForestClassifier</span><br><span class="line"></span><br><span class="line">rf_imputed, best_hyperparams_imputed = holdout_grid_search(rf, X_train_imputed, y_train,</span><br><span class="line">                                                           X_val_imputed, y_val,</span><br><span class="line">                                                           hyperparams, &#123;<span class="string">'random_state'</span>: <span class="number">10</span>&#125;)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Performance for best hyperparameters:"</span>)</span><br><span class="line"></span><br><span class="line">y_train_best = rf_imputed.predict_proba(X_train_imputed)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"- Train C-Index: <span class="subst">&#123;cindex(y_train, y_train_best):<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">y_val_best = rf_imputed.predict_proba(X_val_imputed)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"- Val C-Index: <span class="subst">&#123;cindex(y_val, y_val_best):<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">y_test_imp = rf_imputed.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f"- Test C-Index: <span class="subst">&#123;cindex(y_test, y_test_imp):<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>[1/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.7354751714838482[2/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.7357596207921587[3/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3}Val C-Index: 0.7356792801478268[4/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.745511237919047[5/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.7446752609442414[6/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3}Val C-Index: 0.7453787844243376[7/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 7, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.7486206379915707[8/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 7, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.7506139545185099[9/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 7, &#39;min_samples_leaf&#39;: 3}Val C-Index: 0.749875689138162[10/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.73679102095588[11/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.737142782695928[12/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3}Val C-Index: 0.737118897639505[13/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.7460540801104792[14/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.7457783162772317[15/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3}Val C-Index: 0.746544809451534[16/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 7, &#39;min_samples_leaf&#39;: 1}Val C-Index: 0.7486162952540393[17/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 7, &#39;min_samples_leaf&#39;: 2}Val C-Index: 0.7508050349698939[18/18] {&#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 7, &#39;min_samples_leaf&#39;: 3}Val C-Index: 0.7501840235028955Performance for best hyperparameters:- Train C-Index: 0.8774- Val C-Index: 0.7508- Test C-Index: 0.7834</code></pre><h4 id="Expected-Output-1"><a href="#Expected-Output-1" class="headerlink" title="Expected Output"></a>Expected Output</h4><p>Note, your actual output will vary depending on the hyper-parameters that you chose and the mask that you chose.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Performance <span class="keyword">for</span> best hyperparameters:</span><br><span class="line">- Train C-Index: <span class="number">0.8131</span></span><br><span class="line">- Val C-Index: <span class="number">0.7454</span></span><br><span class="line">- Test C-Index: <span class="number">0.7797</span></span><br></pre></td></tr></table></figure></p><p><a name="10"></a></p><h2 id="10-Comparison"><a href="#10-Comparison" class="headerlink" title="10. Comparison"></a>10. Comparison</h2><p>For good measure, retest on the subgroup from before to see if your new models do better.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">performance, subgroup_size = bad_subset(best_rf, X_test, y_test)</span><br><span class="line">print(<span class="string">f"C-Index (no imputation): <span class="subst">&#123;performance&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">performance, subgroup_size = bad_subset(rf_mean_imputed, X_test, y_test)</span><br><span class="line">print(<span class="string">f"C-Index (mean imputation): <span class="subst">&#123;performance&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">performance, subgroup_size = bad_subset(rf_imputed, X_test, y_test)</span><br><span class="line">print(<span class="string">f"C-Index (multivariate feature imputation): <span class="subst">&#123;performance&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>C-Index (no imputation): 0.670638197475522C-Index (mean imputation): 0.6847548267862058C-Index (multivariate feature imputation): 0.6926978884039164</code></pre><p>We should see that avoiding complete case analysis (i.e. analysis only on observations for which there is no missing data) allows our model to generalize a bit better. Remember to examine your missing cases to judge whether they are missing at random or not!</p><p><a name="11"></a></p><h2 id="11-Explanations-SHAP"><a href="#11-Explanations-SHAP" class="headerlink" title="11. Explanations: SHAP"></a>11. Explanations: SHAP</h2><p>Using a random forest has improved results, but we’ve lost some of the natural interpretability of trees. In this section we’ll try to explain the predictions using slightly more sophisticated techniques. </p><p>You choose to apply <strong>SHAP (SHapley Additive exPlanations) </strong>, a cutting edge method that explains predictions made by black-box machine learning models (i.e. models which are too complex to be understandable by humans as is).</p><blockquote><p>Given a prediction made by a machine learning model, SHAP values explain the prediction by quantifying the additive importance of each feature to the prediction. SHAP values have their roots in cooperative game theory, where Shapley values are used to quantify the contribution of each player to the game.</p><p>Although it is computationally expensive to compute SHAP values for general black-box models, in the case of trees and forests there exists a fast polynomial-time algorithm. For more details, see the <a href="https://arxiv.org/pdf/1802.03888.pdf" target="_blank" rel="noopener">TreeShap paper</a>.</p></blockquote><p>We’ll use the <a href="https://github.com/slundberg/shap" target="_blank" rel="noopener">shap library</a> to do this for our random forest model. Run the next cell to output the most at risk individuals in the test set according to our model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_test_risk = X_test.copy(deep=<span class="keyword">True</span>)</span><br><span class="line">X_test_risk.loc[:, <span class="string">'risk'</span>] = rf_imputed.predict_proba(X_test_risk)[:, <span class="number">1</span>]</span><br><span class="line">X_test_risk = X_test_risk.sort_values(by=<span class="string">'risk'</span>, ascending=<span class="keyword">False</span>)</span><br><span class="line">X_test_risk.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Age</th>      <th>Diastolic BP</th>      <th>Poverty index</th>      <th>Race</th>      <th>Red blood cells</th>      <th>Sedimentation rate</th>      <th>Serum Albumin</th>      <th>Serum Cholesterol</th>      <th>Serum Iron</th>      <th>Serum Magnesium</th>      <th>Serum Protein</th>      <th>Sex</th>      <th>Systolic BP</th>      <th>TIBC</th>      <th>TS</th>      <th>White blood cells</th>      <th>BMI</th>      <th>Pulse pressure</th>      <th>risk</th>    </tr>  </thead>  <tbody>    <tr>      <th>5493</th>      <td>67.0</td>      <td>80.0</td>      <td>30.0</td>      <td>1.0</td>      <td>77.7</td>      <td>59.0</td>      <td>3.4</td>      <td>231.0</td>      <td>36.0</td>      <td>1.40</td>      <td>6.3</td>      <td>1.0</td>      <td>170.0</td>      <td>202.0</td>      <td>17.8</td>      <td>8.4</td>      <td>17.029470</td>      <td>90.0</td>      <td>0.689064</td>    </tr>    <tr>      <th>6337</th>      <td>69.0</td>      <td>80.0</td>      <td>233.0</td>      <td>1.0</td>      <td>77.7</td>      <td>48.0</td>      <td>4.2</td>      <td>159.0</td>      <td>87.0</td>      <td>1.81</td>      <td>6.9</td>      <td>1.0</td>      <td>146.0</td>      <td>291.0</td>      <td>29.9</td>      <td>15.2</td>      <td>17.931276</td>      <td>66.0</td>      <td>0.639300</td>    </tr>    <tr>      <th>2044</th>      <td>74.0</td>      <td>80.0</td>      <td>83.0</td>      <td>1.0</td>      <td>47.6</td>      <td>19.0</td>      <td>4.2</td>      <td>205.0</td>      <td>72.0</td>      <td>1.71</td>      <td>6.9</td>      <td>1.0</td>      <td>180.0</td>      <td>310.0</td>      <td>23.2</td>      <td>10.8</td>      <td>20.900101</td>      <td>100.0</td>      <td>0.582532</td>    </tr>    <tr>      <th>1017</th>      <td>65.0</td>      <td>98.0</td>      <td>16.0</td>      <td>1.0</td>      <td>49.4</td>      <td>30.0</td>      <td>3.4</td>      <td>124.0</td>      <td>129.0</td>      <td>1.59</td>      <td>7.7</td>      <td>1.0</td>      <td>184.0</td>      <td>293.0</td>      <td>44.0</td>      <td>5.9</td>      <td>30.858853</td>      <td>86.0</td>      <td>0.573548</td>    </tr>    <tr>      <th>6609</th>      <td>72.0</td>      <td>90.0</td>      <td>75.0</td>      <td>1.0</td>      <td>29.3</td>      <td>59.0</td>      <td>3.9</td>      <td>216.0</td>      <td>64.0</td>      <td>1.63</td>      <td>7.4</td>      <td>2.0</td>      <td>182.0</td>      <td>322.0</td>      <td>19.9</td>      <td>9.3</td>      <td>22.281793</td>      <td>92.0</td>      <td>0.565259</td>    </tr>  </tbody></table></div><p>We can use SHAP values to try and understand the model output on specific individuals using force plots. Run the cell below to see a force plot on the riskiest individual. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">explainer = shap.TreeExplainer(rf_imputed)</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line">shap_value = explainer.shap_values(X_test.loc[X_test_risk.index[i], :])[<span class="number">1</span>]</span><br><span class="line">shap.force_plot(explainer.expected_value[<span class="number">1</span>], shap_value, feature_names=X_test.columns, matplotlib=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="output_83_0.png" alt="png"></p><p>How to read this chart:</p><ul><li>The red sections on the left are features which push the model towards the final prediction in the positive direction (i.e. a higher Age increases the predicted risk).</li><li>The blue sections on the right are features that push the model towards the final prediction in the negative direction (if an increase in a feature leads to a lower risk, it will be shown in blue).</li><li>Note that the exact output of your chart will differ depending on the hyper-parameters that you choose for your model.</li></ul><p>We can also use SHAP values to understand the model output in aggregate. Run the next cell to initialize the SHAP values (this may take a few minutes).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shap_values = shap.TreeExplainer(rf_imputed).shap_values(X_test)[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>Run the next cell to see a summary plot of the SHAP values for each feature on each of the test examples. The colors indicate the value of the feature.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shap.summary_plot(shap_values, X_test)</span><br></pre></td></tr></table></figure><p><img src="output_87_0.png" alt="png"></p><p>Clearly we see that being a woman (<code>sex = 2.0</code>, as opposed to men for which <code>sex = 1.0</code>) has a negative SHAP value, meaning that it reduces the risk of dying within 10 years. High age and high systolic blood pressure have positive SHAP values, and are therefore related to increased mortality. </p><p>You can see how features interact using dependence plots. These plot the SHAP value for a given feature for each data point, and color the points in using the value for another feature. This lets us begin to explain the variation in SHAP value for a single value of the main feature.</p><p>Run the next cell to see the interaction between Age and Sex.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shap.dependence_plot(<span class="string">'Age'</span>, shap_values, X_test, interaction_index=<span class="string">'Sex'</span>)</span><br></pre></td></tr></table></figure><p><img src="output_89_0.png" alt="png"></p><p>We see that while Age &gt; 50 is generally bad (positive SHAP value), being a woman generally reduces the impact of age. This makes sense since we know that women generally live longer than men.</p><p>Let’s now look at poverty index and age.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shap.dependence_plot(<span class="string">'Poverty index'</span>, shap_values, X_test, interaction_index=<span class="string">'Age'</span>)</span><br></pre></td></tr></table></figure><p><img src="output_91_0.png" alt="png"></p><p>We see that the impact of poverty index drops off quickly, and for higher income individuals age begins to explain much of variation in the impact of poverty index.</p><p>Try some other pairs and see what other interesting relationships you can find!</p><h1 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations!"></a>Congratulations!</h1><p>You have completed the second assignment in Course 2. Along the way you’ve learned to fit decision trees, random forests, and deal with missing data. Now you’re ready to move on to week 3!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Risk-Models-Using-Tree-based-Models&quot;&gt;&lt;a href=&quot;#Risk-Models-Using-Tree-based-Models&quot; class=&quot;headerlink&quot; title=&quot;Risk Models Using Tree
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Machine-Learning/"/>
    
    
      <category term="Medicine" scheme="https://zhangruochi.com/tags/Medicine/"/>
    
  </entry>
  
  <entry>
    <title>Build and Evaluate a Linear Risk model</title>
    <link href="https://zhangruochi.com/Build-and-Evaluate-a-Linear-Risk-model/2020/04/18/"/>
    <id>https://zhangruochi.com/Build-and-Evaluate-a-Linear-Risk-model/2020/04/18/</id>
    <published>2020-04-18T15:50:08.000Z</published>
    <updated>2020-04-19T15:09:57.386Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Build-and-Evaluate-a-Linear-Risk-model"><a href="#Build-and-Evaluate-a-Linear-Risk-model" class="headerlink" title="Build and Evaluate a Linear Risk model"></a>Build and Evaluate a Linear Risk model</h1><p>Welcome to the first assignment in Course 2!</p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul><li><a href="#1">1. Import Packages</a></li><li><a href="#2">2. Load Data</a></li><li><a href="#3">3. Explore the Dataset</a></li><li><a href="#4">4. Mean-Normalize the Data</a><ul><li><a href="#Ex-1">Exercise 1</a></li></ul></li><li><a href="#Ex-2">5. Build the Model</a><ul><li><a href="#Ex-2">Exercise 2</a></li></ul></li><li><a href="#6">6. Evaluate the Model Using the C-Index</a><ul><li><a href="#Ex-3">Exercise 3</a></li></ul></li><li><a href="#7">7. Evaluate the Model on the Test Set</a></li><li><a href="#8">8. Improve the Model</a><ul><li><a href="#Ex-4">Exercise 4</a></li></ul></li><li><a href="#9">9. Evalute the Improved Model</a></li></ul><h2 id="Overview-of-the-Assignment"><a href="#Overview-of-the-Assignment" class="headerlink" title="Overview of the Assignment"></a>Overview of the Assignment</h2><p>In this assignment, you’ll build a risk score model for retinopathy in diabetes patients using logistic regression.</p><p>As we develop the model, we will learn about the following topics:</p><ul><li>Data preprocessing<ul><li>Log transformations</li><li>Standardization</li></ul></li><li>Basic Risk Models<ul><li>Logistic Regression</li><li>C-index</li><li>Interactions Terms</li></ul></li></ul><h3 id="Diabetic-Retinopathy"><a href="#Diabetic-Retinopathy" class="headerlink" title="Diabetic Retinopathy"></a>Diabetic Retinopathy</h3><p>Retinopathy is an eye condition that causes changes to the blood vessels in the part of the eye called the retina.<br>This often leads to vision changes or blindness.<br>Diabetic patients are known to be at high risk for retinopathy. </p><h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>Logistic regression is an appropriate analysis to use for predicting the probability of a binary outcome. In our case, this would be the probability of having or not having diabetic retinopathy.<br>Logistic Regression is one of the most commonly used algorithms for binary classification. It is used to find the best fitting model to describe the relationship between a set of features (also referred to as input, independent, predictor, or explanatory variables) and a binary outcome label (also referred to as an output, dependent, or response variable). Logistic regression has the property that the output prediction is always in the range $[0,1]$. Sometimes this output is used to represent a probability from 0%-100%, but for straight binary classification, the output is converted to either $0$ or $1$ depending on whether it is below or above a certain threshold, usually $0.5$.</p><p>It may be  confusing that the term regression appears in the name even though logistic regression is actually a classification algorithm, but that’s just a name it was given for historical reasons.</p><p><a name="1"></a></p><h2 id="1-Import-Packages"><a href="#1-Import-Packages" class="headerlink" title="1.  Import Packages"></a>1.  Import Packages</h2><p>We’ll first import all the packages that we need for this assignment. </p><ul><li><code>numpy</code> is the fundamental package for scientific computing in python.</li><li><code>pandas</code> is what we’ll use to manipulate our data.</li><li><code>matplotlib</code> is a plotting library.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><p><a name="2"></a></p><h2 id="2-Load-Data"><a href="#2-Load-Data" class="headerlink" title="2. Load Data"></a>2. Load Data</h2><p>First we will load in the dataset that we will use for training and testing our model.</p><ul><li>Run the next cell to load the data using a function imported from our local utils module.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># This function creates randomly generated data</span></span><br><span class="line"><span class="comment"># X, y = load_data(6000)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># For stability, load data from files that were generated using the load_data</span></span><br><span class="line">X = pd.read_csv(<span class="string">'X_data.csv'</span>,index_col=<span class="number">0</span>)</span><br><span class="line">y_df = pd.read_csv(<span class="string">'y_data.csv'</span>,index_col=<span class="number">0</span>)</span><br><span class="line">y = y_df[<span class="string">'y'</span>]</span><br></pre></td></tr></table></figure><p><code>X</code> and <code>y</code> are Pandas DataFrames that hold the data for 6,000 diabetic patients. </p><p><a name="3"></a></p><h2 id="3-Explore-the-Dataset"><a href="#3-Explore-the-Dataset" class="headerlink" title="3. Explore the Dataset"></a>3. Explore the Dataset</h2><p>The features (<code>X</code>) include the following fields:</p><ul><li>Age: (years)</li><li>Systolic_BP: Systolic blood pressure (mmHg)</li><li>Diastolic_BP: Diastolic blood pressure (mmHg)</li><li>Cholesterol: (mg/DL)</li></ul><p>We can use the <code>head()</code> method to display the first few records of each.    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Age</th>      <th>Systolic_BP</th>      <th>Diastolic_BP</th>      <th>Cholesterol</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>77.196340</td>      <td>85.288742</td>      <td>80.021878</td>      <td>79.957109</td>    </tr>    <tr>      <th>1</th>      <td>63.529850</td>      <td>99.379736</td>      <td>84.852361</td>      <td>110.382411</td>    </tr>    <tr>      <th>2</th>      <td>69.003986</td>      <td>111.349455</td>      <td>109.850616</td>      <td>100.828246</td>    </tr>    <tr>      <th>3</th>      <td>82.638210</td>      <td>95.056128</td>      <td>79.666851</td>      <td>87.066303</td>    </tr>    <tr>      <th>4</th>      <td>78.346286</td>      <td>109.154591</td>      <td>90.713220</td>      <td>92.511770</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.shape</span><br></pre></td></tr></table></figure><pre><code>(6000, 4)</code></pre><p>The target (<code>y</code>) is an indicator of whether or not the patient developed retinopathy.</p><ul><li>y = 1 : patient has retinopathy.</li><li>y = 0 : patient does not have retinopathy.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.head()</span><br></pre></td></tr></table></figure><pre><code>0    1.01    1.02    1.03    1.04    1.0Name: y, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.shape</span><br></pre></td></tr></table></figure><pre><code>(6000,)</code></pre><p>Before we build a model, let’s take a closer look at the distribution of our training data. To do this, we will split the data into train and test sets using a 75/25 split.</p><p>For this, we can use the built in function provided by sklearn library.  See the documentation for <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener">sklearn.model_selection.train_test_split</a>. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=<span class="number">0.75</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>Plot the histograms of each column of <code>X_train</code> below: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> X.columns:</span><br><span class="line">    X_train_raw.loc[:, col].hist()</span><br><span class="line">    plt.title(col)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_18_0.png" alt="png"></p><p><img src="output_18_1.png" alt="png"></p><p><img src="output_18_2.png" alt="png"></p><p><img src="output_18_3.png" alt="png"></p><p>As we can see, the distributions have a generally bell shaped distribution, but with slight rightward skew.</p><p>Many statistical models assume that the data is normally distributed, forming a symmetric Gaussian bell shape (with no skew) more like the example below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm</span><br><span class="line">data = np.random.normal(<span class="number">50</span>,<span class="number">12</span>, <span class="number">5000</span>)</span><br><span class="line">fitting_params = norm.fit(data)</span><br><span class="line">norm_dist_fitted = norm(*fitting_params)</span><br><span class="line">t = np.linspace(<span class="number">0</span>,<span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line">plt.hist(data, bins=<span class="number">60</span>, density=<span class="keyword">True</span>)</span><br><span class="line">plt.plot(t, norm_dist_fitted.pdf(t))</span><br><span class="line">plt.title(<span class="string">'Example of Normally Distributed Data'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_20_0.png" alt="png"></p><p>We can transform our data to be closer to a normal distribution by removing the skew. One way to remove the skew is by applying the log function to the data.</p><p>Let’s plot the log of the feature variables to see that it produces the desired effect.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> X_train_raw.columns:</span><br><span class="line">    np.log(X_train_raw.loc[:, col]).hist()</span><br><span class="line">    plt.title(col)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_22_0.png" alt="png"></p><p><img src="output_22_1.png" alt="png"></p><p><img src="output_22_2.png" alt="png"></p><p><img src="output_22_3.png" alt="png"></p><p>We can see that the data is more symmetric after taking the log.</p><p><a name="4"></a></p><h2 id="4-Mean-Normalize-the-Data"><a href="#4-Mean-Normalize-the-Data" class="headerlink" title="4. Mean-Normalize the Data"></a>4. Mean-Normalize the Data</h2><p>Let’s now transform our data so that the distributions are closer to standard normal distributions.</p><p>First we will remove some of the skew from the distribution by using the log transformation.<br>Then we will “standardize” the distribution so that it has a mean of zero and standard deviation of 1. Recall that a standard normal distribution has mean of zero and standard deviation of 1. </p><p><a name="Ex-1"></a></p><h3 id="Exercise-1"><a href="#Exercise-1" class="headerlink" title="Exercise 1"></a>Exercise 1</h3><ul><li>Write a function that first removes some of the skew in the data, and then standardizes the distribution so that for each data point $x$,<script type="math/tex; mode=display">\overline{x} = \frac{x - mean(x)}{std(x)}</script></li><li>Keep in mind that we want to pretend that the test data is “unseen” data. <ul><li>This implies that it is unavailable to us for the purpose of preparing our data, and so we do not want to consider it when evaluating the mean and standard deviation that we use in the above equation. Instead we want to calculate these values using the training data alone, but then use them for standardizing both the training and the test data.</li><li>For a further discussion on the topic, see this article <a href="https://sebastianraschka.com/faq/docs/scale-training-test.html" target="_blank" rel="noopener">“Why do we need to re-use training parameters to transform test data”</a>. </li></ul></li></ul><h4 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h4><ul><li>For the sample standard deviation, please calculate the unbiased estimator:<script type="math/tex; mode=display">s = \sqrt{\frac{\sum_{i=1}^n(x_{i} - \bar{x})^2}{n-1}}</script></li><li>In other words, if you numpy, set the degrees of freedom <code>ddof</code> to 1.</li><li>For pandas, the default <code>ddof</code> is already set to 1.</li></ul><details>    <summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary><p>    <ul>        <li> When working with Pandas DataFrames, you can use the aggregation functions <code>mean</code> and <code>std</code> functions. Note that in order to apply an aggregation function separately for each row or each column, you'll set the axis parameter to either <code>0</code> or <code>1</code>. One produces the aggregation along columns and the other along rows, but it is easy to get them confused. So experiment with each option below to see which one you should use to get an average for each column in the dataframe.<code>avg = df.mean(axis=0)avg = df.mean(axis=1) </code>        </li>        <br><br>    <li>Remember to use <b>training</b> data statistics when standardizing both the training and the test data.</li>    </ul></p></details> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_standard_normal</span><span class="params">(df_train, df_test)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    In order to make the data closer to a normal distribution, take log</span></span><br><span class="line"><span class="string">    transforms to reduce the skew.</span></span><br><span class="line"><span class="string">    Then standardize the distribution with a mean of zero and standard deviation of 1. </span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      df_train (dataframe): unnormalized training data.</span></span><br><span class="line"><span class="string">      df_test (dataframe): unnormalized test data.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      df_train_normalized (dateframe): normalized training data.</span></span><br><span class="line"><span class="string">      df_test_normalized (dataframe): normalized test data.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###  </span></span><br><span class="line">    <span class="comment"># Remove skew by applying the log function to the train set, and to the test set</span></span><br><span class="line">    df_train_unskewed = np.log(df_train)</span><br><span class="line">    df_test_unskewed = np.log(df_test)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#calculate the mean and standard deviation of the training set</span></span><br><span class="line">    mean = df_train_unskewed.mean(axis = <span class="number">0</span>)</span><br><span class="line">    stdev = df_train_unskewed.std(axis = <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># standardize the training set</span></span><br><span class="line">    df_train_standardized = (df_train_unskewed - mean)/ stdev</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># standardize the test set (see instructions and hints above)</span></span><br><span class="line">    df_test_standardized = (df_test_unskewed - mean) / stdev</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> df_train_standardized, df_test_standardized</span><br></pre></td></tr></table></figure><h4 id="Test-Your-Work"><a href="#Test-Your-Work" class="headerlink" title="Test Your Work"></a>Test Your Work</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test</span></span><br><span class="line">tmp_train = pd.DataFrame(&#123;<span class="string">'field1'</span>: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">10</span>], <span class="string">'field2'</span>: [<span class="number">4</span>,<span class="number">5</span>,<span class="number">11</span>]&#125;)</span><br><span class="line">tmp_test = pd.DataFrame(&#123;<span class="string">'field1'</span>: [<span class="number">1</span>,<span class="number">3</span>,<span class="number">10</span>], <span class="string">'field2'</span>: [<span class="number">4</span>,<span class="number">6</span>,<span class="number">11</span>]&#125;)</span><br><span class="line">tmp_train_transformed, tmp_test_transformed = make_standard_normal(tmp_train,tmp_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"Training set transformed field1 has mean <span class="subst">&#123;tmp_train_transformed[<span class="string">'field1'</span>].mean(axis=<span class="number">0</span>):<span class="number">.4</span>f&#125;</span> and standard deviation <span class="subst">&#123;tmp_train_transformed[<span class="string">'field1'</span>].std(axis=<span class="number">0</span>):<span class="number">.4</span>f&#125;</span> "</span>)</span><br><span class="line">print(<span class="string">f"Test set transformed, field1 has mean <span class="subst">&#123;tmp_test_transformed[<span class="string">'field1'</span>].mean(axis=<span class="number">0</span>):<span class="number">.4</span>f&#125;</span> and standard deviation <span class="subst">&#123;tmp_test_transformed[<span class="string">'field1'</span>].std(axis=<span class="number">0</span>):<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Skew of training set field1 before transformation: <span class="subst">&#123;tmp_train[<span class="string">'field1'</span>].skew(axis=<span class="number">0</span>):<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Skew of training set field1 after transformation: <span class="subst">&#123;tmp_train_transformed[<span class="string">'field1'</span>].skew(axis=<span class="number">0</span>):<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Skew of test set field1 before transformation: <span class="subst">&#123;tmp_test[<span class="string">'field1'</span>].skew(axis=<span class="number">0</span>):<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Skew of test set field1 after transformation: <span class="subst">&#123;tmp_test_transformed[<span class="string">'field1'</span>].skew(axis=<span class="number">0</span>):<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Training set transformed field1 has mean -0.0000 and standard deviation 1.0000 Test set transformed, field1 has mean 0.1144 and standard deviation 0.9749Skew of training set field1 before transformation: 1.6523Skew of training set field1 after transformation: 1.0857Skew of test set field1 before transformation: 1.3896Skew of test set field1 after transformation: 0.1371</code></pre><h4 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output:"></a>Expected Output:</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Training <span class="built_in">set</span> transformed field1 has mean <span class="number">-0.0000</span> <span class="keyword">and</span> standard deviation <span class="number">1.0000</span> </span><br><span class="line">Test <span class="built_in">set</span> transformed, field1 has mean <span class="number">0.1144</span> <span class="keyword">and</span> standard deviation <span class="number">0.9749</span></span><br><span class="line">Skew of training <span class="built_in">set</span> field1 before transformation: <span class="number">1.6523</span></span><br><span class="line">Skew of training <span class="built_in">set</span> field1 after transformation: <span class="number">1.0857</span></span><br><span class="line">Skew of test <span class="built_in">set</span> field1 before transformation: <span class="number">1.3896</span></span><br><span class="line">Skew of test <span class="built_in">set</span> field1 after transformation: <span class="number">0.1371</span></span><br></pre></td></tr></table></figure><h4 id="Transform-training-and-test-data"><a href="#Transform-training-and-test-data" class="headerlink" title="Transform training and test data"></a>Transform training and test data</h4><p>Use the function that you just implemented to make the data distribution closer to a standard normal distribution.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test = make_standard_normal(X_train_raw, X_test_raw)</span><br></pre></td></tr></table></figure><p>After transforming the training and test sets, we’ll expect the training set to be centered at zero with a standard deviation of $1$.</p><p>We will avoid observing the test set during model training in order to avoid biasing the model training process, but let’s have a look at the distributions of the transformed training data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> X_train.columns:</span><br><span class="line">    X_train[col].hist()</span><br><span class="line">    plt.title(col)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_35_0.png" alt="png"></p><p><img src="output_35_1.png" alt="png"></p><p><img src="output_35_2.png" alt="png"></p><p><img src="output_35_3.png" alt="png"></p><p><a name="5"></a></p><h2 id="5-Build-the-Model"><a href="#5-Build-the-Model" class="headerlink" title="5. Build the Model"></a>5. Build the Model</h2><p>Now we are ready to build the risk model by training logistic regression with our data.</p><p><a name="Ex-2"></a></p><h3 id="Exercise-2"><a href="#Exercise-2" class="headerlink" title="Exercise 2"></a>Exercise 2</h3><ul><li>Implement the <code>lr_model</code> function to build a model using logistic regression with the <code>LogisticRegression</code> class from <code>sklearn</code>. </li><li>See the documentation for <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit" target="_blank" rel="noopener">sklearn.linear_model.LogisticRegression</a>.</li></ul><details>    <summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary><p>    <ul>        <li>You can leave all the parameters to their default values when constructing an instance of the <code>sklearn.linear_model.LogisticRegression</code> class. If you get a warning message regarding the <code>solver</code> parameter, however, you may want to specify that particular one explicitly with <code>solver='lbfgs'</code>.         </li>        <br><br>    </ul></p></details> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lr_model</span><span class="params">(X_train, y_train)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    <span class="comment"># import the LogisticRegression class</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># create the model object</span></span><br><span class="line">    model = LogisticRegression(solver=<span class="string">'lbfgs'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># fit the model to the training data</span></span><br><span class="line">    model.fit(X_train, y_train)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="comment">#return the fitted model</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h4 id="Test-Your-Work-1"><a href="#Test-Your-Work-1" class="headerlink" title="Test Your Work"></a>Test Your Work</h4><p>Note: the <code>predict</code> method returns the model prediction <em>after</em> converting it from a value in the $[0,1]$ range to a $0$ or $1$ depending on whether it is below or above $0.5$.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test</span></span><br><span class="line">tmp_model = lr_model(X_train[<span class="number">0</span>:<span class="number">3</span>], y_train[<span class="number">0</span>:<span class="number">3</span>] )</span><br><span class="line">print(tmp_model.predict(X_train[<span class="number">4</span>:<span class="number">5</span>]))</span><br><span class="line">print(tmp_model.predict(X_train[<span class="number">5</span>:<span class="number">6</span>]))</span><br></pre></td></tr></table></figure><pre><code>[1.][1.]</code></pre><h4 id="Expected-Output-1"><a href="#Expected-Output-1" class="headerlink" title="Expected Output:"></a>Expected Output:</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.</span>]</span><br><span class="line">[<span class="number">0.</span>]</span><br></pre></td></tr></table></figure><p>Now that we’ve tested our model, we can go ahead and build it. Note that the <code>lr_model</code> function also fits  the model to the training data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_X = lr_model(X_train, y_train)</span><br></pre></td></tr></table></figure><p><a name="6"></a></p><h2 id="6-Evaluate-the-Model-Using-the-C-index"><a href="#6-Evaluate-the-Model-Using-the-C-index" class="headerlink" title="6. Evaluate the Model Using the C-index"></a>6. Evaluate the Model Using the C-index</h2><p>Now that we have a model, we need to evaluate it. We’ll do this using the c-index. </p><ul><li>The c-index measures the discriminatory power of a risk score. </li><li>Intuitively, a higher c-index indicates that the model’s prediction is in agreement with the actual outcomes of a pair of patients.</li><li>The formula for the c-index is</li></ul><script type="math/tex; mode=display">\mbox{cindex} = \frac{\mbox{concordant} + 0.5 \times \mbox{ties}}{\mbox{permissible}}</script><ul><li>A permissible pair is a pair of patients who have different outcomes.</li><li>A concordant pair is a permissible pair in which the patient with the higher risk score also has the worse outcome.</li><li>A tie is a permissible pair where the patients have the same risk score.</li></ul><p><a name="Ex-3"></a></p><h3 id="Exercise-3"><a href="#Exercise-3" class="headerlink" title="Exercise 3"></a>Exercise 3</h3><ul><li>Implement the <code>cindex</code> function to compute c-index.</li><li><code>y_true</code> is the array of actual patient outcomes, 0 if the patient does not eventually get the disease, and 1 if the patient eventually gets the disease.</li><li><code>scores</code> is the risk score of each patient.  These provide relative measures of risk, so they can be any real numbers. By convention, they are always non-negative.</li><li><p>Here is an example of input data and how to interpret it:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_true = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">scores = [<span class="number">0.45</span>, <span class="number">1.25</span>]</span><br></pre></td></tr></table></figure><ul><li>There are two patients. Index 0 of each array is associated with patient 0.  Index 1 is associated with patient 1.</li><li>Patient 0 does not have the disease in the future (<code>y_true</code> is 0), and based on past information, has a risk score of 0.45.</li><li>Patient 1 has the disease at some point in the future (<code>y_true</code> is 1), and based on past information, has a risk score of 1.25.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cindex</span><span class="params">(y_true, scores)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    y_true (np.array): a 1-D array of true binary outcomes (values of zero or one)</span></span><br><span class="line"><span class="string">        0: patient does not get the disease</span></span><br><span class="line"><span class="string">        1: patient does get the disease</span></span><br><span class="line"><span class="string">    scores (np.array): a 1-D array of corresponding risk scores output by the model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">    c_index (float): (concordant pairs + 0.5*ties) / number of permissible pairs</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    n = len(y_true)</span><br><span class="line">    <span class="keyword">assert</span> len(scores) == n</span><br><span class="line"></span><br><span class="line">    concordant = <span class="number">0</span></span><br><span class="line">    permissible = <span class="number">0</span></span><br><span class="line">    ties = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    <span class="comment"># use two nested for loops to go through all unique pairs of patients</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, n): <span class="comment">#choose the range of j so that j&gt;i</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Check if the pair is permissible (the patient outcomes are different)</span></span><br><span class="line">            <span class="keyword">if</span> y_true[i] != y_true[j]:</span><br><span class="line">                <span class="comment"># Count the pair if it's permissible</span></span><br><span class="line">                permissible += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># For permissible pairs, check if they are concordant or are ties</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># check for ties in the score</span></span><br><span class="line">                <span class="keyword">if</span> scores[i] == scores[j]:</span><br><span class="line">                    <span class="comment"># count the tie</span></span><br><span class="line">                    ties += <span class="number">1</span></span><br><span class="line">                    <span class="comment"># if it's a tie, we don't need to check patient outcomes, continue to the top of the for loop.</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># case 1: patient i doesn't get the disease, patient j does</span></span><br><span class="line">                <span class="keyword">if</span> y_true[i] == <span class="number">0</span> <span class="keyword">and</span> y_true[j] == <span class="number">1</span>:</span><br><span class="line">                    <span class="comment"># Check if patient i has a lower risk score than patient j</span></span><br><span class="line">                    <span class="keyword">if</span> scores[i] &lt; scores[j]:</span><br><span class="line">                        <span class="comment"># count the concordant pair</span></span><br><span class="line">                        concordant += <span class="number">1</span></span><br><span class="line">                    <span class="comment"># Otherwise if patient i has a higher risk score, it's not a concordant pair.</span></span><br><span class="line">                    <span class="comment"># Already checked for ties earlier</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># case 2: patient i gets the disease, patient j does not</span></span><br><span class="line">                <span class="keyword">if</span> y_true[i] == <span class="number">1</span> <span class="keyword">and</span> y_true[j] == <span class="number">0</span>:</span><br><span class="line">                    <span class="comment"># Check if patient i has a higher risk score than patient j</span></span><br><span class="line">                    <span class="keyword">if</span> scores[i] &gt; scores[j]:</span><br><span class="line">                        <span class="comment">#count the concordant pair</span></span><br><span class="line">                        concordant += <span class="number">1</span></span><br><span class="line">                    <span class="comment"># Otherwise if patient i has a lower risk score, it's not a concordant pair.</span></span><br><span class="line">                    <span class="comment"># We already checked for ties earlier</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate the c-index using the count of permissible pairs, concordant pairs, and tied pairs.</span></span><br><span class="line">    c_index = (concordant + <span class="number">0.5</span> * ties) / permissible</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> c_index</span><br></pre></td></tr></table></figure><h4 id="Test-Your-Work-2"><a href="#Test-Your-Work-2" class="headerlink" title="Test Your Work"></a>Test Your Work</h4><p>You can use the following test cases to make sure your implementation is correct.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test</span></span><br><span class="line">y_true = np.array([<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 1</span></span><br><span class="line">scores = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">print(<span class="string">'Case 1 Output: &#123;&#125;'</span>.format(cindex(y_true, scores)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 2</span></span><br><span class="line">scores = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">'Case 2 Output: &#123;&#125;'</span>.format(cindex(y_true, scores)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 3</span></span><br><span class="line">scores = np.array([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">print(<span class="string">'Case 3 Output: &#123;&#125;'</span>.format(cindex(y_true, scores)))</span><br><span class="line">cindex(y_true, scores)</span><br></pre></td></tr></table></figure><pre><code>Case 1 Output: 0.0Case 2 Output: 1.0Case 3 Output: 0.8750.875</code></pre><h4 id="Expected-Output-2"><a href="#Expected-Output-2" class="headerlink" title="Expected Output:"></a>Expected Output:</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Case <span class="number">1</span> Output: <span class="number">0.0</span></span><br><span class="line">Case <span class="number">2</span> Output: <span class="number">1.0</span></span><br><span class="line">Case <span class="number">3</span> Output: <span class="number">0.875</span></span><br></pre></td></tr></table></figure><h4 id="Note-1"><a href="#Note-1" class="headerlink" title="Note"></a>Note</h4><p>Please check your implementation of the for loops. </p><ul><li>There is way to make a mistake on the for loops that cannot be caught with unit tests.</li><li>Bonus: Can you think of what this error could be, and why it can’t be caught by unit tests?</li></ul><p><a name="7"></a></p><h2 id="7-Evaluate-the-Model-on-the-Test-Set"><a href="#7-Evaluate-the-Model-on-the-Test-Set" class="headerlink" title="7. Evaluate the Model on the Test Set"></a>7. Evaluate the Model on the Test Set</h2><p>Now, you can evaluate your trained model on the test set.  </p><p>To get the predicted probabilities, we use the <code>predict_proba</code> method. This method will return the result from the model <em>before</em> it is converted to a binary 0 or 1. For each input case, it returns an array of two values which represent the probabilities for both the negative case (patient does not get the disease) and positive case (patient the gets the disease). </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scores = model_X.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line">c_index_X_test = cindex(y_test.values, scores)</span><br><span class="line">print(<span class="string">f"c-index on test set is <span class="subst">&#123;c_index_X_test:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>c-index on test set is 0.8182</code></pre><h4 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c-index on test <span class="built_in">set</span> is <span class="number">0.8336</span></span><br></pre></td></tr></table></figure><p>Let’s plot the coefficients to see which variables (patient features) are having the most effect. You can access the model coefficients by using <code>model.coef_</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">coeffs = pd.DataFrame(data = model_X.coef_, columns = X_train.columns)</span><br><span class="line">coeffs.T.plot.bar(legend=<span class="keyword">None</span>);</span><br></pre></td></tr></table></figure><p><img src="output_56_0.png" alt="png"></p><h3 id="Question"><a href="#Question" class="headerlink" title="Question:"></a>Question:</h3><blockquote><p><strong>Which three variables have the largest impact on the model’s predictions?</strong></p></blockquote><p><a name="8"></a></p><h2 id="8-Improve-the-Model"><a href="#8-Improve-the-Model" class="headerlink" title="8. Improve the Model"></a>8. Improve the Model</h2><p>You can try to improve your model by including interaction terms. </p><ul><li>An interaction term is the product of two variables. <ul><li>For example, if we have data <script type="math/tex; mode=display">x = [x_1, x_2]</script></li><li>We could add the product so that:<script type="math/tex; mode=display">\hat{x} = [x_1, x_2, x_1*x_2]</script></li></ul></li></ul><p><a name="Ex-4"></a></p><h3 id="Exercise-4"><a href="#Exercise-4" class="headerlink" title="Exercise 4"></a>Exercise 4</h3><p>Write code below to add all interactions between every pair of variables to the training and test datasets. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_interactions</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Add interaction terms between columns to dataframe.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">    X (dataframe): Original data</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_int (dataframe): Original data with interaction terms appended. </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    features = X.columns</span><br><span class="line">    m = len(features)</span><br><span class="line">    X_int = X.copy(deep=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    <span class="comment"># 'i' loops through all features in the original dataframe X</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m<span class="number">-1</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get the name of feature 'i'</span></span><br><span class="line">        feature_i_name = features[i]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get the data for feature 'i'</span></span><br><span class="line">        feature_i_data = X[feature_i_name]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># choose the index of column 'j' to be greater than column i</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, m):</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># get the name of feature 'j'</span></span><br><span class="line">            feature_j_name = features[j]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># get the data for feature j'</span></span><br><span class="line">            feature_j_data = X[feature_j_name]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># create the name of the interaction feature by combining both names</span></span><br><span class="line">            <span class="comment"># example: "apple" and "orange" are combined to be "apple_x_orange"</span></span><br><span class="line">            feature_i_j_name = feature_i_name + <span class="string">"_x_"</span> + feature_j_name</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Multiply the data for feature 'i' and feature 'j'</span></span><br><span class="line">            <span class="comment"># store the result as a column in dataframe X_int</span></span><br><span class="line">            X_int[feature_i_j_name] = feature_i_data * feature_j_data</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_int</span><br></pre></td></tr></table></figure><h4 id="Test-Your-Work-3"><a href="#Test-Your-Work-3" class="headerlink" title="Test Your Work"></a>Test Your Work</h4><p>Run the cell below to check your implementation. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Original Data"</span>)</span><br><span class="line">print(X_train.loc[:, [<span class="string">'Age'</span>, <span class="string">'Systolic_BP'</span>]].head())</span><br><span class="line">print(<span class="string">"Data w/ Interactions"</span>)</span><br><span class="line">print(add_interactions(X_train.loc[:, [<span class="string">'Age'</span>, <span class="string">'Systolic_BP'</span>]].head()))</span><br></pre></td></tr></table></figure><pre><code>Original Data           Age  Systolic_BP1824 -0.912451    -0.068019253  -0.302039     1.7195381114  2.576274     0.1559623220  1.163621    -2.0339312108 -0.446238    -0.054554Data w/ Interactions           Age  Systolic_BP  Age_x_Systolic_BP1824 -0.912451    -0.068019           0.062064253  -0.302039     1.719538          -0.5193671114  2.576274     0.155962           0.4018003220  1.163621    -2.033931          -2.3667252108 -0.446238    -0.054554           0.024344</code></pre><h4 id="Expected-Output-3"><a href="#Expected-Output-3" class="headerlink" title="Expected Output:"></a>Expected Output:</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Original Data</span><br><span class="line">           Age  Systolic_BP</span><br><span class="line"><span class="number">2472</span> <span class="number">-2.340711</span>     <span class="number">0.077089</span></span><br><span class="line"><span class="number">4496</span>  <span class="number">0.009916</span>    <span class="number">-1.324053</span></span><br><span class="line"><span class="number">2243</span>  <span class="number">0.927302</span>    <span class="number">-0.424337</span></span><br><span class="line"><span class="number">4311</span> <span class="number">-0.087282</span>     <span class="number">0.399865</span></span><br><span class="line"><span class="number">843</span>   <span class="number">2.204586</span>     <span class="number">0.025521</span></span><br><span class="line">Data w/ Interactions</span><br><span class="line">           Age  Systolic_BP  Age_x_Systolic_BP</span><br><span class="line"><span class="number">2472</span> <span class="number">-2.340711</span>     <span class="number">0.077089</span>          <span class="number">-0.180444</span></span><br><span class="line"><span class="number">4496</span>  <span class="number">0.009916</span>    <span class="number">-1.324053</span>          <span class="number">-0.013129</span></span><br><span class="line"><span class="number">2243</span>  <span class="number">0.927302</span>    <span class="number">-0.424337</span>          <span class="number">-0.393489</span></span><br><span class="line"><span class="number">4311</span> <span class="number">-0.087282</span>     <span class="number">0.399865</span>          <span class="number">-0.034901</span></span><br><span class="line"><span class="number">843</span>   <span class="number">2.204586</span>     <span class="number">0.025521</span>           <span class="number">0.056264</span></span><br></pre></td></tr></table></figure><p>Once you have correctly implemented <code>add_interactions</code>, use it to make transformed version of <code>X_train</code> and <code>X_test</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train_int = add_interactions(X_train)</span><br><span class="line">X_test_int = add_interactions(X_test)</span><br></pre></td></tr></table></figure><p><a name="9"></a></p><h2 id="9-Evaluate-the-Improved-Model"><a href="#9-Evaluate-the-Improved-Model" class="headerlink" title="9. Evaluate the Improved Model"></a>9. Evaluate the Improved Model</h2><p>Now we can train the new and improved version of the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_X_int = lr_model(X_train_int, y_train)</span><br></pre></td></tr></table></figure><p>Let’s evaluate our new model on the test set.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scores_X = model_X.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line">c_index_X_int_test = cindex(y_test.values, scores_X)</span><br><span class="line"></span><br><span class="line">scores_X_int = model_X_int.predict_proba(X_test_int)[:, <span class="number">1</span>]</span><br><span class="line">c_index_X_int_test = cindex(y_test.values, scores_X_int)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"c-index on test set without interactions is <span class="subst">&#123;c_index_X_test:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"c-index on test set with interactions is <span class="subst">&#123;c_index_X_int_test:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>c-index on test set without interactions is 0.8182c-index on test set with interactions is 0.8281</code></pre><p>You should see that the model with interaction terms performs a bit better than the model without interactions.</p><p>Now let’s take another look at the model coefficients to try and see which variables made a difference. Plot the coefficients and report which features seem to be the most important.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int_coeffs = pd.DataFrame(data = model_X_int.coef_, columns = X_train_int.columns)</span><br><span class="line">int_coeffs.T.plot.bar();</span><br></pre></td></tr></table></figure><p><img src="output_71_0.png" alt="png"></p><h3 id="Questions"><a href="#Questions" class="headerlink" title="Questions:"></a>Questions:</h3><blockquote><p><strong>Which variables are most important to the model?</strong><br><br><strong>Have the relevant variables changed?</strong><br><br><strong>What does it mean when the coefficients are positive or negative?</strong><br></p></blockquote><p>You may notice that Age, Systolic_BP, and Cholesterol have a positive coefficient. This means that a higher value in these three features leads to a higher prediction probability for the disease. You also may notice that the interaction of Age x Cholesterol has a negative coefficient. This means that a higher value for the Age x Cholesterol product reduces the prediction probability for the disease.</p><p>To understand the effect of interaction terms, let’s compare the output of the model we’ve trained on sample cases with and without the interaction. Run the cell below to choose an index and look at the features corresponding to that case in the training set. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index = index = <span class="number">3432</span></span><br><span class="line">case = X_train_int.iloc[index, :]</span><br><span class="line">print(case)</span><br></pre></td></tr></table></figure><pre><code>Age                           2.502061Systolic_BP                   1.713547Diastolic_BP                  0.268265Cholesterol                   2.146349Age_x_Systolic_BP             4.287400Age_x_Diastolic_BP            0.671216Age_x_Cholesterol             5.370296Systolic_BP_x_Diastolic_BP    0.459685Systolic_BP_x_Cholesterol     3.677871Diastolic_BP_x_Cholesterol    0.575791Name: 5970, dtype: float64</code></pre><p>We can see that they have above average Age and Cholesterol. We can now see what our original model would have output by zero-ing out the value for Cholesterol and Age.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_case = case.copy(deep=<span class="keyword">True</span>)</span><br><span class="line">new_case.loc[<span class="string">"Age_x_Cholesterol"</span>] = <span class="number">0</span></span><br><span class="line">new_case</span><br></pre></td></tr></table></figure><pre><code>Age                           2.502061Systolic_BP                   1.713547Diastolic_BP                  0.268265Cholesterol                   2.146349Age_x_Systolic_BP             4.287400Age_x_Diastolic_BP            0.671216Age_x_Cholesterol             0.000000Systolic_BP_x_Diastolic_BP    0.459685Systolic_BP_x_Cholesterol     3.677871Diastolic_BP_x_Cholesterol    0.575791Name: 5970, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f"Output with interaction: \t<span class="subst">&#123;model_X_int.predict_proba([case.values])[:, <span class="number">1</span>][<span class="number">0</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Output without interaction: \t<span class="subst">&#123;model_X_int.predict_proba([new_case.values])[:, <span class="number">1</span>][<span class="number">0</span>]:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Output with interaction:    0.9448Output without interaction:     0.9965</code></pre><h4 id="Expected-output-1"><a href="#Expected-output-1" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Output with interaction: <span class="number">0.9448</span></span><br><span class="line">Output without interaction: <span class="number">0.9965</span></span><br></pre></td></tr></table></figure><p>We see that the model is less confident in its prediction with the interaction term than without (the prediction value is lower when including the interaction term). With the interaction term, the model has adjusted for the fact that the effect of high cholesterol becomes less important for older patients compared to younger patients.</p><h1 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations!"></a>Congratulations!</h1><p>You have finished the first assignment of Course 2. </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Build-and-Evaluate-a-Linear-Risk-model&quot;&gt;&lt;a href=&quot;#Build-and-Evaluate-a-Linear-Risk-model&quot; class=&quot;headerlink&quot; title=&quot;Build and Evalua
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Machine Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Machine-Learning/"/>
    
    
      <category term="Medicine" scheme="https://zhangruochi.com/tags/Medicine/"/>
    
  </entry>
  
  <entry>
    <title>Brain Tumor Auto-Segmentation for Magnetic Resonance Imaging (MRI)</title>
    <link href="https://zhangruochi.com/Brain-Tumor-Auto-Segmentation-for-Magnetic-Resonance-Imaging-MRI/2020/04/17/"/>
    <id>https://zhangruochi.com/Brain-Tumor-Auto-Segmentation-for-Magnetic-Resonance-Imaging-MRI/2020/04/17/</id>
    <published>2020-04-17T15:10:42.000Z</published>
    <updated>2020-04-19T15:10:02.269Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://miro.medium.com/max/2652/1*eTkBMyqdg9JodNcG_O4-Kw.jpeg" width="100%"></p><p><a href="https://medium.com/stanford-ai-for-healthcare/its-a-no-brainer-deep-learning-for-brain-mr-images-f60116397472" target="_blank" rel="noopener">Image Source</a></p><h1 id="Brain-Tumor-Auto-Segmentation-for-Magnetic-Resonance-Imaging-MRI"><a href="#Brain-Tumor-Auto-Segmentation-for-Magnetic-Resonance-Imaging-MRI" class="headerlink" title="Brain Tumor Auto-Segmentation for Magnetic Resonance Imaging (MRI)"></a>Brain Tumor Auto-Segmentation for Magnetic Resonance Imaging (MRI)</h1><p>Welcome to the final part of the “Artificial Intelligence for Medicine” course 1!</p><p>You will learn how to build a neural network to automatically segment tumor regions in brain, using <a href="https://en.wikipedia.org/wiki/Magnetic_resonance_imaging" target="_blank" rel="noopener">MRI (Magnetic Resonance Imaging</a>) scans.</p><p>The MRI scan is one of the most common image modalities that we encounter in the radiology field.<br>Other data modalities include: </p><ul><li><a href="https://en.wikipedia.org/wiki/CT_scan" target="_blank" rel="noopener">Computer Tomography (CT)</a>, </li><li><a href="https://en.wikipedia.org/wiki/Ultrasound" target="_blank" rel="noopener">Ultrasound</a></li><li><a href="https://en.wikipedia.org/wiki/X-ray" target="_blank" rel="noopener">X-Rays</a>. </li></ul><p>In this assignment we will be focusing on MRIs but many of our learnings applies to other mentioned modalities as well.  We’ll walk you through some of the steps of training a deep learning model for segmentation.</p><p><strong>You will learn:</strong></p><ul><li>What is in an MR image</li><li>Standard data preparation techniques for MRI datasets</li><li>Metrics and loss functions for segmentation</li><li>Visualizing and evaluating segmentation models</li></ul><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><p>Use these links to jump to particular sections of this assignment!</p><ul><li><a href="#1">1. Dataset</a><ul><li><a href="#1-1">1.1 What is an MRI?</a></li><li><a href="#1-2">1.2 MRI Data Processing</a></li><li><a href="#1-3">1.3 Exploring the Dataset</a></li><li><a href="#1-4">1.4 Data Preprocessing</a><ul><li><a href="#1-4-1">1.4.1 Sub-volume Sampling</a></li><li><a href="#1-4-2">1.4.2 Standardization</a></li></ul></li></ul></li><li><a href="#2">2. Model: 3D U-Net</a></li><li><a href="#3">3. Metrics</a><ul><li><a href="#3-1">3.1 Dice Coefficient</a></li><li><a href="#3-2">3.2 Soft Dice Loss</a></li></ul></li><li><a href="#4">4. Training</a></li><li><a href="#5">5. Evaluation</a><ul><li><a href="#5-1">5.1 Overall Performance</a></li><li><a href="#5-2">5.2 Patch-level Predictions</a></li><li><a href="#5-3">5.3 Running on Entire Scans</a></li></ul></li></ul><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>In this assignment, we’ll make use of the following packages:</p><ul><li><code>keras</code> is a framework for building deep learning models.</li><li><code>keras.backend</code> allows us to perform math operations on tensors.</li><li><code>nibabel</code> will let us extract the images and labels from the files in our dataset.</li><li><code>numpy</code> is a library for mathematical and scientific operations.</li><li><code>pandas</code> is what we’ll use to manipulate our data.</li></ul><h2 id="Import-Packages"><a href="#Import-Packages" class="headerlink" title="Import Packages"></a>Import Packages</h2><p>Run the next cell to import all the necessary packages, dependencies and custom util functions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> nibabel <span class="keyword">as</span> nib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> backend <span class="keyword">as</span> K </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> util</span><br></pre></td></tr></table></figure><pre><code>Using TensorFlow backend.</code></pre><p><a name="1"></a></p><h1 id="1-Dataset"><a href="#1-Dataset" class="headerlink" title="1 Dataset"></a>1 Dataset</h1><p><a name="1-1"></a></p><h2 id="1-1-What-is-an-MRI"><a href="#1-1-What-is-an-MRI" class="headerlink" title="1.1 What is an MRI?"></a>1.1 What is an MRI?</h2><p>Magnetic resonance imaging (MRI) is an advanced imaging technique that is used to observe a variety of diseases and parts of the body. </p><p>As we will see later, neural networks can analyze these images individually (as a radiologist would) or combine them into a single 3D volume to make predictions.</p><p>At a high level, MRI works by measuring the radio waves emitting by atoms subjected to a magnetic field. </p><p><img src="https://miro.medium.com/max/1740/1*yC1Bt3IOzNv8Pp7t1v7F1Q.png"></p><p>In this assignment, we’ll build a multi-class segmentation model. We’ll  identify 3 different abnormalities in each image: edemas, non-enhancing tumors, and enhancing tumors.</p><p><a name="1-2"></a></p><h2 id="1-2-MRI-Data-Processing"><a href="#1-2-MRI-Data-Processing" class="headerlink" title="1.2 MRI Data Processing"></a>1.2 MRI Data Processing</h2><p>We often encounter MR images in the <a href="https://en.wikipedia.org/wiki/DICOM" target="_blank" rel="noopener">DICOM format</a>. </p><ul><li>The DICOM format is the output format for most commercial MRI scanners. This type of data can be processed using the <a href="https://pydicom.github.io/pydicom/stable/getting_started.html" target="_blank" rel="noopener">pydicom</a> Python library. </li></ul><p>In this assignment, we will be using the data from the <a href="https://decathlon-10.grand-challenge.org" target="_blank" rel="noopener">Decathlon 10 Challenge</a>. This data has been mostly pre-processed for the competition participants, however in real practice, MRI data needs to be significantly pre-preprocessed before we can use it to train our models.</p><p><a name="1-3"></a></p><h2 id="1-3-Exploring-the-Dataset"><a href="#1-3-Exploring-the-Dataset" class="headerlink" title="1.3 Exploring the Dataset"></a>1.3 Exploring the Dataset</h2><p>Our dataset is stored in the <a href="https://nifti.nimh.nih.gov/nifti-1/" target="_blank" rel="noopener">NifTI-1 format</a> and we will be using the <a href="https://github.com/nipy/nibabel" target="_blank" rel="noopener">NiBabel library</a> to interact with the files. Each training sample is composed of two separate files:</p><p>The first file is an image file containing a 4D array of MR image in the shape of (240, 240, 155, 4). </p><ul><li>The first 3 dimensions are the X, Y, and Z values for each point in the 3D volume, which is commonly called a voxel. </li><li>The 4th dimension is the values for 4 different sequences<ul><li>0: FLAIR: “Fluid Attenuated Inversion Recovery” (FLAIR)</li><li>1: T1w: “T1-weighted”</li><li>2: t1gd: “T1-weighted with gadolinium contrast enhancement” (T1-Gd)</li><li>3: T2w: “T2-weighted”</li></ul></li></ul><p>The second file in each training example is a label file containing a 3D array with the shape of (240, 240, 155).  </p><ul><li>The integer values in this array indicate the “label” for each voxel in the corresponding image files:<ul><li>0: background</li><li>1: edema</li><li>2: non-enhancing tumor</li><li>3: enhancing tumor</li></ul></li></ul><p>We have access to a total of 484 training images which we will be splitting into a training (80%) and validation (20%) dataset.</p><p>Let’s begin by looking at one single case and visualizing the data! You have access to 10 different cases via this notebook and we strongly encourage you to explore the data further on your own.</p><p>We’ll use the <a href="https://nipy.org/nibabel/nibabel_images.html" target="_blank" rel="noopener">NiBabel library</a> to load the image and label for a case. The function is shown below to give you a sense of how it works. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set home directory and data directory</span></span><br><span class="line">HOME_DIR = <span class="string">"./BraTS-Data/"</span></span><br><span class="line">DATA_DIR = HOME_DIR</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_case</span><span class="params">(image_nifty_file, label_nifty_file)</span>:</span></span><br><span class="line">    <span class="comment"># load the image and label file, get the image content and return a numpy array for each</span></span><br><span class="line">    image = np.array(nib.load(image_nifty_file).get_fdata())</span><br><span class="line">    label = np.array(nib.load(label_nifty_file).get_fdata())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure><p>We’ll now visualize an example.  For this, we use a pre-defined function we have written in the <code>util.py</code> file that uses <code>matplotlib</code> to generate a summary of the image. </p><p>The colors correspond to each class.</p><ul><li>Red is edema</li><li>Green is a non-enhancing tumor</li><li>Blue is an enhancing tumor. </li></ul><p>Do feel free to look at this function at your own time to understand how this is achieved.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">image, label = load_case(DATA_DIR + <span class="string">"imagesTr/BRATS_003.nii.gz"</span>, DATA_DIR + <span class="string">"labelsTr/BRATS_003.nii.gz"</span>)</span><br><span class="line">image = util.get_labeled_image(image, label)</span><br><span class="line"></span><br><span class="line">util.plot_image_grid(image)</span><br></pre></td></tr></table></figure><p><img src="output_10_0.png" alt="png"></p><p>We’ve also written a utility function which generates a GIF that shows what it looks like to iterate over each axis.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">image, label = load_case(DATA_DIR + <span class="string">"imagesTr/BRATS_003.nii.gz"</span>, DATA_DIR + <span class="string">"labelsTr/BRATS_003.nii.gz"</span>)</span><br><span class="line">util.visualize_data_gif(util.get_labeled_image(image, label))</span><br></pre></td></tr></table></figure><p><img src="output_12_0.png" alt="png"></p><p><strong>Reminder:</strong> You can explore more images in the <code>imagesTr</code> directory by changing the image name file.</p><p><a name="1-4"></a></p><h2 id="1-4-Data-Preprocessing-using-patches"><a href="#1-4-Data-Preprocessing-using-patches" class="headerlink" title="1.4 Data Preprocessing using patches"></a>1.4 Data Preprocessing using patches</h2><p>While our dataset is provided to us post-registration and in the NIfTI format, we still have to do some minor pre-processing before feeding the data to our model. </p><h5 id="Generate-sub-volumes"><a href="#Generate-sub-volumes" class="headerlink" title="Generate sub-volumes"></a>Generate sub-volumes</h5><p>We are going to first generate “patches” of our data which you can think of as sub-volumes of the whole MR images. </p><ul><li>The reason that we are generating patches is because a network that can process the entire volume at once will simply not fit inside our current environment’s memory/GPU.</li><li>Therefore we will be using this common technique to generate spatially consistent sub-volumes of our data, which can be fed into our network.</li><li>Specifically, we will be generating randomly sampled sub-volumes of shape [160, 160, 16] from our images. </li><li>Furthermore, given that a large portion of the MRI volumes are just brain tissue or black background without any tumors, we want to make sure that we pick patches that at least include some amount of tumor data. </li><li>Therefore, we are only going to pick patches that have at most 95% non-tumor regions (so at least 5% tumor). </li><li>We do this by filtering the volumes based on the values present in the background labels.</li></ul><h5 id="Standardization-mean-0-stdev-1"><a href="#Standardization-mean-0-stdev-1" class="headerlink" title="Standardization (mean 0, stdev 1)"></a>Standardization (mean 0, stdev 1)</h5><p>Lastly, given that the values in MR images cover a very wide range, we will standardize the values to have a mean of zero and standard deviation of 1. </p><ul><li>This is a common technique in deep image processing since standardization makes it much easier for the network to learn.</li></ul><p>Let’s walk through these steps in the following exercises.</p><p><a name="1-4-1"></a></p><h3 id="1-4-1-Sub-volume-Sampling"><a href="#1-4-1-Sub-volume-Sampling" class="headerlink" title="1.4.1 Sub-volume Sampling"></a>1.4.1 Sub-volume Sampling</h3><p>Fill in the function below takes in:</p><ul><li>a 4D image (shape: [240, 240, 155, 4])</li><li>its 3D label (shape: [240, 240, 155]) arrays, </li></ul><p>The function returns:</p><ul><li>A randomly generated sub-volume of size [160, 160, 16]</li><li>Its corresponding label in a 1-hot format which has the shape [3, 160, 160, 160]</li></ul><p>Additionally: </p><ol><li>Make sure that at most 95% of the returned patch is non-tumor regions. </li><li>Given that our network expects the channels for our images to appear as the first dimension (instead of the last one in our current setting) reorder the dimensions of the image to have the channels appear as the first dimension.</li><li>Reorder the dimensions of the label array to have the first dimension as the classes (instead of the last one in our current setting)</li><li>Reduce the labels array dimension to only include the non-background classes (total of 3 instead of 4)</li></ol><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Check the lecture notebook for a similar example in 1 dimension</li>    <li>To check the ratio of background to the whole sub-volume, the numerator is the number of background labels in the sub-volume.  The last dimension of the label array at index 0 contains the labels to identify whether the voxel is a background (value of 1) or not a a background (value of 0).        </li>    <li>For the denominator of the background ratio, this is the volume of the output (see <code>output_x</code>, <code>output_y</code>, <code>output_z</code> in the function parameters).</li>    <li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical" target="_blank" rel="noopener">keras.utils.to_categorical(y, num_classes=)</a></li>    <li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.moveaxis.html" target="_blank" rel="noopener"> np.moveaxis </a> can help you re-arrange the dimensions of the arrays </li>    <li> <a href="https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randint.html" target="_blank" rel="noopener">np.random.randint</a> for random sampling</li>    <li> When taking a subset of the label <code>'y'</code> that excludes the background class, remember which dimension contains the <code>'num_classes'</code> channel after re-ordering the axes. </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sub_volume</span><span class="params">(image, label, </span></span></span><br><span class="line"><span class="function"><span class="params">                   orig_x = <span class="number">240</span>, orig_y = <span class="number">240</span>, orig_z = <span class="number">155</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                   output_x = <span class="number">160</span>, output_y = <span class="number">160</span>, output_z = <span class="number">16</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                   num_classes = <span class="number">4</span>, max_tries = <span class="number">1000</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                   background_threshold=<span class="number">0.95</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Extract random sub-volume from original images.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        image (np.array): original image, </span></span><br><span class="line"><span class="string">            of shape (orig_x, orig_y, orig_z, num_channels)</span></span><br><span class="line"><span class="string">        label (np.array): original label. </span></span><br><span class="line"><span class="string">            labels coded using discrete values rather than</span></span><br><span class="line"><span class="string">            a separate dimension, </span></span><br><span class="line"><span class="string">            so this is of shape (orig_x, orig_y, orig_z)</span></span><br><span class="line"><span class="string">        orig_x (int): x_dim of input image</span></span><br><span class="line"><span class="string">        orig_y (int): y_dim of input image</span></span><br><span class="line"><span class="string">        orig_z (int): z_dim of input image</span></span><br><span class="line"><span class="string">        output_x (int): desired x_dim of output</span></span><br><span class="line"><span class="string">        output_y (int): desired y_dim of output</span></span><br><span class="line"><span class="string">        output_z (int): desired z_dim of output</span></span><br><span class="line"><span class="string">        num_classes (int): number of class labels</span></span><br><span class="line"><span class="string">        max_tries (int): maximum trials to do when sampling</span></span><br><span class="line"><span class="string">        background_threshold (float): limit on the fraction </span></span><br><span class="line"><span class="string">            of the sample which can be the background</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    returns:</span></span><br><span class="line"><span class="string">        X (np.array): sample of original image of dimension </span></span><br><span class="line"><span class="string">            (num_channels, output_x, output_y, output_z)</span></span><br><span class="line"><span class="string">        y (np.array): labels which correspond to X, of dimension </span></span><br><span class="line"><span class="string">            (num_classes, output_x, output_y, output_z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Initialize features and labels with `None`</span></span><br><span class="line">    X = <span class="keyword">None</span></span><br><span class="line">    y = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    tries = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> tries &lt; max_tries:</span><br><span class="line">        <span class="comment"># randomly sample sub-volume by sampling the corner voxel</span></span><br><span class="line">        <span class="comment"># hint: make sure to leave enough room for the output dimensions!</span></span><br><span class="line">        start_x = np.random.randint(orig_x - output_x + <span class="number">1</span> )</span><br><span class="line">        start_y = np.random.randint(orig_y - output_y + <span class="number">1</span> )</span><br><span class="line">        start_z = np.random.randint(orig_z - output_z + <span class="number">1</span> )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># extract relevant area of label</span></span><br><span class="line">        y = label[start_x: start_x + output_x,</span><br><span class="line">                  start_y: start_y + output_y,</span><br><span class="line">                  start_z: start_z + output_z]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># One-hot encode the categories.</span></span><br><span class="line">        <span class="comment"># This adds a 4th dimension, 'num_classes'</span></span><br><span class="line">        <span class="comment"># (output_x, output_y, output_z, num_classes)</span></span><br><span class="line">        y = keras.utils.to_categorical(y, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute the background ratio</span></span><br><span class="line">        bgrd_ratio = y[:,:,:, <span class="number">0</span>].sum() / (output_x * output_y * output_z)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># increment tries counter</span></span><br><span class="line">        tries += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># if background ratio is below the desired threshold,</span></span><br><span class="line">        <span class="comment"># use that sub-volume.</span></span><br><span class="line">        <span class="comment"># otherwise continue the loop and try another random sub-volume</span></span><br><span class="line">        <span class="keyword">if</span> bgrd_ratio &lt; background_threshold:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># make copy of the sub-volume</span></span><br><span class="line">            X = np.copy(image[start_x: start_x + output_x,</span><br><span class="line">                              start_y: start_y + output_y,</span><br><span class="line">                              start_z: start_z + output_z, :])</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># change dimension of X</span></span><br><span class="line">            <span class="comment"># from (x_dim, y_dim, z_dim, num_channels)</span></span><br><span class="line">            <span class="comment"># to (num_channels, x_dim, y_dim, z_dim)</span></span><br><span class="line">            X = np.moveaxis(X, <span class="number">-1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># change dimension of y</span></span><br><span class="line">            <span class="comment"># from (x_dim, y_dim, z_dim, num_classes)</span></span><br><span class="line">            <span class="comment"># to (num_classes, x_dim, y_dim, z_dim)</span></span><br><span class="line">            y = np.moveaxis(y, <span class="number">-1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment">### END CODE HERE ###</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># take a subset of y that excludes the background class</span></span><br><span class="line">            <span class="comment"># in the 'num_classes' dimension</span></span><br><span class="line">            y = y[<span class="number">1</span>:, :, :, :]</span><br><span class="line">    </span><br><span class="line">            <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line">    <span class="comment"># if we've tried max_tries number of samples</span></span><br><span class="line">    <span class="comment"># Give up in order to avoid looping forever.</span></span><br><span class="line">    print(<span class="string">f"Tried <span class="subst">&#123;tries&#125;</span> times to find a sub-volume. Giving up..."</span>)</span><br></pre></td></tr></table></figure><h3 id="Test-Case"><a href="#Test-Case" class="headerlink" title="Test Case:"></a>Test Case:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">image = np.zeros((<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">label = np.zeros((<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">            image[i, j, k, <span class="number">0</span>] = i*j*k</span><br><span class="line">            label[i, j, k] = k</span><br><span class="line"></span><br><span class="line">print(<span class="string">"image:"</span>)</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    print(<span class="string">f"z = <span class="subst">&#123;k&#125;</span>"</span>)</span><br><span class="line">    print(image[:, :, k, <span class="number">0</span>])</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">"label:"</span>)</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    print(<span class="string">f"z = <span class="subst">&#123;k&#125;</span>"</span>)</span><br><span class="line">    print(label[:, :, k])</span><br></pre></td></tr></table></figure><pre><code>image:z = 0[[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]]z = 1[[0. 0. 0. 0.] [0. 1. 2. 3.] [0. 2. 4. 6.] [0. 3. 6. 9.]]z = 2[[ 0.  0.  0.  0.] [ 0.  2.  4.  6.] [ 0.  4.  8. 12.] [ 0.  6. 12. 18.]]label:z = 0[[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]]z = 1[[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]]z = 2[[2. 2. 2. 2.] [2. 2. 2. 2.] [2. 2. 2. 2.] [2. 2. 2. 2.]]</code></pre><h4 id="Test-Extracting-2-2-2-sub-volume"><a href="#Test-Extracting-2-2-2-sub-volume" class="headerlink" title="Test: Extracting (2, 2, 2) sub-volume"></a>Test: Extracting (2, 2, 2) sub-volume</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sample_image, sample_label = get_sub_volume(image, </span><br><span class="line">                                            label,</span><br><span class="line">                                            orig_x=<span class="number">4</span>, </span><br><span class="line">                                            orig_y=<span class="number">4</span>, </span><br><span class="line">                                            orig_z=<span class="number">3</span>,</span><br><span class="line">                                            output_x=<span class="number">2</span>, </span><br><span class="line">                                            output_y=<span class="number">2</span>, </span><br><span class="line">                                            output_z=<span class="number">2</span>,</span><br><span class="line">                                            num_classes = <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Sampled Image:"</span>)</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    print(<span class="string">"z = "</span> + str(k))</span><br><span class="line">    print(sample_image[<span class="number">0</span>, :, :, k])</span><br></pre></td></tr></table></figure><pre><code>Sampled Image:z = 0[[0. 2.] [0. 3.]]z = 1[[0. 4.] [0. 6.]]</code></pre><h4 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Sampled Image:</span><br><span class="line">z = <span class="number">0</span></span><br><span class="line">[[<span class="number">0.</span> <span class="number">2.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">3.</span>]]</span><br><span class="line">z = <span class="number">1</span></span><br><span class="line">[[<span class="number">0.</span> <span class="number">4.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">6.</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Sampled Label:"</span>)</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    print(<span class="string">"class = "</span> + str(c))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        print(<span class="string">"z = "</span> + str(k))</span><br><span class="line">        print(sample_label[c, :, :, k])</span><br></pre></td></tr></table></figure><pre><code>Sampled Label:class = 0z = 0[[1. 1.] [1. 1.]]z = 1[[0. 0.] [0. 0.]]class = 1z = 0[[0. 0.] [0. 0.]]z = 1[[1. 1.] [1. 1.]]</code></pre><h4 id="Expected-output-1"><a href="#Expected-output-1" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Sampled Label:</span><br><span class="line">class = 0</span><br><span class="line">z = <span class="number">0</span></span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span>]]</span><br><span class="line">z = <span class="number">1</span></span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">class = 1</span><br><span class="line">z = <span class="number">0</span></span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">z = <span class="number">1</span></span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span>]]</span><br></pre></td></tr></table></figure><p>You can run the following cell to look at a candidate patch and ensure that the function works correctly. We’ll look at the enhancing tumor part of the label.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">image, label = load_case(DATA_DIR + <span class="string">"imagesTr/BRATS_001.nii.gz"</span>, DATA_DIR + <span class="string">"labelsTr/BRATS_001.nii.gz"</span>)</span><br><span class="line">X, y = get_sub_volume(image, label)</span><br><span class="line"><span class="comment"># enhancing tumor is channel 2 in the class label</span></span><br><span class="line"><span class="comment"># you can change indexer for y to look at different classes</span></span><br><span class="line">util.visualize_patch(X[<span class="number">0</span>, :, :, :], y[<span class="number">2</span>])</span><br></pre></td></tr></table></figure><p><img src="output_26_0.png" alt="png"></p><p><a name="1-4-2"></a></p><h3 id="1-4-2-Standardization"><a href="#1-4-2-Standardization" class="headerlink" title="1.4.2 Standardization"></a>1.4.2 Standardization</h3><p>Next, fill in the following function that given a patch (sub-volume), standardizes the values across each channel and each Z plane to have a mean of zero and standard deviation of 1. </p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li> Check that the standard deviation is not zero before dividing by it.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standardize</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Standardize mean and standard deviation </span></span><br><span class="line"><span class="string">        of each channel and z_dimension.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        image (np.array): input image, </span></span><br><span class="line"><span class="string">            shape (num_channels, dim_x, dim_y, dim_z)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        standardized_image (np.array): standardized version of input image</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize to array of zeros, with same shape as the image</span></span><br><span class="line">    standardized_image = np.empty(image.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># iterate over channels</span></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(image.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="comment"># iterate over the `z` dimension</span></span><br><span class="line">        <span class="keyword">for</span> z <span class="keyword">in</span> range(image.shape[<span class="number">3</span>]):</span><br><span class="line">            <span class="comment"># get a slice of the image </span></span><br><span class="line">            <span class="comment"># at channel c and z-th dimension `z`</span></span><br><span class="line">            image_slice = image[c,:,:,z]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># subtract the mean from image_slice</span></span><br><span class="line">            centered = image_slice - image_slice.mean()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> image_slice.std() != <span class="number">0</span>:</span><br><span class="line">                centered_scaled = image_slice / image_slice.std()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                centered_scaled = centered</span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">            <span class="comment"># update  the slice of standardized image</span></span><br><span class="line">            <span class="comment"># with the scaled centered and scaled image</span></span><br><span class="line">            standardized_image[c, :, :, z] = centered_scaled</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> standardized_image</span><br></pre></td></tr></table></figure><p>And to sanity check, let’s look at the output of our function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_norm = standardize(X)</span><br><span class="line">print(<span class="string">"standard deviation for a slice should be 1.0"</span>)</span><br><span class="line">print(<span class="string">f"stddv for X_norm[0, :, :, 0]: <span class="subst">&#123;X_norm[<span class="number">0</span>,:,:,<span class="number">0</span>].std():<span class="number">.2</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>standard deviation for a slice should be 1.0stddv for X_norm[0, :, :, 0]: 1.00</code></pre><p>Let’s visualize our patch again just to make sure (it won’t look different since the <code>imshow</code> function we use to visualize automatically normalizes the pixels when displaying in black and white).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.visualize_patch(X_norm[<span class="number">0</span>, :, :, :], y[<span class="number">2</span>])</span><br></pre></td></tr></table></figure><p><img src="output_33_0.png" alt="png"></p><p><a name="2"></a></p><h1 id="2-Model-3D-U-Net"><a href="#2-Model-3D-U-Net" class="headerlink" title="2 Model: 3D U-Net"></a>2 Model: 3D U-Net</h1><p>Now let’s build our model. In this assignment we will be building a <a href="https://arxiv.org/abs/1606.06650" target="_blank" rel="noopener">3D U-net</a>. </p><ul><li>This architecture will take advantage of the volumetric shape of MR images and is one of the best performing models for this task. </li><li>Feel free to familiarize yourself with the architecture by reading <a href="https://arxiv.org/abs/1606.06650" target="_blank" rel="noopener">this paper</a>.</li></ul><p><img src="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png" width="50%"></p><p><a name="3"></a></p><h1 id="3-Metrics"><a href="#3-Metrics" class="headerlink" title="3 Metrics"></a>3 Metrics</h1><p><a name="3-1"></a></p><h2 id="3-1-Dice-Similarity-Coefficient"><a href="#3-1-Dice-Similarity-Coefficient" class="headerlink" title="3.1 Dice Similarity Coefficient"></a>3.1 Dice Similarity Coefficient</h2><p>Aside from the architecture, one of the most important elements of any deep learning method is the choice of our loss function. </p><p>A natural choice that you may be familiar with is the cross-entropy loss function. </p><ul><li>However, this loss function is not ideal for segmentation tasks due to heavy class imbalance (there are typically not many positive regions). </li></ul><p>A much more common loss for segmentation tasks is the Dice similarity coefficient, which is a measure of how well two contours overlap. </p><ul><li>The Dice index ranges from 0 (complete mismatch) </li><li>To 1 (perfect match).</li></ul><p>In general, for two sets $A$ and $B$, the Dice similarity coefficient is defined as: </p><script type="math/tex; mode=display">\text{DSC}(A, B) = \frac{2 \times |A \cap B|}{|A| + |B|}.</script><p>Here we can interpret $A$ and $B$ as sets of voxels, $A$ being the predicted tumor region and $B$ being the ground truth. </p><p>Our model will map each voxel to 0 or 1</p><ul><li>0 means it is a background voxel</li><li>1 means it is part of the segmented region.</li></ul><p>In the dice coefficient, the variables in the formula are:</p><ul><li>$x$ : the input image</li><li>$f(x)$ : the model output (prediction)</li><li>$y$ : the label (actual ground truth)</li></ul><p>The dice coefficient “DSC” is:</p><script type="math/tex; mode=display">\text{DSC}(f, x, y) = \frac{2 \times \sum_{i, j} f(x)_{ij} \times y_{ij} + \epsilon}{\sum_{i,j} f(x)_{ij} + \sum_{i, j} y_{ij} + \epsilon}</script><ul><li>$\epsilon$ is a small number that is added to avoid division by zero</li></ul><p><img src="https://www.researchgate.net/publication/328671987/figure/fig4/AS:688210103529478@1541093483784/Calculation-of-the-Dice-similarity-coefficient-The-deformed-contour-of-the-liver-from.ppm" width="30%"></p><p><a href="https://www.researchgate.net/figure/Calculation-of-the-Dice-similarity-coefficient-The-deformed-contour-of-the-liver-from_fig4_328671987" target="_blank" rel="noopener">Image Source</a></p><p>Implement the dice coefficient for a single output class below.</p><ul><li>Please use the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/backend/sum" target="_blank" rel="noopener">Keras.sum(x,axis=)</a> function to compute the numerator and denominator of the dice coefficient.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">single_class_dice_coefficient</span><span class="params">(y_true, y_pred, axis=<span class="params">(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>)</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                                  epsilon=<span class="number">0.00001</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute dice coefficient for single class.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y_true (Tensorflow tensor): tensor of ground truth values for single class.</span></span><br><span class="line"><span class="string">                                    shape: (x_dim, y_dim, z_dim)</span></span><br><span class="line"><span class="string">        y_pred (Tensorflow tensor): tensor of predictions for single class.</span></span><br><span class="line"><span class="string">                                    shape: (x_dim, y_dim, z_dim)</span></span><br><span class="line"><span class="string">        axis (tuple): spatial axes to sum over when computing numerator and</span></span><br><span class="line"><span class="string">                      denominator of dice coefficient.</span></span><br><span class="line"><span class="string">                      Hint: pass this as the 'axis' argument to the K.sum function.</span></span><br><span class="line"><span class="string">        epsilon (float): small constant added to numerator and denominator to</span></span><br><span class="line"><span class="string">                        avoid divide by 0 errors.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dice_coefficient (float): computed value of dice coefficient.     </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    dice_numerator = K.sum(<span class="number">2</span> * y_true * y_pred, axis= axis) + epsilon</span><br><span class="line">    dice_denominator = K.sum(y_true,axis= axis) + K.sum(y_pred, axis = axis) + epsilon</span><br><span class="line">    dice_coefficient = dice_numerator / dice_denominator </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dice_coefficient</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEST CASES</span></span><br><span class="line">sess = K.get_session()</span><br><span class="line"><span class="comment">#sess = tf.compat.v1.Session()</span></span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    pred = np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Test Case #1"</span>)</span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(pred[:, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(label[:, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># choosing a large epsilon to help check for implementation errors</span></span><br><span class="line">    dc = single_class_dice_coefficient(pred, label,epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"dice coefficient: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Test Case #2"</span>)</span><br><span class="line">    pred = np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]]), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(pred[:, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(label[:, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># choosing a large epsilon to help check for implementation errors</span></span><br><span class="line">    dc = single_class_dice_coefficient(pred, label,epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"dice_coefficient: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #1pred:[[1. 0.] [0. 1.]]label:[[1. 1.] [0. 0.]]dice coefficient: 0.6000Test Case #2pred:[[1. 0.] [0. 1.]]label:[[1. 1.] [0. 1.]]dice_coefficient: 0.8333</code></pre><h5 id="Expected-output-2"><a href="#Expected-output-2" class="headerlink" title="Expected output"></a>Expected output</h5><p>If you get a different result, please check that you implemented the equation completely.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#1</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">dice coefficient: <span class="number">0.6000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Test Case <span class="comment">#2</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">dice_coefficient: <span class="number">0.8333</span></span><br></pre></td></tr></table></figure></p><h3 id="Dice-Coefficient-for-Multiple-classes"><a href="#Dice-Coefficient-for-Multiple-classes" class="headerlink" title="Dice Coefficient for Multiple classes"></a>Dice Coefficient for Multiple classes</h3><p>Now that we have the single class case, we can think about how to approach the multi class context. </p><ul><li>Remember that for this task, we want segmentations for each of the 3 classes of abnormality (edema, enhancing tumor, non-enhancing tumor). </li><li>This will give us 3 different dice coefficients (one for each abnormality class). </li><li>To combine these, we can just take the average. We can write that the overall dice coefficient is: </li></ul><script type="math/tex; mode=display">DC(f, x, y) = \frac{1}{3} \left ( DC_{1}(f, x, y) + DC_{2}(f, x, y) + DC_{3}(f, x, y) \right )</script><ul><li>$DC_{1}$, $DC_{2}$ and $DC_{3}$ are edema, enhancing tumor, and non-enhancing tumor dice coefficients.</li></ul><p>For any number of classes, the equation becomes:  </p><script type="math/tex; mode=display">DC(f, x, y) = \frac{1}{N} \sum_{c=1}^{C} \left ( DC_{c}(f, x, y) \right )</script><p>In this case, with three categories, $C = 3$</p><p>Implement the mean dice coefficient below. This should not be very different from your singe-class implementation.</p><p>Please use the <a href="https://keras.io/backend/#mean" target="_blank" rel="noopener">K.mean</a> function to take the average of the three classes.  </p><ul><li>Apply the mean to the ratio that you calculate in the last line of code that you’ll implement.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dice_coefficient</span><span class="params">(y_true, y_pred, axis=<span class="params">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                     epsilon=<span class="number">0.00001</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute mean dice coefficient over all abnormality classes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y_true (Tensorflow tensor): tensor of ground truth values for all classes.</span></span><br><span class="line"><span class="string">                                    shape: (num_classes, x_dim, y_dim, z_dim)</span></span><br><span class="line"><span class="string">        y_pred (Tensorflow tensor): tensor of predictions for all classes.</span></span><br><span class="line"><span class="string">                                    shape: (num_classes, x_dim, y_dim, z_dim)</span></span><br><span class="line"><span class="string">        axis (tuple): spatial axes to sum over when computing numerator and</span></span><br><span class="line"><span class="string">                      denominator of dice coefficient.</span></span><br><span class="line"><span class="string">                      Hint: pass this as the 'axis' argument to the K.sum</span></span><br><span class="line"><span class="string">                            and K.mean functions.</span></span><br><span class="line"><span class="string">        epsilon (float): small constant add to numerator and denominator to</span></span><br><span class="line"><span class="string">                        avoid divide by 0 errors.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dice_coefficient (float): computed value of dice coefficient.     </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    dice_numerator = K.sum(<span class="number">2</span> * y_true * y_pred, axis= axis) + epsilon</span><br><span class="line">    dice_denominator = K.sum(y_true,axis= axis) + K.sum(y_pred, axis = axis) + epsilon</span><br><span class="line">    dice_coefficient = K.mean(dice_numerator / dice_denominator,axis = <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dice_coefficient</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEST CASES</span></span><br><span class="line">sess = K.get_session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Test Case #1"</span>)</span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    dc = dice_coefficient(pred, label,epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"dice coefficient: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Test Case #2"</span>)</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    dc = dice_coefficient(pred, label,epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"dice coefficient: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">    print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Test Case #3"</span>)</span><br><span class="line">    pred = np.zeros((<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">    pred[<span class="number">0</span>, :, :, :] = np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">-1</span>)</span><br><span class="line">    pred[<span class="number">1</span>, :, :, :] = np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    label = np.zeros((<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">    label[<span class="number">0</span>, :, :, :] = np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">-1</span>)</span><br><span class="line">    label[<span class="number">1</span>, :, :, :] = np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]]), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(<span class="string">"class = 0"</span>)</span><br><span class="line">    print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"class = 1"</span>)</span><br><span class="line">    print(pred[<span class="number">1</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(<span class="string">"class = 0"</span>)</span><br><span class="line">    print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"class = 1"</span>)</span><br><span class="line">    print(label[<span class="number">1</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    dc = dice_coefficient(pred, label,epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"dice coefficient: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #1pred:[[1. 0.] [0. 1.]]label:[[1. 1.] [0. 0.]]dice coefficient: 0.6000Test Case #2pred:[[1. 0.] [0. 1.]]label:[[1. 1.] [0. 1.]]dice coefficient: 0.8333Test Case #3pred:class = 0[[1. 0.] [0. 1.]]class = 1[[1. 0.] [0. 1.]]label:class = 0[[1. 1.] [0. 0.]]class = 1[[1. 1.] [0. 1.]]dice coefficient: 0.7167</code></pre><h4 id="Expected-output-3"><a href="#Expected-output-3" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#1</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">dice coefficient: <span class="number">0.6000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Test Case <span class="comment">#2</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">dice coefficient: <span class="number">0.8333</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Test Case <span class="comment">#3</span></span><br><span class="line">pred:</span><br><span class="line">class = 0</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">class = 1</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">class = 0</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">class = 1</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">dice coefficient: <span class="number">0.7167</span></span><br></pre></td></tr></table></figure><p><a name="3-2"></a></p><h2 id="3-2-Soft-Dice-Loss"><a href="#3-2-Soft-Dice-Loss" class="headerlink" title="3.2 Soft Dice Loss"></a>3.2 Soft Dice Loss</h2><p>While the Dice Coefficient makes intuitive sense, it is not the best for training. </p><ul><li>This is because it takes in discrete values (zeros and ones). </li><li>The model outputs <em>probabilities</em> that each pixel is, say, a tumor or not, and we want to be able to backpropagate through those outputs. </li></ul><p>Therefore, we need an analogue of the Dice loss which takes real valued input. This is where the <strong>Soft Dice loss</strong> comes in. The formula is: </p><script type="math/tex; mode=display">\mathcal{L}_{Dice}(p, q) = 1 - \frac{2\times\sum_{i, j} p_{ij}q_{ij} + \epsilon}{\left(\sum_{i, j} p_{ij}^2 \right) + \left(\sum_{i, j} q_{ij}^2 \right) + \epsilon}</script><ul><li>$p$ is our predictions</li><li>$q$ is the ground truth </li><li>In practice each $q_i$ will either be 0 or 1. </li><li>$\epsilon$ is a small number that is added to avoid division by zero</li></ul><p>The soft Dice loss ranges between </p><ul><li>0: perfectly matching the ground truth distribution $q$</li><li>1: complete mismatch with the ground truth.</li></ul><p>You can also check that if $p_i$ and $q_i$ are each 0 or 1, then the soft Dice loss is just one minus the dice coefficient.</p><h3 id="Multi-Class-Soft-Dice-Loss"><a href="#Multi-Class-Soft-Dice-Loss" class="headerlink" title="Multi-Class Soft Dice Loss"></a>Multi-Class Soft Dice Loss</h3><p>We’ve explained the single class case for simplicity, but the multi-class generalization is exactly the same as that of the dice coefficient. </p><ul><li>Since you’ve already implemented the multi-class dice coefficient, we’ll have you jump directly to the multi-class soft dice loss.</li></ul><p>For any number of categories of diseases, the expression becomes:</p><script type="math/tex; mode=display">\mathcal{L}_{Dice}(p, q) = 1 - \frac{1}{N} \sum_{c=1}^{C} \frac{2\times\sum_{i, j} p_{cij}q_{cij} + \epsilon}{\left(\sum_{i, j} p_{cij}^2 \right) + \left(\sum_{i, j} q_{cij}^2 \right) + \epsilon}</script><p>Please implement the soft dice loss below!</p><p>As before, you will use K.mean()</p><ul><li>Apply the average the mean to ratio that you’ll calculate in the last line of code that you’ll implement.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">soft_dice_loss</span><span class="params">(y_true, y_pred, axis=<span class="params">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                   epsilon=<span class="number">0.00001</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute mean soft dice loss over all abnormality classes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y_true (Tensorflow tensor): tensor of ground truth values for all classes.</span></span><br><span class="line"><span class="string">                                    shape: (num_classes, x_dim, y_dim, z_dim)</span></span><br><span class="line"><span class="string">        y_pred (Tensorflow tensor): tensor of soft predictions for all classes.</span></span><br><span class="line"><span class="string">                                    shape: (num_classes, x_dim, y_dim, z_dim)</span></span><br><span class="line"><span class="string">        axis (tuple): spatial axes to sum over when computing numerator and</span></span><br><span class="line"><span class="string">                      denominator in formula for dice loss.</span></span><br><span class="line"><span class="string">                      Hint: pass this as the 'axis' argument to the K.sum</span></span><br><span class="line"><span class="string">                            and K.mean functions.</span></span><br><span class="line"><span class="string">        epsilon (float): small constant added to numerator and denominator to</span></span><br><span class="line"><span class="string">                        avoid divide by 0 errors.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dice_loss (float): computed value of dice loss.     </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line"></span><br><span class="line">    dice_numerator = <span class="number">2</span> * K.sum(y_true * y_pred, axis=axis) + epsilon</span><br><span class="line">    dice_denominator = K.sum(K.square(y_true), axis = axis) + K.sum(K.square(y_pred), axis = axis) + epsilon</span><br><span class="line">    dice_loss = <span class="number">1</span> - K.mean(dice_numerator / dice_denominator, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dice_loss</span><br></pre></td></tr></table></figure><h4 id="Test-Case-1"><a href="#Test-Case-1" class="headerlink" title="Test Case 1"></a>Test Case 1</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEST CASES</span></span><br><span class="line">sess = K.get_session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Test Case #1"</span>)</span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    dc = soft_dice_loss(pred, label, epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"soft dice loss:<span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #1pred:[[1. 0.] [0. 1.]]label:[[1. 1.] [0. 0.]]soft dice loss:0.4000</code></pre><h4 id="Expected-output-4"><a href="#Expected-output-4" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#1</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">soft dice loss:<span class="number">0.4000</span></span><br></pre></td></tr></table></figure><h4 id="Test-Case-2"><a href="#Test-Case-2" class="headerlink" title="Test Case 2"></a>Test Case 2</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sess = K.get_session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"Test Case #2"</span>)</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(<span class="number">0.5</span>*np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    dc = soft_dice_loss(pred, label, epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"soft dice loss: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #2pred:[[0.5 0. ] [0.  0.5]]label:[[1. 1.] [0. 0.]]soft dice loss: 0.4286</code></pre><h4 id="Expected-output-5"><a href="#Expected-output-5" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#2</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">0.5</span> <span class="number">0.</span> ]</span><br><span class="line"> [<span class="number">0.</span>  <span class="number">0.5</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">soft dice loss: <span class="number">0.4286</span></span><br></pre></td></tr></table></figure><h4 id="Test-Case-3"><a href="#Test-Case-3" class="headerlink" title="Test Case 3"></a>Test Case 3</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sess = K.get_session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"Test Case #3"</span>)</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    dc = soft_dice_loss(pred, label, epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"soft dice loss: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #3pred:[[1. 0.] [0. 1.]]label:[[1. 1.] [0. 1.]]soft dice loss: 0.1667</code></pre><h4 id="Expected-output-6"><a href="#Expected-output-6" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#3</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">soft dice loss: <span class="number">0.1667</span></span><br></pre></td></tr></table></figure><h4 id="Test-Case-4"><a href="#Test-Case-4" class="headerlink" title="Test Case 4"></a>Test Case 4</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">sess = K.get_session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Test Case #4"</span>)</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    pred[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>] = <span class="number">0.8</span></span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    dc = soft_dice_loss(pred, label, epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"soft dice loss: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #4pred:[[1.  0.8] [0.  1. ]]label:[[1. 1.] [0. 1.]]soft dice loss: 0.0060</code></pre><h4 id="Expected-output-7"><a href="#Expected-output-7" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#4</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span>  <span class="number">0.8</span>]</span><br><span class="line"> [<span class="number">0.</span>  <span class="number">1.</span> ]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">soft dice loss: <span class="number">0.0060</span></span><br></pre></td></tr></table></figure><h4 id="Test-Case-5"><a href="#Test-Case-5" class="headerlink" title="Test Case 5"></a>Test Case 5</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">sess = K.get_session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"Test Case #5"</span>)</span><br><span class="line">    pred = np.zeros((<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">    pred[<span class="number">0</span>, :, :, :] = np.expand_dims(<span class="number">0.5</span>*np.eye(<span class="number">2</span>), <span class="number">-1</span>)</span><br><span class="line">    pred[<span class="number">1</span>, :, :, :] = np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">-1</span>)</span><br><span class="line">    pred[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>] = <span class="number">0.8</span></span><br><span class="line"></span><br><span class="line">    label = np.zeros((<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">    label[<span class="number">0</span>, :, :, :] = np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">-1</span>)</span><br><span class="line">    label[<span class="number">1</span>, :, :, :] = np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]]), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"pred:"</span>)</span><br><span class="line">    print(<span class="string">"class = 0"</span>)</span><br><span class="line">    print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"class = 1"</span>)</span><br><span class="line">    print(pred[<span class="number">1</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"label:"</span>)</span><br><span class="line">    print(<span class="string">"class = 0"</span>)</span><br><span class="line">    print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    print(<span class="string">"class = 1"</span>)</span><br><span class="line">    print(label[<span class="number">1</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    dc = soft_dice_loss(pred, label, epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"soft dice loss: <span class="subst">&#123;dc.eval():<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #5pred:class = 0[[0.5 0. ] [0.  0.5]]class = 1[[1.  0.8] [0.  1. ]]label:class = 0[[1. 1.] [0. 0.]]class = 1[[1. 1.] [0. 1.]]soft dice loss: 0.2173</code></pre><h4 id="Expected-output-8"><a href="#Expected-output-8" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#5</span></span><br><span class="line">pred:</span><br><span class="line">class = 0</span><br><span class="line">[[<span class="number">0.5</span> <span class="number">0.</span> ]</span><br><span class="line"> [<span class="number">0.</span>  <span class="number">0.5</span>]]</span><br><span class="line">class = 1</span><br><span class="line">[[<span class="number">1.</span>  <span class="number">0.8</span>]</span><br><span class="line"> [<span class="number">0.</span>  <span class="number">1.</span> ]]</span><br><span class="line">label:</span><br><span class="line">class = 0</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">class = 1</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">soft dice loss: <span class="number">0.2173</span></span><br></pre></td></tr></table></figure><h4 id="Test-Case-6"><a href="#Test-Case-6" class="headerlink" title="Test Case 6"></a>Test Case 6</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test case 6</span></span><br><span class="line">pred = np.array([</span><br><span class="line">                    [</span><br><span class="line">                        [ </span><br><span class="line">                            [<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">                        ],</span><br><span class="line">                        [</span><br><span class="line">                            [<span class="number">1.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]</span><br><span class="line">                        ]</span><br><span class="line">                    ],</span><br><span class="line">                    [</span><br><span class="line">                        [ </span><br><span class="line">                            [<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">                        ],</span><br><span class="line">                        [</span><br><span class="line">                            [<span class="number">1.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]</span><br><span class="line">                        ]</span><br><span class="line">                    ],</span><br><span class="line">                  ])</span><br><span class="line">label = np.array([</span><br><span class="line">                    [</span><br><span class="line">                        [ </span><br><span class="line">                            [<span class="number">1.0</span>, <span class="number">0.0</span>], [<span class="number">1.0</span>, <span class="number">0.0</span>]</span><br><span class="line">                        ],</span><br><span class="line">                        [</span><br><span class="line">                            [<span class="number">1.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">                        ]</span><br><span class="line">                    ],</span><br><span class="line">                    [</span><br><span class="line">                        [ </span><br><span class="line">                            [<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">                        ],</span><br><span class="line">                        [</span><br><span class="line">                            [<span class="number">1.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">                        ]</span><br><span class="line">                    ]</span><br><span class="line">                  ])</span><br><span class="line"></span><br><span class="line">sess = K.get_session()</span><br><span class="line">print(<span class="string">"Test case #6"</span>)</span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    dc = soft_dice_loss(pred, label, epsilon=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">f"soft dice loss"</span>,dc.eval())</span><br></pre></td></tr></table></figure><pre><code>Test case #6soft dice loss 0.4375</code></pre><h4 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Test case <span class="comment">#6</span></span><br><span class="line">soft dice loss: <span class="number">0.4375</span></span><br></pre></td></tr></table></figure><p>Note, if you don’t have a scalar, and have an array with more than one value, please check your implementation!</p><p><a name="4"></a></p><h1 id="4-Create-and-Train-the-model"><a href="#4-Create-and-Train-the-model" class="headerlink" title="4 Create and Train the model"></a>4 Create and Train the model</h1><p>Once you’ve finished implementing the soft dice loss, we can create the model! </p><p>We’ll use the <code>unet_model_3d</code> function in <code>utils</code> which we implemented for you.</p><ul><li>This creates the model architecture and compiles the model with the specified loss functions and metrics. </li><li>Check out function <code>util.unet_model_3d(loss_function)</code> in the <code>util.py</code> file.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = util.unet_model_3d(loss_function=soft_dice_loss, metrics=[dice_coefficient])</span><br></pre></td></tr></table></figure><pre><code>WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.Instructions for updating:If using Keras pass *_constraint arguments to layers.</code></pre><p><a name="4-1"></a></p><h2 id="4-1-Training-on-a-Large-Dataset"><a href="#4-1-Training-on-a-Large-Dataset" class="headerlink" title="4.1 Training on a Large Dataset"></a>4.1 Training on a Large Dataset</h2><p>In order to facilitate the training on the large dataset:</p><ul><li>We have pre-processed the entire dataset into patches and stored the patches in the <a href="http://docs.h5py.org/en/stable/" target="_blank" rel="noopener"><code>h5py</code></a> format. </li><li>We also wrote a custom Keras <a href="https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence" target="_blank" rel="noopener"><code>Sequence</code></a> class which can be used as a <code>Generator</code> for the keras model to train on large datasets. </li><li>Feel free to look at the <code>VolumeDataGenerator</code> class in <code>util.py</code> to learn about how such a generator can be coded.</li></ul><p>Note: <a href="https://www.geeksforgeeks.org/keras-fit-and-keras-fit_generator/" target="_blank" rel="noopener">Here</a> you can check the difference between <code>fit</code> and <code>fit_generator</code> functions.</p><p>To get a flavor of the training on the larger dataset, you can run the following cell to train the model on a small subset of the dataset (85 patches). You should see the loss going down and the dice coefficient going up. </p><p>Running <code>model.fit()</code> on the Coursera workspace may cause the kernel to die.</p><ul><li>Soon, we will load a pre-trained version of this model, so that you don’t need to train the model on this workspace.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run this on your local machine only</span></span><br><span class="line"><span class="comment"># May cause the kernel to die if running in the Coursera platform</span></span><br><span class="line"></span><br><span class="line">base_dir = HOME_DIR + <span class="string">"processed/"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(base_dir + <span class="string">"config.json"</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    config = json.load(json_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get generators for training and validation sets</span></span><br><span class="line">train_generator = util.VolumeDataGenerator(config[<span class="string">"train"</span>], base_dir + <span class="string">"train/"</span>, batch_size=<span class="number">3</span>, dim=(<span class="number">160</span>, <span class="number">160</span>, <span class="number">16</span>), verbose=<span class="number">0</span>)</span><br><span class="line">valid_generator = util.VolumeDataGenerator(config[<span class="string">"valid"</span>], base_dir + <span class="string">"valid/"</span>, batch_size=<span class="number">3</span>, dim=(<span class="number">160</span>, <span class="number">160</span>, <span class="number">16</span>), verbose=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">steps_per_epoch = <span class="number">20</span></span><br><span class="line">n_epochs=<span class="number">10</span></span><br><span class="line">validation_steps = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">model.fit_generator(generator=train_generator,</span><br><span class="line">        steps_per_epoch=steps_per_epoch,</span><br><span class="line">        epochs=n_epochs,</span><br><span class="line">        use_multiprocessing=<span class="keyword">True</span>,</span><br><span class="line">        validation_data=valid_generator,</span><br><span class="line">        validation_steps=validation_steps)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run this cell if you to save the weights of your trained model in cell section 4.1</span></span><br><span class="line"><span class="comment">#model.save_weights(base_dir + 'my_model_pretrained.hdf5')</span></span><br></pre></td></tr></table></figure><p><a name="4-2"></a></p><h2 id="4-2-Loading-a-Pre-Trained-Model"><a href="#4-2-Loading-a-Pre-Trained-Model" class="headerlink" title="4.2 Loading a Pre-Trained Model"></a>4.2 Loading a Pre-Trained Model</h2><p>As in assignment 1, instead of having the model train for longer, we’ll give you access to a pretrained version. We’ll use this to extract predictions and measure performance.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># run this cell if you didn't run the training cell in section 4.1</span></span><br><span class="line">base_dir = HOME_DIR + <span class="string">"processed/"</span></span><br><span class="line"><span class="keyword">with</span> open(base_dir + <span class="string">"config.json"</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    config = json.load(json_file)</span><br><span class="line"><span class="comment"># Get generators for training and validation sets</span></span><br><span class="line">train_generator = util.VolumeDataGenerator(config[<span class="string">"train"</span>], base_dir + <span class="string">"train/"</span>, batch_size=<span class="number">3</span>, dim=(<span class="number">160</span>, <span class="number">160</span>, <span class="number">16</span>), verbose=<span class="number">0</span>)</span><br><span class="line">valid_generator = util.VolumeDataGenerator(config[<span class="string">"valid"</span>], base_dir + <span class="string">"valid/"</span>, batch_size=<span class="number">3</span>, dim=(<span class="number">160</span>, <span class="number">160</span>, <span class="number">16</span>), verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_weights(HOME_DIR + <span class="string">"model_pretrained.hdf5"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;model_1&quot;__________________________________________________________________________________________________Layer (type)                    Output Shape         Param #     Connected to                     ==================================================================================================input_1 (InputLayer)            (None, 4, 160, 160,  0                                            __________________________________________________________________________________________________conv3d_1 (Conv3D)               (None, 32, 160, 160, 3488        input_1[0][0]                    __________________________________________________________________________________________________activation_1 (Activation)       (None, 32, 160, 160, 0           conv3d_1[0][0]                   __________________________________________________________________________________________________conv3d_2 (Conv3D)               (None, 64, 160, 160, 55360       activation_1[0][0]               __________________________________________________________________________________________________activation_2 (Activation)       (None, 64, 160, 160, 0           conv3d_2[0][0]                   __________________________________________________________________________________________________max_pooling3d_1 (MaxPooling3D)  (None, 64, 80, 80, 8 0           activation_2[0][0]               __________________________________________________________________________________________________conv3d_3 (Conv3D)               (None, 64, 80, 80, 8 110656      max_pooling3d_1[0][0]            __________________________________________________________________________________________________activation_3 (Activation)       (None, 64, 80, 80, 8 0           conv3d_3[0][0]                   __________________________________________________________________________________________________conv3d_4 (Conv3D)               (None, 128, 80, 80,  221312      activation_3[0][0]               __________________________________________________________________________________________________activation_4 (Activation)       (None, 128, 80, 80,  0           conv3d_4[0][0]                   __________________________________________________________________________________________________max_pooling3d_2 (MaxPooling3D)  (None, 128, 40, 40,  0           activation_4[0][0]               __________________________________________________________________________________________________conv3d_5 (Conv3D)               (None, 128, 40, 40,  442496      max_pooling3d_2[0][0]            __________________________________________________________________________________________________activation_5 (Activation)       (None, 128, 40, 40,  0           conv3d_5[0][0]                   __________________________________________________________________________________________________conv3d_6 (Conv3D)               (None, 256, 40, 40,  884992      activation_5[0][0]               __________________________________________________________________________________________________activation_6 (Activation)       (None, 256, 40, 40,  0           conv3d_6[0][0]                   __________________________________________________________________________________________________max_pooling3d_3 (MaxPooling3D)  (None, 256, 20, 20,  0           activation_6[0][0]               __________________________________________________________________________________________________conv3d_7 (Conv3D)               (None, 256, 20, 20,  1769728     max_pooling3d_3[0][0]            __________________________________________________________________________________________________activation_7 (Activation)       (None, 256, 20, 20,  0           conv3d_7[0][0]                   __________________________________________________________________________________________________conv3d_8 (Conv3D)               (None, 512, 20, 20,  3539456     activation_7[0][0]               __________________________________________________________________________________________________activation_8 (Activation)       (None, 512, 20, 20,  0           conv3d_8[0][0]                   __________________________________________________________________________________________________up_sampling3d_1 (UpSampling3D)  (None, 512, 40, 40,  0           activation_8[0][0]               __________________________________________________________________________________________________concatenate_1 (Concatenate)     (None, 768, 40, 40,  0           up_sampling3d_1[0][0]                                                                             activation_6[0][0]               __________________________________________________________________________________________________conv3d_9 (Conv3D)               (None, 256, 40, 40,  5308672     concatenate_1[0][0]              __________________________________________________________________________________________________activation_9 (Activation)       (None, 256, 40, 40,  0           conv3d_9[0][0]                   __________________________________________________________________________________________________conv3d_10 (Conv3D)              (None, 256, 40, 40,  1769728     activation_9[0][0]               __________________________________________________________________________________________________activation_10 (Activation)      (None, 256, 40, 40,  0           conv3d_10[0][0]                  __________________________________________________________________________________________________up_sampling3d_2 (UpSampling3D)  (None, 256, 80, 80,  0           activation_10[0][0]              __________________________________________________________________________________________________concatenate_2 (Concatenate)     (None, 384, 80, 80,  0           up_sampling3d_2[0][0]                                                                             activation_4[0][0]               __________________________________________________________________________________________________conv3d_11 (Conv3D)              (None, 128, 80, 80,  1327232     concatenate_2[0][0]              __________________________________________________________________________________________________activation_11 (Activation)      (None, 128, 80, 80,  0           conv3d_11[0][0]                  __________________________________________________________________________________________________conv3d_12 (Conv3D)              (None, 128, 80, 80,  442496      activation_11[0][0]              __________________________________________________________________________________________________activation_12 (Activation)      (None, 128, 80, 80,  0           conv3d_12[0][0]                  __________________________________________________________________________________________________up_sampling3d_3 (UpSampling3D)  (None, 128, 160, 160 0           activation_12[0][0]              __________________________________________________________________________________________________concatenate_3 (Concatenate)     (None, 192, 160, 160 0           up_sampling3d_3[0][0]                                                                             activation_2[0][0]               __________________________________________________________________________________________________conv3d_13 (Conv3D)              (None, 64, 160, 160, 331840      concatenate_3[0][0]              __________________________________________________________________________________________________activation_13 (Activation)      (None, 64, 160, 160, 0           conv3d_13[0][0]                  __________________________________________________________________________________________________conv3d_14 (Conv3D)              (None, 64, 160, 160, 110656      activation_13[0][0]              __________________________________________________________________________________________________activation_14 (Activation)      (None, 64, 160, 160, 0           conv3d_14[0][0]                  __________________________________________________________________________________________________conv3d_15 (Conv3D)              (None, 3, 160, 160,  195         activation_14[0][0]              __________________________________________________________________________________________________activation_15 (Activation)      (None, 3, 160, 160,  0           conv3d_15[0][0]                  ==================================================================================================Total params: 16,318,307Trainable params: 16,318,307Non-trainable params: 0__________________________________________________________________________________________________</code></pre><p><a name="5"></a></p><h1 id="5-Evaluation"><a href="#5-Evaluation" class="headerlink" title="5 Evaluation"></a>5 Evaluation</h1><p>Now that we have a trained model, we’ll learn to extract its predictions and evaluate its performance on scans from our validation set.</p><p><a name="5-1"></a></p><h2 id="5-1-Overall-Performance"><a href="#5-1-Overall-Performance" class="headerlink" title="5.1 Overall Performance"></a>5.1 Overall Performance</h2><p>First let’s measure the overall performance on the validation set. </p><ul><li>We can do this by calling the keras <a href="https://keras.io/models/model/#evaluate_generator" target="_blank" rel="noopener">evaluate_generator</a> function and passing in the validation generator, created in section 4.1. </li></ul><h4 id="Using-the-validation-set-for-testing"><a href="#Using-the-validation-set-for-testing" class="headerlink" title="Using the validation set for testing"></a>Using the validation set for testing</h4><ul><li>Note: since we didn’t do cross validation tuning on the final model, it’s okay to use the validation set.</li><li>For real life implementations, however, you would want to do cross validation as usual to choose hyperparamters and then use a hold out test set to assess performance</li></ul><p>Python Code for measuring the overall performance on the validation set:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val_loss, val_dice = model.evaluate_generator(valid_generator)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"validation soft dice loss: <span class="subst">&#123;val_loss:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"validation dice coefficient: <span class="subst">&#123;val_dice:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><h4 id="Expected-output-9"><a href="#Expected-output-9" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">validation soft dice loss: <span class="number">0.4742</span></span><br><span class="line">validation dice coefficient: <span class="number">0.5152</span></span><br></pre></td></tr></table></figure><p><strong>NOTE:</strong> Do not run the code shown above on the Coursera platform as it will exceed the platform’s memory limitations. However, you can run the code shown above locally on your machine or in Colab to practice measuring the overall performance on the validation set.</p><p>Like we mentioned above, due to memory limitiations on the Coursera platform we won’t be runing the above code, however, you should take note of the <strong>expected output</strong> below it. We should note that due to the randomness in choosing sub-volumes, the values for soft dice loss and dice coefficient will be different each time that you run it.</p><p><a name="5-2"></a></p><h2 id="5-2-Patch-level-predictions"><a href="#5-2-Patch-level-predictions" class="headerlink" title="5.2 Patch-level predictions"></a>5.2 Patch-level predictions</h2><p>When applying the model, we’ll want to look at segmentations for individual scans (entire scans, not just the sub-volumes)</p><ul><li>This will be a bit complicated because of our sub-volume approach. </li><li>First let’s keep things simple and extract model predictions for sub-volumes.</li><li>We can use the sub-volume which we extracted at the beginning of the assignment.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.visualize_patch(X_norm[<span class="number">0</span>, :, :, :], y[<span class="number">2</span>])</span><br></pre></td></tr></table></figure><p><img src="output_76_0.png" alt="png"></p><h4 id="Add-a-‘batch’-dimension"><a href="#Add-a-‘batch’-dimension" class="headerlink" title="Add a ‘batch’ dimension"></a>Add a ‘batch’ dimension</h4><p>We can extract predictions by calling <code>model.predict</code> on the patch. </p><ul><li>We’ll add an <code>images_per_batch</code> dimension, since the <code>predict</code> method is written to take in batches. </li><li>The dimensions of the input should be <code>(images_per_batch, num_channels, x_dim, y_dim, z_dim)</code>.</li><li>Use <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.expand_dims.html" target="_blank" rel="noopener">numpy.expand_dims</a> to add a new dimension as the zero-th dimension by setting axis=0</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_norm_with_batch_dimension = np.expand_dims(X_norm, axis=<span class="number">0</span>)</span><br><span class="line">patch_pred = model.predict(X_norm_with_batch_dimension)</span><br></pre></td></tr></table></figure><h4 id="Convert-prediction-from-probability-into-a-category"><a href="#Convert-prediction-from-probability-into-a-category" class="headerlink" title="Convert prediction from probability into a category"></a>Convert prediction from probability into a category</h4><p>Currently, each element of <code>patch_pred</code> is a number between 0.0 and 1.0.</p><ul><li>Each number is the model’s confidence that a voxel is part of a given class. </li><li>You will convert these to discrete 0 and 1 integers by using a threshold. </li><li>We’ll use a threshold of 0.5. </li><li>In real applications, you would tune this to achieve your required level  of sensitivity or specificity.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set threshold.</span></span><br><span class="line">threshold = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># use threshold to get hard predictions</span></span><br><span class="line">patch_pred[patch_pred &gt; threshold] = <span class="number">1.0</span></span><br><span class="line">patch_pred[patch_pred &lt;= threshold] = <span class="number">0.0</span></span><br></pre></td></tr></table></figure><p>Now let’s visualize the original patch and ground truth alongside our thresholded predictions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Patch and ground truth"</span>)</span><br><span class="line">util.visualize_patch(X_norm[<span class="number">0</span>, :, :, :], y[<span class="number">2</span>])</span><br><span class="line">plt.show()</span><br><span class="line">print(<span class="string">"Patch and prediction"</span>)</span><br><span class="line">util.visualize_patch(X_norm[<span class="number">0</span>, :, :, :], patch_pred[<span class="number">0</span>, <span class="number">2</span>, :, :, :])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>Patch and ground truth</code></pre><p><img src="output_82_1.png" alt="png"></p><pre><code>Patch and prediction</code></pre><p><img src="output_82_3.png" alt="png"></p><h4 id="Sensitivity-and-Specificity"><a href="#Sensitivity-and-Specificity" class="headerlink" title="Sensitivity and Specificity"></a>Sensitivity and Specificity</h4><p>The model is covering some of the relevant areas, but it’s definitely not perfect. </p><ul><li>To quantify its performance, we can use per-pixel sensitivity and specificity. </li></ul><p>Recall that in terms of the true positives, true negatives, false positives, and false negatives, </p><script type="math/tex; mode=display">\text{sensitivity} = \frac{\text{true positives}}{\text{true positives} + \text{false negatives}}</script><script type="math/tex; mode=display">\text{specificity} = \frac{\text{true negatives}}{\text{true negatives} + \text{false positives}}</script><p>Below let’s write a function to compute the sensitivity and specificity per output class.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Recall that a true positive occurs when the class prediction is equal to 1, and the class label is also equal to 1</li>    <li>Use <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html" target="_blank" rel="noopener"> numpy.sum() </a> </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_class_sens_spec</span><span class="params">(pred, label, class_num)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute sensitivity and specificity for a particular example</span></span><br><span class="line"><span class="string">    for a given class.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pred (np.array): binary arrary of predictions, shape is</span></span><br><span class="line"><span class="string">                         (num classes, height, width, depth).</span></span><br><span class="line"><span class="string">        label (np.array): binary array of labels, shape is</span></span><br><span class="line"><span class="string">                          (num classes, height, width, depth).</span></span><br><span class="line"><span class="string">        class_num (int): number between 0 - (num_classes -1) which says</span></span><br><span class="line"><span class="string">                         which prediction class to compute statistics</span></span><br><span class="line"><span class="string">                         for.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        sensitivity (float): precision for given class_num.</span></span><br><span class="line"><span class="string">        specificity (float): recall for given class_num</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># extract sub-array for specified class</span></span><br><span class="line">    class_pred = pred[class_num]</span><br><span class="line">    class_label = label[class_num]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># true positives</span></span><br><span class="line">    tp = np.sum((class_label == <span class="number">1</span>) &amp; (class_pred == <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># true negatives</span></span><br><span class="line">    tn = np.sum((class_label == <span class="number">0</span>) &amp; (class_pred == <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#false positives</span></span><br><span class="line">    fp = np.sum((class_label == <span class="number">0</span>) &amp; (class_pred == <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># false negatives</span></span><br><span class="line">    fn = np.sum((class_label == <span class="number">1</span>) &amp; (class_pred == <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute sensitivity and specificity</span></span><br><span class="line">    sensitivity = tp / (tp + fn)</span><br><span class="line">    specificity = tn / (tn + fp)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sensitivity, specificity</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEST CASES</span></span><br><span class="line">pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">0.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test Case #1"</span>)</span><br><span class="line">print(<span class="string">"pred:"</span>)</span><br><span class="line">print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">print(<span class="string">"label:"</span>)</span><br><span class="line">print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">sensitivity, specificity = compute_class_sens_spec(pred, label, <span class="number">0</span>)</span><br><span class="line">print(<span class="string">f"sensitivity: <span class="subst">&#123;sensitivity:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"specificity: <span class="subst">&#123;specificity:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #1pred:[[1. 0.] [0. 1.]]label:[[1. 1.] [0. 0.]]sensitivity: 0.5000specificity: 0.5000</code></pre><h4 id="Expected-output-10"><a href="#Expected-output-10" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#1</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">sensitivity: <span class="number">0.5000</span></span><br><span class="line">specificity: <span class="number">0.5000</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Test Case #2"</span>)</span><br><span class="line"></span><br><span class="line">pred = np.expand_dims(np.expand_dims(np.eye(<span class="number">2</span>), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">label = np.expand_dims(np.expand_dims(np.array([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]]), <span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"pred:"</span>)</span><br><span class="line">print(pred[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">print(<span class="string">"label:"</span>)</span><br><span class="line">print(label[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">sensitivity, specificity = compute_class_sens_spec(pred, label, <span class="number">0</span>)</span><br><span class="line">print(<span class="string">f"sensitivity: <span class="subst">&#123;sensitivity:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"specificity: <span class="subst">&#123;specificity:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #2pred:[[1. 0.] [0. 1.]]label:[[1. 1.] [0. 1.]]sensitivity: 0.6667specificity: 1.0000</code></pre><h4 id="Expected-output-11"><a href="#Expected-output-11" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Test Case <span class="comment">#2</span></span><br><span class="line">pred:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">label:</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">sensitivity: <span class="number">0.6667</span></span><br><span class="line">specificity: <span class="number">1.0000</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Note: we must explicity import 'display' in order for the autograder to compile the submitted code</span></span><br><span class="line"><span class="comment"># Even though we could use this function without importing it, keep this import in order to allow the grader to work</span></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display</span><br><span class="line">print(<span class="string">"Test Case #3"</span>)</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">'y_test'</span>: [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                   <span class="string">'preds_test'</span>: [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                   <span class="string">'category'</span>: [<span class="string">'TP'</span>,<span class="string">'TP'</span>,<span class="string">'TN'</span>,<span class="string">'TN'</span>,<span class="string">'TN'</span>,<span class="string">'FP'</span>,<span class="string">'FP'</span>,<span class="string">'FP'</span>,<span class="string">'FP'</span>,<span class="string">'FN'</span>,<span class="string">'FN'</span>,<span class="string">'FN'</span>,<span class="string">'FN'</span>,<span class="string">'FN'</span>]</span><br><span class="line">                  &#125;)</span><br><span class="line"></span><br><span class="line">display(df)</span><br><span class="line">pred = np.array( [df[<span class="string">'preds_test'</span>]])</span><br><span class="line">label = np.array( [df[<span class="string">'y_test'</span>]])</span><br><span class="line"></span><br><span class="line">sensitivity, specificity = compute_class_sens_spec(pred, label, <span class="number">0</span>)</span><br><span class="line">print(<span class="string">f"sensitivity: <span class="subst">&#123;sensitivity:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"specificity: <span class="subst">&#123;specificity:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test Case #3</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>y_test</th>      <th>preds_test</th>      <th>category</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>1</td>      <td>TP</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>1</td>      <td>TP</td>    </tr>    <tr>      <th>2</th>      <td>0</td>      <td>0</td>      <td>TN</td>    </tr>    <tr>      <th>3</th>      <td>0</td>      <td>0</td>      <td>TN</td>    </tr>    <tr>      <th>4</th>      <td>0</td>      <td>0</td>      <td>TN</td>    </tr>    <tr>      <th>5</th>      <td>0</td>      <td>1</td>      <td>FP</td>    </tr>    <tr>      <th>6</th>      <td>0</td>      <td>1</td>      <td>FP</td>    </tr>    <tr>      <th>7</th>      <td>0</td>      <td>1</td>      <td>FP</td>    </tr>    <tr>      <th>8</th>      <td>0</td>      <td>1</td>      <td>FP</td>    </tr>    <tr>      <th>9</th>      <td>1</td>      <td>0</td>      <td>FN</td>    </tr>    <tr>      <th>10</th>      <td>1</td>      <td>0</td>      <td>FN</td>    </tr>    <tr>      <th>11</th>      <td>1</td>      <td>0</td>      <td>FN</td>    </tr>    <tr>      <th>12</th>      <td>1</td>      <td>0</td>      <td>FN</td>    </tr>    <tr>      <th>13</th>      <td>1</td>      <td>0</td>      <td>FN</td>    </tr>  </tbody></table></div><pre><code>sensitivity: 0.2857specificity: 0.4286</code></pre><h4 id="Expected-Output-1"><a href="#Expected-Output-1" class="headerlink" title="Expected Output"></a>Expected Output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Test case <span class="comment">#3</span></span><br><span class="line">...</span><br><span class="line">sensitivity: <span class="number">0.2857</span></span><br><span class="line">specificity: <span class="number">0.4286</span></span><br></pre></td></tr></table></figure><h4 id="Sensitivity-and-Specificity-for-the-patch-prediction"><a href="#Sensitivity-and-Specificity-for-the-patch-prediction" class="headerlink" title="Sensitivity and Specificity for the patch prediction"></a>Sensitivity and Specificity for the patch prediction</h4><p>Next let’s compute the sensitivity and specificity on that patch for expanding tumors. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sensitivity, specificity = compute_class_sens_spec(patch_pred[<span class="number">0</span>], y, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"Sensitivity: <span class="subst">&#123;sensitivity:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Specificity: <span class="subst">&#123;specificity:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Sensitivity: 0.8049Specificity: 0.9924</code></pre><h4 id="Expected-output-12"><a href="#Expected-output-12" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Sensitivity: <span class="number">0.7891</span></span><br><span class="line">Specificity: <span class="number">0.9960</span></span><br></pre></td></tr></table></figure><p>We can also display the sensitivity and specificity for each class.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sens_spec_df</span><span class="params">(pred, label)</span>:</span></span><br><span class="line">    patch_metrics = pd.DataFrame(</span><br><span class="line">        columns = [<span class="string">'Edema'</span>, </span><br><span class="line">                   <span class="string">'Non-Enhancing Tumor'</span>, </span><br><span class="line">                   <span class="string">'Enhancing Tumor'</span>], </span><br><span class="line">        index = [<span class="string">'Sensitivity'</span>,</span><br><span class="line">                 <span class="string">'Specificity'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i, class_name <span class="keyword">in</span> enumerate(patch_metrics.columns):</span><br><span class="line">        sens, spec = compute_class_sens_spec(pred, label, i)</span><br><span class="line">        patch_metrics.loc[<span class="string">'Sensitivity'</span>, class_name] = round(sens,<span class="number">4</span>)</span><br><span class="line">        patch_metrics.loc[<span class="string">'Specificity'</span>, class_name] = round(spec,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> patch_metrics</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = get_sens_spec_df(patch_pred[<span class="number">0</span>], y)</span><br><span class="line"></span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure><pre><code>              Edema Non-Enhancing Tumor Enhancing TumorSensitivity  0.8746              0.9419          0.8049Specificity    0.97              0.9957          0.9924</code></pre><h4 id="Expected-output-13"><a href="#Expected-output-13" class="headerlink" title="Expected output"></a>Expected output</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">              Edema Non-Enhancing Tumor Enhancing Tumor</span><br><span class="line">Sensitivity  <span class="number">0.9085</span>              <span class="number">0.9505</span>          <span class="number">0.7891</span></span><br><span class="line">Specificity  <span class="number">0.9848</span>              <span class="number">0.9961</span>           <span class="number">0.996</span></span><br></pre></td></tr></table></figure><p><a name="5-3"></a></p><h2 id="5-3-Running-on-entire-scans"><a href="#5-3-Running-on-entire-scans" class="headerlink" title="5.3 Running on entire scans"></a>5.3 Running on entire scans</h2><p>As of now, our model just runs on patches, but what we really want to see is our model’s result on a whole MRI scan. </p><ul><li>To do this, generate patches for the scan.</li><li>Then we run the model on the patches. </li><li>Then combine the results together to get a fully labeled MR image.</li></ul><p>The output of our model will be a 4D array with 3 probability values for each voxel in our data. </p><ul><li>We then can use a threshold (which you can find by a calibration process) to decide whether or not to report a label for each voxel. </li></ul><p>We have written a function that stitches the patches together:  <code>predict_and_viz(image, label, model, threshold)</code> </p><ul><li>Inputs: an image, label and model.</li><li>Ouputs: the model prediction over the whole image, and a visual of the ground truth and prediction. </li></ul><p>Run the following cell to see this function in action!</p><h4 id="Note-the-prediction-takes-some-time"><a href="#Note-the-prediction-takes-some-time" class="headerlink" title="Note: the prediction takes some time!"></a>Note: the prediction takes some time!</h4><ul><li>The first prediction will take about 7 to 8 minutes to run.</li><li>You can skip running this first prediction to save time.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># uncomment this code to run it</span></span><br><span class="line"><span class="comment"># image, label = load_case(DATA_DIR + "imagesTr/BRATS_001.nii.gz", DATA_DIR + "labelsTr/BRATS_001.nii.gz")</span></span><br><span class="line"><span class="comment"># pred = util.predict_and_viz(image, label, model, .5, loc=(130, 130, 77))</span></span><br></pre></td></tr></table></figure><p>Here’s a second prediction.</p><ul><li>Takes about 7 to 8 minutes to run</li></ul><p>Please run this second prediction so that we can check the predictions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.getsizeof(image) / <span class="number">1000</span>  / <span class="number">1000</span></span><br></pre></td></tr></table></figure><pre><code>285696144</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">image, label = load_case(DATA_DIR + <span class="string">"imagesTr/BRATS_003.nii.gz"</span>, DATA_DIR + <span class="string">"labelsTr/BRATS_003.nii.gz"</span>)</span><br><span class="line">pred = util.predict_and_viz(image, label, model, <span class="number">.5</span>, loc=(<span class="number">130</span>, <span class="number">130</span>, <span class="number">77</span>))</span><br></pre></td></tr></table></figure><p><img src="output_103_0.png" alt="png"></p><h4 id="Check-how-well-the-predictions-do"><a href="#Check-how-well-the-predictions-do" class="headerlink" title="Check how well the predictions do"></a>Check how well the predictions do</h4><p>We can see some of the discrepancies between the model and the ground truth visually. </p><ul><li>We can also use the functions we wrote previously to compute sensitivity and specificity for each class over the whole scan.</li><li>First we need to format the label and prediction to match our functions expect.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">whole_scan_label = keras.utils.to_categorical(label, num_classes = <span class="number">4</span>)</span><br><span class="line">whole_scan_pred = pred</span><br><span class="line"></span><br><span class="line"><span class="comment"># move axis to match shape expected in functions</span></span><br><span class="line">whole_scan_label = np.moveaxis(whole_scan_label, <span class="number">3</span> ,<span class="number">0</span>)[<span class="number">1</span>:<span class="number">4</span>]</span><br><span class="line">whole_scan_pred = np.moveaxis(whole_scan_pred, <span class="number">3</span>, <span class="number">0</span>)[<span class="number">1</span>:<span class="number">4</span>]</span><br></pre></td></tr></table></figure><p>Now we can compute sensitivity and specificity for each class just like before.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">whole_scan_df = get_sens_spec_df(whole_scan_pred, whole_scan_label)</span><br><span class="line"></span><br><span class="line">print(whole_scan_df)</span><br></pre></td></tr></table></figure><pre><code>              Edema Non-Enhancing Tumor Enhancing TumorSensitivity   0.902              0.2617          0.8496Specificity  0.9894              0.9998          0.9982</code></pre><h1 id="That’s-all-for-now"><a href="#That’s-all-for-now" class="headerlink" title="That’s all for now!"></a>That’s all for now!</h1><p>Congratulations on finishing this challenging assignment! You now know all the basics for building a neural auto-segmentation model for MRI images. We hope that you end up using these skills on interesting and challenging problems that you face in the real world.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/2652/1*eTkBMyqdg9JodNcG_O4-Kw.jpeg&quot; width=&quot;100%&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://medium.com/stanford-ai-fo
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Medicine" scheme="https://zhangruochi.com/tags/Medicine/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow.js Converter</title>
    <link href="https://zhangruochi.com/TensorFlow-js-Converter/2020/04/17/"/>
    <id>https://zhangruochi.com/TensorFlow-js-Converter/2020/04/17/</id>
    <published>2020-04-17T15:05:16.000Z</published>
    <updated>2020-04-18T03:07:31.945Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Converting-a-Keras-Model-to-JSON-Format"><a href="#Converting-a-Keras-Model-to-JSON-Format" class="headerlink" title="Converting a Keras Model to JSON Format"></a>Converting a Keras Model to JSON Format</h1><p>In the previous lesson you saw how to use a CNN to make your recognition of the handwriting digits more efficient. In this lesson you’ll take that to the next level, recognizing real images of Cats and Dogs in order to classify an incoming image as one or the other. In particular the handwriting recognition made your life a little easier by having all the images be the same size and shape, and they were all monochrome color. Real-world images aren’t like that — they’re in different shapes, aspect ratios etc, and they’re usually in color!</p><p>So, as part of the task you need to process your data — not least resizing it to be uniform in shape. </p><p>You’ll follow these steps:</p><ol><li>Explore the Example Data of Cats and Dogs.</li><li>Build and Train a Neural Network to recognize the difference between the two.</li><li>Evaluate the Training and Validation accuracy.</li><li>Save the trained model as a Keras HDF5 file.</li><li>Use the tensorflow.js converter to convert the saved Keras model into JSON format.</li></ol><h1 id="Import-Resources"><a href="#Import-Resources" class="headerlink" title="Import Resources"></a>Import Resources</h1><p>In order to use the tensorflow.js converter we need to install <code>tensorflowjs</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\u2022 Using TensorFlow Version:'</span>, tf.__version__)</span><br></pre></td></tr></table></figure><pre><code>• Using TensorFlow Version: 2.1.0</code></pre><h2 id="Explore-the-Example-Data"><a href="#Explore-the-Example-Data" class="headerlink" title="Explore the Example Data"></a>Explore the Example Data</h2><p>Let’s start by downloading our example data, a .zip of 2,000 JPG pictures of cats and dogs, and extracting it locally in <code>/tmp</code>.</p><p><strong>NOTE:</strong> The 2,000 images used in this exercise are excerpted from the <a href="https://www.kaggle.com/c/dogs-vs-cats/data" target="_blank" rel="noopener">“Dogs vs. Cats” dataset</a> available on Kaggle, which contains 25,000 images. Here, we use a subset of the full dataset to decrease training time for educational purposes.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!wget --no-check-certificate \</span><br><span class="line">  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \</span><br><span class="line">  -O /tmp/cats_and_dogs_filtered.zip</span><br></pre></td></tr></table></figure><pre><code>Warning: Failed to set locale category LC_NUMERIC to en_CN.Warning: Failed to set locale category LC_TIME to en_CN.Warning: Failed to set locale category LC_COLLATE to en_CN.Warning: Failed to set locale category LC_MONETARY to en_CN.Warning: Failed to set locale category LC_MESSAGES to en_CN.--2020-02-14 21:04:38--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zipResolving storage.googleapis.com (storage.googleapis.com)... 172.217.12.240Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.12.240|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 68606236 (65M) [application/zip]Saving to: ‘/tmp/cats_and_dogs_filtered.zip’/tmp/cats_and_dogs_ 100%[===================&gt;]  65.43M  26.3MB/s    in 2.5s    2020-02-14 21:04:40 (26.3 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]</code></pre><p>The following python code will use the OS library to use Operating System libraries, giving you access to the file system, and the zipfile library allowing you to unzip the data. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line">local_zip = <span class="string">'/tmp/cats_and_dogs_filtered.zip'</span></span><br><span class="line"></span><br><span class="line">zip_ref = zipfile.ZipFile(local_zip, <span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">zip_ref.extractall(<span class="string">'/tmp'</span>)</span><br><span class="line">zip_ref.close()</span><br></pre></td></tr></table></figure><p>The contents of the .zip are extracted to the base directory <code>/tmp/cats_and_dogs_filtered</code>, which contains <code>train</code> and <code>validation</code> subdirectories for the training and validation datasets (see the <a href="https://developers.google.com/machine-learning/crash-course/validation/check-your-intuition" target="_blank" rel="noopener">Machine Learning Crash Course</a> for a refresher on training, validation, and test sets), which in turn each contain <code>cats</code> and <code>dogs</code> subdirectories.</p><p>In short: The training set is the data that is used to tell the neural network model that ‘this is what a cat looks like’, ‘this is what a dog looks like’ etc. The validation data set is images of cats and dogs that the neural network will not see as part of the training, so you can test how well or how badly it does in evaluating if an image contains a cat or a dog.</p><p>One thing to pay attention to in this sample: We do not explicitly label the images as cats or dogs. If you remember with the handwriting example earlier, we had labelled ‘this is a 1’, ‘this is a 7’ etc.  Later you’ll see something called an ImageGenerator being used — and this is coded to read images from subdirectories, and automatically label them from the name of that subdirectory. So, for example, you will have a ‘training’ directory containing a ‘cats’ directory and a ‘dogs’ one. ImageGenerator will label the images appropriately for you, reducing a coding step. </p><p>Let’s define each of these directories:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">base_dir = <span class="string">'/tmp/cats_and_dogs_filtered'</span></span><br><span class="line"></span><br><span class="line">train_dir = os.path.join(base_dir, <span class="string">'train'</span>)</span><br><span class="line">validation_dir = os.path.join(base_dir, <span class="string">'validation'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Directory with our training cat/dog pictures</span></span><br><span class="line">train_cats_dir = os.path.join(train_dir, <span class="string">'cats'</span>)</span><br><span class="line">train_dogs_dir = os.path.join(train_dir, <span class="string">'dogs'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Directory with our validation cat/dog pictures</span></span><br><span class="line">validation_cats_dir = os.path.join(validation_dir, <span class="string">'cats'</span>)</span><br><span class="line">validation_dogs_dir = os.path.join(validation_dir, <span class="string">'dogs'</span>)</span><br></pre></td></tr></table></figure><p>Now, let’s see what the filenames look like in the <code>cats</code> and <code>dogs</code> <code>train</code> directories (file naming conventions are the same in the <code>validation</code> directory):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_cat_fnames = os.listdir( train_cats_dir )</span><br><span class="line">train_dog_fnames = os.listdir( train_dogs_dir )</span><br><span class="line"></span><br><span class="line">print(train_cat_fnames[:<span class="number">10</span>])</span><br><span class="line">print(train_dog_fnames[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>[&#39;cat.952.jpg&#39;, &#39;cat.946.jpg&#39;, &#39;cat.6.jpg&#39;, &#39;cat.749.jpg&#39;, &#39;cat.991.jpg&#39;, &#39;cat.985.jpg&#39;, &#39;cat.775.jpg&#39;, &#39;cat.761.jpg&#39;, &#39;cat.588.jpg&#39;, &#39;cat.239.jpg&#39;][&#39;dog.775.jpg&#39;, &#39;dog.761.jpg&#39;, &#39;dog.991.jpg&#39;, &#39;dog.749.jpg&#39;, &#39;dog.985.jpg&#39;, &#39;dog.952.jpg&#39;, &#39;dog.946.jpg&#39;, &#39;dog.211.jpg&#39;, &#39;dog.577.jpg&#39;, &#39;dog.563.jpg&#39;]</code></pre><p>Let’s find out the total number of cat and dog images in the <code>train</code> and <code>validation</code> directories:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'total training cat images :'</span>, len(os.listdir(      train_cats_dir ) ))</span><br><span class="line">print(<span class="string">'total training dog images :'</span>, len(os.listdir(      train_dogs_dir ) ))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'total validation cat images :'</span>, len(os.listdir( validation_cats_dir ) ))</span><br><span class="line">print(<span class="string">'total validation dog images :'</span>, len(os.listdir( validation_dogs_dir ) ))</span><br></pre></td></tr></table></figure><pre><code>total training cat images : 1000total training dog images : 1000total validation cat images : 500total validation dog images : 500</code></pre><p>For both cats and dogs, we have 1,000 training images and 500 validation images.</p><p>Now let’s take a look at a few pictures to get a better sense of what the cat and dog datasets look like. First, we configure the <code>matplotlib</code> parameters:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters for our graph; we'll output images in a 4x4 configuration</span></span><br><span class="line">nrows = <span class="number">4</span></span><br><span class="line">ncols = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">pic_index = <span class="number">0</span> <span class="comment"># Index for iterating over images</span></span><br></pre></td></tr></table></figure><p>Now, we display a batch of 8 cat and 8 dog pictures. You can re-run the cell to see a fresh batch each time:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set up matplotlib fig, and size it to fit 4x4 pics</span></span><br><span class="line">fig = plt.gcf()</span><br><span class="line">fig.set_size_inches(ncols*<span class="number">4</span>, nrows*<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">pic_index+=<span class="number">8</span></span><br><span class="line"></span><br><span class="line">next_cat_pix = [os.path.join(train_cats_dir, fname) </span><br><span class="line">                <span class="keyword">for</span> fname <span class="keyword">in</span> train_cat_fnames[ pic_index<span class="number">-8</span>:pic_index] </span><br><span class="line">               ]</span><br><span class="line"></span><br><span class="line">next_dog_pix = [os.path.join(train_dogs_dir, fname) </span><br><span class="line">                <span class="keyword">for</span> fname <span class="keyword">in</span> train_dog_fnames[ pic_index<span class="number">-8</span>:pic_index]</span><br><span class="line">               ]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, img_path <span class="keyword">in</span> enumerate(next_cat_pix+next_dog_pix):</span><br><span class="line">    <span class="comment"># Set up subplot; subplot indices start at 1</span></span><br><span class="line">    sp = plt.subplot(nrows, ncols, i + <span class="number">1</span>)</span><br><span class="line">    sp.axis(<span class="string">'Off'</span>) <span class="comment"># Don't show axes (or gridlines)</span></span><br><span class="line">    </span><br><span class="line">    img = mpimg.imread(img_path)</span><br><span class="line">    plt.imshow(img)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_17_0.png" alt="png"></p><p>It may not be obvious from looking at the images in this grid, but an important note here, and a significant difference from the previous lesson is that these images come in all shapes and sizes. When you did the handwriting recognition example, you had 28x28 greyscale images to work with. These are color and in a variety of shapes. Before training a Neural network with them you’ll need to tweak the images. You’ll see that in the next section.</p><p>Ok, now that you have an idea for what your data looks like, the next step is to define the model that will be trained to recognize cats or dogs from these images </p><h2 id="Building-a-Small-Model-from-Scratch-to-Get-to-72-Accuracy"><a href="#Building-a-Small-Model-from-Scratch-to-Get-to-72-Accuracy" class="headerlink" title="Building a Small Model from Scratch to Get to ~72% Accuracy"></a>Building a Small Model from Scratch to Get to ~72% Accuracy</h2><p>In the previous section you saw that the images were in a variety of shapes and sizes. In order to train a neural network to handle them you’ll need them to be in a uniform size. We’ve chosen 150x150 for this, and you’ll see the code that preprocesses the images to that shape shortly. </p><p>But before we continue, let’s start defining the model. We will define a Sequential layer as before, adding some convolutional layers first. Note the input shape parameter this time. In the earlier example it was 28x28x1, because the image was 28x28 in greyscale (8 bits, 1 byte for color depth). This time it is 150x150 for the size and 3 (24 bits, 3 bytes) for the color depth. </p><p>We then add a couple of convolutional layers as in the previous example, and flatten the final result to feed into the densely connected layers.</p><p>Finally we add the densely connected layers. </p><p>Note that because we are facing a two-class classification problem, i.e. a <em>binary classification problem</em>, we will end our network with a <a href="https://wikipedia.org/wiki/Sigmoid_function" target="_blank" rel="noopener"><em>sigmoid</em> activation</a>, so that the output of our network will be a single scalar between 0 and 1, encoding the probability that the current image is class 1 (as opposed to class 0).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    <span class="comment"># Note the input shape is the desired size of the image 150x150 with 3 bytes color</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">16</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">'relu'</span>, input_shape=(<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>)),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>), </span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">'relu'</span>), </span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># Flatten the results to feed into a DNN</span></span><br><span class="line">    tf.keras.layers.Flatten(), </span><br><span class="line">    <span class="comment"># 512 neuron hidden layer</span></span><br><span class="line">    tf.keras.layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>), </span><br><span class="line">    <span class="comment"># Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')</span></span><br><span class="line">    tf.keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)  </span><br><span class="line">])</span><br></pre></td></tr></table></figure><p>The <code>model.summary()</code> method call prints a summary of the NN </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d (Conv2D)              (None, 148, 148, 16)      448       _________________________________________________________________max_pooling2d (MaxPooling2D) (None, 74, 74, 16)        0         _________________________________________________________________conv2d_1 (Conv2D)            (None, 72, 72, 32)        4640      _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 36, 36, 32)        0         _________________________________________________________________conv2d_2 (Conv2D)            (None, 34, 34, 64)        18496     _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 17, 17, 64)        0         _________________________________________________________________flatten (Flatten)            (None, 18496)             0         _________________________________________________________________dense (Dense)                (None, 512)               9470464   _________________________________________________________________dense_1 (Dense)              (None, 1)                 513       =================================================================Total params: 9,494,561Trainable params: 9,494,561Non-trainable params: 0_________________________________________________________________</code></pre><p>The “output shape” column shows how the size of your feature map evolves in each successive layer. The convolution layers reduce the size of the feature maps by a bit due to padding, and each pooling layer halves the dimensions.</p><p>Next, we’ll configure the specifications for model training. We will train our model with the <code>binary_crossentropy</code> loss, because it’s a binary classification problem and our final activation is a sigmoid. (For a refresher on loss metrics, see the <a href="https://developers.google.com/machine-learning/crash-course/descending-into-ml/video-lecture" target="_blank" rel="noopener">Machine Learning Crash Course</a>.) We will use the <code>rmsprop</code> optimizer with a learning rate of <code>0.001</code>. During training, we will want to monitor classification accuracy.</p><p><strong>NOTE</strong>: In this case, using the <a href="https://wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp" target="_blank" rel="noopener">RMSprop optimization algorithm</a> is preferable to <a href="https://developers.google.com/machine-learning/glossary/#SGD" target="_blank" rel="noopener">stochastic gradient descent</a> (SGD), because RMSprop automates learning-rate tuning for us. (Other optimizers, such as <a href="https://wikipedia.org/wiki/Stochastic_gradient_descent#Adam" target="_blank" rel="noopener">Adam</a> and <a href="https://developers.google.com/machine-learning/glossary/#AdaGrad" target="_blank" rel="noopener">Adagrad</a>, also automatically adapt the learning rate during training, and would work equally well here.)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics = [<span class="string">'acc'</span>])</span><br></pre></td></tr></table></figure><h3 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h3><p>Let’s set up data generators that will read pictures in our source folders, convert them to <code>float32</code> tensors, and feed them (with their labels) to our network. We’ll have one generator for the training images and one for the validation images. Our generators will yield batches of 20 images of size 150x150 and their labels (binary).</p><p>As you may already know, data that goes into neural networks should usually be normalized in some way to make it more amenable to processing by the network. (It is uncommon to feed raw pixels into a convnet.) In our case, we will preprocess our images by normalizing the pixel values to be in the <code>[0, 1]</code> range (originally all values are in the <code>[0, 255]</code> range).</p><p>In Keras this can be done via the <code>keras.preprocessing.image.ImageDataGenerator</code> class using the <code>rescale</code> parameter. This <code>ImageDataGenerator</code> class allows you to instantiate generators of augmented image batches (and their labels) via <code>.flow(data, labels)</code> or <code>.flow_from_directory(directory)</code>. These generators can then be used with the Keras model methods that accept data generators as inputs: <code>fit_generator</code>, <code>evaluate_generator</code>, and <code>predict_generator</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line"><span class="comment"># All images will be rescaled by 1./255.</span></span><br><span class="line">train_datagen = ImageDataGenerator( rescale = <span class="number">1.0</span>/<span class="number">255.</span> )</span><br><span class="line">test_datagen  = ImageDataGenerator( rescale = <span class="number">1.0</span>/<span class="number">255.</span> )</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------</span></span><br><span class="line"><span class="comment"># Flow training images in batches of 20 using train_datagen generator</span></span><br><span class="line"><span class="comment"># --------------------</span></span><br><span class="line">train_generator = train_datagen.flow_from_directory(train_dir,</span><br><span class="line">                                                    batch_size=<span class="number">20</span>,</span><br><span class="line">                                                    class_mode=<span class="string">'binary'</span>,</span><br><span class="line">                                                    target_size=(<span class="number">150</span>, <span class="number">150</span>))     </span><br><span class="line"><span class="comment"># --------------------</span></span><br><span class="line"><span class="comment"># Flow validation images in batches of 20 using test_datagen generator</span></span><br><span class="line"><span class="comment"># --------------------</span></span><br><span class="line">validation_generator =  test_datagen.flow_from_directory(validation_dir,</span><br><span class="line">                                                         batch_size=<span class="number">20</span>,</span><br><span class="line">                                                         class_mode  = <span class="string">'binary'</span>,</span><br><span class="line">                                                         target_size = (<span class="number">150</span>, <span class="number">150</span>))</span><br></pre></td></tr></table></figure><pre><code>Found 2000 images belonging to 2 classes.Found 1000 images belonging to 2 classes.</code></pre><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>Let’s train on all 2,000 images available, for 15 epochs, and validate on all 1,000 test images. (This may take a few minutes to run.)</p><p>Do note the values per epoch.</p><p>You’ll see 4 values per epoch — Loss, Accuracy, Validation Loss and Validation Accuracy. </p><p>The Loss and Accuracy are a great indication of progress of training. It’s making a guess as to the classification of the training data, and then measuring it against the known label, calculating the result. Accuracy is the portion of correct guesses. The Validation accuracy is the measurement with the data that has not been used in training. As expected this would be a bit lower. You’ll learn about why this occurs in the section on overfitting later in this course.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit_generator(train_generator,</span><br><span class="line">                              validation_data=validation_generator,</span><br><span class="line">                              steps_per_epoch=<span class="number">100</span>,</span><br><span class="line">                              epochs=<span class="number">15</span>,</span><br><span class="line">                              validation_steps=<span class="number">50</span>,</span><br><span class="line">                              verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><pre><code>WARNING:tensorflow:From &lt;ipython-input-17-c57227122236&gt;:6: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.Instructions for updating:Please use Model.fit, which supports generators.WARNING:tensorflow:sample_weight modes were coerced from  ...    to    [&#39;...&#39;]WARNING:tensorflow:sample_weight modes were coerced from  ...    to    [&#39;...&#39;]Train for 100 steps, validate for 50 stepsEpoch 1/15100/100 - 24s - loss: 0.9314 - acc: 0.5660 - val_loss: 0.6656 - val_acc: 0.6120Epoch 2/15100/100 - 25s - loss: 0.6632 - acc: 0.6310 - val_loss: 0.7892 - val_acc: 0.5310Epoch 3/15100/100 - 24s - loss: 0.5777 - acc: 0.7025 - val_loss: 0.5888 - val_acc: 0.6900Epoch 4/15100/100 - 23s - loss: 0.4736 - acc: 0.7785 - val_loss: 0.6290 - val_acc: 0.6970Epoch 5/15100/100 - 23s - loss: 0.3826 - acc: 0.8310 - val_loss: 0.6550 - val_acc: 0.6990Epoch 6/15100/100 - 23s - loss: 0.3170 - acc: 0.8695 - val_loss: 0.7551 - val_acc: 0.6940Epoch 7/15100/100 - 23s - loss: 0.2174 - acc: 0.9075 - val_loss: 1.0175 - val_acc: 0.6640Epoch 8/15100/100 - 23s - loss: 0.1598 - acc: 0.9420 - val_loss: 1.3129 - val_acc: 0.6650Epoch 9/15100/100 - 23s - loss: 0.1329 - acc: 0.9570 - val_loss: 1.1711 - val_acc: 0.6890Epoch 10/15100/100 - 23s - loss: 0.0960 - acc: 0.9735 - val_loss: 1.3053 - val_acc: 0.7130Epoch 11/15100/100 - 23s - loss: 0.0709 - acc: 0.9800 - val_loss: 1.5509 - val_acc: 0.6970Epoch 12/15100/100 - 23s - loss: 0.0496 - acc: 0.9865 - val_loss: 5.7583 - val_acc: 0.5370Epoch 13/15100/100 - 25s - loss: 0.0885 - acc: 0.9800 - val_loss: 3.9976 - val_acc: 0.5910Epoch 14/15100/100 - 23s - loss: 0.0423 - acc: 0.9830 - val_loss: 1.8770 - val_acc: 0.7040Epoch 15/15100/100 - 23s - loss: 0.0480 - acc: 0.9900 - val_loss: 2.3820 - val_acc: 0.6860</code></pre><h3 id="Evaluating-Accuracy-and-Loss-for-the-Model"><a href="#Evaluating-Accuracy-and-Loss-for-the-Model" class="headerlink" title="Evaluating Accuracy and Loss for the Model"></a>Evaluating Accuracy and Loss for the Model</h3><p>Let’s plot the training/validation accuracy and loss as collected during training:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-----------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Retrieve a list of list results on training and test data</span></span><br><span class="line"><span class="comment"># sets for each training epoch</span></span><br><span class="line"><span class="comment">#-----------------------------------------------------------</span></span><br><span class="line">acc      = history.history[     <span class="string">'acc'</span> ]</span><br><span class="line">val_acc  = history.history[ <span class="string">'val_acc'</span> ]</span><br><span class="line">loss     = history.history[    <span class="string">'loss'</span> ]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span> ]</span><br><span class="line"></span><br><span class="line">epochs   = range(len(acc)) <span class="comment"># Get number of epochs</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#------------------------------------------------</span></span><br><span class="line"><span class="comment"># Plot training and validation accuracy per epoch</span></span><br><span class="line"><span class="comment">#------------------------------------------------</span></span><br><span class="line">plt.plot  ( epochs,     acc, label=<span class="string">'Training'</span>)</span><br><span class="line">plt.plot  ( epochs, val_acc, label=<span class="string">'Validation'</span>)</span><br><span class="line">plt.title (<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="comment">#------------------------------------------------</span></span><br><span class="line"><span class="comment"># Plot training and validation loss per epoch</span></span><br><span class="line"><span class="comment">#------------------------------------------------</span></span><br><span class="line">plt.plot  ( epochs,     loss, label=<span class="string">'Training'</span>)</span><br><span class="line">plt.plot  ( epochs, val_loss, label=<span class="string">'Validation'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title (<span class="string">'Training and validation loss'</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5, 1.0, &#39;Training and validation loss&#39;)</code></pre><p><img src="output_31_1.png" alt="png"></p><p><img src="output_31_2.png" alt="png"></p><p>As you can see, we are <strong>overfitting</strong> like it’s getting out of fashion. Our training accuracy (in blue) gets close to 100% (!) while our validation accuracy (in orange) stalls as 70%. Our validation loss reaches its minimum after only five epochs.</p><p>Since we have a relatively small number of training examples (2000), overfitting should be our number one concern. Overfitting happens when a model exposed to too few examples learns patterns that do not generalize to new data, i.e. when the model starts using irrelevant features for making predictions. For instance, if you, as a human, only see three images of people who are lumberjacks, and three images of people who are sailors, and among them the only person wearing a cap is a lumberjack, you might start thinking that wearing a cap is a sign of being a lumberjack as opposed to a sailor. You would then make a pretty lousy lumberjack/sailor classifier.</p><p>Overfitting is the central problem in machine learning: given that we are fitting the parameters of our model to a given dataset, how can we make sure that the representations learned by the model will be applicable to data never seen before? How do we avoid learning things that are specific to the training data?</p><p>In the next exercise, we’ll look at ways to prevent overfitting in the cat vs. dog classification model.</p><h2 id="Save-the-Model"><a href="#Save-the-Model" class="headerlink" title="Save the Model"></a>Save the Model</h2><p>In the cell below, save the trained model as a Keras model (<code>.h5</code> file).</p><p><strong>HINT</strong>: Use <code>model.save()</code>. Feel free to take a look at the <code>Linear-to-JavaScript.ipynb</code> example.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Save the trained model as a Keras HDF5 file. </span></span><br><span class="line"></span><br><span class="line">saved_model_path = <span class="string">"./my_model.h5"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># YOUR CODE HERE</span></span><br><span class="line">model.save(saved_model_path)</span><br></pre></td></tr></table></figure><h2 id="Run-the-TensorFlow-js-Converter-on-The-Saved-Keras-Model"><a href="#Run-the-TensorFlow-js-Converter-on-The-Saved-Keras-Model" class="headerlink" title="Run the TensorFlow.js Converter on The Saved Keras Model"></a>Run the TensorFlow.js Converter on The Saved Keras Model</h2><p>In the cell below, use the <code>tensorflowjs_converter</code> to convert the saved Keras model into JSON format.</p><p><strong>HINT</strong>: Make sure you specify the format of the input model as Keras by using the <code>--input_format</code> option. Feel free to take a look at the <code>Linear-to-JavaScript.ipynb</code> example and the <a href="https://github.com/tensorflow/tfjs/tree/master/tfjs-converter#step-1-converting-a-tensorflow-savedmodel-tensorflow-hub-module-keras-hdf5-or-tfkeras-savedmodel-to-a-web-friendly-format" target="_blank" rel="noopener">TensorFlow.js converter documentation</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXERCISE: Use the tensorflow.js converter to convert the saved Keras model into JSON format.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># YOUR CODE HERE</span></span><br><span class="line">! tensorflowjs_converter \</span><br><span class="line">    --input_format=keras \</span><br><span class="line">    &#123;saved_model_path&#125; \</span><br><span class="line">    <span class="string">"./"</span></span><br></pre></td></tr></table></figure><p>If you did things correctly, you should now have a <strong>JSON</strong> file named <code>model.json</code> and various <code>.bin</code> files, such as <code>group1-shard1of10.bin</code>. The number of <code>.bin</code> files will depend on the size of your model: the larger your model, the greater the number of <code>.bin</code> files. The <code>model.json</code> file contains the architecture of your model and the <code>.bin</code> files will contain the weights of your model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Converting-a-Keras-Model-to-JSON-Format&quot;&gt;&lt;a href=&quot;#Converting-a-Keras-Model-to-JSON-Format&quot; class=&quot;headerlink&quot; title=&quot;Converting a K
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
      <category term="AI Workflow" scheme="https://zhangruochi.com/categories/AI-Workflow/"/>
    
      <category term="Deployment" scheme="https://zhangruochi.com/categories/AI-Workflow/Deployment/"/>
    
    
  </entry>
  
  <entry>
    <title>Evaluation of Diagnostic Models</title>
    <link href="https://zhangruochi.com/Evaluation-of-Diagnostic-Models/2020/04/17/"/>
    <id>https://zhangruochi.com/Evaluation-of-Diagnostic-Models/2020/04/17/</id>
    <published>2020-04-17T10:54:09.000Z</published>
    <updated>2020-04-19T15:10:09.341Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Evaluation-of-Diagnostic-Models"><a href="#Evaluation-of-Diagnostic-Models" class="headerlink" title="Evaluation of Diagnostic Models"></a>Evaluation of Diagnostic Models</h1><p>Welcome to the second assignment of course 1. In this assignment, we will be working with the results of the X-ray classification model we developed in the previous assignment. In order to make the data processing a bit more manageable, we will be working with a subset of our training, and validation datasets. We will also use our manually labeled test dataset of 420 X-rays.  </p><p>As a reminder, our dataset contains X-rays from 14 different conditions diagnosable from an X-ray. We’ll evaluate our performance on each of these classes using the classification metrics we learned in lecture.</p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><p>Click on these links to jump to a particular section of this assignment!</p><ul><li><a href="#1">1. Packages</a></li><li><a href="#2">2. Overview</a></li><li><a href="#3">3. Metrics</a><ul><li><a href="#3-1">3.1 True Positives, False Positives, True Negatives, and False Negatives</a></li><li><a href="#3-2">3.2 Accuracy</a></li><li><a href="#3-3">3.3 Prevalence</a></li><li><a href="#3-4">3.4 Sensitivity and Specificity</a></li><li><a href="#3-5">3.5 PPV and NPV</a></li><li><a href="#3-6">3.6 ROC Curve</a></li></ul></li><li><a href="#4">4. Confidence Intervals</a></li><li><a href="#5">5. Precision-Recall Curve</a></li><li><a href="#6">6. F1 Score</a></li><li><a href="#7">7. Calibration</a></li></ul><p><strong>By the end of this assignment you will learn about:</strong></p><ol><li>Accuracy</li><li>Prevalence</li><li>Specificity &amp; Sensitivity</li><li>PPV and NPV</li><li>ROC curve and AUCROC (c-statistic)</li><li>Confidence Intervals</li></ol><p><a name="1"></a></p><h2 id="1-Packages"><a href="#1-Packages" class="headerlink" title="1. Packages"></a>1. Packages</h2><p>In this assignment, we’ll make use of the following packages:</p><ul><li><a href="https://docs.scipy.org/doc/numpy/" target="_blank" rel="noopener">numpy</a> is a popular library for scientific computing</li><li><a href="https://matplotlib.org/3.1.1/contents.html" target="_blank" rel="noopener">matplotlib</a> is a plotting library compatible with numpy</li><li><a href="https://pandas.pydata.org/docs/" target="_blank" rel="noopener">pandas</a> is what we’ll use to manipulate our data</li><li><a href="https://scikit-learn.org/stable/index.html" target="_blank" rel="noopener">sklearn</a> will be used to measure the performance of our model</li></ul><p>Run the next cell to import all the necessary packages as well as custom util functions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd  </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> util</span><br></pre></td></tr></table></figure><p><a name="2"></a></p><h2 id="2-Overview"><a href="#2-Overview" class="headerlink" title="2. Overview"></a>2. Overview</h2><p>We’ll go through our evaluation metrics in the following order.</p><ul><li>Metrics<ul><li>TP, TN, FP, FN</li><li>Accuracy</li><li>Prevalence</li><li>Sensitivity and Specificity</li><li>PPV and NPV</li><li>AUC</li></ul></li><li>Confidence Intervals</li></ul><p>Let’s take a quick peek at our dataset. The data is stored in two CSV files called <code>train_preds.csv</code> and <code>valid_preds.csv</code>. We have precomputed the model outputs for our test cases. We’ll work with these predictions and the true class labels throughout the assignment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">train_results = pd.read_csv(<span class="string">"train_preds.csv"</span>)</span><br><span class="line">valid_results = pd.read_csv(<span class="string">"valid_preds.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># the labels in our dataset</span></span><br><span class="line">class_labels = [<span class="string">'Cardiomegaly'</span>,</span><br><span class="line"> <span class="string">'Emphysema'</span>,</span><br><span class="line"> <span class="string">'Effusion'</span>,</span><br><span class="line"> <span class="string">'Hernia'</span>,</span><br><span class="line"> <span class="string">'Infiltration'</span>,</span><br><span class="line"> <span class="string">'Mass'</span>,</span><br><span class="line"> <span class="string">'Nodule'</span>,</span><br><span class="line"> <span class="string">'Atelectasis'</span>,</span><br><span class="line"> <span class="string">'Pneumothorax'</span>,</span><br><span class="line"> <span class="string">'Pleural_Thickening'</span>,</span><br><span class="line"> <span class="string">'Pneumonia'</span>,</span><br><span class="line"> <span class="string">'Fibrosis'</span>,</span><br><span class="line"> <span class="string">'Edema'</span>,</span><br><span class="line"> <span class="string">'Consolidation'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># the labels for prediction values in our dataset</span></span><br><span class="line">pred_labels = [l + <span class="string">"_pred"</span> <span class="keyword">for</span> l <span class="keyword">in</span> class_labels]</span><br></pre></td></tr></table></figure><p>Extract the labels (y) and the predictions (pred).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = valid_results[class_labels].values</span><br><span class="line">pred = valid_results[pred_labels].values</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.shape, pred.shape</span><br></pre></td></tr></table></figure><pre><code>((1000, 14), (1000, 14))</code></pre><p>Run the next cell to view them side by side.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># let's take a peek at our dataset</span></span><br><span class="line">valid_results[np.concatenate([class_labels, pred_labels])].head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Cardiomegaly</th>      <th>Emphysema</th>      <th>Effusion</th>      <th>Hernia</th>      <th>Infiltration</th>      <th>Mass</th>      <th>Nodule</th>      <th>Atelectasis</th>      <th>Pneumothorax</th>      <th>Pleural_Thickening</th>      <th>...</th>      <th>Infiltration_pred</th>      <th>Mass_pred</th>      <th>Nodule_pred</th>      <th>Atelectasis_pred</th>      <th>Pneumothorax_pred</th>      <th>Pleural_Thickening_pred</th>      <th>Pneumonia_pred</th>      <th>Fibrosis_pred</th>      <th>Edema_pred</th>      <th>Consolidation_pred</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0.256020</td>      <td>0.266928</td>      <td>0.312440</td>      <td>0.460342</td>      <td>0.079453</td>      <td>0.271495</td>      <td>0.276861</td>      <td>0.398799</td>      <td>0.015867</td>      <td>0.156320</td>    </tr>    <tr>      <th>1</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0.382199</td>      <td>0.176825</td>      <td>0.465807</td>      <td>0.489424</td>      <td>0.084595</td>      <td>0.377318</td>      <td>0.363582</td>      <td>0.638024</td>      <td>0.025948</td>      <td>0.144419</td>    </tr>    <tr>      <th>2</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0.427727</td>      <td>0.115513</td>      <td>0.249030</td>      <td>0.035105</td>      <td>0.238761</td>      <td>0.167095</td>      <td>0.166389</td>      <td>0.262463</td>      <td>0.007758</td>      <td>0.125790</td>    </tr>    <tr>      <th>3</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0.158596</td>      <td>0.259460</td>      <td>0.334870</td>      <td>0.266489</td>      <td>0.073371</td>      <td>0.229834</td>      <td>0.191281</td>      <td>0.344348</td>      <td>0.008559</td>      <td>0.119153</td>    </tr>    <tr>      <th>4</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>...</td>      <td>0.536762</td>      <td>0.198797</td>      <td>0.273110</td>      <td>0.186771</td>      <td>0.242122</td>      <td>0.309786</td>      <td>0.411771</td>      <td>0.244666</td>      <td>0.126930</td>      <td>0.342409</td>    </tr>  </tbody></table><p>5 rows × 28 columns</p></div><p>To further understand our dataset details, here’s a histogram of the number of samples for each label in the validation dataset:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line">plt.bar(x = class_labels, height= y.sum(axis=<span class="number">0</span>));</span><br></pre></td></tr></table></figure><p><img src="output_14_0.png" alt="png"></p><p>It seem like our dataset has an imbalanced population of samples. Specifically, our dataset has a small number of patients diagnosed with a <code>Hernia</code>.</p><p><a name="3"></a></p><h2 id="3-Metrics"><a href="#3-Metrics" class="headerlink" title="3 Metrics"></a>3 Metrics</h2><p><a name="3-1"></a></p><h3 id="3-1-True-Positives-False-Positives-True-Negatives-and-False-Negatives"><a href="#3-1-True-Positives-False-Positives-True-Negatives-and-False-Negatives" class="headerlink" title="3.1 True Positives, False Positives, True Negatives, and False Negatives"></a>3.1 True Positives, False Positives, True Negatives, and False Negatives</h3><p>The most basic statistics to compute from the model predictions are the true positives, true negatives, false positives, and false negatives. </p><p>As the name suggests</p><ul><li>true positive (TP): The model classifies the example as positive, and the actual label also positive.</li><li>false positive (FP): The model classifies the example as positive, <strong>but</strong> the actual label is negative.</li><li>true negative (TN): The model classifies the example as negative, and the actual label is also negative.</li><li>false negative (FN): The model classifies the example as negative, <strong>but</strong> the label is actually positive.</li></ul><p>We will count the number of TP, FP, TN and FN in the given data.  All of our metrics can be built off of these four statistics. </p><p>Recall that the model outputs real numbers between 0 and 1.</p><ul><li>To compute binary class predictions, we need to convert these to either 0 or 1. </li><li>We’ll do this using a threshold value $th$.</li><li>Any model outputs above $th$ are set to 1, and below $th$ are set to 0. </li></ul><p>All of our metrics (except for AUC at the end) will depend on the choice of this threshold. </p><p>Fill in the functions to compute the TP, FP, TN, and FN for a given threshold below. </p><p>The first one has been done for you.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">true_positives</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Count true positives.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        TP (int): true positives</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    TP = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get thresholded predictions</span></span><br><span class="line">    thresholded_preds = pred &gt;= th</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute TP</span></span><br><span class="line">    TP = np.sum((y == <span class="number">1</span>) &amp; (thresholded_preds == <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> TP</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">true_negatives</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Count true negatives.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        TN (int): true negatives</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    TN = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get thresholded predictions</span></span><br><span class="line">    thresholded_preds = pred &gt;= th</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute TN</span></span><br><span class="line">    TN = np.sum((y == <span class="number">0</span>) &amp; (thresholded_preds == <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> TN</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">false_positives</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Count false positives.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        FP (int): false positives</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    FP = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get thresholded predictions</span></span><br><span class="line">    thresholded_preds = pred &gt;= th</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute FP</span></span><br><span class="line">    FP = np.sum((y == <span class="number">0</span>) &amp; (thresholded_preds == <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> FP</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">false_negatives</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Count false positives.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        FN (int): false negatives</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    FN = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get thresholded predictions</span></span><br><span class="line">    thresholded_preds = pred &gt;= th</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute FN</span></span><br><span class="line">    FN = np.sum((y == <span class="number">1</span>) &amp; (thresholded_preds == <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> FN</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Note: we must explicity import 'display' in order for the autograder to compile the submitted code</span></span><br><span class="line"><span class="comment"># Even though we could use this function without importing it, keep this import in order to allow the grader to work</span></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display</span><br><span class="line"><span class="comment"># Test</span></span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">'y_test'</span>: [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                   <span class="string">'preds_test'</span>: [<span class="number">0.8</span>,<span class="number">0.7</span>,<span class="number">0.4</span>,<span class="number">0.3</span>,<span class="number">0.2</span>,<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0</span>],</span><br><span class="line">                   <span class="string">'category'</span>: [<span class="string">'TP'</span>,<span class="string">'TP'</span>,<span class="string">'TN'</span>,<span class="string">'TN'</span>,<span class="string">'TN'</span>,<span class="string">'FP'</span>,<span class="string">'FP'</span>,<span class="string">'FP'</span>,<span class="string">'FP'</span>,<span class="string">'FN'</span>,<span class="string">'FN'</span>,<span class="string">'FN'</span>,<span class="string">'FN'</span>,<span class="string">'FN'</span>]</span><br><span class="line">                  &#125;)</span><br><span class="line"></span><br><span class="line">display(df)</span><br><span class="line"><span class="comment">#y_test = np.array([1, 0, 0, 1, 1])</span></span><br><span class="line">y_test = df[<span class="string">'y_test'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#preds_test = np.array([0.8, 0.8, 0.4, 0.6, 0.3])</span></span><br><span class="line">preds_test = df[<span class="string">'preds_test'</span>]</span><br><span class="line"></span><br><span class="line">threshold = <span class="number">0.5</span></span><br><span class="line">print(<span class="string">f"threshold: <span class="subst">&#123;threshold&#125;</span>\n"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"""Our functions calcualted: </span></span><br><span class="line"><span class="string">TP: <span class="subst">&#123;true_positives(y_test, preds_test, threshold)&#125;</span></span></span><br><span class="line"><span class="string">TN: <span class="subst">&#123;true_negatives(y_test, preds_test, threshold)&#125;</span></span></span><br><span class="line"><span class="string">FP: <span class="subst">&#123;false_positives(y_test, preds_test, threshold)&#125;</span></span></span><br><span class="line"><span class="string">FN: <span class="subst">&#123;false_negatives(y_test, preds_test, threshold)&#125;</span></span></span><br><span class="line"><span class="string">"""</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Expected results"</span>)</span><br><span class="line">print(<span class="string">f"There are <span class="subst">&#123;sum(df[<span class="string">'category'</span>] == <span class="string">'TP'</span>)&#125;</span> TP"</span>)</span><br><span class="line">print(<span class="string">f"There are <span class="subst">&#123;sum(df[<span class="string">'category'</span>] == <span class="string">'TN'</span>)&#125;</span> TN"</span>)</span><br><span class="line">print(<span class="string">f"There are <span class="subst">&#123;sum(df[<span class="string">'category'</span>] == <span class="string">'FP'</span>)&#125;</span> FP"</span>)</span><br><span class="line">print(<span class="string">f"There are <span class="subst">&#123;sum(df[<span class="string">'category'</span>] == <span class="string">'FN'</span>)&#125;</span> FN"</span>)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>y_test</th>      <th>preds_test</th>      <th>category</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>0.8</td>      <td>TP</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>0.7</td>      <td>TP</td>    </tr>    <tr>      <th>2</th>      <td>0</td>      <td>0.4</td>      <td>TN</td>    </tr>    <tr>      <th>3</th>      <td>0</td>      <td>0.3</td>      <td>TN</td>    </tr>    <tr>      <th>4</th>      <td>0</td>      <td>0.2</td>      <td>TN</td>    </tr>    <tr>      <th>5</th>      <td>0</td>      <td>0.5</td>      <td>FP</td>    </tr>    <tr>      <th>6</th>      <td>0</td>      <td>0.6</td>      <td>FP</td>    </tr>    <tr>      <th>7</th>      <td>0</td>      <td>0.7</td>      <td>FP</td>    </tr>    <tr>      <th>8</th>      <td>0</td>      <td>0.8</td>      <td>FP</td>    </tr>    <tr>      <th>9</th>      <td>1</td>      <td>0.1</td>      <td>FN</td>    </tr>    <tr>      <th>10</th>      <td>1</td>      <td>0.2</td>      <td>FN</td>    </tr>    <tr>      <th>11</th>      <td>1</td>      <td>0.3</td>      <td>FN</td>    </tr>    <tr>      <th>12</th>      <td>1</td>      <td>0.4</td>      <td>FN</td>    </tr>    <tr>      <th>13</th>      <td>1</td>      <td>0.0</td>      <td>FN</td>    </tr>  </tbody></table></div><pre><code>threshold: 0.5Our functions calcualted: TP: 2TN: 3FP: 4FN: 5Expected resultsThere are 2 TPThere are 3 TNThere are 4 FPThere are 5 FN</code></pre><p>Run the next cell to see a summary of evaluative metrics for the model predictions for each class. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.get_performance_metrics(y, pred, class_labels)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>TP</th>      <th>TN</th>      <th>FP</th>      <th>FN</th>      <th>Accuracy</th>      <th>Prevalence</th>      <th>Sensitivity</th>      <th>Specificity</th>      <th>PPV</th>      <th>NPV</th>      <th>AUC</th>      <th>F1</th>      <th>Threshold</th>    </tr>    <tr>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Cardiomegaly</th>      <td>16</td>      <td>814</td>      <td>169</td>      <td>1</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Emphysema</th>      <td>20</td>      <td>869</td>      <td>103</td>      <td>8</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Effusion</th>      <td>99</td>      <td>690</td>      <td>196</td>      <td>15</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Hernia</th>      <td>1</td>      <td>743</td>      <td>255</td>      <td>1</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Infiltration</th>      <td>114</td>      <td>543</td>      <td>265</td>      <td>78</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Mass</th>      <td>40</td>      <td>789</td>      <td>158</td>      <td>13</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Nodule</th>      <td>28</td>      <td>731</td>      <td>220</td>      <td>21</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Atelectasis</th>      <td>64</td>      <td>657</td>      <td>249</td>      <td>30</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumothorax</th>      <td>24</td>      <td>785</td>      <td>183</td>      <td>8</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pleural_Thickening</th>      <td>24</td>      <td>713</td>      <td>259</td>      <td>4</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumonia</th>      <td>14</td>      <td>661</td>      <td>320</td>      <td>5</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Fibrosis</th>      <td>10</td>      <td>725</td>      <td>261</td>      <td>4</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Edema</th>      <td>15</td>      <td>767</td>      <td>213</td>      <td>5</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Consolidation</th>      <td>36</td>      <td>658</td>      <td>297</td>      <td>9</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>  </tbody></table></div><p>Right now it only has TP, TN, FP, FN. Throughout this assignment we’ll fill in all the other metrics to learn more about our model performance.</p><p><a name="3-2"></a></p><h3 id="3-2-Accuracy"><a href="#3-2-Accuracy" class="headerlink" title="3.2 Accuracy"></a>3.2 Accuracy</h3><p>Let’s use a threshold of .5 for the probability cutoff for our predictions for all classes and calculate our model’s accuracy as we would normally do in a machine learning problem. </p><script type="math/tex; mode=display">accuracy = \frac{\text{true positives} + \text{true negatives}}{\text{true positives} + \text{true negatives} + \text{false positives} + \text{false negatives}}</script><p>Use this formula to compute accuracy below:</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>Remember to set the value for the threshold when calling the functions.</li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_accuracy</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute accuracy of predictions at threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        accuracy (float): accuracy of predictions at threshold</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    accuracy = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get TP, FP, TN, FN using our previously defined functions</span></span><br><span class="line">    TP = true_positives(y, pred, th)</span><br><span class="line">    FP = false_positives(y, pred, th)</span><br><span class="line">    TN = true_negatives(y, pred, th)</span><br><span class="line">    FN = false_negatives(y, pred, th)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute accuracy using TP, FP, TN, FN</span></span><br><span class="line">    accuracy = (TP + TN)/(TP + TN + FP + FN)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test</span></span><br><span class="line">print(<span class="string">"Test case:"</span>)</span><br><span class="line"></span><br><span class="line">y_test = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">'test labels: &#123;y_test&#125;'</span>)</span><br><span class="line"></span><br><span class="line">preds_test = np.array([<span class="number">0.8</span>, <span class="number">0.8</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.3</span>])</span><br><span class="line">print(<span class="string">f'test predictions: <span class="subst">&#123;preds_test&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">threshold = <span class="number">0.5</span></span><br><span class="line">print(<span class="string">f"threshold: <span class="subst">&#123;threshold&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"computed accuracy: <span class="subst">&#123;get_accuracy(y_test, preds_test, threshold)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test case:test labels: {y_test}test predictions: [0.8 0.8 0.4 0.6 0.3]threshold: 0.5computed accuracy: 0.6</code></pre><h4 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test labels: &#123;y_test&#125;</span><br><span class="line">test predictions: [<span class="number">0.8</span> <span class="number">0.8</span> <span class="number">0.4</span> <span class="number">0.6</span> <span class="number">0.3</span>]</span><br><span class="line">threshold: <span class="number">0.5</span></span><br><span class="line">computed accuracy: <span class="number">0.6</span></span><br></pre></td></tr></table></figure><p>Run the next cell to see the accuracy of the model output for each class, as well as the number of true positives, true negatives, false positives, and false negatives.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>TP</th>      <th>TN</th>      <th>FP</th>      <th>FN</th>      <th>Accuracy</th>      <th>Prevalence</th>      <th>Sensitivity</th>      <th>Specificity</th>      <th>PPV</th>      <th>NPV</th>      <th>AUC</th>      <th>F1</th>      <th>Threshold</th>    </tr>    <tr>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Cardiomegaly</th>      <td>16</td>      <td>814</td>      <td>169</td>      <td>1</td>      <td>0.83</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Emphysema</th>      <td>20</td>      <td>869</td>      <td>103</td>      <td>8</td>      <td>0.889</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Effusion</th>      <td>99</td>      <td>690</td>      <td>196</td>      <td>15</td>      <td>0.789</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Hernia</th>      <td>1</td>      <td>743</td>      <td>255</td>      <td>1</td>      <td>0.744</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Infiltration</th>      <td>114</td>      <td>543</td>      <td>265</td>      <td>78</td>      <td>0.657</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Mass</th>      <td>40</td>      <td>789</td>      <td>158</td>      <td>13</td>      <td>0.829</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Nodule</th>      <td>28</td>      <td>731</td>      <td>220</td>      <td>21</td>      <td>0.759</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Atelectasis</th>      <td>64</td>      <td>657</td>      <td>249</td>      <td>30</td>      <td>0.721</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumothorax</th>      <td>24</td>      <td>785</td>      <td>183</td>      <td>8</td>      <td>0.809</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pleural_Thickening</th>      <td>24</td>      <td>713</td>      <td>259</td>      <td>4</td>      <td>0.737</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumonia</th>      <td>14</td>      <td>661</td>      <td>320</td>      <td>5</td>      <td>0.675</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Fibrosis</th>      <td>10</td>      <td>725</td>      <td>261</td>      <td>4</td>      <td>0.735</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Edema</th>      <td>15</td>      <td>767</td>      <td>213</td>      <td>5</td>      <td>0.782</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Consolidation</th>      <td>36</td>      <td>658</td>      <td>297</td>      <td>9</td>      <td>0.694</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>  </tbody></table></div><p>If we were to judge our model’s performance based on the accuracy metric, we would say that our model is not very accurate for detecting the <code>Infiltration</code> cases (accuracy of 0.657) but pretty accurate for detecting <code>Emphysema</code> (accuracy of 0.889). </p><p><strong>But is that really the case?…</strong></p><p>Let’s imagine a model that simply predicts that any patient does <strong>Not</strong> have <code>Emphysema</code>, regardless of patient’s measurements. Let’s calculate the accuracy for such a model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_accuracy(valid_results[<span class="string">"Emphysema"</span>].values, np.zeros(len(valid_results)))</span><br></pre></td></tr></table></figure><p>As you can see above, such a model would be 97% accurate! Even better than our deep learning based model. </p><p>But is this really a good model? Wouldn’t this model be wrong 100% of the time if the patient actually had this condition?</p><p>In the following sections, we will address this concern with more advanced model measures - <strong>sensitivity and specificity</strong> - that evaluate how well the model predicts positives for patients with the condition and negatives for cases that actually do not have the condition.</p><p><a name="3-3"></a></p><h3 id="3-3-Prevalence"><a href="#3-3-Prevalence" class="headerlink" title="3.3 Prevalence"></a>3.3 Prevalence</h3><p>Another important concept is <strong>prevalence</strong>. </p><ul><li>In a medical context, prevalence is the proportion of people in the population who have the disease (or condition, etc). </li><li>In machine learning terms, this is the proportion of positive examples. The expression for prevalence is:</li></ul><script type="math/tex; mode=display">prevalence = \frac{1}{N} \sum_{i} y_i</script><p>where $y_i = 1$ when the example is ‘positive’ (has the disease).</p><p>Let’s measure prevalence for each disease:</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li>    You can use <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html" target="_blank" rel="noopener"> np.mean </a> to calculate the formula.</li>    <li>Actually, the automatic grader is expecting numpy.mean, so please use it instead of using an equally valid but different way of calculating the prevalence. =) </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_prevalence</span><span class="params">(y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute accuracy of predictions at threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        prevalence (float): prevalence of positive cases</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    prevalence = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    prevalence = np.mean(y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> prevalence</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test</span></span><br><span class="line">print(<span class="string">"Test case:\n"</span>)</span><br><span class="line"></span><br><span class="line">y_test = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">f'test labels: <span class="subst">&#123;y_test&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"computed prevalence: <span class="subst">&#123;get_prevalence(y_test)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test case:test labels: [1 0 0 1 1 0 0 0 0 1]computed prevalence: 0.4</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>TP</th>      <th>TN</th>      <th>FP</th>      <th>FN</th>      <th>Accuracy</th>      <th>Prevalence</th>      <th>Sensitivity</th>      <th>Specificity</th>      <th>PPV</th>      <th>NPV</th>      <th>AUC</th>      <th>F1</th>      <th>Threshold</th>    </tr>    <tr>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Cardiomegaly</th>      <td>16</td>      <td>814</td>      <td>169</td>      <td>1</td>      <td>0.83</td>      <td>0.017</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Emphysema</th>      <td>20</td>      <td>869</td>      <td>103</td>      <td>8</td>      <td>0.889</td>      <td>0.028</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Effusion</th>      <td>99</td>      <td>690</td>      <td>196</td>      <td>15</td>      <td>0.789</td>      <td>0.114</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Hernia</th>      <td>1</td>      <td>743</td>      <td>255</td>      <td>1</td>      <td>0.744</td>      <td>0.002</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Infiltration</th>      <td>114</td>      <td>543</td>      <td>265</td>      <td>78</td>      <td>0.657</td>      <td>0.192</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Mass</th>      <td>40</td>      <td>789</td>      <td>158</td>      <td>13</td>      <td>0.829</td>      <td>0.053</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Nodule</th>      <td>28</td>      <td>731</td>      <td>220</td>      <td>21</td>      <td>0.759</td>      <td>0.049</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Atelectasis</th>      <td>64</td>      <td>657</td>      <td>249</td>      <td>30</td>      <td>0.721</td>      <td>0.094</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumothorax</th>      <td>24</td>      <td>785</td>      <td>183</td>      <td>8</td>      <td>0.809</td>      <td>0.032</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pleural_Thickening</th>      <td>24</td>      <td>713</td>      <td>259</td>      <td>4</td>      <td>0.737</td>      <td>0.028</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumonia</th>      <td>14</td>      <td>661</td>      <td>320</td>      <td>5</td>      <td>0.675</td>      <td>0.019</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Fibrosis</th>      <td>10</td>      <td>725</td>      <td>261</td>      <td>4</td>      <td>0.735</td>      <td>0.014</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Edema</th>      <td>15</td>      <td>767</td>      <td>213</td>      <td>5</td>      <td>0.782</td>      <td>0.02</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Consolidation</th>      <td>36</td>      <td>658</td>      <td>297</td>      <td>9</td>      <td>0.694</td>      <td>0.045</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>  </tbody></table></div><p><code>Hernia</code> has a prevalence 0.002, which is the rarest among the studied conditions in our dataset.</p><p><a name="3-4"></a></p><h3 id="3-4-Sensitivity-and-Specificity"><a href="#3-4-Sensitivity-and-Specificity" class="headerlink" title="3.4 Sensitivity and Specificity"></a>3.4 Sensitivity and Specificity</h3><p><img src="sens_spec.png" width="30%"></p><p>Sensitivity and specificity are two of the most prominent numbers that are used to measure diagnostics tests.</p><ul><li>Sensitivity is the probability that our test outputs positive given that the case is actually positive.</li><li>Specificity is the probability that the test outputs negative given that the case is actually negative. </li></ul><p>We can phrase this easily in terms of true positives, true negatives, false positives, and false negatives: </p><script type="math/tex; mode=display">sensitivity = \frac{\text{true positives}}{\text{true positives} + \text{false negatives}}</script><script type="math/tex; mode=display">specificity = \frac{\text{true negatives}}{\text{true negatives} + \text{false positives}}</script><p>Let’s calculate sensitivity and specificity for our model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sensitivity</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute sensitivity of predictions at threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        sensitivity (float): probability that our test outputs positive given that the case is actually positive</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    sensitivity = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get TP and FN using our previously defined functions</span></span><br><span class="line">    TP = true_positives(y, pred, th)</span><br><span class="line">    FN = false_negatives(y, pred, th)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># use TP and FN to compute sensitivity</span></span><br><span class="line">    sensitivity = TP / (TP + FN)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> sensitivity</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_specificity</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute specificity of predictions at threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        specificity (float): probability that the test outputs negative given that the case is actually negative</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    specificity = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get TN and FP using our previously defined functions</span></span><br><span class="line">    TN = true_negatives(y, pred, th)</span><br><span class="line">    FP = false_positives(y , pred, th)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># use TN and FP to compute specificity </span></span><br><span class="line">    specificity = TN / (TN + FP)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> specificity</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test</span></span><br><span class="line">print(<span class="string">"Test case"</span>)</span><br><span class="line"></span><br><span class="line">y_test = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">f'test labels: <span class="subst">&#123;y_test&#125;</span>\n'</span>)</span><br><span class="line"></span><br><span class="line">preds_test = np.array([<span class="number">0.8</span>, <span class="number">0.8</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.3</span>])</span><br><span class="line">print(<span class="string">f'test predictions: <span class="subst">&#123;preds_test&#125;</span>\n'</span>)</span><br><span class="line"></span><br><span class="line">threshold = <span class="number">0.5</span></span><br><span class="line">print(<span class="string">f"threshold: <span class="subst">&#123;threshold&#125;</span>\n"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"computed sensitivity: <span class="subst">&#123;get_sensitivity(y_test, preds_test, threshold):<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"computed specificity: <span class="subst">&#123;get_specificity(y_test, preds_test, threshold):<span class="number">.2</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test casetest labels: [1 0 0 1 1]test predictions: [0.8 0.8 0.4 0.6 0.3]threshold: 0.5computed sensitivity: 0.67computed specificity: 0.50</code></pre><h4 id="Expected-output-1"><a href="#Expected-output-1" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Test case</span><br><span class="line">test labels: [<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">test predictions: [<span class="number">0.8</span> <span class="number">0.8</span> <span class="number">0.4</span> <span class="number">0.6</span> <span class="number">0.3</span>]</span><br><span class="line"></span><br><span class="line">threshold: <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">computed sensitivity: <span class="number">0.67</span></span><br><span class="line">computed specificity: <span class="number">0.50</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, </span><br><span class="line">                        sens=get_sensitivity, spec=get_specificity)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>TP</th>      <th>TN</th>      <th>FP</th>      <th>FN</th>      <th>Accuracy</th>      <th>Prevalence</th>      <th>Sensitivity</th>      <th>Specificity</th>      <th>PPV</th>      <th>NPV</th>      <th>AUC</th>      <th>F1</th>      <th>Threshold</th>    </tr>    <tr>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Cardiomegaly</th>      <td>16</td>      <td>814</td>      <td>169</td>      <td>1</td>      <td>0.83</td>      <td>0.017</td>      <td>0.941</td>      <td>0.828</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Emphysema</th>      <td>20</td>      <td>869</td>      <td>103</td>      <td>8</td>      <td>0.889</td>      <td>0.028</td>      <td>0.714</td>      <td>0.894</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Effusion</th>      <td>99</td>      <td>690</td>      <td>196</td>      <td>15</td>      <td>0.789</td>      <td>0.114</td>      <td>0.868</td>      <td>0.779</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Hernia</th>      <td>1</td>      <td>743</td>      <td>255</td>      <td>1</td>      <td>0.744</td>      <td>0.002</td>      <td>0.5</td>      <td>0.744</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Infiltration</th>      <td>114</td>      <td>543</td>      <td>265</td>      <td>78</td>      <td>0.657</td>      <td>0.192</td>      <td>0.594</td>      <td>0.672</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Mass</th>      <td>40</td>      <td>789</td>      <td>158</td>      <td>13</td>      <td>0.829</td>      <td>0.053</td>      <td>0.755</td>      <td>0.833</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Nodule</th>      <td>28</td>      <td>731</td>      <td>220</td>      <td>21</td>      <td>0.759</td>      <td>0.049</td>      <td>0.571</td>      <td>0.769</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Atelectasis</th>      <td>64</td>      <td>657</td>      <td>249</td>      <td>30</td>      <td>0.721</td>      <td>0.094</td>      <td>0.681</td>      <td>0.725</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumothorax</th>      <td>24</td>      <td>785</td>      <td>183</td>      <td>8</td>      <td>0.809</td>      <td>0.032</td>      <td>0.75</td>      <td>0.811</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pleural_Thickening</th>      <td>24</td>      <td>713</td>      <td>259</td>      <td>4</td>      <td>0.737</td>      <td>0.028</td>      <td>0.857</td>      <td>0.734</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumonia</th>      <td>14</td>      <td>661</td>      <td>320</td>      <td>5</td>      <td>0.675</td>      <td>0.019</td>      <td>0.737</td>      <td>0.674</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Fibrosis</th>      <td>10</td>      <td>725</td>      <td>261</td>      <td>4</td>      <td>0.735</td>      <td>0.014</td>      <td>0.714</td>      <td>0.735</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Edema</th>      <td>15</td>      <td>767</td>      <td>213</td>      <td>5</td>      <td>0.782</td>      <td>0.02</td>      <td>0.75</td>      <td>0.783</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Consolidation</th>      <td>36</td>      <td>658</td>      <td>297</td>      <td>9</td>      <td>0.694</td>      <td>0.045</td>      <td>0.8</td>      <td>0.689</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>  </tbody></table></div><p>Note that specificity and sensitivity do not depend on the prevalence of the positive class in the dataset. </p><ul><li>This is because the statistics are only computed within people of the same class</li><li>Sensitivity only considers output on people in the positive class</li><li>Similarly, specificity only considers output on people in the negative class.</li></ul><p><a name="3-5"></a></p><h3 id="3-5-PPV-and-NPV"><a href="#3-5-PPV-and-NPV" class="headerlink" title="3.5 PPV and NPV"></a>3.5 PPV and NPV</h3><p>Diagnostically, however, sensitivity and specificity are not helpful. Sensitivity, for example, tells us the probability our test outputs positive given that the person already has the condition. Here, we are conditioning on the thing we would like to find out (whether the patient has the condition)!</p><p>What would be more helpful is the probability that the person has the disease given that our test outputs positive. That brings us to positive predictive value (PPV) and negative predictive value (NPV).</p><ul><li>Positive predictive value (PPV) is the probability that subjects with a positive screening test truly have the disease.</li><li>Negative predictive value (NPV) is the probability that subjects with a negative screening test truly don’t have the disease.</li></ul><p>Again, we can formulate these in terms of true positives, true negatives, false positives, and false negatives: </p><script type="math/tex; mode=display">PPV = \frac{\text{true positives}}{\text{true positives} + \text{false positives}}</script><script type="math/tex; mode=display">NPV = \frac{\text{true negatives}}{\text{true negatives} + \text{false negatives}}</script><p>Let’s calculate PPV &amp; NPV for our model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ppv</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute PPV of predictions at threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        PPV (float): positive predictive value of predictions at threshold</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    PPV = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get TP and FP using our previously defined functions</span></span><br><span class="line">    TP = true_positives(y, pred, th)</span><br><span class="line">    FP = false_positives(y, pred, th)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># use TP and FP to compute PPV</span></span><br><span class="line">    PPV = TP / (TP + FP)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> PPV</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_npv</span><span class="params">(y, pred, th=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute NPV of predictions at threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y (np.array): ground truth, size (n_examples)</span></span><br><span class="line"><span class="string">        pred (np.array): model output, size (n_examples)</span></span><br><span class="line"><span class="string">        th (float): cutoff value for positive prediction from model</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        NPV (float): negative predictive value of predictions at threshold</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    NPV = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get TN and FN using our previously defined functions</span></span><br><span class="line">    TN = true_negatives(y, pred, th)</span><br><span class="line">    FN = false_negatives(y, pred, th)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># use TN and FN to compute NPV</span></span><br><span class="line">    NPV = TN / (TN + FN)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> NPV</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test</span></span><br><span class="line">print(<span class="string">"Test case:\n"</span>)</span><br><span class="line"></span><br><span class="line">y_test = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">f'test labels: <span class="subst">&#123;y_test&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">preds_test = np.array([<span class="number">0.8</span>, <span class="number">0.8</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.3</span>])</span><br><span class="line">print(<span class="string">f'test predictions: <span class="subst">&#123;preds_test&#125;</span>\n'</span>)</span><br><span class="line"></span><br><span class="line">threshold = <span class="number">0.5</span></span><br><span class="line">print(<span class="string">f"threshold: <span class="subst">&#123;threshold&#125;</span>\n"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"computed ppv: <span class="subst">&#123;get_ppv(y_test, preds_test, threshold):<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"computed npv: <span class="subst">&#123;get_npv(y_test, preds_test, threshold):<span class="number">.2</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test case:test labels: [1 0 0 1 1]test predictions: [0.8 0.8 0.4 0.6 0.3]threshold: 0.5computed ppv: 0.67computed npv: 0.50</code></pre><h4 id="Expected-output-2"><a href="#Expected-output-2" class="headerlink" title="Expected output:"></a>Expected output:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Test case:</span><br><span class="line"></span><br><span class="line">test labels: [<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line">test predictions: [<span class="number">0.8</span> <span class="number">0.8</span> <span class="number">0.4</span> <span class="number">0.6</span> <span class="number">0.3</span>]</span><br><span class="line"></span><br><span class="line">threshold: <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">computed ppv: <span class="number">0.67</span></span><br><span class="line">computed npv: <span class="number">0.50</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, </span><br><span class="line">                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>TP</th>      <th>TN</th>      <th>FP</th>      <th>FN</th>      <th>Accuracy</th>      <th>Prevalence</th>      <th>Sensitivity</th>      <th>Specificity</th>      <th>PPV</th>      <th>NPV</th>      <th>AUC</th>      <th>F1</th>      <th>Threshold</th>    </tr>    <tr>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Cardiomegaly</th>      <td>16</td>      <td>814</td>      <td>169</td>      <td>1</td>      <td>0.83</td>      <td>0.017</td>      <td>0.941</td>      <td>0.828</td>      <td>0.086</td>      <td>0.999</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Emphysema</th>      <td>20</td>      <td>869</td>      <td>103</td>      <td>8</td>      <td>0.889</td>      <td>0.028</td>      <td>0.714</td>      <td>0.894</td>      <td>0.163</td>      <td>0.991</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Effusion</th>      <td>99</td>      <td>690</td>      <td>196</td>      <td>15</td>      <td>0.789</td>      <td>0.114</td>      <td>0.868</td>      <td>0.779</td>      <td>0.336</td>      <td>0.979</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Hernia</th>      <td>1</td>      <td>743</td>      <td>255</td>      <td>1</td>      <td>0.744</td>      <td>0.002</td>      <td>0.5</td>      <td>0.744</td>      <td>0.004</td>      <td>0.999</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Infiltration</th>      <td>114</td>      <td>543</td>      <td>265</td>      <td>78</td>      <td>0.657</td>      <td>0.192</td>      <td>0.594</td>      <td>0.672</td>      <td>0.301</td>      <td>0.874</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Mass</th>      <td>40</td>      <td>789</td>      <td>158</td>      <td>13</td>      <td>0.829</td>      <td>0.053</td>      <td>0.755</td>      <td>0.833</td>      <td>0.202</td>      <td>0.984</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Nodule</th>      <td>28</td>      <td>731</td>      <td>220</td>      <td>21</td>      <td>0.759</td>      <td>0.049</td>      <td>0.571</td>      <td>0.769</td>      <td>0.113</td>      <td>0.972</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Atelectasis</th>      <td>64</td>      <td>657</td>      <td>249</td>      <td>30</td>      <td>0.721</td>      <td>0.094</td>      <td>0.681</td>      <td>0.725</td>      <td>0.204</td>      <td>0.956</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumothorax</th>      <td>24</td>      <td>785</td>      <td>183</td>      <td>8</td>      <td>0.809</td>      <td>0.032</td>      <td>0.75</td>      <td>0.811</td>      <td>0.116</td>      <td>0.99</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pleural_Thickening</th>      <td>24</td>      <td>713</td>      <td>259</td>      <td>4</td>      <td>0.737</td>      <td>0.028</td>      <td>0.857</td>      <td>0.734</td>      <td>0.085</td>      <td>0.994</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumonia</th>      <td>14</td>      <td>661</td>      <td>320</td>      <td>5</td>      <td>0.675</td>      <td>0.019</td>      <td>0.737</td>      <td>0.674</td>      <td>0.042</td>      <td>0.992</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Fibrosis</th>      <td>10</td>      <td>725</td>      <td>261</td>      <td>4</td>      <td>0.735</td>      <td>0.014</td>      <td>0.714</td>      <td>0.735</td>      <td>0.037</td>      <td>0.995</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Edema</th>      <td>15</td>      <td>767</td>      <td>213</td>      <td>5</td>      <td>0.782</td>      <td>0.02</td>      <td>0.75</td>      <td>0.783</td>      <td>0.066</td>      <td>0.994</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Consolidation</th>      <td>36</td>      <td>658</td>      <td>297</td>      <td>9</td>      <td>0.694</td>      <td>0.045</td>      <td>0.8</td>      <td>0.689</td>      <td>0.108</td>      <td>0.987</td>      <td>Not Defined</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>  </tbody></table></div><p>Notice that despite having very high sensitivity and accuracy, the PPV of the predictions could still be very low. </p><p>This is the case with <code>Edema</code>, for example. </p><ul><li>The sensitivity for <code>Edema</code> is 0.75.</li><li>However, given that the model predicted positive, the probability that a person has Edema (its PPV) is only 0.066!</li></ul><p><a name="3-6"></a></p><h3 id="3-6-ROC-Curve"><a href="#3-6-ROC-Curve" class="headerlink" title="3.6 ROC Curve"></a>3.6 ROC Curve</h3><p>So far we have been operating under the assumption that our model’s prediction of <code>0.5</code> and above should be treated as positive and otherwise it should be treated as negative. This however was a rather arbitrary choice. One way to see this, is to look at a very informative visualization called the receiver operating characteristic (ROC) curve.</p><p>The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The ideal point is at the top left, with a true positive rate of 1 and a false positive rate of 0. The various points on the curve are generated by gradually changing the threshold.</p><p>Let’s look at this curve for our model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.get_curve(y, pred, class_labels)</span><br></pre></td></tr></table></figure><p><img src="output_53_0.png" alt="png"></p><p>The area under the ROC curve is also called AUCROC or C-statistic and is a measure of goodness of fit. In medical literature this number also gives the probability that a randomly selected patient who experienced a condition had a higher risk score than a patient who had not experienced the event. This summarizes the model output across all thresholds, and provides a good sense of the discriminative power of a given model.</p><p>Let’s use the <code>sklearn</code> metric function of <code>roc_auc_score</code> to add this score to our metrics table.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, </span><br><span class="line">                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>TP</th>      <th>TN</th>      <th>FP</th>      <th>FN</th>      <th>Accuracy</th>      <th>Prevalence</th>      <th>Sensitivity</th>      <th>Specificity</th>      <th>PPV</th>      <th>NPV</th>      <th>AUC</th>      <th>F1</th>      <th>Threshold</th>    </tr>    <tr>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Cardiomegaly</th>      <td>16</td>      <td>814</td>      <td>169</td>      <td>1</td>      <td>0.83</td>      <td>0.017</td>      <td>0.941</td>      <td>0.828</td>      <td>0.086</td>      <td>0.999</td>      <td>0.933</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Emphysema</th>      <td>20</td>      <td>869</td>      <td>103</td>      <td>8</td>      <td>0.889</td>      <td>0.028</td>      <td>0.714</td>      <td>0.894</td>      <td>0.163</td>      <td>0.991</td>      <td>0.935</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Effusion</th>      <td>99</td>      <td>690</td>      <td>196</td>      <td>15</td>      <td>0.789</td>      <td>0.114</td>      <td>0.868</td>      <td>0.779</td>      <td>0.336</td>      <td>0.979</td>      <td>0.891</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Hernia</th>      <td>1</td>      <td>743</td>      <td>255</td>      <td>1</td>      <td>0.744</td>      <td>0.002</td>      <td>0.5</td>      <td>0.744</td>      <td>0.004</td>      <td>0.999</td>      <td>0.644</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Infiltration</th>      <td>114</td>      <td>543</td>      <td>265</td>      <td>78</td>      <td>0.657</td>      <td>0.192</td>      <td>0.594</td>      <td>0.672</td>      <td>0.301</td>      <td>0.874</td>      <td>0.696</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Mass</th>      <td>40</td>      <td>789</td>      <td>158</td>      <td>13</td>      <td>0.829</td>      <td>0.053</td>      <td>0.755</td>      <td>0.833</td>      <td>0.202</td>      <td>0.984</td>      <td>0.888</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Nodule</th>      <td>28</td>      <td>731</td>      <td>220</td>      <td>21</td>      <td>0.759</td>      <td>0.049</td>      <td>0.571</td>      <td>0.769</td>      <td>0.113</td>      <td>0.972</td>      <td>0.745</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Atelectasis</th>      <td>64</td>      <td>657</td>      <td>249</td>      <td>30</td>      <td>0.721</td>      <td>0.094</td>      <td>0.681</td>      <td>0.725</td>      <td>0.204</td>      <td>0.956</td>      <td>0.781</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumothorax</th>      <td>24</td>      <td>785</td>      <td>183</td>      <td>8</td>      <td>0.809</td>      <td>0.032</td>      <td>0.75</td>      <td>0.811</td>      <td>0.116</td>      <td>0.99</td>      <td>0.826</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pleural_Thickening</th>      <td>24</td>      <td>713</td>      <td>259</td>      <td>4</td>      <td>0.737</td>      <td>0.028</td>      <td>0.857</td>      <td>0.734</td>      <td>0.085</td>      <td>0.994</td>      <td>0.868</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumonia</th>      <td>14</td>      <td>661</td>      <td>320</td>      <td>5</td>      <td>0.675</td>      <td>0.019</td>      <td>0.737</td>      <td>0.674</td>      <td>0.042</td>      <td>0.992</td>      <td>0.762</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Fibrosis</th>      <td>10</td>      <td>725</td>      <td>261</td>      <td>4</td>      <td>0.735</td>      <td>0.014</td>      <td>0.714</td>      <td>0.735</td>      <td>0.037</td>      <td>0.995</td>      <td>0.801</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Edema</th>      <td>15</td>      <td>767</td>      <td>213</td>      <td>5</td>      <td>0.782</td>      <td>0.02</td>      <td>0.75</td>      <td>0.783</td>      <td>0.066</td>      <td>0.994</td>      <td>0.856</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>    <tr>      <th>Consolidation</th>      <td>36</td>      <td>658</td>      <td>297</td>      <td>9</td>      <td>0.694</td>      <td>0.045</td>      <td>0.8</td>      <td>0.689</td>      <td>0.108</td>      <td>0.987</td>      <td>0.799</td>      <td>Not Defined</td>      <td>0.5</td>    </tr>  </tbody></table></div><p><a name="4"></a></p><h2 id="4-Confidence-Intervals"><a href="#4-Confidence-Intervals" class="headerlink" title="4. Confidence Intervals"></a>4. Confidence Intervals</h2><p>Of course our dataset is only a sample of the real world, and our calculated values for all above metrics is an estimate of the real world values. It would be good to quantify this uncertainty due to the sampling of our dataset. We’ll do this through the use of confidence intervals. A 95\% confidence interval for an estimate $\hat{s}$ of a parameter $s$ is an interval $I = (a, b)$ such that 95\% of the time when the experiment is run, the true value $s$ is contained in $I$. More concretely, if we were to run the experiment many times, then the fraction of those experiments for which $I$ contains the true parameter would tend towards 95\%.</p><p>While some estimates come with methods for computing the confidence interval analytically, more complicated statistics, such as the AUC for example, are difficult. For these we can use a method called the <em>bootstrap</em>. The bootstrap estimates the uncertainty by resampling the dataset with replacement. For each resampling $i$, we will get a new estimate, $\hat{s}_i$. We can then estimate the distribution of $\hat{s}$ by using the distribution of $\hat{s}_i$ for our bootstrap samples.</p><p>In the code below, we create bootstrap samples and compute sample AUCs from those samples. Note that we use stratified random sampling (sampling from the positive and negative classes separately) to make sure that members of each class are represented. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bootstrap_auc</span><span class="params">(y, pred, classes, bootstraps = <span class="number">100</span>, fold_size = <span class="number">1000</span>)</span>:</span></span><br><span class="line">    statistics = np.zeros((len(classes), bootstraps))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">        df = pd.DataFrame(columns=[<span class="string">'y'</span>, <span class="string">'pred'</span>])</span><br><span class="line">        df.loc[:, <span class="string">'y'</span>] = y[:, c]</span><br><span class="line">        df.loc[:, <span class="string">'pred'</span>] = pred[:, c]</span><br><span class="line">        <span class="comment"># get positive examples for stratified sampling</span></span><br><span class="line">        df_pos = df[df.y == <span class="number">1</span>]</span><br><span class="line">        df_neg = df[df.y == <span class="number">0</span>]</span><br><span class="line">        prevalence = len(df_pos) / len(df)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(bootstraps):</span><br><span class="line">            <span class="comment"># stratified sampling of positive and negative examples</span></span><br><span class="line">            pos_sample = df_pos.sample(n = int(fold_size * prevalence), replace=<span class="keyword">True</span>)</span><br><span class="line">            neg_sample = df_neg.sample(n = int(fold_size * (<span class="number">1</span>-prevalence)), replace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">            y_sample = np.concatenate([pos_sample.y.values, neg_sample.y.values])</span><br><span class="line">            pred_sample = np.concatenate([pos_sample.pred.values, neg_sample.pred.values])</span><br><span class="line">            score = roc_auc_score(y_sample, pred_sample)</span><br><span class="line">            statistics[c][i] = score</span><br><span class="line">    <span class="keyword">return</span> statistics</span><br><span class="line"></span><br><span class="line">statistics = bootstrap_auc(y, pred, class_labels)</span><br></pre></td></tr></table></figure><p>Now we can compute confidence intervals from the sample statistics that we computed.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.print_confidence_intervals(class_labels, statistics)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Mean AUC (CI 5%-95%)</th>    </tr>  </thead>  <tbody>    <tr>      <th>Cardiomegaly</th>      <td>0.93 (0.90-0.97)</td>    </tr>    <tr>      <th>Emphysema</th>      <td>0.93 (0.90-0.96)</td>    </tr>    <tr>      <th>Effusion</th>      <td>0.89 (0.87-0.91)</td>    </tr>    <tr>      <th>Hernia</th>      <td>0.67 (0.30-0.98)</td>    </tr>    <tr>      <th>Infiltration</th>      <td>0.69 (0.65-0.73)</td>    </tr>    <tr>      <th>Mass</th>      <td>0.89 (0.86-0.92)</td>    </tr>    <tr>      <th>Nodule</th>      <td>0.74 (0.68-0.80)</td>    </tr>    <tr>      <th>Atelectasis</th>      <td>0.78 (0.75-0.81)</td>    </tr>    <tr>      <th>Pneumothorax</th>      <td>0.83 (0.75-0.91)</td>    </tr>    <tr>      <th>Pleural_Thickening</th>      <td>0.87 (0.82-0.93)</td>    </tr>    <tr>      <th>Pneumonia</th>      <td>0.76 (0.65-0.84)</td>    </tr>    <tr>      <th>Fibrosis</th>      <td>0.81 (0.75-0.86)</td>    </tr>    <tr>      <th>Edema</th>      <td>0.85 (0.81-0.90)</td>    </tr>    <tr>      <th>Consolidation</th>      <td>0.80 (0.75-0.84)</td>    </tr>  </tbody></table></div><p>As you can see, our confidence intervals are much wider for some classes than for others. Hernia, for example, has an interval around (0.30 - 0.98), indicating that we can’t be certain it is better than chance (at 0.5). </p><p><a name="5"></a></p><h2 id="5-Precision-Recall-Curve"><a href="#5-Precision-Recall-Curve" class="headerlink" title="5. Precision-Recall Curve"></a>5. Precision-Recall Curve</h2><p>Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. </p><p>In information retrieval</p><ul><li>Precision is a measure of result relevancy and that is equivalent to our previously defined PPV. </li><li>Recall is a measure of how many truly relevant results are returned and that is equivalent to our previously defined sensitivity measure.</li></ul><p>The precision-recall curve (PRC) shows the trade-off between precision and recall for different thresholds. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. </p><p>High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).</p><p>Run the following cell to generate a PRC:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.get_curve(y, pred, class_labels, curve=<span class="string">'prc'</span>)</span><br></pre></td></tr></table></figure><p><img src="output_64_0.png" alt="png"></p><p><a name="6"></a></p><h2 id="6-F1-Score"><a href="#6-F1-Score" class="headerlink" title="6. F1 Score"></a>6. F1 Score</h2><p>F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. </p><p>Again, we can simply use <code>sklearn</code>‘s utility metric function of <code>f1_score</code> to add this measure to our performance table.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line">util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, </span><br><span class="line">                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score,f1=f1_score)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>TP</th>      <th>TN</th>      <th>FP</th>      <th>FN</th>      <th>Accuracy</th>      <th>Prevalence</th>      <th>Sensitivity</th>      <th>Specificity</th>      <th>PPV</th>      <th>NPV</th>      <th>AUC</th>      <th>F1</th>      <th>Threshold</th>    </tr>    <tr>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Cardiomegaly</th>      <td>16</td>      <td>814</td>      <td>169</td>      <td>1</td>      <td>0.83</td>      <td>0.017</td>      <td>0.941</td>      <td>0.828</td>      <td>0.086</td>      <td>0.999</td>      <td>0.933</td>      <td>0.158</td>      <td>0.5</td>    </tr>    <tr>      <th>Emphysema</th>      <td>20</td>      <td>869</td>      <td>103</td>      <td>8</td>      <td>0.889</td>      <td>0.028</td>      <td>0.714</td>      <td>0.894</td>      <td>0.163</td>      <td>0.991</td>      <td>0.935</td>      <td>0.265</td>      <td>0.5</td>    </tr>    <tr>      <th>Effusion</th>      <td>99</td>      <td>690</td>      <td>196</td>      <td>15</td>      <td>0.789</td>      <td>0.114</td>      <td>0.868</td>      <td>0.779</td>      <td>0.336</td>      <td>0.979</td>      <td>0.891</td>      <td>0.484</td>      <td>0.5</td>    </tr>    <tr>      <th>Hernia</th>      <td>1</td>      <td>743</td>      <td>255</td>      <td>1</td>      <td>0.744</td>      <td>0.002</td>      <td>0.5</td>      <td>0.744</td>      <td>0.004</td>      <td>0.999</td>      <td>0.644</td>      <td>0.008</td>      <td>0.5</td>    </tr>    <tr>      <th>Infiltration</th>      <td>114</td>      <td>543</td>      <td>265</td>      <td>78</td>      <td>0.657</td>      <td>0.192</td>      <td>0.594</td>      <td>0.672</td>      <td>0.301</td>      <td>0.874</td>      <td>0.696</td>      <td>0.399</td>      <td>0.5</td>    </tr>    <tr>      <th>Mass</th>      <td>40</td>      <td>789</td>      <td>158</td>      <td>13</td>      <td>0.829</td>      <td>0.053</td>      <td>0.755</td>      <td>0.833</td>      <td>0.202</td>      <td>0.984</td>      <td>0.888</td>      <td>0.319</td>      <td>0.5</td>    </tr>    <tr>      <th>Nodule</th>      <td>28</td>      <td>731</td>      <td>220</td>      <td>21</td>      <td>0.759</td>      <td>0.049</td>      <td>0.571</td>      <td>0.769</td>      <td>0.113</td>      <td>0.972</td>      <td>0.745</td>      <td>0.189</td>      <td>0.5</td>    </tr>    <tr>      <th>Atelectasis</th>      <td>64</td>      <td>657</td>      <td>249</td>      <td>30</td>      <td>0.721</td>      <td>0.094</td>      <td>0.681</td>      <td>0.725</td>      <td>0.204</td>      <td>0.956</td>      <td>0.781</td>      <td>0.314</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumothorax</th>      <td>24</td>      <td>785</td>      <td>183</td>      <td>8</td>      <td>0.809</td>      <td>0.032</td>      <td>0.75</td>      <td>0.811</td>      <td>0.116</td>      <td>0.99</td>      <td>0.826</td>      <td>0.201</td>      <td>0.5</td>    </tr>    <tr>      <th>Pleural_Thickening</th>      <td>24</td>      <td>713</td>      <td>259</td>      <td>4</td>      <td>0.737</td>      <td>0.028</td>      <td>0.857</td>      <td>0.734</td>      <td>0.085</td>      <td>0.994</td>      <td>0.868</td>      <td>0.154</td>      <td>0.5</td>    </tr>    <tr>      <th>Pneumonia</th>      <td>14</td>      <td>661</td>      <td>320</td>      <td>5</td>      <td>0.675</td>      <td>0.019</td>      <td>0.737</td>      <td>0.674</td>      <td>0.042</td>      <td>0.992</td>      <td>0.762</td>      <td>0.079</td>      <td>0.5</td>    </tr>    <tr>      <th>Fibrosis</th>      <td>10</td>      <td>725</td>      <td>261</td>      <td>4</td>      <td>0.735</td>      <td>0.014</td>      <td>0.714</td>      <td>0.735</td>      <td>0.037</td>      <td>0.995</td>      <td>0.801</td>      <td>0.07</td>      <td>0.5</td>    </tr>    <tr>      <th>Edema</th>      <td>15</td>      <td>767</td>      <td>213</td>      <td>5</td>      <td>0.782</td>      <td>0.02</td>      <td>0.75</td>      <td>0.783</td>      <td>0.066</td>      <td>0.994</td>      <td>0.856</td>      <td>0.121</td>      <td>0.5</td>    </tr>    <tr>      <th>Consolidation</th>      <td>36</td>      <td>658</td>      <td>297</td>      <td>9</td>      <td>0.694</td>      <td>0.045</td>      <td>0.8</td>      <td>0.689</td>      <td>0.108</td>      <td>0.987</td>      <td>0.799</td>      <td>0.19</td>      <td>0.5</td>    </tr>  </tbody></table></div><p><a name="7"></a></p><h2 id="7-Calibration"><a href="#7-Calibration" class="headerlink" title="7. Calibration"></a>7. Calibration</h2><p>When performing classification we often want not only to predict the class label, but also obtain a probability of each label. This probability would ideally give us some kind of confidence on the prediction. In order to observe how our model’s generated probabilities are aligned with the real probabilities, we can plot what’s called a <em>calibration curve</em>. </p><p>In order to generate a calibration plot, we first bucketize our predictions to a fixed number of separate bins (e.g. 5) between 0 and 1. We then calculate a point for each bin: the x-value for each point is the mean for the probability that our model has assigned to these points and the y-value for each point fraction of true positives in that bin. We then plot these points in a linear plot. A well-calibrated model has a calibration curve that almost aligns with the y=x line.</p><p>The <code>sklearn</code> library has a utility <code>calibration_curve</code> for generating a calibration plot. Let’s use it and take a look at our model’s calibration:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.calibration <span class="keyword">import</span> calibration_curve</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_calibration_curve</span><span class="params">(y, pred)</span>:</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">20</span>, <span class="number">20</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(class_labels)):</span><br><span class="line">        plt.subplot(<span class="number">4</span>, <span class="number">4</span>, i + <span class="number">1</span>)</span><br><span class="line">        fraction_of_positives, mean_predicted_value = calibration_curve(y[:,i], pred[:,i], n_bins=<span class="number">20</span>)</span><br><span class="line">        plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], linestyle=<span class="string">'--'</span>)</span><br><span class="line">        plt.plot(mean_predicted_value, fraction_of_positives, marker=<span class="string">'.'</span>)</span><br><span class="line">        plt.xlabel(<span class="string">"Predicted Value"</span>)</span><br><span class="line">        plt.ylabel(<span class="string">"Fraction of Positives"</span>)</span><br><span class="line">        plt.title(class_labels[i])</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_calibration_curve(y, pred)</span><br></pre></td></tr></table></figure><p><img src="output_71_0.png" alt="png"></p><p>As the above plots show, for most predictions our model’s calibration plot does not resemble a well calibrated plot. How can we fix that?…</p><p>Thankfully, there is a very useful method called <a href="https://en.wikipedia.org/wiki/Platt_scaling" target="_blank" rel="noopener">Platt scaling</a> which works by fitting a logistic regression model to our model’s scores. To build this model, we will be using the training portion of our dataset to generate the linear model and then will use the model to calibrate the predictions for our test portion.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="keyword">as</span> LR </span><br><span class="line"></span><br><span class="line">y_train = train_results[class_labels].values</span><br><span class="line">pred_train = train_results[pred_labels].values</span><br><span class="line">pred_calibrated = np.zeros_like(pred)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(class_labels)):</span><br><span class="line">    lr = LR(solver=<span class="string">'liblinear'</span>, max_iter=<span class="number">10000</span>)</span><br><span class="line">    lr.fit(pred_train[:, i].reshape(<span class="number">-1</span>, <span class="number">1</span>), y_train[:, i])    </span><br><span class="line">    pred_calibrated[:, i] = lr.predict_proba(pred[:, i].reshape(<span class="number">-1</span>, <span class="number">1</span>))[:,<span class="number">1</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_calibration_curve(y[:,], pred_calibrated)</span><br></pre></td></tr></table></figure><p><img src="output_74_0.png" alt="png"></p><h1 id="That’s-it"><a href="#That’s-it" class="headerlink" title="That’s it!"></a>That’s it!</h1><p>Congratulations! That was a lot of metrics to get familiarized with.<br>We hope that you feel a lot more confident in your understanding of medical diagnostic evaluation and test your models correctly in your future work :)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Evaluation-of-Diagnostic-Models&quot;&gt;&lt;a href=&quot;#Evaluation-of-Diagnostic-Models&quot; class=&quot;headerlink&quot; title=&quot;Evaluation of Diagnostic Model
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Medicine" scheme="https://zhangruochi.com/tags/Medicine/"/>
    
  </entry>
  
  <entry>
    <title>Chest X-Ray Medical Diagnosis with Deep Learning</title>
    <link href="https://zhangruochi.com/Chest-X-Ray-Medical-Diagnosis-with-Deep-Learning/2020/04/15/"/>
    <id>https://zhangruochi.com/Chest-X-Ray-Medical-Diagnosis-with-Deep-Learning/2020/04/15/</id>
    <published>2020-04-15T13:11:55.000Z</published>
    <updated>2020-04-16T01:49:43.444Z</updated>
    
    <content type="html"><![CDATA[<ul><li>This is the assignment of coursera course <a href="https://www.coursera.org/learn/ai-for-medical-diagnosis/" target="_blank" rel="noopener">Medical Diagnosis</a> from deeplearning.ai </li></ul><h1 id="Chest-X-Ray-Medical-Diagnosis-with-Deep-Learning"><a href="#Chest-X-Ray-Medical-Diagnosis-with-Deep-Learning" class="headerlink" title="Chest X-Ray Medical Diagnosis with Deep Learning"></a>Chest X-Ray Medical Diagnosis with Deep Learning</h1><p><img src="output_1.png" style="padding-top: 50px;width: 87%;left: 0px;margin-left: 0px;margin-right: 0px;"></p><p>Welcome to the first assignment of course 1</p><p>In this assignment! You will explore medical image diagnosis by building a state-of-the-art chest X-ray classifier using Keras. </p><p>The assignment will walk through some of the steps of building and evaluating this deep learning classifier model. In particular, you will:</p><ul><li>Pre-process and prepare a real-world X-ray dataset</li><li>Use transfer learning to retrain a DenseNet model for X-ray image classification</li><li>Learn a technique to handle class imbalance</li><li>Measure diagnostic performance by computing the AUC (Area Under the Curve) for the ROC (Receiver Operating Characteristic) curve</li><li>Visualize model activity using GradCAMs</li></ul><p>In completing this assignment you will learn about the following topics: </p><ul><li>Data preparation<ul><li>Visualizing data</li><li>Preventing data leakage</li></ul></li><li>Model Development<ul><li>Addressing class imbalance</li><li>Leveraging pre-trained models using transfer learning</li></ul></li><li>Evaluation<ul><li>AUC and ROC curves</li></ul></li></ul><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><p>Use these links to jump to specific sections of this assignment!</p><ul><li><a href="#1">1. Import Packages and Function</a></li><li><a href="#2">2. Load the Datasets</a><ul><li><a href="#2-1">2.1 Preventing Data Leakage</a><ul><li><a href="#Ex-1">Exercise 1 - Checking Data Leakage</a></li></ul></li><li><a href="#2-2">2.2 Preparing Images</a></li></ul></li><li><a href="#3">3. Model Development</a><ul><li><a href="#3-1">3.1 Addressing Class Imbalance</a><ul><li><a href="#Ex-2">Exercise 2 - Computing Class Frequencies</a></li><li><a href="#Ex-3">Exercise 3 - Weighted Loss</a></li></ul></li><li><a href="#3-3">3.3 DenseNet121</a></li></ul></li><li><a href="#4">4. Training [optional]</a><ul><li><a href="#4-1">4.1 Training on the Larger Dataset</a></li></ul></li><li><a href="#5">5. Prediction and Evaluation</a><ul><li><a href="#5-1">5.1 ROC Curve and AUROC</a></li><li><a href="#5-2">5.2 Visualizing Learning with GradCAM</a></li></ul></li></ul><p><a name="1"></a></p><h2 id="1-Import-Packages-and-Functions¶"><a href="#1-Import-Packages-and-Functions¶" class="headerlink" title="1. Import Packages and Functions¶"></a>1. Import Packages and Functions¶</h2><p>We’ll make use of the following packages:</p><ul><li><code>numpy</code> and <code>pandas</code> is what we’ll use to manipulate our data</li><li><code>matplotlib.pyplot</code> and <code>seaborn</code> will be used to produce plots for visualization</li><li><code>util</code> will provide the locally defined utility functions that have been provided for this assignment</li></ul><p>We will also use several modules from the <code>keras</code> framework for building deep learning models.</p><p>Run the next cell to import all the necessary packages.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"><span class="keyword">from</span> keras.applications.densenet <span class="keyword">import</span> DenseNet121</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, GlobalAveragePooling2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> util</span><br></pre></td></tr></table></figure><p><a name="2"></a></p><h2 id="2-Load-the-Datasets"><a href="#2-Load-the-Datasets" class="headerlink" title="2 Load the Datasets"></a>2 Load the Datasets</h2><p>For this assignment, we will be using the <a href="https://arxiv.org/abs/1705.02315" target="_blank" rel="noopener">ChestX-ray8 dataset</a> which contains 108,948 frontal-view X-ray images of 32,717 unique patients. </p><ul><li>Each image in the data set contains multiple text-mined labels identifying 14 different pathological conditions. </li><li>These in turn can be used by physicians to diagnose 8 different diseases. </li><li>We will use this data to develop a single model that will provide binary classification predictions for each of the 14 labeled pathologies. </li><li>In other words it will predict ‘positive’ or ‘negative’ for each of the pathologies.</li></ul><p>You can download the entire dataset for free <a href="https://nihcc.app.box.com/v/ChestXray-NIHCC" target="_blank" rel="noopener">here</a>. </p><ul><li>We have provided a ~1000 image subset of the images for you.</li><li>These can be accessed in the folder path stored in the <code>IMAGE_DIR</code> variable.</li></ul><p>The dataset includes a CSV file that provides the labels for each X-ray. </p><p>To make your job a bit easier, we have processed the labels for our small sample and generated three new files to get you started. These three files are:</p><ol><li><code>nih/train-small.csv</code>: 875 images from our dataset to be used for training.</li><li><code>nih/valid-small.csv</code>: 109 images from our dataset to be used for validation.</li><li><code>nih/test.csv</code>: 420 images from our dataset to be used for testing. </li></ol><p>This dataset has been annotated by consensus among four different radiologists for 5 of our 14 pathologies:</p><ul><li><code>Consolidation</code></li><li><code>Edema</code></li><li><code>Effusion</code></li><li><code>Cardiomegaly</code></li><li><code>Atelectasis</code></li></ul><h4 id="Sidebar-on-meaning-of-‘class’"><a href="#Sidebar-on-meaning-of-‘class’" class="headerlink" title="Sidebar on meaning of ‘class’"></a>Sidebar on meaning of ‘class’</h4><p>It is worth noting that the word <strong>‘class’</strong> is used in multiple ways is these discussions. </p><ul><li>We sometimes refer to each of the 14 pathological conditions that are labeled in our dataset as a class. </li><li>But for each of those pathologies we are attempting to predict whether a certain condition is present (i.e. positive result) or absent (i.e. negative result). <ul><li>These two possible labels of ‘positive’ or ‘negative’ (or the numerical equivalent of 1 or 0) are also typically referred to as classes. </li></ul></li><li>Moreover, we also use the term in reference to software code ‘classes’ such as <code>ImageDataGenerator</code>.</li></ul><p>As long as you are aware of all this though, it should not cause you any confusion as the term ‘class’ is usually clear from the context in which it is used.</p><h4 id="Read-in-the-data"><a href="#Read-in-the-data" class="headerlink" title="Read in the data"></a>Read in the data</h4><p>Let’s open these files using the <a href="https://pandas.pydata.org/" target="_blank" rel="noopener">pandas</a> library</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_df = pd.read_csv(<span class="string">"nih/train-small.csv"</span>)</span><br><span class="line">valid_df = pd.read_csv(<span class="string">"nih/valid-small.csv"</span>)</span><br><span class="line"></span><br><span class="line">test_df = pd.read_csv(<span class="string">"nih/test.csv"</span>)</span><br><span class="line"></span><br><span class="line">train_df.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Image</th>      <th>Atelectasis</th>      <th>Cardiomegaly</th>      <th>Consolidation</th>      <th>Edema</th>      <th>Effusion</th>      <th>Emphysema</th>      <th>Fibrosis</th>      <th>Hernia</th>      <th>Infiltration</th>      <th>Mass</th>      <th>Nodule</th>      <th>PatientId</th>      <th>Pleural_Thickening</th>      <th>Pneumonia</th>      <th>Pneumothorax</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>00027079_001.png</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>27079</td>      <td>1</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>00004477_001.png</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>4477</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>00018530_002.png</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>18530</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>00026928_001.png</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>26928</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>00016687_000.png</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>1</td>      <td>16687</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_df.shape</span><br></pre></td></tr></table></figure><pre><code>(875, 16)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">labels = [<span class="string">'Cardiomegaly'</span>, </span><br><span class="line">          <span class="string">'Emphysema'</span>, </span><br><span class="line">          <span class="string">'Effusion'</span>, </span><br><span class="line">          <span class="string">'Hernia'</span>, </span><br><span class="line">          <span class="string">'Infiltration'</span>, </span><br><span class="line">          <span class="string">'Mass'</span>, </span><br><span class="line">          <span class="string">'Nodule'</span>, </span><br><span class="line">          <span class="string">'Atelectasis'</span>,</span><br><span class="line">          <span class="string">'Pneumothorax'</span>,</span><br><span class="line">          <span class="string">'Pleural_Thickening'</span>, </span><br><span class="line">          <span class="string">'Pneumonia'</span>, </span><br><span class="line">          <span class="string">'Fibrosis'</span>, </span><br><span class="line">          <span class="string">'Edema'</span>, </span><br><span class="line">          <span class="string">'Consolidation'</span>]</span><br></pre></td></tr></table></figure><p><a name="2-1"></a></p><h3 id="2-1-Preventing-Data-Leakage"><a href="#2-1-Preventing-Data-Leakage" class="headerlink" title="2.1 Preventing Data Leakage"></a>2.1 Preventing Data Leakage</h3><p>It is worth noting that our dataset contains multiple images for each patient. This could be the case, for example, when a patient has taken multiple X-ray images at different times during their hospital visits. In our data splitting, we have ensured that the split is done on the patient level so that there is no data “leakage” between the train, validation, and test datasets.</p><p><a name="Ex-1"></a></p><h3 id="Exercise-1-Checking-Data-Leakage"><a href="#Exercise-1-Checking-Data-Leakage" class="headerlink" title="Exercise 1 - Checking Data Leakage"></a>Exercise 1 - Checking Data Leakage</h3><p>In the cell below, write a function to check whether there is leakage between two datasets. We’ll use this to make sure there are no patients in the test set that are also present in either the train or validation sets.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li> Make use of python's set.intersection() function. </li>    <li> In order to match the automatic grader's expectations, please start the line of code with <code>df1_patients_unique...[continue your code here]</code> </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_for_leakage</span><span class="params">(df1, df2, patient_col)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return True if there any patients are in both df1 and df2.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        df1 (dataframe): dataframe describing first dataset</span></span><br><span class="line"><span class="string">        df2 (dataframe): dataframe describing second dataset</span></span><br><span class="line"><span class="string">        patient_col (str): string name of column with patient IDs</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        leakage (bool): True if there is leakage, otherwise False</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    df1_patients_unique = set(df1[patient_col].unique().tolist())</span><br><span class="line">    df2_patients_unique = set(df2[patient_col].unique().tolist())</span><br><span class="line">    </span><br><span class="line">    patients_in_both_groups = df1_patients_unique.intersection(df2_patients_unique)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># leakage contains true if there is patient overlap, otherwise false.</span></span><br><span class="line">    leakage = len(patients_in_both_groups) &gt;= <span class="number">1</span> <span class="comment"># boolean (true if there is at least 1 patient in both groups)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> leakage</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test</span></span><br><span class="line">print(<span class="string">"test case 1"</span>)</span><br><span class="line">df1 = pd.DataFrame(&#123;<span class="string">'patient_id'</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]&#125;)</span><br><span class="line">df2 = pd.DataFrame(&#123;<span class="string">'patient_id'</span>: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]&#125;)</span><br><span class="line">print(<span class="string">"df1"</span>)</span><br><span class="line">print(df1)</span><br><span class="line">print(<span class="string">"df2"</span>)</span><br><span class="line">print(df2)</span><br><span class="line">print(<span class="string">f"leakage output: <span class="subst">&#123;check_for_leakage(df1, df2, <span class="string">'patient_id'</span>)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">"-------------------------------------"</span>)</span><br><span class="line">print(<span class="string">"test case 2"</span>)</span><br><span class="line">df1 = pd.DataFrame(&#123;<span class="string">'patient_id'</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]&#125;)</span><br><span class="line">df2 = pd.DataFrame(&#123;<span class="string">'patient_id'</span>: [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]&#125;)</span><br><span class="line">print(<span class="string">"df1:"</span>)</span><br><span class="line">print(df1)</span><br><span class="line">print(<span class="string">"df2:"</span>)</span><br><span class="line">print(df2)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"leakage output: <span class="subst">&#123;check_for_leakage(df1, df2, <span class="string">'patient_id'</span>)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>test case 1df1   patient_id0           01           12           2df2   patient_id0           21           32           4leakage output: True-------------------------------------test case 2df1:   patient_id0           01           12           2df2:   patient_id0           31           42           5leakage output: False</code></pre><h5 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">test case <span class="number">1</span></span><br><span class="line">df1</span><br><span class="line">   patient_id</span><br><span class="line"><span class="number">0</span>           <span class="number">0</span></span><br><span class="line"><span class="number">1</span>           <span class="number">1</span></span><br><span class="line"><span class="number">2</span>           <span class="number">2</span></span><br><span class="line">df2</span><br><span class="line">   patient_id</span><br><span class="line"><span class="number">0</span>           <span class="number">2</span></span><br><span class="line"><span class="number">1</span>           <span class="number">3</span></span><br><span class="line"><span class="number">2</span>           <span class="number">4</span></span><br><span class="line">leakage output: <span class="keyword">True</span></span><br><span class="line">-------------------------------------</span><br><span class="line">test case <span class="number">2</span></span><br><span class="line">df1:</span><br><span class="line">   patient_id</span><br><span class="line"><span class="number">0</span>           <span class="number">0</span></span><br><span class="line"><span class="number">1</span>           <span class="number">1</span></span><br><span class="line"><span class="number">2</span>           <span class="number">2</span></span><br><span class="line">df2:</span><br><span class="line">   patient_id</span><br><span class="line"><span class="number">0</span>           <span class="number">3</span></span><br><span class="line"><span class="number">1</span>           <span class="number">4</span></span><br><span class="line"><span class="number">2</span>           <span class="number">5</span></span><br><span class="line">leakage output: <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p>Run the next cell to check if there are patients in both train and test or in both valid and test.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"leakage between train and test: &#123;&#125;"</span>.format(check_for_leakage(train_df, test_df, <span class="string">'PatientId'</span>)))</span><br><span class="line">print(<span class="string">"leakage between valid and test: &#123;&#125;"</span>.format(check_for_leakage(valid_df, test_df, <span class="string">'PatientId'</span>)))</span><br></pre></td></tr></table></figure><pre><code>leakage between train and test: Falseleakage between valid and test: False</code></pre><p>If we get <code>False</code> for both, then we’re ready to start preparing the datasets for training. Remember to always check for data leakage!</p><p><a name="2-2"></a></p><h3 id="2-2-Preparing-Images"><a href="#2-2-Preparing-Images" class="headerlink" title="2.2 Preparing Images"></a>2.2 Preparing Images</h3><p>With our dataset splits ready, we can now proceed with setting up our model to consume them. </p><ul><li>For this we will use the off-the-shelf <a href="https://keras.io/preprocessing/image/" target="_blank" rel="noopener">ImageDataGenerator</a> class from the Keras framework, which allows us to build a “generator” for images specified in a dataframe. </li><li>This class also provides support for basic data augmentation such as random horizontal flipping of images.</li><li>We also use the generator to transform the values in each batch so that their mean is $0$ and their standard deviation is 1. <ul><li>This will facilitate model training by standardizing the input distribution. </li></ul></li><li>The generator also converts our single channel X-ray images (gray-scale) to a three-channel format by repeating the values in the image across all channels.<ul><li>We will want this because the pre-trained model that we’ll use requires three-channel inputs.</li></ul></li></ul><p>Since it is mainly a matter of reading and understanding Keras documentation, we have implemented the generator for you. There are a few things to note: </p><ol><li>We normalize the mean and standard deviation of the data</li><li>We shuffle the input after each epoch.</li><li>We set the image size to be 320px by 320px</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_train_generator</span><span class="params">(df, image_dir, x_col, y_cols, shuffle=True, batch_size=<span class="number">8</span>, seed=<span class="number">1</span>, target_w = <span class="number">320</span>, target_h = <span class="number">320</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return generator for training set, normalizing using batch</span></span><br><span class="line"><span class="string">    statistics.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      train_df (dataframe): dataframe specifying training data.</span></span><br><span class="line"><span class="string">      image_dir (str): directory where image files are held.</span></span><br><span class="line"><span class="string">      x_col (str): name of column in df that holds filenames.</span></span><br><span class="line"><span class="string">      y_cols (list): list of strings that hold y labels for images.</span></span><br><span class="line"><span class="string">      sample_size (int): size of sample to use for normalization statistics.</span></span><br><span class="line"><span class="string">      batch_size (int): images per batch to be fed into model during training.</span></span><br><span class="line"><span class="string">      seed (int): random seed.</span></span><br><span class="line"><span class="string">      target_w (int): final width of input images.</span></span><br><span class="line"><span class="string">      target_h (int): final height of input images.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        train_generator (DataFrameIterator): iterator over training set</span></span><br><span class="line"><span class="string">    """</span>        </span><br><span class="line">    print(<span class="string">"getting train generator..."</span>) </span><br><span class="line">    <span class="comment"># normalize images</span></span><br><span class="line">    image_generator = ImageDataGenerator(</span><br><span class="line">        samplewise_center=<span class="keyword">True</span>,</span><br><span class="line">        samplewise_std_normalization= <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># flow from directory with specified batch size</span></span><br><span class="line">    <span class="comment"># and target image size</span></span><br><span class="line">    generator = image_generator.flow_from_dataframe(</span><br><span class="line">            dataframe=df,</span><br><span class="line">            directory=image_dir,</span><br><span class="line">            x_col=x_col,</span><br><span class="line">            y_col=y_cols,</span><br><span class="line">            class_mode=<span class="string">"raw"</span>,</span><br><span class="line">            batch_size=batch_size,</span><br><span class="line">            shuffle=shuffle,</span><br><span class="line">            seed=seed,</span><br><span class="line">            target_size=(target_w,target_h))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> generator</span><br></pre></td></tr></table></figure><h4 id="Build-a-separate-generator-for-valid-and-test-sets"><a href="#Build-a-separate-generator-for-valid-and-test-sets" class="headerlink" title="Build a separate generator for valid and test sets"></a>Build a separate generator for valid and test sets</h4><p>Now we need to build a new generator for validation and testing data. </p><p><strong>Why can’t we use the same generator as for the training data?</strong></p><p>Look back at the generator we wrote for the training data. </p><ul><li>It normalizes each image <strong>per batch</strong>, meaning that it uses batch statistics. </li><li>We should not do this with the test and validation data, since in a real life scenario we don’t process incoming images a batch at a time (we process one image at a time). </li><li>Knowing the average per batch of test data would effectively give our model an advantage.  <ul><li>The model should not have any information about the test data.</li></ul></li></ul><p>What we need to do is normalize incoming test data using the statistics <strong>computed from the training set</strong>. </p><ul><li>We implement this in the function below. </li><li>There is one technical note. Ideally, we would want to compute our sample mean and standard deviation using the entire training set. </li><li>However, since this is extremely large, that would be very time consuming. </li><li>In the interest of time, we’ll take a random sample of the dataset and calcualte the sample mean and sample standard deviation.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_test_and_valid_generator</span><span class="params">(valid_df, test_df, train_df, image_dir, x_col, y_cols, sample_size=<span class="number">100</span>, batch_size=<span class="number">8</span>, seed=<span class="number">1</span>, target_w = <span class="number">320</span>, target_h = <span class="number">320</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return generator for validation set and test test set using </span></span><br><span class="line"><span class="string">    normalization statistics from training set.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      valid_df (dataframe): dataframe specifying validation data.</span></span><br><span class="line"><span class="string">      test_df (dataframe): dataframe specifying test data.</span></span><br><span class="line"><span class="string">      train_df (dataframe): dataframe specifying training data.</span></span><br><span class="line"><span class="string">      image_dir (str): directory where image files are held.</span></span><br><span class="line"><span class="string">      x_col (str): name of column in df that holds filenames.</span></span><br><span class="line"><span class="string">      y_cols (list): list of strings that hold y labels for images.</span></span><br><span class="line"><span class="string">      sample_size (int): size of sample to use for normalization statistics.</span></span><br><span class="line"><span class="string">      batch_size (int): images per batch to be fed into model during training.</span></span><br><span class="line"><span class="string">      seed (int): random seed.</span></span><br><span class="line"><span class="string">      target_w (int): final width of input images.</span></span><br><span class="line"><span class="string">      target_h (int): final height of input images.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        test_generator (DataFrameIterator) and valid_generator: iterators over test set and validation set respectively</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    print(<span class="string">"getting train and valid generators..."</span>)</span><br><span class="line">    <span class="comment"># get generator to sample dataset</span></span><br><span class="line">    raw_train_generator = ImageDataGenerator().flow_from_dataframe(</span><br><span class="line">        dataframe=train_df, </span><br><span class="line">        directory=IMAGE_DIR, </span><br><span class="line">        x_col=<span class="string">"Image"</span>, </span><br><span class="line">        y_col=labels, </span><br><span class="line">        class_mode=<span class="string">"raw"</span>, </span><br><span class="line">        batch_size=sample_size, </span><br><span class="line">        shuffle=<span class="keyword">True</span>, </span><br><span class="line">        target_size=(target_w, target_h))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get data sample</span></span><br><span class="line">    batch = raw_train_generator.next()</span><br><span class="line">    data_sample = batch[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># use sample to fit mean and std for test set generator</span></span><br><span class="line">    image_generator = ImageDataGenerator(</span><br><span class="line">        featurewise_center=<span class="keyword">True</span>,</span><br><span class="line">        featurewise_std_normalization= <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># fit generator to sample from training data</span></span><br><span class="line">    image_generator.fit(data_sample)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get test generator</span></span><br><span class="line">    valid_generator = image_generator.flow_from_dataframe(</span><br><span class="line">            dataframe=valid_df,</span><br><span class="line">            directory=image_dir,</span><br><span class="line">            x_col=x_col,</span><br><span class="line">            y_col=y_cols,</span><br><span class="line">            class_mode=<span class="string">"raw"</span>,</span><br><span class="line">            batch_size=batch_size,</span><br><span class="line">            shuffle=<span class="keyword">False</span>,</span><br><span class="line">            seed=seed,</span><br><span class="line">            target_size=(target_w,target_h))</span><br><span class="line"></span><br><span class="line">    test_generator = image_generator.flow_from_dataframe(</span><br><span class="line">            dataframe=test_df,</span><br><span class="line">            directory=image_dir,</span><br><span class="line">            x_col=x_col,</span><br><span class="line">            y_col=y_cols,</span><br><span class="line">            class_mode=<span class="string">"raw"</span>,</span><br><span class="line">            batch_size=batch_size,</span><br><span class="line">            shuffle=<span class="keyword">False</span>,</span><br><span class="line">            seed=seed,</span><br><span class="line">            target_size=(target_w,target_h))</span><br><span class="line">    <span class="keyword">return</span> valid_generator, test_generator</span><br></pre></td></tr></table></figure><p>With our generator function ready, let’s make one generator for our training data and one each of our test and  validation datasets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">IMAGE_DIR = <span class="string">"nih/images-small/"</span></span><br><span class="line">train_generator = get_train_generator(train_df, IMAGE_DIR, <span class="string">"Image"</span>, labels)</span><br><span class="line">valid_generator, test_generator= get_test_and_valid_generator(valid_df, test_df, train_df, IMAGE_DIR, <span class="string">"Image"</span>, labels)</span><br></pre></td></tr></table></figure><pre><code>getting train generator.../opt/conda/lib/python3.6/site-packages/keras_preprocessing/image/dataframe_iterator.py:273: UserWarning: Found 866 invalid image filename(s) in x_col=&quot;Image&quot;. These filename(s) will be ignored.  .format(n_invalid, x_col)/opt/conda/lib/python3.6/site-packages/keras_preprocessing/image/dataframe_iterator.py:273: UserWarning: Found 866 invalid image filename(s) in x_col=&quot;Image&quot;. These filename(s) will be ignored.  .format(n_invalid, x_col)Found 9 validated image filenames.getting train and valid generators...Found 9 validated image filenames./opt/conda/lib/python3.6/site-packages/keras_preprocessing/image/dataframe_iterator.py:273: UserWarning: Found 108 invalid image filename(s) in x_col=&quot;Image&quot;. These filename(s) will be ignored.  .format(n_invalid, x_col)Found 1 validated image filenames.Found 420 validated image filenames.</code></pre><p>Let’s peek into what the generator gives our model during training and validation. We can do this by calling the <code>__get_item__(index)</code> function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x, y = train_generator.__getitem__(<span class="number">0</span>)</span><br><span class="line">plt.imshow(x[<span class="number">0</span>]);</span><br></pre></td></tr></table></figure><pre><code>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</code></pre><p><img src="output_28_1.png" alt="png"></p><p><a name="3"></a></p><h2 id="3-Model-Development"><a href="#3-Model-Development" class="headerlink" title="3 Model Development"></a>3 Model Development</h2><p>Now we’ll move on to model training and development. We have a few practical challenges to deal with before actually training a neural network, though. The first is class imbalance.</p><p><a name="3-1"></a></p><h3 id="3-1-Addressing-Class-Imbalance"><a href="#3-1-Addressing-Class-Imbalance" class="headerlink" title="3.1 Addressing Class Imbalance"></a>3.1 Addressing Class Imbalance</h3><p>One of the challenges with working with medical diagnostic datasets is the large class imbalance present in such datasets. Let’s plot the frequency of each of the labels in our dataset:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line">plt.bar(x=labels, height=np.mean(train_generator.labels, axis=<span class="number">0</span>))</span><br><span class="line">plt.title(<span class="string">"Frequency of Each Class"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_31_0.png" alt="png"></p><p>We can see from this plot that the prevalance of positive cases varies significantly across the different pathologies. (These trends mirror the ones in the full dataset as well.) </p><ul><li>The <code>Hernia</code> pathology has the greatest imbalance with the proportion of positive training cases being about 0.2%. </li><li>But even the <code>Infiltration</code> pathology, which has the least amount of imbalance, has only 17.5% of the training cases labelled positive.</li></ul><p>Ideally, we would train our model using an evenly balanced dataset so that the positive and negative training cases would contribute equally to the loss. </p><p>If we use a normal cross-entropy loss function with a highly unbalanced dataset, as we are seeing here, then the algorithm will be incentivized to prioritize the majority class (i.e negative in our case), since it contributes more to the loss. </p><h4 id="Impact-of-class-imbalance-on-loss-function"><a href="#Impact-of-class-imbalance-on-loss-function" class="headerlink" title="Impact of class imbalance on loss function"></a>Impact of class imbalance on loss function</h4><p>Let’s take a closer look at this. Assume we would have used a normal cross-entropy loss for each pathology. We recall that the cross-entropy loss contribution from the $i^{th}$ training data case is:</p><script type="math/tex; mode=display">\mathcal{L}_{cross-entropy}(x_i) = -(y_i \log(f(x_i)) + (1-y_i) \log(1-f(x_i))),</script><p>where $x_i$ and $y_i$ are the input features and the label, and $f(x_i)$ is the output of the model, i.e. the probability that it is positive. </p><p>Note that for any training case, either $y_i=0$ or else $(1-y_i)=0$, so only one of these terms contributes to the loss (the other term is multiplied by zero, and becomes zero). </p><p>We can rewrite the overall average cross-entropy loss over the entire training set $\mathcal{D}$ of size $N$ as follows: </p><script type="math/tex; mode=display">\mathcal{L}_{cross-entropy}(\mathcal{D}) = - \frac{1}{N}\big( \sum_{\text{positive examples}} \log (f(x_i)) + \sum_{\text{negative examples}} \log(1-f(x_i)) \big).</script><p>Using this formulation, we can see that if there is a large imbalance with very few positive training cases, for example, then the loss will be dominated by the negative class. Summing the contribution over all the training cases for each class (i.e. pathological condition), we see that the contribution of each class (i.e. positive or negative) is: </p><script type="math/tex; mode=display">freq_{p} = \frac{\text{number of positive examples}}{N}</script><script type="math/tex; mode=display">\text{and}</script><script type="math/tex; mode=display">freq_{n} = \frac{\text{number of negative examples}}{N}.</script><p><a name="Ex-2"></a></p><h3 id="Exercise-2-Computing-Class-Frequencies"><a href="#Exercise-2-Computing-Class-Frequencies" class="headerlink" title="Exercise 2 - Computing Class Frequencies"></a>Exercise 2 - Computing Class Frequencies</h3><p>Complete the function below to calculate these frequences for each label in our dataset.</p><p><details>    </details></p><p><summary>    <font size="3" color="darkgreen"><b>Hints</b></font></summary></p><p><ul>    <li> Use numpy.sum(a, axis=), and choose the axis (0 or 1) </li></ul></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_class_freqs</span><span class="params">(labels)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute positive and negative frequences for each class.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        labels (np.array): matrix of labels, size (num_examples, num_classes)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        positive_frequencies (np.array): array of positive frequences for each</span></span><br><span class="line"><span class="string">                                         class, size (num_classes)</span></span><br><span class="line"><span class="string">        negative_frequencies (np.array): array of negative frequences for each</span></span><br><span class="line"><span class="string">                                         class, size (num_classes)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># total number of patients (rows)</span></span><br><span class="line">    N = labels.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    positive_frequencies = np.sum(labels, axis=<span class="number">0</span>) / labels.shape[<span class="number">0</span>]</span><br><span class="line">    negative_frequencies = <span class="number">1</span> - positive_frequencies</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> positive_frequencies, negative_frequencies</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test</span></span><br><span class="line">labels_matrix = np.array(</span><br><span class="line">    [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">     [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">     [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">     [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">     [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">)</span><br><span class="line">print(<span class="string">"labels:"</span>)</span><br><span class="line">print(labels_matrix)</span><br><span class="line"></span><br><span class="line">test_pos_freqs, test_neg_freqs = compute_class_freqs(labels_matrix)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"pos freqs: <span class="subst">&#123;test_pos_freqs&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"neg freqs: <span class="subst">&#123;test_neg_freqs&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>labels:[[1 0 0] [0 1 1] [1 0 1] [1 1 1] [1 0 1]]pos freqs: [0.8 0.4 0.8]neg freqs: [0.2 0.6 0.2]</code></pre><h5 id="Expected-output-1"><a href="#Expected-output-1" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">labels:</span><br><span class="line">[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]]</span><br><span class="line">pos freqs: [<span class="number">0.8</span> <span class="number">0.4</span> <span class="number">0.8</span>]</span><br><span class="line">neg freqs: [<span class="number">0.2</span> <span class="number">0.6</span> <span class="number">0.2</span>]</span><br></pre></td></tr></table></figure><p>Now we’ll compute frequencies for our training data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">freq_pos, freq_neg = compute_class_freqs(train_generator.labels)</span><br><span class="line">freq_pos</span><br></pre></td></tr></table></figure><pre><code>array([0.        , 0.11111111, 0.22222222, 0.        , 0.22222222,       0.11111111, 0.        , 0.11111111, 0.        , 0.        ,       0.        , 0.        , 0.        , 0.        ])</code></pre><p>Let’s visualize these two contribution ratios next to each other for each of the pathologies:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = pd.DataFrame(&#123;<span class="string">"Class"</span>: labels, <span class="string">"Label"</span>: <span class="string">"Positive"</span>, <span class="string">"Value"</span>: freq_pos&#125;)</span><br><span class="line">data = data.append([&#123;<span class="string">"Class"</span>: labels[l], <span class="string">"Label"</span>: <span class="string">"Negative"</span>, <span class="string">"Value"</span>: v&#125; <span class="keyword">for</span> l,v <span class="keyword">in</span> enumerate(freq_neg)], ignore_index=<span class="keyword">True</span>)</span><br><span class="line">plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line">f = sns.barplot(x=<span class="string">"Class"</span>, y=<span class="string">"Value"</span>, hue=<span class="string">"Label"</span> ,data=data)</span><br></pre></td></tr></table></figure><p><img src="output_42_0.png" alt="png"></p><p>As we see in the above plot, the contributions of positive cases is significantly lower than that of the negative ones. However, we want the contributions to be equal. One way of doing this is by multiplying each example from each class by a class-specific weight factor, $w_{pos}$ and $w_{neg}$, so that the overall contribution of each class is the same. </p><p>To have this, we want </p><script type="math/tex; mode=display">w_{pos} \times freq_{p} = w_{neg} \times freq_{n},</script><p>which we can do simply by taking </p><script type="math/tex; mode=display">w_{pos} = freq_{neg}</script><script type="math/tex; mode=display">w_{neg} = freq_{pos}</script><p>This way, we will be balancing the contribution of positive and negative labels.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pos_weights = freq_neg</span><br><span class="line">neg_weights = freq_pos</span><br><span class="line">pos_contribution = freq_pos * pos_weights </span><br><span class="line">neg_contribution = freq_neg * neg_weights</span><br></pre></td></tr></table></figure><p>Let’s verify this by graphing the two contributions next to each other again:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = pd.DataFrame(&#123;<span class="string">"Class"</span>: labels, <span class="string">"Label"</span>: <span class="string">"Positive"</span>, <span class="string">"Value"</span>: pos_contribution&#125;)</span><br><span class="line">data = data.append([&#123;<span class="string">"Class"</span>: labels[l], <span class="string">"Label"</span>: <span class="string">"Negative"</span>, <span class="string">"Value"</span>: v&#125; </span><br><span class="line">                        <span class="keyword">for</span> l,v <span class="keyword">in</span> enumerate(neg_contribution)], ignore_index=<span class="keyword">True</span>)</span><br><span class="line">plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line">sns.barplot(x=<span class="string">"Class"</span>, y=<span class="string">"Value"</span>, hue=<span class="string">"Label"</span> ,data=data);</span><br></pre></td></tr></table></figure><p><img src="output_46_0.png" alt="png"></p><p>As the above figure shows, by applying these weightings the positive and negative labels within each class would have the same aggregate contribution to the loss function. Now let’s implement such a loss function. </p><p>After computing the weights, our final weighted loss for each training case will be </p><script type="math/tex; mode=display">\mathcal{L}_{cross-entropy}^{w}(x) = - (w_{p} y \log(f(x)) + w_{n}(1-y) \log( 1 - f(x) ) ).</script><p><a name="Ex-3"></a></p><h3 id="Exercise-3-Weighted-Loss"><a href="#Exercise-3-Weighted-Loss" class="headerlink" title="Exercise 3 - Weighted Loss"></a>Exercise 3 - Weighted Loss</h3><p>Fill out the <code>weighted_loss</code> function below to return a loss function that calculates the weighted loss for each batch. Recall that for the multi-class loss, we add up the average loss for each individual class. Note that we also want to add a small value, $\epsilon$, to the predicted values before taking their logs. This is simply to avoid a numerical error that would otherwise occur if the predicted value happens to be zero.</p><h5 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h5><p>Please use Keras functions to calculate the mean and the log.</p><ul><li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/backend/mean" target="_blank" rel="noopener">Keras.mean</a></li><li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/backend/log" target="_blank" rel="noopener">Keras.log</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weighted_loss</span><span class="params">(pos_weights, neg_weights, epsilon=<span class="number">1e-7</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return weighted loss function given negative weights and positive weights.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      pos_weights (np.array): array of positive weights for each class, size (num_classes)</span></span><br><span class="line"><span class="string">      neg_weights (np.array): array of negative weights for each class, size (num_classes)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      weighted_loss (function): weighted loss function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weighted_loss</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Return weighted loss value. </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes)</span></span><br><span class="line"><span class="string">            y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes)</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            loss (Tensor): overall scalar loss summed across all classes</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># initialize loss to zero</span></span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(pos_weights)):</span><br><span class="line">            <span class="comment"># for each class, add average weighted loss for that class </span></span><br><span class="line">            loss += -(K.mean( pos_weights[i] * y_true[:,i] * K.log(y_pred[:,i] + epsilon) + \</span><br><span class="line">                                neg_weights[i] * (<span class="number">1</span> - y_true[:,i]) * K.log(<span class="number">1</span> - y_pred[:,i] + epsilon), axis = <span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> weighted_loss</span><br></pre></td></tr></table></figure><p>Now let’s test our function with some simple cases. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test</span></span><br><span class="line">sess = K.get_session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default() <span class="keyword">as</span> sess:</span><br><span class="line">    print(<span class="string">"Test example:\n"</span>)</span><br><span class="line">    y_true = K.constant(np.array(</span><br><span class="line">        [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">    ))</span><br><span class="line">    print(<span class="string">"y_true:\n"</span>)</span><br><span class="line">    print(y_true.eval())</span><br><span class="line"></span><br><span class="line">    w_p = np.array([<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.5</span>])</span><br><span class="line">    w_n = np.array([<span class="number">0.75</span>, <span class="number">0.75</span>, <span class="number">0.5</span>])</span><br><span class="line">    print(<span class="string">"\nw_p:\n"</span>)</span><br><span class="line">    print(w_p)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"\nw_n:\n"</span>)</span><br><span class="line">    print(w_n)</span><br><span class="line"></span><br><span class="line">    y_pred_1 = K.constant(<span class="number">0.7</span>*np.ones(y_true.shape))</span><br><span class="line">    print(<span class="string">"\ny_pred_1:\n"</span>)</span><br><span class="line">    print(y_pred_1.eval())</span><br><span class="line"></span><br><span class="line">    y_pred_2 = K.constant(<span class="number">0.3</span>*np.ones(y_true.shape))</span><br><span class="line">    print(<span class="string">"\ny_pred_2:\n"</span>)</span><br><span class="line">    print(y_pred_2.eval())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># test with a large epsilon in order to catch errors</span></span><br><span class="line">    L = get_weighted_loss(w_p, w_n, epsilon=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"\nIf we weighted them correctly, we expect the two losses to be the same."</span>)</span><br><span class="line">    L1 = L(y_true, y_pred_1).eval()</span><br><span class="line">    L2 = L(y_true, y_pred_2).eval()</span><br><span class="line">    print(<span class="string">f"\nL(y_pred_1)= <span class="subst">&#123;L1:<span class="number">.4</span>f&#125;</span>, L(y_pred_2)= <span class="subst">&#123;L2:<span class="number">.4</span>f&#125;</span>"</span>)</span><br><span class="line">    print(<span class="string">f"Difference is L1 - L2 = <span class="subst">&#123;L1 - L2:<span class="number">.4</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Test example:y_true:[[1. 1. 1.] [1. 1. 0.] [0. 1. 0.] [1. 0. 1.]]w_p:[0.25 0.25 0.5 ]w_n:[0.75 0.75 0.5 ]y_pred_1:[[0.7 0.7 0.7] [0.7 0.7 0.7] [0.7 0.7 0.7] [0.7 0.7 0.7]]y_pred_2:[[0.3 0.3 0.3] [0.3 0.3 0.3] [0.3 0.3 0.3] [0.3 0.3 0.3]]If we weighted them correctly, we expect the two losses to be the same.L(y_pred_1)= -0.4956, L(y_pred_2)= -0.4956Difference is L1 - L2 = 0.0000</code></pre><h4 id="Additional-check"><a href="#Additional-check" class="headerlink" title="Additional check"></a>Additional check</h4><p>If you implemented the function correctly, then if the epsilon for the <code>get_weighted_loss</code> is set to <code>1</code>, the weighted losses will be as follows:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">L(y_pred_1)= <span class="number">-0.4956</span>, L(y_pred_2)= <span class="number">-0.4956</span></span><br></pre></td></tr></table></figure></p><p>If you are missing something in your implementation, you will see a different set of losses for L1 and L2 (even though L1 and L2 will be the same).</p><p><a name="3-3"></a></p><h3 id="3-3-DenseNet121"><a href="#3-3-DenseNet121" class="headerlink" title="3.3 DenseNet121"></a>3.3 DenseNet121</h3><p>Next, we will use a pre-trained <a href="https://www.kaggle.com/pytorch/densenet121" target="_blank" rel="noopener">DenseNet121</a> model which we can load directly from Keras and then add two layers on top of it:</p><ol><li>A <code>GlobalAveragePooling2D</code> layer to get the average of the last convolution layers from DenseNet121.</li><li>A <code>Dense</code> layer with <code>sigmoid</code> activation to get the prediction logits for each of our classes.</li></ol><p>We can set our custom loss function for the model by specifying the <code>loss</code> parameter in the <code>compile()</code> function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create the base pre-trained model</span></span><br><span class="line">base_model = DenseNet121(weights=<span class="string">'./nih/densenet.hdf5'</span>, include_top=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">x = base_model.output</span><br><span class="line"></span><br><span class="line"><span class="comment"># add a global spatial average pooling layer</span></span><br><span class="line">x = GlobalAveragePooling2D()(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># and a logistic layer</span></span><br><span class="line">predictions = Dense(len(labels), activation=<span class="string">"sigmoid"</span>)(x)</span><br><span class="line"></span><br><span class="line">model = Model(inputs=base_model.input, outputs=predictions)</span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=get_weighted_loss(pos_weights, neg_weights))</span><br></pre></td></tr></table></figure><pre><code>WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.Instructions for updating:If using Keras pass *_constraint arguments to layers.WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.</code></pre><p><a name="4"></a></p><h2 id="4-Training-optional"><a href="#4-Training-optional" class="headerlink" title="4 Training [optional]"></a>4 Training [optional]</h2><p>With our model ready for training, we will use the <code>model.fit()</code> function in Keras to train our model. </p><ul><li>We are training on a small subset of the dataset (~1%).  </li><li>So what we care about at this point is to make sure that the loss on the training set is decreasing.</li></ul><p>Since training can take a considerable time, for pedagogical purposes we have chosen not to train the model here but rather to load a set of pre-trained weights in the next section. However, you can use the code shown below to practice training the model locally on your machine or in Colab.</p><p><strong>NOTE:</strong> Do not run the code below on the Coursera platform as it will exceed the platform’s memory limitations.</p><p>Python Code for training the model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit_generator(train_generator, </span><br><span class="line">                              validation_data=valid_generator,</span><br><span class="line">                              steps_per_epoch=<span class="number">100</span>, </span><br><span class="line">                              validation_steps=<span class="number">25</span>, </span><br><span class="line">                              epochs = <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(history.history[<span class="string">'loss'</span>])</span><br><span class="line">plt.ylabel(<span class="string">"loss"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"epoch"</span>)</span><br><span class="line">plt.title(<span class="string">"Training Loss Curve"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><a name="4-1"></a></p><h3 id="4-1-Training-on-the-Larger-Dataset"><a href="#4-1-Training-on-the-Larger-Dataset" class="headerlink" title="4.1 Training on the Larger Dataset"></a>4.1 Training on the Larger Dataset</h3><p>Given that the original dataset is 40GB+ in size and the training process on the full dataset takes a few hours, we have trained the model on a GPU-equipped machine for you and provided the weights file from our model (with a batch size of 32 instead) to be used for the rest of this assignment. </p><p>The model architecture for our pre-trained model is exactly the same, but we used a few useful Keras “callbacks” for this training. Do spend time to read about these callbacks at your leisure as they will be very useful for managing long-running training sessions:</p><ol><li>You can use <code>ModelCheckpoint</code> callback to monitor your model’s <code>val_loss</code> metric and keep a snapshot of your model at the point. </li><li>You can use the <code>TensorBoard</code> to use the Tensorflow Tensorboard utility to monitor your runs in real-time. </li><li>You can use the <code>ReduceLROnPlateau</code> to slowly decay the learning rate for your model as it stops getting better on a metric such as <code>val_loss</code> to fine-tune the model in the final steps of training.</li><li>You can use the <code>EarlyStopping</code> callback to stop the training job when your model stops getting better in it’s validation loss. You can set a <code>patience</code> value which is the number of epochs the model does not improve after which the training is terminated. This callback can also conveniently restore the weights for the best metric at the end of training to your model.</li></ol><p>You can read about these callbacks and other useful Keras callbacks <a href="https://keras.io/callbacks/" target="_blank" rel="noopener">here</a>.</p><p>Let’s load our pre-trained weights into the model now:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_weights(<span class="string">"./nih/pretrained_model.h5"</span>)</span><br></pre></td></tr></table></figure><p><a name="5"></a></p><h2 id="5-Prediction-and-Evaluation"><a href="#5-Prediction-and-Evaluation" class="headerlink" title="5 Prediction and Evaluation"></a>5 Prediction and Evaluation</h2><p>Now that we have a model, let’s evaluate it using our test set. We can conveniently use the <code>predict_generator</code> function to generate the predictions for the images in our test set.</p><p><strong>Note:</strong> The following cell can take about 4 minutes to run.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predicted_vals = model.predict_generator(test_generator, steps = len(test_generator))</span><br></pre></td></tr></table></figure><pre><code>WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.</code></pre><p><a name="5-1"></a></p><h3 id="5-1-ROC-Curve-and-AUROC"><a href="#5-1-ROC-Curve-and-AUROC" class="headerlink" title="5.1 ROC Curve and AUROC"></a>5.1 ROC Curve and AUROC</h3><p>We’ll cover topic of model evaluation in much more detail in later weeks, but for now we’ll walk through computing a metric called the AUC (Area Under the Curve) from the ROC (<a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank" rel="noopener">Receiver Operating Characteristic</a>) curve. This is also referred to as the AUROC value, but you will see all three terms in reference to the technique, and often used almost interchangeably. </p><p>For now, what you need to know in order to interpret the plot is that a curve that is more to the left and the top has more “area” under it, and indicates that the model is performing better.</p><p>We will use the <code>util.get_roc_curve()</code> function which has been provided for you in <code>util.py</code>. Look through this function and note the use of the <code>sklearn</code> library functions to generate the ROC curves and AUROC values for our model. </p><ul><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html" target="_blank" rel="noopener">roc_curve</a></li><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html" target="_blank" rel="noopener">roc_auc_score</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">auc_rocs = util.get_roc_curve(labels, predicted_vals, test_generator)</span><br></pre></td></tr></table></figure><p><img src="output_62_0.png" alt="png"></p><p>You can compare the performance to the AUCs reported in the original ChexNeXt paper in the table below: </p><p>For reference, here’s the AUC figure from the ChexNeXt paper which includes AUC values for their model as well as radiologists on this dataset:</p><p><img src="https://journals.plos.org/plosmedicine/article/figure/image?size=large&id=10.1371/journal.pmed.1002686.t001" width="80%"></p><p>This method does take advantage of a few other tricks such as self-training and ensembling as well, which can give a significant boost to the performance.</p><p>For details about the best performing methods and their performance on this dataset, we encourage you to read the following papers:</p><ul><li><a href="https://arxiv.org/abs/1711.05225" target="_blank" rel="noopener">CheXNet</a></li><li><a href="https://arxiv.org/pdf/1901.07031.pdf" target="_blank" rel="noopener">CheXpert</a></li><li><a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002686" target="_blank" rel="noopener">ChexNeXt</a></li></ul><p><a name="5-2"></a></p><h3 id="5-2-Visualizing-Learning-with-GradCAM"><a href="#5-2-Visualizing-Learning-with-GradCAM" class="headerlink" title="5.2 Visualizing Learning with GradCAM"></a>5.2 Visualizing Learning with GradCAM</h3><p>One of the challenges of using deep learning in medicine is that the complex architecture used for neural networks makes them much harder to interpret compared to traditional machine learning models (e.g. linear models). </p><p>One of the most common approaches aimed at increasing the interpretability of models for computer vision tasks is to use Class Activation Maps (CAM). </p><ul><li>Class activation maps are useful for understanding where the model is “looking” when classifying an image. </li></ul><p>In this section we will use a <a href="https://arxiv.org/abs/1610.02391" target="_blank" rel="noopener">GradCAM’s</a> technique to produce a heatmap highlighting the important regions in the image for predicting the pathological condition. </p><ul><li>This is done by extracting the gradients of each predicted class, flowing into our model’s final convolutional layer. Look at the <code>util.compute_gradcam</code> which has been provided for you in <code>util.py</code> to see how this is done with the Keras framework. </li></ul><p>It is worth mentioning that GradCAM does not provide a full explanation of the reasoning for each classification probability. </p><ul><li>However, it is still a useful tool for “debugging” our model and augmenting our prediction so that an expert could validate that a prediction is indeed due to the model focusing on the right regions of the image.</li></ul><p>First we will load the small training set and setup to look at the 4 classes with the highest performing AUC measures.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">"nih/train-small.csv"</span>)</span><br><span class="line">IMAGE_DIR = <span class="string">"nih/images-small/"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># only show the lables with top 4 AUC</span></span><br><span class="line">labels_to_show = np.take(labels, np.argsort(auc_rocs)[::<span class="number">-1</span>])[:<span class="number">4</span>]</span><br></pre></td></tr></table></figure><p>Now let’s look at a few specific images.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.compute_gradcam(model, <span class="string">'00008270_015.png'</span>, IMAGE_DIR, df, labels, labels_to_show)</span><br></pre></td></tr></table></figure><pre><code>Loading original imageGenerating gradcam for class CardiomegalyGenerating gradcam for class MassGenerating gradcam for class PneumothoraxGenerating gradcam for class Edema</code></pre><p><img src="output_71_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.compute_gradcam(model, <span class="string">'00011355_002.png'</span>, IMAGE_DIR, df, labels, labels_to_show)</span><br></pre></td></tr></table></figure><pre><code>Loading original imageGenerating gradcam for class CardiomegalyGenerating gradcam for class MassGenerating gradcam for class PneumothoraxGenerating gradcam for class Edema</code></pre><p><img src="output_72_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.compute_gradcam(model, <span class="string">'00029855_001.png'</span>, IMAGE_DIR, df, labels, labels_to_show)</span><br></pre></td></tr></table></figure><pre><code>Loading original imageGenerating gradcam for class CardiomegalyGenerating gradcam for class MassGenerating gradcam for class PneumothoraxGenerating gradcam for class Edema</code></pre><p><img src="output_73_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">util.compute_gradcam(model, <span class="string">'00005410_000.png'</span>, IMAGE_DIR, df, labels, labels_to_show)</span><br></pre></td></tr></table></figure><pre><code>Loading original imageGenerating gradcam for class CardiomegalyGenerating gradcam for class MassGenerating gradcam for class PneumothoraxGenerating gradcam for class Edema</code></pre><p><img src="output_74_1.png" alt="png"></p><p>Congratulations, you’ve completed the first assignment of course one! You’ve learned how to preprocess data, check for data leakage, train a pre-trained model, and evaluate using the AUC. Great work!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;This is the assignment of coursera course &lt;a href=&quot;https://www.coursera.org/learn/ai-for-medical-diagnosis/&quot; target=&quot;_blank&quot; rel=&quot;n
      
    
    </summary>
    
    
      <category term="Artificial Intelligence" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/"/>
    
      <category term="Deep Learning" scheme="https://zhangruochi.com/categories/Artificial-Intelligence/Deep-Learning/"/>
    
    
      <category term="Medicine" scheme="https://zhangruochi.com/tags/Medicine/"/>
    
  </entry>
  
</feed>
